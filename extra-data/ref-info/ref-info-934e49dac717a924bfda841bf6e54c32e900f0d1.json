{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18649966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11463e2a6ed218e87e22cba2c2f24fb5992d0293",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "THE DIFFICULTIES OF LEARNING IN MULTILAYERED NETWORKS OF COMPUTATIONAL UNITS HAS LIMITED THE USE OF CONNECTIONIST SYSTEMS IN COMPLEX DOMAINS. THIS DISSERTATION ELUCIDATES THE ISSUES OF LEARNING IN A NETWORK''S HIDDEN UNITS, AND REVIEWS METHODS FOR ADDRESSING THESE ISSUES THAT HAVE BEEN DEVELOPED THROUGH THE YEARS. ISSUES OF LEARNING IN HIDDEN UNITS ARE SHOWN TO BE ANALOGOUS TO LEARNING ISSUES FOR MULTILAYER SYSTEMS EMPLOYING SYMBOLIC REPRSENTATIONS. COMPARISONS OF A NUMBER OF ALGORITHMS FOR LEARNING IN HIDDEN UNITS ARE MADE BY APPLYING THEM IN A CONSISTENT MANNER TO SEVERAL TASKS. RECENTLY DEVELOPED ALGORITHMS, INCLUDING RUMELHART, ET AL''S, ERROR BACK-PROPOGATIONS ALGORITHM AND BARTO, ET AL''S, REINFORCEMENT-LEARNING ALGORITHMS, LEARN THE SOLUTIONS TO THE TASKS MUCH MORE SUCCESSFULLY THAN METHODS OF THE PAST. A NOVEL ALGORITHM IS EXAMINED THAT COMBINES ASPECTS OF REINFORCEMENT LEARNING AND A DATA-DIRECTED SEARCH FOR USEFUL WEIGHTS, AND IS SHOWN TO OUT PERFORM REINFORMCEMENT-LEARNING ALGORITHMS. A CONNECTIONIST FRAMEWORK FOR THE LEARNING OF STRATEGIES IS DESCRIBED WHICH COMBINES THE ERROR BACK-PROPOGATION ALGORITHM FOR LEARNING IN HIDDEN UNITS WITH SUTTON''S AHC ALGORITHM TO LEARN EVALUATION FUNCTIONS AND WITH A REINFORCEMENT-LEARNING ALGORITHM TO LEARN SEARCH HEURISTICS. THE GENERAL- ITY OF THIS HYBRID SYSTEM IS DEMONSTRATED THROUGH SUCCESSFUL APPLICATIONS"
            },
            "slug": "Learning-and-problem-solving-with-multilayer-neural-Anderson",
            "title": {
                "fragments": [],
                "text": "Learning and problem-solving with multilayer connectionist systems (adaptive, strategy learning, neural networks, reinforcement learning)"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel algorithm is examined that combines ASPECTS of REINFORCEMENT LEARNING and a DATA-DIRECTED SEARCH for USEFUL WEIGHTS, and is shown to out perform reinFORMCEMENT-LEARNING ALGORITHMS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15150815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a42b2104ca8ff891ae77c40a915d4c94c8f8428",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Rumelhart, Hinton and Williams (Rumelhart 86) describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in weight space . We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the analog networks described by Hopefield and Tank (Hopfield 85). The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "slug": "Experiments-on-Learning-by-Back-Propagation.-Plaut-Nowlan",
            "title": {
                "fragments": [],
                "text": "Experiments on Learning by Back Propagation."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941182"
                        ],
                        "name": "Y. Bard",
                        "slug": "Y.-Bard",
                        "structuredName": {
                            "firstName": "Yonathan",
                            "lastName": "Bard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Bard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122334592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48594d32602bf88b83f5633ad7be15e9d558365a",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The Davidon-Fletcher-Powell method of function minimization [2], [3] has attained widespread popularity. Yet it goes wrong from time to time. Among the conditions reported are: 1. Broyden [1] states that negative steps had to be taken occasionally. 2. McCormick [5] noted that reinitialization of the matrix every now and then improved the method's performance. 3. Wolfe [6] has reported cases of convergence to nonstationary points. The author has encountered similar behavior in his own work, and has found it invariably the result of the matrix turning singular, due to the cause detailed below. The author believes other workers' difficulties probably originate in the same cause. We wish to find the minimum of a continuously differentiable function F(x) (boldface lower case letters denote vectors). In Davidon's method we proceed iteratively: If xi is the value of x at the ith iteration, then"
            },
            "slug": "On-a-numerical-instability-of-Davidon-like-methods-Bard",
            "title": {
                "fragments": [],
                "text": "On a numerical instability of Davidon-like methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The author has encountered similar behavior in his own work, and has found it invariably the result of the matrix turning singular, due to the cause detailed below, when other workers' difficulties probably originate in the same cause."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48462607"
                        ],
                        "name": "L. Shastri",
                        "slug": "L.-Shastri",
                        "structuredName": {
                            "firstName": "Lokendra",
                            "lastName": "Shastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12357500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1e40283ecd4633c36c70fbc8dbb14e9a4afb37f",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for learning phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. The network uses hidden units with recurrent links to capture spectral/temporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. \n \nA simple connectionist network with recurrent links was trained on a single instance of the word pair \"no\" and \"go\", and successful learned a discriminatory mechanism. The trained network also correctly discriminated 98% of 25 other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items."
            },
            "slug": "Learning-Phonetic-Features-Using-Connectionist-Watrous-Shastri",
            "title": {
                "fragments": [],
                "text": "Learning Phonetic Features Using Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A method for learning phonetic features from speech data using connectionist networks is described and a supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964979"
                        ],
                        "name": "C. G. Broyden",
                        "slug": "C.-G.-Broyden",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Broyden",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Broyden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53666480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0f1c76c1f8824cd974a4b76f126045e1e8bd223",
            "isKey": false,
            "numCitedBy": 1425,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Convergence of a Class of Double-rank Minimization Algorithms 2. The New Algorithm d where q and ql are uniquely determined orthonormal vectors. The parameter 1/ is . ntially arbitrary in that it depends upon p. It was suggested in Part 1 that a suitable ice for I] would be zero since if it were negative, or large and positive, the matrix KI hence HI might become needlessly badly conditioned. It was noted moreover that osing I] in this way gives rise to a new algorithm. the two algorithms in this class already published, that due to Davidon (1959) modified by Fletcher & Powell (1963) is obtained by putting P equal to zero and s shown in Part I that this led, in general, to negative values of 1]. We thus expect quence of matrices {HI} obtained by that algorithm to exhibit a tendency to arity and this tendency has been noted by, among others, Broyden (1967) and on (1969). In a more recent algorithm, due to Greenstadt (1967), if H is positive ite the values of 1] are even more negative than those occurring in the DFP ithm. One result of this is that for this algorithm the matrices H cannot, unlike for the DFP algorithm, be proved to be positive definite and this has serious tions when considering numerical stability. this paper we show theoretically that the new algorithm is stable and we prove is the only member of the class considered for which a certain matrix error is reduced strictly monotonically when minimizing quadratic functions. We the effect of rounding and of poor conditioning of H on the attainable accuracy solution and conclude by presenting the results of a numerical survey in he performance of the new algorithm for a variety of test problem is compared t of the DFP algorithm. C. G. BROYDEN Computing Centre, University of Essex, Wivenhoe Park, Colchester, Essex"
            },
            "slug": "The-Convergence-of-a-Class-of-Double-rank-2.-The-Broyden",
            "title": {
                "fragments": [],
                "text": "The Convergence of a Class of Double-rank Minimization Algorithms 2. The New Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown theoretically that the new algorithm is stable and it is proved is the only member of the class considered for which a certain matrix error is reduced strictly monotonically when minimizing quadratic functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96635129"
                        ],
                        "name": "J. D. Pearson",
                        "slug": "J.-D.-Pearson",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Pearson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Pearson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117407502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "deab7b2aef4b5e129c0edfa5e9675ad8e4707734",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Two basic approaches to the generation of conjugate directions are considered for the problem of unconstrained minimization of a quadratic function. Using the principle of choosing a step direction orthogonal to the previous gradient changes, a projected gradient algorithm and a class of variable metric algorithms are derived. Three variants of the class are developed into algorithms, one of which is the Fletcher-Powell-Davidon scheme. Numerical results indicate the merits of the new algorithms compared to several now in use, for a variety of nonquadratic problems."
            },
            "slug": "ON-VARIABLE-METRIC-METHODS-OF-MINIMIZATION-Pearson",
            "title": {
                "fragments": [],
                "text": "ON VARIABLE METRIC METHODS OF MINIMIZATION"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Two basic approaches to the generation of conjugate directions are considered for the problem of unconstrained minimization of a quadratic function, and a projected gradient algorithm and a class of variable metric algorithms are derived."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 784288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98b4d4e24aab57ab4e1124ff8106909050645cfa",
            "isKey": false,
            "numCitedBy": 16694,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
            },
            "slug": "Neural-networks-and-physical-systems-with-emergent-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural networks and physical systems with emergent collective computational abilities."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20678424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c86590e947c28e8791d1e8bab8fc8ab53302341f",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In the work described here, the backpropagation neural network learning procedure is applied to the analysis and recognition of speech. This procedure takes a set of input/output pattern pairs and attempts to learn their functional relationship; it develops the necessary representational features during the course of learning. A series of computer simulation studies was carried out to assess the ability of these networks to accurately label sounds, to learn to recognize sounds without labels, and to learn feature representations of continuous speech. These studies demonstrated that the networks can learn to label presegmented test tokens with accuracies of up to 95%. Networks trained on segmented sounds using a strategy that requires no external labels were able to recognize and delineate sounds in continuous speech. These networks developed rich internal representations that included units which corresponded to such traditional distinctions as vowels and consonants, as well as units that were sensitive to novel and nonstandard features. Networks trained on a large corpus of unsegmented, continuous speech without labels also developed interesting feature representations, which may be useful in both segmentation and label learning. The results of these studies, while preliminary, demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition."
            },
            "slug": "Learning-the-hidden-structure-of-speech.-Elman-Zipser",
            "title": {
                "fragments": [],
                "text": "Learning the hidden structure of speech."
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The results of these studies demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48681841"
                        ],
                        "name": "J. Wegstein",
                        "slug": "J.-Wegstein",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Wegstein",
                            "middleNames": [
                                "Henry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wegstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "One early such method involved a weighted averaging of successive minima [24], while other methods combined successive gradient vectors in order to generate a search direction more consistently in the direction of the solution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11574324,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a4546b0faf648cd3dc98ec52c18780af6239e3c3",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A technique is discussed which, when applied to an iterative procedure for the solution of an equation, accelerates the rate of convergence if the iteration converges and induces convergence if the iteration diverges. An illustrative example is given. 1. Introduction This paper discusses a technique for accelerating the convergence of iterative procedures employed in the solution of non-linear algebraic and transcendental equations. Iterative procedures in problems of this kind are of considerable importance since, with the aid of high speed computers, they very often provide the only effective means of solving such equations. The methods described here are of interest because they are capable of producing solutions even in those cases where the iteration algorithm may be divergent."
            },
            "slug": "Accelerating-convergence-of-iterative-processes-Wegstein",
            "title": {
                "fragments": [],
                "text": "Accelerating convergence of iterative processes"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A technique is discussed which, when applied to an iterative procedure for the solution of an equation, accelerates the rate of convergenceif the iteration converges and induces convergence if the iteration diverges."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1384952,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "52d44a7937d076ea82726911cb79ce76e6597e40",
            "isKey": false,
            "numCitedBy": 4226,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a9 The British Computer Society Issue Section: Articles Download all figures A powerful iterative descent method for finding a local minimum of a function of several variables is described. A number of theorems are proved to show that it always converges and that it converges rapidly. Numerical tests on a variety of functions confirm these theorems. The method has been used to solve a system of one hundred non-linear simultaneous equations. Related articles in Web of Science"
            },
            "slug": "A-Rapidly-Convergent-Descent-Method-for-Fletcher-Powell",
            "title": {
                "fragments": [],
                "text": "A Rapidly Convergent Descent Method for Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A number of theorems are proved to show that it always converges and that it converges rapidly, and this method has been used to solve a system of one hundred non-linear simultaneous equations."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145323121"
                        ],
                        "name": "J. Nelder",
                        "slug": "J.-Nelder",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Nelder",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nelder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189443"
                        ],
                        "name": "R. Mead",
                        "slug": "R.-Mead",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Mead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mead"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2208295,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "017ddb7e815236defd0566bc46f6ed8401cc6ba6",
            "isKey": false,
            "numCitedBy": 25604,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems."
            },
            "slug": "A-Simplex-Method-for-Function-Minimization-Nelder-Mead",
            "title": {
                "fragments": [],
                "text": "A Simplex Method for Function Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": false,
            "numCitedBy": 9912,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "This measure can be defined [22] as an expected value where Q(x,c) is the cost functional of the solution space vector x and the input vector c, from the space of input vectors C."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62742604,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "624d7d56f98f29e96c36493b2a9e1820ec53725d",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Concrete"
            },
            "slug": "Adaptation-and-learning-in-automatic-systems-Narendra",
            "title": {
                "fragments": [],
                "text": "Adaptation and learning in automatic systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87160862"
                        ],
                        "name": "T. M. Williams",
                        "slug": "T.-M.-Williams",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Williams",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. M. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "The line search algorithm, however, can affect the overall performance significantly [7], and should be considered carefully."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "This avoids the direct computation of the second derivative, and the computational complexity is reduced by a factor of O ( N ) [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "There are three aspects of the line search, bracketing, sectioning, and interpolation [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62396338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e10e980f2c2a14ebeaa113c1c96062d2b2643e9",
            "isKey": true,
            "numCitedBy": 723,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Practical-Methods-of-Optimization.-Vol.-1:-Williams",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization. Vol. 1: Unconstrained Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064698238"
                        ],
                        "name": "D. J. Bell",
                        "slug": "D.-J.-Bell",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. J. Bell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109071061,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "da0c7a1d4be25830c30995587434363b56e2b543",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Methods-for-Unconstrained-Optimization-Bell",
            "title": {
                "fragments": [],
                "text": "Numerical Methods for Unconstrained Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144681785"
                        ],
                        "name": "S. Vajda",
                        "slug": "S.-Vajda",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Vajda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vajda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61914400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0221dc67db0acc1d9495f896a3f4e4fb151abf7d",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Methods-for-Non-Linear-Optimization-Vajda",
            "title": {
                "fragments": [],
                "text": "Numerical Methods for Non-Linear Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061638288"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26597321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb787c61efda995b99fb939b743c99cda3f0b9f4",
            "isKey": false,
            "numCitedBy": 3278,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-New-Approach-to-Variable-Metric-Algorithms-Fletcher",
            "title": {
                "fragments": [],
                "text": "A New Approach to Variable Metric Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parker . LearningLogic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 228
                            }
                        ],
                        "text": "The strength of the connectionist approach seems to be that the multiple sources of knowledge can be appropriately integrated, and that statistical covariance of multiple cues can be estimated accurately from small data samples [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knowledge representation in connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings ofthe Sixth National Conference on Artijcial Intelligence"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Davidon . Variable Metric Methods for Minimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Watrous and Lokendra Shastri . Learning phonetic features using connectionist networks Accelerating convergence of iterative processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knowledge representation in connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings ofthe Sixth National Conference on Artijcial Intelligence,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Davidon . Variable Metric Methods for Minimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning- Logic. Center for Computational Research in Economics and Management Science TR-47"
            },
            "venue": {
                "fragments": [],
                "text": "Massachusetts Institute of Technology,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boltzmann Machines: Constraint Satisfaction Networks That Learn"
            },
            "venue": {
                "fragments": [],
                "text": "Boltzmann Machines: Constraint Satisfaction Networks That Learn"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NETtaEk: A Parallel Network that Learns to Read Aloz~d"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report JHU/EECS86/01,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Burr . Knowledge representation in connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings ofthe Sixth National Conference on Artijcial Intelligence , July 1987 . submitted . [ lo ]"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Algorithms-for-Connectionist-Networks:-of-Watrous/934e49dac717a924bfda841bf6e54c32e900f0d1?sort=total-citations"
}