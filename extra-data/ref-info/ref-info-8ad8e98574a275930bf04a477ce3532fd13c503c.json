{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71486292"
                        ],
                        "name": "M. Tomita",
                        "slug": "M.-Tomita",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Tomita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tomita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27194707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b57a3f720c52b26b7437e6b1099eaade9b222b8",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "MLR, an extended LR parser, is introduced, and its application to natural language parsing is discussed. An LR parser is a shift-reduce parser which is deterministically guided by a parsing table. A parsing table can be obtained automatically from a context-free phrase structure grammar. LR parsers cannot manage ambiguous grammars such as natural language grammars, because their parsing tables would have multiply-defined entries, which precludes deterministic parsing. MLR, however, can handle multiply-defined entries, using a dynamic programming method. When an input sentence is ambiguous, the MLR parser produces all possible parse trees without parsing any part of the input sentence more than once in the same way, despite the fact that the parser does not maintain a chart as in chart parsing. Our method also provides an elegant solution to the problem of multi-part-of-speech words such as \"that\". The MLR parser and its parsing table generator have been implemented at Carnegie-Mellon University."
            },
            "slug": "LR-Parsers-For-Natural-Languages-Tomita",
            "title": {
                "fragments": [],
                "text": "LR Parsers For Natural Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "When an input sentence is ambiguous, the MLR parser produces all possible parse trees without parsing any part of the input sentence more than once in the same way, despite the fact that the parser does not maintain a chart as in chart parsing."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "\u2026our approach corresponds to making the probability of rule application conditional on other rules having applied during the parse derivation (e.g. Magerman and Marcus 1991) and the lexical category of the next word; for example, it would be possible to create a grammatical representation of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 178
                            }
                        ],
                        "text": "Probabilistic CFGs also will not model the context dependence of rule use; for example, an NP is more likely to be expanded as a pronoun in subject position than elsewhere (e.g. Magerman and Marcus 1991), but only one global probability can be associated with the relevant CF production."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2376935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da838db79e7593018894ada44db35eee670941d6",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar."
            },
            "slug": "Pearl:-A-Probabilistic-Chart-Parser-Magerman-Marcus",
            "title": {
                "fragments": [],
                "text": "Pearl: A Probabilistic Chart Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence and provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Carroll and Briscoe (1992) present a heuristic algorithm for parse forest unpacking that interleaves normalization of competing sub-analyses with best-first extraction of the n most probable analyses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 22
                            }
                        ],
                        "text": "' In subsequent work (Carroll and Briscoe 1992) we have developed such a heuristic technique for best-first search of the parse forest which, in practice, makes the recovery of the most probable analyses much more efficient (allowing analysis of sentences containing over 30 words)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 3
                            }
                        ],
                        "text": "In Carroll and Briscoe (1992) we present a more motivated technique for normalizing the probability of competing sub-analyses in the parse forest. experimenting with techniques for probabilistically unpacking the packed parse forest to recover the first few most probable derivations without the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1816604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96cb7602a8b77bed593732c6d6d7ec903fe72e7e",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The research described below forms part of a wider programme to develop a practical parser for naturally-occurring natural language input which is capable of returning the n-best syntacticallydeterminate analyses, containing that which is semantically and pragmatically most appropriate (preferably as the highest ranked) from the exponential (in sentence length) syntactically legitimate possibilities (Church & Patil 1983), which can frequently run into the thousands with realistic sentences and grammars. We have opted to develop a domain-independent solution to this problem based on integrating statistical Markov modelling techniques, which offer the potential for rapid tuning to different sublanguages / corpora on the basis of supervised training, with linguistically-adequate grammatical (language) models, capable of returning analyses detailed enough to support semantic interpretation"
            },
            "slug": "Probabilistic-Normalisation-and-Unpacking-of-Packed-Carroll-Briscoe",
            "title": {
                "fragments": [],
                "text": "Probabilistic Normalisation and Unpacking of Packed Parse Forests for Unification-based Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A domain-independent solution based on integrating statistical Markov modelling techniques, which offer the potential for rapid tuning to different sublanguages / corpora on the basis of supervised training, with linguistically-adequate grammatical models, capable of returning analyses detailed enough to support semantic interpretation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117843148"
                        ],
                        "name": "Jerry Wright",
                        "slug": "Jerry-Wright",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Wright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jerry Wright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1396125786"
                        ],
                        "name": "A. Wrigley",
                        "slug": "A.-Wrigley",
                        "structuredName": {
                            "firstName": "Ave",
                            "lastName": "Wrigley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wrigley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32597159"
                        ],
                        "name": "R. Sharman",
                        "slug": "R.-Sharman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sharman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sharman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 79
                            }
                        ],
                        "text": "Several researchers (Wright and Wrigley 1989; Wright 1990; Ng and Tomita 1991; Wright, Wrigley, and Sharman 1991) have proposed using LR parsers as a practical method of parsing with a probabilistic context-free grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 0
                            }
                        ],
                        "text": "Wright, Wrigley, and Sharman (1991) describe a Viterbi-like algorithm for unpacking parse forests containing probabilities of (sub-)analyses to find the n-best analyses, but this approach does not generalize (except in a heuristic way) to our approach in which unification failure on the different\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60520470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1543569618fb83dd23fd66118c082f02b71d1f01",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Various issues in the implementation of generalized LR parsing with probability are discussed. A method for preventing the generation of infinite numbers of states is described and the space requirements of the parsing tables are assessed for a substantial natural-language grammar. Because of a high degree of ambiguity in the grammar, there are many multiple entries and the tables are rather large. A new method for grammar adaptation is introduced which may help to reduce this problem. A probabilistic version of the Tomita parse forest is also described."
            },
            "slug": "Adaptive-Probabilistic-Generalized-LR-Parsing-Wright-Wrigley",
            "title": {
                "fragments": [],
                "text": "Adaptive Probabilistic Generalized LR Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Various issues in the implementation of generalized LR parsing with probability are discussed, including a method for preventing the generation of infinite numbers of states and a probabilistic version of the Tomita parse forest."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643864881"
                        ],
                        "name": "BriscoeTed",
                        "slug": "BriscoeTed",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "BriscoeTed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "BriscoeTed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643864964"
                        ],
                        "name": "CarrollJohn",
                        "slug": "CarrollJohn",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "CarrollJohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "CarrollJohn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215847957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8cd3950d73c9536b8b1f1fbfc367e56a4553c1f",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large num..."
            },
            "slug": "Generalized-probabilistic-LR-parsing-of-natural-BriscoeTed-CarrollJohn",
            "title": {
                "fragments": [],
                "text": "Generalized probabilistic LR parsing of natural language (Corpora) with unification-based grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A very wide-coverage probabilistic parsing system for natural language (NL) based on LR parsing techniques, intended to rank the large numbers of words in a language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107485398"
                        ],
                        "name": "J. H. Wright",
                        "slug": "J.-H.-Wright",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 46
                            }
                        ],
                        "text": "Several researchers (Wright and Wrigley 1989; Wright 1990; Ng and Tomita 1991; Wright, Wrigley, and Sharman 1991) have proposed using LR parsers as a practical method of parsing with a probabilistic context-free grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 166
                            }
                        ],
                        "text": "The most straightforward technique for associating probabilities with the parse table is to assign a probability to each action in the action part of the table (e.g. Wright 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61129334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55c94cfb3ffab340e35c7e59130d40d82f161068",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LR-parsing-of-probabilistic-grammars-with-input-for-Wright",
            "title": {
                "fragments": [],
                "text": "LR parsing of probabilistic grammars with input uncertainty for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49095270"
                        ],
                        "name": "Jeremy H. Wright",
                        "slug": "Jeremy-H.-Wright",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremy H. Wright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34301560"
                        ],
                        "name": "E. Wrigley",
                        "slug": "E.-Wrigley",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Wrigley",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wrigley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 21
                            }
                        ],
                        "text": "Several researchers (Wright and Wrigley 1989; Wright 1990; Ng and Tomita 1991; Wright, Wrigley, and Sharman 1991) have proposed using LR parsers as a practical method of parsing with a probabilistic context-free grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60156140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c609552cfa5fecef32282579ee76bc0551fda36a",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "An LR parser for probabilistic context-free grammars is described. Each of the standard versions of parser generator (SLR, canonical and LALR) may be applied. A graph-structured stack permits action conflicts and allows the parser to be used with uncertain input, typical of speech recognition applications. The sentence uncertainty is measured using entropy and is significantly lower for the grammar than for a first-order Markov model."
            },
            "slug": "Probabilistic-LR-Parsing-for-Speech-Recognition-Wright-Wrigley",
            "title": {
                "fragments": [],
                "text": "Probabilistic LR Parsing for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An LR parser for probabilistic context-free grammars is described, using a graph-structured stack that permits action conflicts and allows the parser to be used with uncertain input, typical of speech recognition applications."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71486292"
                        ],
                        "name": "M. Tomita",
                        "slug": "M.-Tomita",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Tomita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tomita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "From these figures, the ANLT grammar is more than twice the size of Tomita's (combined morphological and syntactic) grammar for Japanese (Tomita 1987:45)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Tomita (1987) describes a system for nondeterministic LR parsing of context-free grammars consisting of atomic categories, in which each CF production may be augmented with a set of tests (which perform similar types of operations to those available in a unification grammar)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5558941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d6e100aead59980ac0fe88ad53bb7613adbbf22",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed. The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar. Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a \"graph-structured stack\". The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way. We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables. The algorithm is fast, due to the LR table precomputation. In several experiments with different English grammars and sentences, timings indicate a five- to tenfold speed advantage over Earley's context-free parsing algorithm.The algorithm parses a sentence strictly from left to right on-line, that is, it starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence. A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP AI workstations. The parser is used in the multi-lingual machine translation project at CMU. Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU."
            },
            "slug": "An-Efficient-Augmented-Context-Free-Parsing-Tomita",
            "title": {
                "fragments": [],
                "text": "An Efficient Augmented-Context-Free Parsing Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692491"
                        ],
                        "name": "S. Shieber",
                        "slug": "S.-Shieber",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Shieber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shieber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 279
                            }
                        ],
                        "text": "\u2026and defined independently of parsing procedure, since different definitions of the CF portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by Shieber (1985)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 216804411,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f4b6c7af3389150c115c5c6bb8310feda363a70",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Grammar formalisms based on the encoding of grammatical information in complex-valued feature systems enjoy some currency both in linguistics and natural-language-processing research. Such formalisms can be thought of by analogy to context-free grammars as generalizing the notion of nonterminal symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort. Unfortunately, in moving to an infinite nonterminal domain, standard methods of parsing may no longer be applicable to the formalism. Typically, the problem manifests itself as gross inefficiency or even nontermination of the algorithms. In this paper, we discuss a solution to the problem of extending parsing algorithms to formalisms with possibly infinite nonterminal domains, a solution based on a general technique we call restriction. As a particular example of such an extension, we present a complete, correct, terminating extension of Earley's algorithm that uses restriction to perform top-down filtering. Our implementation of this algorithm demonstrates the drastic elimination of chart edges that can be achieved by this technique. Finally, we describe further uses for the technique---including parsing other grammar formalisms, including definite-clause grammars; extending other parsing algorithms, including LR methods and syntactic preference modeling algorithms; and efficient indexing."
            },
            "slug": "Using-Restriction-to-Extend-Parsing-Algorithms-for-Shieber",
            "title": {
                "fragments": [],
                "text": "Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A solution to the problem of extending parsing algorithms to formalisms with possibly infinite nonterminal domains, a solution based on a general technique the authors call restriction is discussed, including a complete, correct, terminating extension of Earley's algorithm that uses restriction to perform top-down filtering."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086270"
                        ],
                        "name": "K. Cul\u00edk",
                        "slug": "K.-Cul\u00edk",
                        "structuredName": {
                            "firstName": "Karel",
                            "lastName": "Cul\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cul\u00edk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114392068"
                        ],
                        "name": "Rina S. Cohen",
                        "slug": "Rina-S.-Cohen",
                        "structuredName": {
                            "firstName": "Rina",
                            "lastName": "Cohen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rina S. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1482996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "650a05529d63aac7e9665c4681946fd01e24ddf0",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LR-Regular-Grammars-an-Extension-of-LR(k)-Grammars-Cul\u00edk-Cohen",
            "title": {
                "fragments": [],
                "text": "LR-Regular Grammars - an Extension of LR(k) Grammars"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2341559"
                        ],
                        "name": "N. Waegner",
                        "slug": "N.-Waegner",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Waegner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Waegner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 9
                            }
                        ],
                        "text": "However, Briscoe and Waegner (1992) describe an experiment in which, firstly, Baum-Welch re-estimation was used in conjunction with other more linguistically motivated constraints on the class of grammars that could be inferred, such as 'headedness'; and secondly, initial probabilities were heavily\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14216855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d286045910b498f848a5581f24b7b1c7871a031",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes a parser of sequences of (English) part-of-speech labels which utilises a probabilistic grammar trained using the inside-outside algorithm. The initial (meta)grammar is defined by a linguist and further rules compatible with metagrammatical constraints are automatically generated. During training, rules with very low probability are rejected yielding a wide-coverage parser capable of ranking alternative analyses. A series of corpus-based experiments describe the parser's performance."
            },
            "slug": "Robust-stochastic-parsing-using-the-inside-outside-Briscoe-Waegner",
            "title": {
                "fragments": [],
                "text": "Robust stochastic parsing using the inside-outside algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A parser of sequences of (English) part-of-speech labels which utilises a probabilistic grammar trained using the inside-outside algorithm yielding a wide-coverage parser capable of ranking alternative analyses."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3215185"
                        ],
                        "name": "G. Sampson",
                        "slug": "G.-Sampson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Sampson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sampson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143618150"
                        ],
                        "name": "R. Haigh",
                        "slug": "R.-Haigh",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Haigh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haigh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144214753"
                        ],
                        "name": "E. Atwell",
                        "slug": "E.-Atwell",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Atwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Atwell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 87
                            }
                        ],
                        "text": "If these results are interpreted in terms of a goodness of fit measure such as that of Sampson, Haigh, and Atwell (1989), the measure would be better ttian 96%."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 0
                            }
                        ],
                        "text": "Sampson, Haigh, and Atwell (1989) propose a more thorough-going probabilistic approach in which the parser uses a statistically defined measure of 'closest fit' to the set of analyses contained in a 'tree bank' of training data to assign an analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 214
                            }
                        ],
                        "text": "These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g. Sampson, Haigh, and Atwell 1989), are inadequate (taken alone)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30109304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b7ad2faf132ab14192babee10c45cec425fee32",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Parsing techniques based on rules defining grammaticality are difficult to use with authentic natural-language inputs, which are often grammatically messy. Instead, the APRIL system seeks a labelled tree structure which maximizes a numerical measure of conformity to statistical norms derived from a sample of parsed text. No distinction between legal and illegal trees arises: any labelled tree has a value. Because the search space is large and has an irregular geometry, APRIL seeks the best tree using simulated annealing, a stochastic optimization technique. Beginning with an arbitrary tree, many randomly-generated local modifications are considered and adopted or rejected according to their effect on tree-value: acceptance decisions are made probabilistically, subject to a bias against adverse moves which is very weak at the outset but is made to increase as the random walk through the search space continues. This enables the system to converge on the global optimum without getting trapped in loc..."
            },
            "slug": "Natural-language-analysis-by-stochastic-a-progress-Sampson-Haigh",
            "title": {
                "fragments": [],
                "text": "Natural language analysis by stochastic optimization: a progress report on Project APRIL"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The APRIL system seeks a labelled tree structure which maximizes a numerical measure of conformity to statistical norms derived from a sample of parsed text, which enables the system to converge on the global optimum without getting trapped in loc..."
            },
            "venue": {
                "fragments": [],
                "text": "J. Exp. Theor. Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145098645"
                        ],
                        "name": "J. Grosch",
                        "slug": "J.-Grosch",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Grosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Grosch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 18
                            }
                        ],
                        "text": "As a rough guide, Grosch (1990) quotes LALR(1) table construction for a grammar for Modula-2 taking from about 5 to 50 seconds, so scaling up two orders of magnitude, our timings for the ANLT grammar fall in the expected region."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9210575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb3589969c238e02fa2f77eb2a86d5ba9da862bf",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "1 SUMMARY Lalr is a parser generator that generates very fast and powerful parsers. The design goals have been to generate portable, table-drivenparsers that areasefficient as possible and which include all the features needed for practical applications. Like Yacc it accepts LALR(1) grammars, resolves ambiguities with precedence and associativ-ity of operators, generates table-drivenp arsers, and has a mechanism for S-attribution. Unlike Yacc it allows grammars to be written in extended BNF,i ncludes automatic error reporting, recovery,a nd repair,a nd generates parsers in C or Modula-2. In case of LR-conflicts, a derivation tree is printed instead of the involved states and items in order to aid the location of the problem. The parsers aretwo tothree times faster as those of Yacc. Using a MC 68020 processor,35,000 tokens per second or 580,000 lines per minute can be parsed. The sources of Lalr exist in C and in Modula-2. We describe in detail the development steps of the generated parsers. We show howsoftwareengineering methods likepseudo code and stepwise refinement can turnaparsing algorithm from a textbook into a complete and efficient implementation. Wepresent the details of the generated parsers and show howt he performance is achievedw ith a relatively simple and short program. Wed iscuss important features of the generator and finally present a comparison of some parser generators."
            },
            "slug": "Lalr-A-Generator-for-Efficient-Parsers-Grosch",
            "title": {
                "fragments": [],
                "text": "Lalr - A Generator for Efficient Parsers"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Lalr is a parser generator that generates very fast and powerful parsers that accepts LALR(1) grammars, resolves ambiguities with precedence and associativ-ity of operators, generates table-driven parsers, and has a mechanism for S-attribution."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003169"
                        ],
                        "name": "Claire Grover",
                        "slug": "Claire-Grover",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Grover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Grover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713574"
                        ],
                        "name": "B. Boguraev",
                        "slug": "B.-Boguraev",
                        "structuredName": {
                            "firstName": "Branimir",
                            "lastName": "Boguraev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boguraev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 127
                            }
                        ],
                        "text": "Previous work has demonstrated that the ANLT system is, in principle, able to assign the correct parse to a high proportion of English noun phrases drawn from a variety of corpora."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 241
                            }
                        ],
                        "text": "An example of a possible ANLT object grammar rule is: IN -, V +, BAR 2, PER x, PLU y, VFORM z] --+ [N +, V -, BAR 2, PER x, PLU y, CASE Nom] [N -, V +, BAR i, PER x, PLU y, VFORM z] This rule provides a (simple) analysis of the structure of English clauses, corresponding to S --* NP VP, using a feature system based loosely on that of GPSG (Gazdar et al. 1985)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 255
                            }
                        ],
                        "text": "The feasibility and usefulness of our approach has been investigated in a preliminary way by analyzing a ~small corpus of Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing noun definitions drawn from the Longman Dictionary of Contemporary English (LDOCE) (Procter 1978)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 108
                            }
                        ],
                        "text": "However, these rules are natural treatments of noun compounding and prepositional phrase (PP) attachment in English, and the different derivations correlate with different interpretations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 141
                            }
                        ],
                        "text": "In what follows, we will assume that the unification-based grammars we are considering are represented in the ANLT object grammar formalism (Briscoe et al. 1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 109
                            }
                        ],
                        "text": "For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 175
                            }
                        ],
                        "text": "This approach resulted in a simple tag sequence grammar often able to assign coherent and semantically/pragmatically plausible analyses to tag sequences drawn from the Spoken English Corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "The Alvey Natural Language Tools (ANLT) system is a wide-coverage lexical, morphological, and syntactic analysis system for English (Briscoe et al. 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14287805,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "1cd2098be359ad26c60daa42a5e34a9fda4261eb",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language grammars with large coverage are typically the result of many person-years of effort, working with clumsy formalisms and sub-optimal software support for grammar development. This paper describes our approach to the task of writing a substantial grammar, as part of a collaboration to produce a general purpose morphological and syntactic analyser for English. The grammatical formalism we have developed for the task is a metagrammatical notation which is a more expressive and computationally tractable variant of Generalized Phrase Structure Grammar. We have also implemented a software system which provides a highly integrated and very powerful set of tools for developing and managing a large grammar based on this notation. The system provides a grammarian with an environment which we have found to be essential for rapid but successful production of a substantial grammar."
            },
            "slug": "A-Formalism-and-Environment-for-the-Development-of-Briscoe-Grover",
            "title": {
                "fragments": [],
                "text": "A Formalism and Environment for the Development of a Large Grammar of English"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The grammatical formalism developed for the task is a metagrammatical notation which is a more expressive and computationally tractable variant of Generalized Phrase Structure Grammar and a software system is implemented which provides a grammarian with an environment which they have found to be essential for rapid but successful production of a substantial grammar."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692491"
                        ],
                        "name": "S. Shieber",
                        "slug": "S.-Shieber",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Shieber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shieber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 96
                            }
                        ],
                        "text": "To avoid asking the user about lexical ambiguity, we use the technique of preterminal delaying (Shieber 1983), in which the assignment of an atomic preterminal category to a lexical item is not made until the choice is forced by the use of a particular production in a later reduce action."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 91
                            }
                        ],
                        "text": "One implication of this finding is that an approach to conflict resolution such as that of Shieber (1983) where reduce-reduce conflicts are resolved in favor of the longer reduction may not suffice to select a unique analysis for realistic NL grammars."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215514040,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "b78b881dec3334abf5b8c6390c40a2d93a177294",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Native speakers of English show definite and consistent preferences for certain readings of syntactically ambiguous sentences. A user of a natural-language-processing system would naturally expect it to reflect the same preferences. Thus, such systems must model in some way the linguistic performance as well as the linguistic competence of the native speaker. We have developed a parsing algorithm--a variant of the LALR(I) shift-reduce algorithm--that models the preference behavior of native speakers for a range of syntactic preference phenomena reported in the psycholinguistic literature, including the recent data on lexical preferences. The algorithm yields the preferred parse deterministically, without building multiple parse trees and choosing among them. As a side effect, it displays appropriate behavior in processing the much discussed garden-path sentences. The parsing algorithm has been implemented and has confirmed the feasibility of our approach to the modeling of these phenomena."
            },
            "slug": "Sentence-Disambiguation-by-a-Shift-Reduce-Parsing-Shieber",
            "title": {
                "fragments": [],
                "text": "Sentence Disambiguation by a Shift-Reduce Parsing Technique"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A parsing algorithm is developed--a variant of the LALR(I) shift-reduce algorithm--that models the preference behavior of native speakers for a range of syntactic preference phenomena reported in the psycholinguistic literature, including the recent data on lexical preferences."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144713247"
                        ],
                        "name": "E. Klein",
                        "slug": "E.-Klein",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150220413"
                        ],
                        "name": "M. Martin",
                        "slug": "M.-Martin",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Martin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Martin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2 Figures given by  Klein and Martin (1989) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the grammars we have investigated, this representation achieves a similar order of space saving to the comb vector representation suggested by Aho, Sethi, and Ullman (1986:244ff) for unambiguous grammars (see  Klein and Martin [1989]  for a survey of representation techniques)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10358940,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec34568c3e432d90bbeef0d5b2a8d0a310135b28",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "PGS is a parser generating system accepting LALR(1) and related grammars in extended BNF notation and producing parsers based on table\u2010driven stack automata. To enable syntax\u2010directed translation, semantic actions can be attached to rules of the input grammar. An attribution mechanism allows the transfer of information between rules. The generated parsers have an automatic error recovery which can be tailored to satisfy specific needs of the language to be accepted. PGS generates parsers written in Pascal, Modula\u20102, C or Ada. Compared with existing systems, e.g. YACC1, a parser generated by PGS is twice as fast and the parse tables require 25 per cent less storage. This paper gives a survey of algorithms involved in the generator and the generated parsers, and compares them with algorithms used in other systems. In detail, it compares several parse\u2010table representations and their implications for space and time efficiency of the generated parsers."
            },
            "slug": "The-parser-generating-system-PGS-Klein-Martin",
            "title": {
                "fragments": [],
                "text": "The parser generating system PGS"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper gives a survey of algorithms involved in the generator and the generated parsers, and compares them with algorithms used in other systems."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7510969"
                        ],
                        "name": "S. DeRose",
                        "slug": "S.-DeRose",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "DeRose",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. DeRose"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 92
                            }
                        ],
                        "text": "These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g. Sampson, Haigh, and Atwell 1989), are inadequate (taken alone)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1275545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a8de92b304729f15d9bd6c3d22a56ab9b31e212",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Several algorithms have been developed in the past that attempt to resolve categorial ambiguities in natural language text without recourse to syntactic or semantic level information. An innovative method (called \"CLAWS\") was recently developed by those working with the Lancaster-Oslo/Bergen Corpus of British English. This algorithm uses a systematic calculation based upon the probabilities of co-occurrence of particular tags. Its accuracy is high, but it is very slow, and it has been manually augmented in a number of ways. The effects upon accuracy of this manual augmentation are not individually known.The current paper presents an algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments. Tests of the algorithm using the million words of the Brown Standard Corpus of English are reported; the overall accuracy is 96%. This algorithm can provide a fast and accurate front end to any parsing or natural language processing system for English."
            },
            "slug": "Grammatical-Category-Disambiguation-by-Statistical-DeRose",
            "title": {
                "fragments": [],
                "text": "Grammatical Category Disambiguation by Statistical Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32597159"
                        ],
                        "name": "R. Sharman",
                        "slug": "R.-Sharman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sharman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sharman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32405574"
                        ],
                        "name": "R. Hercer",
                        "slug": "R.-Hercer",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Hercer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 67
                            }
                        ],
                        "text": "Results comparable to those obtained by Fujisaki et al. (1989) and Sharman, Jelinek, and Mercer (1990) are possible on the basis of a quite modest amount of manual effort and a very much smaller training corpus, because the parse histories contain little 'noise' and usefully reflect the\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 0
                            }
                        ],
                        "text": "Sharman, Jelinek, and Mercer (1990) conducted a similar experiment with a grammar in ID/LP format (Gazdar et al. 1985; Sharman 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17402234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "860dfdaa8187bd22809f00396b30c66a2fc1ef24",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Parsing sentences of a Natural Language(NL) is an essential requirement for a variety of NL applications, and has been extensively studied. In particular, the sort of tasks which it would be desirable to do, include the ability to tag each word with its part-of-speech; to delineate with brackets, and label with a category name, each syntactic phrase; and to be able to adapt to different types of source material. Despite some 30 years of active research performing these tasks with a high degree of accuracy on unrestricted text is still an unsolved problem."
            },
            "slug": "Generating-a-grammar-for-statistical-training-Sharman-Jelinek",
            "title": {
                "fragments": [],
                "text": "Generating a grammar for statistical training"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Parsing sentences of a Natural Language is an essential requirement for a variety of NL applications, and has been extensively studied."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145060166"
                        ],
                        "name": "J. Kimball",
                        "slug": "J.-Kimball",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kimball",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kimball"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "Looking at these 21 cases in more detail, in 8 there is an inappropriate structural preference for 'low' or 'local' attachment (see Kimball 1973), in 4, an inappropriate preference for compounds, and in 6 of the remaining 9 cases, the highest ranked result contains a misanalysis of a single\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143411351,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "37a5b98e31d8f1d0b6739c5f506df570d3c3536e",
            "isKey": false,
            "numCitedBy": 786,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Seven-principles-of-surface-structure-parsing-in-Kimball",
            "title": {
                "fragments": [],
                "text": "Seven principles of surface structure parsing in natural language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169600"
                        ],
                        "name": "T. Fujisaki",
                        "slug": "T.-Fujisaki",
                        "structuredName": {
                            "firstName": "Tetsunosuke",
                            "lastName": "Fujisaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fujisaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716964"
                        ],
                        "name": "J. Cocke",
                        "slug": "J.-Cocke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cocke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cocke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795982"
                        ],
                        "name": "T. Nishino",
                        "slug": "T.-Nishino",
                        "structuredName": {
                            "firstName": "Tetsuro",
                            "lastName": "Nishino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nishino"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59759783,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "e7470c416e13fcf91396cc29fa43a7903ea6d519",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Constructing a grammar which can parse sentences selected from a natural language corpus is a difficult task. One of the most serious problems is the unmanageably large number of ambiguities. Pure syntactic analysis based only on syntactic knowledge will sometimes result in hundreds of ambiguous parses."
            },
            "slug": "Probabilistic-Parsing-Method-for-Sentence-Fujisaki-Jelinek",
            "title": {
                "fragments": [],
                "text": "Probabilistic Parsing Method for Sentence Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A grammar which can parse sentences selected from a natural language corpus is a difficult task and pure syntactic analysis based only on syntactic knowledge will sometimes result in hundreds of ambiguous parses."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235876"
                        ],
                        "name": "M. Bermudez",
                        "slug": "M.-Bermudez",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Bermudez",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bermudez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 99
                            }
                        ],
                        "text": "Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions ; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 224
                            }
                        ],
                        "text": "The existing chart parser, although slower, has been retained since it is more suited to grammar development, because of the speed with which modifications to the grammar can be compiled and its better debugging facilities (Boguraev et al. 1988). Our nondeterministic LR parser is based on Kipps' (1989) reformulation of Tomita's (1987) parsing algorithm and uses a graph-structured stack in the same way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 76
                            }
                        ],
                        "text": "Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28879555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278ffcf5aab7e67bd35000614cc05bfa77f70b2c",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A construction method is presented for lookahead LR passers that unifies many of the construction algorithms available in the literature. The model allows single, multiple, and arbitrary symbol lookahead, each only when required. Three user-supplied parameters are used by the construction method; specific settings of these parameters yield well-known grammar classes such as LALR(k), SLR(k), and several subsets of the LR-regular class, among others. Thus. rather than using several severely incompatible parser generators, or making unnatural changes to the grammar to accommodate the parsing techniques, one can manipulate these parameters to obtain a suitable parsing technique. The model captures the essence of the problem of computing lookahead for LR parsers, and provides a better understanding of the relationships among the corresponding classes of context-free grammars.<<ETX>>"
            },
            "slug": "A-unifying-model-for-lookahead-LR-parsing-Bermudez",
            "title": {
                "fragments": [],
                "text": "A unifying model for lookahead LR parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A construction method is presented for lookahead LR passers that unifies many of the construction algorithms available in the literature, and provides a better understanding of the relationships among the corresponding classes of context-free grammars."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1988 International Conference on Computer Languages"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 278
                            }
                        ],
                        "text": "In fact, LR parsing is the most effectively predictive parsing technique for which an automatic compilation procedure is known, but this is somewhat undermined by our use of features, which will block some derivations so that the valid prefix property will no longer hold (e.g. Schabes 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8586936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0ed4b7868de7f24ace313e252c834d2d55f6cab",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an algorithm for designing a predictive left to right shift-reduce non-determinisic push-down machine corresponding to an arbitrary unrestricted context-free grammar and an algorithm for efficiently driving this machine in pseudo-parallel. The performance of the resulting parser is formally proven to be superior to Earley's parser (1970).The technique employed consists in constructing before run-time a parsing table that encodes a non-deterministic machine in the which the predictive behavior has been compiled out. At run time, the machine is driven in pseudo-parallel with the help of a chart.The recognizer behaves in the worst case in O(|G|2n3)-time and O(|G|n2)-space. However in practice it is always superior to Earley's parser since the prediction steps have been compiled before run-time.Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portions of the basic non-deterministic push-down machine while still using the same pseudo-parallel driver."
            },
            "slug": "Polynomial-Time-and-Space-Shift-Reduce-Parsing-of-Schabes",
            "title": {
                "fragments": [],
                "text": "Polynomial Time and Space Shift-Reduce Parsing of Arbitrary Context-free Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An algorithm for designing a predictive left to right shift-reduce non-determinisic push-down machine corresponding to an arbitrary unrestricted context-free grammar and an algorithm for efficiently driving this machine in pseudo-parallel are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879397"
                        ],
                        "name": "D. Warren",
                        "slug": "D.-Warren",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warren",
                            "middleNames": [
                                "H.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Warren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "This formalism is a notational variant of Definite Clause Grammar (e.g. Pereira and Warren 1980), in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2133116,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbc04a1951003ba164303b2898fb7f3c6b4e9083",
            "isKey": false,
            "numCitedBy": 1034,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Definite-Clause-Grammars-for-Language-Analysis-A-of-Pereira-Warren",
            "title": {
                "fragments": [],
                "text": "Definite Clause Grammars for Language Analysis - A Survey of the Formalism and a Comparison with Augmented Transition Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2385937"
                        ],
                        "name": "Tsuneko Nakazawa",
                        "slug": "Tsuneko-Nakazawa",
                        "structuredName": {
                            "firstName": "Tsuneko",
                            "lastName": "Nakazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsuneko Nakazawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 223
                            }
                        ],
                        "text": "In this case, the LR parse table would be based on complex categories, with unification of complex categories taking the place of equality of atomic ones in the standard LR parse table construction algorithm (Osborne 1990; Nakazawa 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 972946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b7a956b7df5eb520c81e4b49697b0e15673fd36",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an LR parsing algorithm modified for grammars with feature-based categories. The proposed algorithm does not instantiate categories during preprocessing of a grammar as proposed elsewhere. As a result, it constructs a minimal size of GOTO/ACTION table and eliminates the necessity of search for GOTO table entries during parsing."
            },
            "slug": "An-Extended-LR-Parsing-Algorithm-for-Grammars-Using-Nakazawa",
            "title": {
                "fragments": [],
                "text": "An Extended LR Parsing Algorithm for Grammars Using Feature-Based Syntactic Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The proposed algorithm does not instantiate categories during preprocessing of a grammar as proposed elsewhere, and as a result, it constructs a minimal size of GOTO/ACTION table and eliminates the necessity of search for GOTO table entries during parsing."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144954740"
                        ],
                        "name": "J. Earley",
                        "slug": "J.-Earley",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Earley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Earley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b",
            "isKey": false,
            "numCitedBy": 1339,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(<italic>k</italic>) algorithm and the familiar top-down algorithm. It has a time bound proportional to <italic>n</italic><supscrpt>3</supscrpt> (where <italic>n</italic> is the length of the string being parsed) in general; it has an <italic>n</italic><supscrpt>2</supscrpt> bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick."
            },
            "slug": "An-efficient-context-free-parsing-algorithm-Earley",
            "title": {
                "fragments": [],
                "text": "An efficient context-free parsing algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A parsing algorithm which seems to be the most efficient general context-free algorithm known is described and appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701400"
                        ],
                        "name": "G. Gazdar",
                        "slug": "G.-Gazdar",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Gazdar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gazdar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145606490"
                        ],
                        "name": "Ewan Klein",
                        "slug": "Ewan-Klein",
                        "structuredName": {
                            "firstName": "Ewan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ewan Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2770201"
                        ],
                        "name": "G. Pullum",
                        "slug": "G.-Pullum",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Pullum",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pullum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393013"
                        ],
                        "name": "I. Sag",
                        "slug": "I.-Sag",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Sag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 202
                            }
                        ],
                        "text": "\u2026the gapped daughter--for each such daughter an extra rule is added to the backbone grammar expanding the gap category to the null string; secondly, the formalism allows Kleene star and plus operators (Gazdar et al. 1985)-- in the ANLT grammar these operators are utilized in rules for coordination."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 340,
                                "start": 336
                            }
                        ],
                        "text": "An example of a possible ANLT object grammar rule is: IN -, V +, BAR 2, PER x, PLU y, VFORM z] --+ [N +, V -, BAR 2, PER x, PLU y, CASE Nom] [N -, V +, BAR i, PER x, PLU y, VFORM z] This rule provides a (simple) analysis of the structure of English clauses, corresponding to S --* NP VP, using a feature system based loosely on that of GPSG (Gazdar et al. 1985)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 209
                            }
                        ],
                        "text": "This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g. Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 99
                            }
                        ],
                        "text": "Sharman, Jelinek, and Mercer (1990) conducted a similar experiment with a grammar in ID/LP format (Gazdar et al. 1985; Sharman 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Although Tomita (1984:357) anticipates LR parsing techniques being applied to large NL grammars written in formalisms such as GPSG, the sizes of parse tables for such grammars grow more rapidly than he predicts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 278
                            }
                        ],
                        "text": "\u2026BAR 2, PER x, PLU y, VFORM z] --+ [N +, V -, BAR 2, PER x, PLU y, CASE Nom] [N -, V +, BAR i, PER x, PLU y, VFORM z] This rule provides a (simple) analysis of the structure of English clauses, corresponding to S --* NP VP, using a feature system based loosely on that of GPSG (Gazdar et al. 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 414,
                                "start": 394
                            }
                        ],
                        "text": "Two other aspects of the ANLT grammar formalism require further minor elaborations to the basic algorithm: firstly, a rule may introduce a gap by including the feature specification [NULL +] on the gapped daughter--for each such daughter an extra rule is added to the backbone grammar expanding the gap category to the null string; secondly, the formalism allows Kleene star and plus operators (Gazdar et al. 1985)-in the ANLT grammar these operators are utilized in rules for coordination."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61087257,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "cdfefdebd4686a878e6572cb8ba2da9d8efbe552",
            "isKey": true,
            "numCitedBy": 2056,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Generalized Phrase Structure Grammar\" provides the definitive exposition of the theory of grammar originally proposed by Gerald Gazdar and developed during half a dozen years' work with his colleagues Ewan Klein, Geoffrey Pullum, and Ivan Sag. This long-awaited book contains both detailed specifications of the theory and extensive illustrations of its power to describe large parts of English grammar. Experts who wish to evaluate the theory and students learning GPSP for the first time will find this book an invaluable guide.The initial chapters lay out the theoretical machinery of GPSP in a readily intelligible way. Combining informal discussion with precise formalization, the authors describe all major aspects of their grammatical system, including a complete theory of syntactic features, phrase structure rules, meta rules, and feature instantiation principles. The book then shows just what a GPSP analysis of English syntax can accomplish. Topics include the internal structure of phrases, unbounded dependency constructions of many varieties, and coordinate conjunction a construction long considered the sticking point for phrase structure approaches to syntax.The book concludes with a well developed proposal for a model theoretic semantic system to go along with GPSP syntax. Throughout, the authors maintain the highest standards of explicitness and rigor in developing and assessing their grammatical system. Their aim is to provide the best possible test of the hypothesis that syntactic description can be accomplished in a single-level system. And more generally, it is their intention to formulate a grammatical framework in which linguistic universals follow directly from the form of the system and therefore require no explicit statement. Their book sets new methodological standards for work in generative grammar while presenting a grammatical system of extraordinary scope.\""
            },
            "slug": "Generalized-Phrase-Structure-Grammar-Gazdar-Klein",
            "title": {
                "fragments": [],
                "text": "Generalized Phrase Structure Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "\"Generalized Phrase Structure Grammar\" provides the definitive exposition of the theory of grammar originally proposed by Gerald Gazdar and developed during half a dozen years' work with his colleagues Ewan Klein, Geoffrey Pullum, and Ivan Sag."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49497622"
                        ],
                        "name": "K. Lari",
                        "slug": "K.-Lari",
                        "structuredName": {
                            "firstName": "Kaveh",
                            "lastName": "Lari",
                            "middleNames": [
                                "Sookhak"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, there is no guarantee that the global optimum will be found, and the a priori initial probabilities chosen are critical for convergence on useful probabilities (e.g.  Lari and Young 1990 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 175
                            }
                        ],
                        "text": "However, there is no guarantee that the global optimum will be found, and the a priori initial probabilities chosen are critical for convergence on useful probabilities (e.g. Lari and Young 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 201
                            }
                        ],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following  Lari and Young (1990)  and Pereira and Schabes (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53736294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74",
            "isKey": true,
            "numCitedBy": 713,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-stochastic-context-free-grammars-Lari-Young",
            "title": {
                "fragments": [],
                "text": "Applications of stochastic context-free grammars using the Inside-Outside algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713574"
                        ],
                        "name": "B. Boguraev",
                        "slug": "B.-Boguraev",
                        "structuredName": {
                            "firstName": "Branimir",
                            "lastName": "Boguraev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boguraev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003169"
                        ],
                        "name": "Claire Grover",
                        "slug": "Claire-Grover",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Grover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Grover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 224
                            }
                        ],
                        "text": "The existing chart parser, although slower, has been retained since it is more suited to grammar development, because of the speed with which modifications to the grammar can be compiled and its better debugging facilities (Boguraev et al. 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12738746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4589f5fc7b6c5d1fab8271e96c97ddc060051bd3",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Even though progress in theoretical linguistics does not necessarily rely on the construction of working programs, a large proportion of current research in syntactic theory is facilitated by suitable computational tools. However, when natural language processing applications seek to draw on the results from new developments in theories of grammar, not only the nature of the tools has to change, but they face the challenge of reconciling the seemingly contradictory requirements of notational perspicuity and efficiency of performance. In this paper, we present a comparison and an evaluation of a number of software systems for grammar development, and argue that they are inadequate as practical tools for building wide-coverage grammars. We discuss a number of factors characteristic of this task, demonstrate how they influence the design of a suitable software environment, and describe the implementation of a system which has supported efficient development of a large computational grammar of English."
            },
            "slug": "Software-Support-for-Practical-Grammar-Development-Boguraev-Carroll",
            "title": {
                "fragments": [],
                "text": "Software Support for Practical Grammar Development"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a comparison and an evaluation of a number of software systems for grammar development, and argues that they are inadequate as practical tools for building wide-coverage grammars."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144741427"
                        ],
                        "name": "C. Pollard",
                        "slug": "C.-Pollard",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pollard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393013"
                        ],
                        "name": "I. Sag",
                        "slug": "I.-Sag",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Sag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g. Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 183
                            }
                        ],
                        "text": "To date, we have not attempted to compute CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out of the categories concerned would be necessary for an LR parser to work effectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62081394,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "aff1b791866d308ea37cca86e2aa76e852f23604",
            "isKey": false,
            "numCitedBy": 1194,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A long-standing, near-universal, and erroneous practice of teaching syntax in a void exists, as if the communicative function of language had nothing to do with syntax. And semantics has customarily been taught in sequence after syntax, or else not at all. Based upon graduate courses taught at Stanford University, this work seeks to redress this situation by building up syntactic and semantic aspects of grammatical theory in an integrated way from the start, under the assumption that neither is of linguistic interest divorced from the other. The particular theory presented, head-driven phrase structure grammar (HPSG) - so-called because of its central notion of the grammatical head - is an information-based (or 'unification-based' theory that has its roots in a number of different research programs within linguistics and neighboring disciplines such as philosophy and computer science.Thus HPSG draws upon and attempts to synthesize insights and perspectives from several families of contemporary syntactic theories, such as categorial grammar, lexical-functional grammar, generalized phrase structure grammar, and government-binding theory; but many of its key ideas arise from semantic theories like situation semantics and discourse representation theory, and from computational work in such areas as knowledge representation, data type theory, and formalisms based upon the unification of partial information."
            },
            "slug": "Information-based-syntax-and-semantics-Pollard-Sag",
            "title": {
                "fragments": [],
                "text": "Information-based syntax and semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The particular theory presented, head-driven phrase structure grammar (HPSG) - so-called because of its central notion of the grammatical head - is an information-based (or 'unification-based' theory that has its roots in a number of different research programs within linguistics and neighboring disciplines such as philosophy and computer science."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Pereira and Schabes (1992) report an experiment using Baum-Welch re-estimation to infer a grammar and associated rule probabilities from a category set containing 15 nonterminals and 48 terminals, corresponding to the Penn Treebank lexical tagset (Santorini 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 227
                            }
                        ],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 696805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c0eab87d4855c42ae6395bf2e27eefe55003b4a",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
            },
            "slug": "Inside-Outside-Reestimation-From-Partially-Corpora-Pereira-Schabes",
            "title": {
                "fragments": [],
                "text": "Inside-Outside Reestimation From Partially Bracketed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus to achieve faster convergence and better modelling of hierarchical structure than the original one."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153201"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121084921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c79a9bb8f885050cad70b4c69e016b186ffa538",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms which are based on modeling speech as a finite\u2010state, hidden Markov process have been very successful in recent years. This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes. This algorithm permits automatic training of the stochastic analog of an arbitrary context free grammar. In particular, in contrast to many grammatical inference methods, the new algorithm allows the grammar to have an arbitrary degree of ambiguity. Since natural language is often syntactically ambiguous, it is necessary for the grammatical inference algorithm to allow for this ambiguity. Furthermore, allowing ambiguity in the grammar allows errors in the recognition process to be explicitly modeled in the grammar rather than added as an extra component."
            },
            "slug": "Trainable-grammars-for-speech-recognition-Baker",
            "title": {
                "fragments": [],
                "text": "Trainable grammars for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes that permits automatic training of the stochastic analog of an arbitrary context free grammar."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50539436"
                        ],
                        "name": "Lita Taylor",
                        "slug": "Lita-Taylor",
                        "structuredName": {
                            "firstName": "Lita",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lita Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003169"
                        ],
                        "name": "Claire Grover",
                        "slug": "Claire-Grover",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Grover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Grover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96.8% of a corpus of 10,000 noun phrases extracted (without regard for their internal form) from a variety of corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8811626,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "df0a809cbe66fb95c5670d21f197e418943cfba7",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximately, 10,000 naturally occurring noun phrases taken from the LOB corpus were used firstly, to evaluate the NP component of the Alvey ANLT grammar (Grover et al., 1987, 1989) and secondly, to retest Sampson's (1987a) claim that this data provide evidence for the lack of a clear-cut distinction between grammatical and 'deviant' examples. The examples were sorted and classified on the basis of the lexical and syntactic analysis undertaken as part of the LOB corpus project (Sampson, 1987b). Tokens of each resulting type were parsed using the ANLT grammar and the results analysed to determine the success rate of the parses and the generality of the rules employed."
            },
            "slug": "The-Syntactic-Regularity-of-English-Noun-Phrases-Taylor-Grover",
            "title": {
                "fragments": [],
                "text": "The Syntactic Regularity of English Noun Phrases"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Approximately, 10,000 naturally occurring noun phrases taken from the LOB corpus were used firstly, to evaluate the NP component of the Alvey ANLT grammar and secondly, to retest Sampson's (1987a) claim that this data provide evidence for the lack of a clear-cut distinction between grammatical and 'deviant' examples."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "By combining such techniques and relaxing the CNF constraint, for example, by adopting the trellis algorithm version of Baum-Welch re-estimation (Kupiec 1991), it might be possible to create a computationally tractable system operating with a realistic NL grammar that would only infer a new rule from a finite space of linguistically motivated possibilities in the face of parse failure or improbability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "By combining such techniques and relaxing the CNF constraint, for example, by adopting the trellis algorithm version of Baum-Welch re-estimation (Kupiec 1991), it might be possible to create a computationally tractable system operating with a realistic NL grammar that would only infer a new rule\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Kupiec (1991) extends Baum-Welch re-estimation to arbitrary (non- CNF) CFGs. Baum-Welch re-estimation can be used with restricted or unrestricted grammars/models in the sense that some of the parameters corresponding to possible productions over a given (non-)terminal category set/set of states can\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2197535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d11ac5f5b46f8936cd12854295b5bb95d2bfc7e4",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a new algorithm for estimating the parameters of a hidden stochastic context-free grammar. In contrast to the Inside/Outside (I/O) algorithm it does not require the grammar to be expressed in Chomsky normal form, and thus can operate directly on more natural representations of a grammar. The algorithm uses a trellis-based structure as opposed to the binary branching tree structure used by the I/O algorithm. The form of the trellis is an extension of that used by the Forward/Backward algorithm, and as a result the algorithm reduces to the latter for components that can be modeled as finite-state networks. In the same way that a hidden Markov model (HMM) is a stochastic analogue of a finite-state network, the representation used by the new algorithm is a stochastic analogue of a recursive transition network, in which a state may be simple or itself contain an underlying structure."
            },
            "slug": "A-Trellis-Based-Algorithm-For-Estimating-The-Of-Kupiec",
            "title": {
                "fragments": [],
                "text": "A Trellis-Based Algorithm For Estimating The Parameters Of Hidden Stochastic Context-Free Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new algorithm for estimating the parameters of a hidden stochastic context-free grammar that does not require the grammar to be expressed in Chomsky normal form, and thus can operate directly on more natural representations of a grammar."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34739267"
                        ],
                        "name": "G. Ritchie",
                        "slug": "G.-Ritchie",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Ritchie",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ritchie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46898028"
                        ],
                        "name": "G. Russell",
                        "slug": "G.-Russell",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 77
                            }
                        ],
                        "text": "The 10,600 resultant entries were loaded into the ANLT morphological system (Ritchie et al. 1987) and this sublexicon and the full ANLT grammar formed the starting point for the training process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1296471,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a40ecce3d36a2fd3252f5fd3ed12470505ddc0a4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "To achieve generality, natural language parsers require dictionaries which handle lexical information in a linguistically motivated but computationally viable manner. Various rule formalisms are presented which process orthographic effects, word structure, and lexicai redundancy in a manner which allows the statement of linguistic generalisations with a clear computational interpretation. A compact description of a medium-sized subset of the English lexicon can be stated using these formalisms. The proposed mechanisms have been implemented and tested, but require to be refined further if they are to be regarded as an interesting linguistic theory."
            },
            "slug": "A-Computational-Framework-for-Lexical-Description-Ritchie-Pulman",
            "title": {
                "fragments": [],
                "text": "A Computational Framework for Lexical Description"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Various rule formalisms are presented which process orthographic effects, word structure, and lexicai redundancy in a manner which allows the statement of linguistic generalisations with a clear computational interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069371110"
                        ],
                        "name": "Karen Jensen",
                        "slug": "Karen-Jensen",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Jensen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Jensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2488519"
                        ],
                        "name": "G. Heidorn",
                        "slug": "G.-Heidorn",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Heidorn",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heidorn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413105"
                        ],
                        "name": "L. A. Miller",
                        "slug": "L.-A.-Miller",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. A. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776993"
                        ],
                        "name": "Yael Ravin",
                        "slug": "Yael-Ravin",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Ravin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yael Ravin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Jensen et al. (1983) describe an approach to parsing such examples based on parse 'fitting' or rule 'relaxation' to deal with 'ill-formed' input."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9483784,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9de9fd1835d7cac1b46685b3ffa93e3da49a3be4",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Processing syntactically ill-formed language is an important mission of the EPISTLE system, Ill-formed input is treated by this system in various ways. Misspellings are highlighted by a standard spelling checker; syntactic errors are detected and corrections are suggested; and stylistic infelicities are called to the user's attention.Central to the EPISTLE processing strategy is its technique of fitted parsing. When the rules of a conventional syntactic grammar are unable to produce a parse for an input string, this technique can be used to produce a reasonable approximate parse that can serve as input to the remaining stages of processing.This paper first describes the fitting process and gives examples of ill-formed language situations where it is called into play. We then show how a fitted parse allows EPISTLE to carry on its text-critiquing mission where conventional grammars would fail either because of input problems or because of limitations in the grammars themselves. Some inherent difficulties of the fitting technique are also discussed. In addition, we explore how style critiquing relates to the handling of ill-formed input, and how a fitted parse can be used in style checking."
            },
            "slug": "Parse-Fitting-and-Prose-Fixing:-Getting-a-Hold-on-Jensen-Heidorn",
            "title": {
                "fragments": [],
                "text": "Parse Fitting and Prose Fixing: Getting a Hold on III-Formedness"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper first describes the fitting process and gives examples of ill-formed language situations where it is called into play, and shows how a fitted parse allows EPISTLE to carry on its text-critiquing mission where conventional grammars would fail either because of input problems or because of limitations in the Grammars themselves."
            },
            "venue": {
                "fragments": [],
                "text": "Am. J. Comput. Linguistics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741927"
                        ],
                        "name": "M. Covington",
                        "slug": "M.-Covington",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Covington",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Covington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767307"
                        ],
                        "name": "H. Alshawi",
                        "slug": "H.-Alshawi",
                        "structuredName": {
                            "firstName": "Hiyan",
                            "lastName": "Alshawi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Alshawi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887897"
                        ],
                        "name": "Bj\u00f6rn Gamb\u00e4ck",
                        "slug": "Bj\u00f6rn-Gamb\u00e4ck",
                        "structuredName": {
                            "firstName": "Bj\u00f6rn",
                            "lastName": "Gamb\u00e4ck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bj\u00f6rn Gamb\u00e4ck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 164
                            }
                        ],
                        "text": "However, we generalize the technique of atomic category packing described by Tomita, driven by atomic category names, to complex feature-based categories following Alshawi (1992): the packing of sub-analyses is driven by the subsumption relationship between the feature values in their top nodes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1463,
                                "start": 164
                            }
                        ],
                        "text": "However, we generalize the technique of atomic category packing described by Tomita, driven by atomic category names, to complex feature-based categories following Alshawi (1992): the packing of sub-analyses is driven by the subsumption relationship between the feature values in their top nodes. An analysis is only packed into one that has already been found if its top node is subsumed by, or is equal to that of the one already found. An analysis, once packed, will thus never need to be unpacked during parsing (as in Tomita's system) since the value of each feature will always be uniquely determined. Our use of local ambiguity packing does not in practice seem to result in exponentially bad performance with respect to sentence length (cf. Johnson 1989) since we have been able to generate packed parse forests for sentences of over 30 words having many thousands of parses. We have implemented a unification version of Schabes' (1991a) chart-based LR-like parser (which is polynomial in sentence length for CF grammars), but experiments with the ANLT grammar suggest that it offers no practical advantages over our Tomita-style parser, and Schabes' table construction algorithm yields less fine-grained and, therefore, less predictive parse tables. Nevertheless, searching the parse forest exhaustively to recover each distinct analysis proved computationally intractable for sentences over about 22 words in length. Wright, Wrigley, and Sharman (1991) describe a Viterbi-like algorithm for unpacking parse forests containing probabilities of (sub-)analyses to find the n-best analyses, but this approach does not generalize (except in a heuristic way) to our approach in which unification failure on the different extensions of packed nodes (resulting from differing super- or sub-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 946,
                                "start": 164
                            }
                        ],
                        "text": "However, we generalize the technique of atomic category packing described by Tomita, driven by atomic category names, to complex feature-based categories following Alshawi (1992): the packing of sub-analyses is driven by the subsumption relationship between the feature values in their top nodes. An analysis is only packed into one that has already been found if its top node is subsumed by, or is equal to that of the one already found. An analysis, once packed, will thus never need to be unpacked during parsing (as in Tomita's system) since the value of each feature will always be uniquely determined. Our use of local ambiguity packing does not in practice seem to result in exponentially bad performance with respect to sentence length (cf. Johnson 1989) since we have been able to generate packed parse forests for sentences of over 30 words having many thousands of parses. We have implemented a unification version of Schabes' (1991a) chart-based LR-like parser (which is polynomial in sentence length for CF grammars), but experiments with the ANLT grammar suggest that it offers no practical advantages over our Tomita-style parser, and Schabes' table construction algorithm yields less fine-grained and, therefore, less predictive parse tables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121989119,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1afd689e024fe23d8be8ce3832b5999ce97f3bfc",
            "isKey": true,
            "numCitedBy": 486,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to the CLE, Hiyan Alshawi and Robert C. Moore - language analysis and interpretation, overview of CLE components logical forms, Jan van Eijck and Hiyan Alshawi - levels of logical form in the CLE, resolved logical form, quasi logical form categories and rules, Hiyan Alshawi - constraints, components and rules, categories for linguistic analysis, CLE categories, category unification and subsumption, Boolean expression feature values, feature sets, defaults and macros, internal category representation, grammar rules, syntax and morphology rules, semantic rules, lexical entries unification-based syntactic analysis, Stephen G. Pulman - theoretical background, subcategorization, start categories, sentence types, subject-auxilliary inversion, unbounded dependencies, passives, conjunctions semantic rules for English, Jan van Eijck and Robert C. Moore - semantic rules and sences, general principles of the CLE semantics, the semantics of specific constructions lexical analysis, David Carter - tokenizing the input, segmenting the tokens, recovering from unknown tokens syntactic and semantic processing, Robert C. Moore and Hiyan Alshawi - parsing, semantic analysis, morphological processing, ambiguities and packing quantifier scoping, Douglas B. Moran and Fernando C.N. Pereira - quantifier scoping problem, scoping rules and references, the scoping algorithm, refinements to the basic algorithm, implementation sortal restrictions, Hiyan Alshawi and David Carter - applying sortal restrictions, encoding sorts as terms, the external representation of sorts, specifying the sort hierarchy, developments resolving quasi logical forms, Hiyan Alshawi - deriving LFs from QLFs, anaphoric terms, context model, constraints on resolved LFs, anaphoric relations and formulae, further work on QLF resolution lexical acquisition, David Carter - strategy adopted, assumptions behind the strategy eliciting syntactic behaviour and semantic informtion, incorporating new entries, the core lexicon the CLE in application development, Arnold Smith - linguistic applications, model-based systems, the LF-Prolog query evaluator, the order-processing exemplar, extending the interface, new directions ellipsis, comparatives and generation, Hiyan Alshawi and Stephen G. Pulman Swedish-English QLF translation, Hiyan Alshawi, et al - the Swedish CLE, QLF-based translation, disambiguation and interaction."
            },
            "slug": "The-Core-Language-Engine-Covington-Alshawi",
            "title": {
                "fragments": [],
                "text": "The Core Language Engine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34886242"
                        ],
                        "name": "R. Patil",
                        "slug": "R.-Patil",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Patil",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Patil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 151
                            }
                        ],
                        "text": "For example, in noun compounds analyzed using a recursive binary-branching rule (N --* N N) the number of analyses correlates with the Catalan series (Church and Patil, 1982), 4 so a 3-word compound has 2 analyses, 4 has 5, 5 has 14, 9 has 1430, and so forth."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 10
                            }
                        ],
                        "text": "Secondly, Church and Patil (1982) demonstrate that for a realistic grammar parsing realistic input, the set of possible analyses licensed by the grammar can be in the thousands."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9330325,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a0876f4589bd112e5b6e7c67580dc0ab8102ad83",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentences are far more ambiguous than one might have thought. There may be hundreds, perhaps thousands, of syntactic parse trees for certain very natural sentences of English. This fact has been a major problem confronting natural language processing, especially when a large percentage of the syntactic parse trees are enumerated during semantic/pragmatic processing. In this paper we propose some methods for dealing with syntactic ambiguity in ways that exploit certain regularities among alternative parse trees. These regularities will be expressed as linear combinations of ATN networks, and also as sums and products of formal power series. We believe that such encoding of ambiguity will enhance processing, whether syntactic and semantic constraints are processed separately in sequence or interleaved together."
            },
            "slug": "Coping-with-Syntactic-Ambiguity-or-How-to-Put-the-Church-Patil",
            "title": {
                "fragments": [],
                "text": "Coping with Syntactic Ambiguity or How to Put the Block in the Box on the Table"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes some methods for dealing with syntactic ambiguity in ways that exploit certain regularities among alternative parse trees, and believes that such encoding of ambiguity will enhance processing, whether syntactic and semantic constraints are processed separately in sequence or interleaved together."
            },
            "venue": {
                "fragments": [],
                "text": "Am. J. Comput. Linguistics"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 278
                            }
                        ],
                        "text": "In fact, LR parsing is the most effectively predictive parsing technique for which an automatic compilation procedure is known, but this is somewhat undermined by our use of features, which will block some derivations so that the valid prefix property will no longer hold (e.g. Schabes 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53814149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfca6a3a2b4fe35e435d32a52b49729462a4615c",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The valid prefix property (VPP), the capability of a left to right parser to detect errors as soon as possible, often goes unnoticed in parsing CFGs. Earley\u2019s parser for CFGs (Earley, 1968; Earley, 1970) maintains the valid prefix property and obtains an O(n^3)-time worst case complexity, as good as parsers that do not maintain such as the CKY parser (Younger, 1967; Kasami, 1965). Contrary to CFGs, maintaining the valid prefix property for TAGs is costly. In 1988, Schabes and Joshi proposed an Earley-type parser for TAGs. It maintains the valid prefix property at the expense of its worst case complexity (O(n^9)-time). To our knowledge, it is the only known polynomial time parser for TAGs that maintains the valid prefix property. In this paper, we explain why the valid prefix property is expensive to maintain for TAGs and we introduce a predictive left to right parser for TAGs that does not maintain the valid prefix property but that achieves an O(n^6)-time worst case behavior, O(n^4)-time for unambiguous grammars and linear time for a large class of grammars."
            },
            "slug": "The-Valid-Prefix-Property-and-Left-to-Right-Parsing-Schabes",
            "title": {
                "fragments": [],
                "text": "The Valid Prefix Property and Left to Right Parsing of Tree-Adjoining Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A predictive left to right parser for TAGs is introduced that does not maintain the valid prefix property but that achieves an O(n^6)-time worst case behavior, O( n^4)-time for unambiguous grammars and linear time for a large class of Grammars."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37013516"
                        ],
                        "name": "D. Spector",
                        "slug": "D.-Spector",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spector",
                            "middleNames": [
                                "H.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spector"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19808567,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "d35c3210e9c5c9eef1515411ca97c51d1e6ce275",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper attempts to be a complete description, from the implementor's viewpoint, of the basic lexing and parsing requirements of the Modula-2 language. The lexical structure is described informally, while the syntax is specified formally by an LALR(1) grammar expressed in BNF (Backus-Naur Form).Further information is available in the Modula-2 language definition [1]."
            },
            "slug": "Lexing-and-parsing-Modula-2-Spector",
            "title": {
                "fragments": [],
                "text": "Lexing and parsing Modula-2"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper attempts to be a complete description, from the implementor's viewpoint, of the basic lexing and parsing requirements of the Modula-2 language."
            },
            "venue": {
                "fragments": [],
                "text": "SIGP"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15379653"
                        ],
                        "name": "Ann A. Copestake",
                        "slug": "Ann-A.-Copestake",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Copestake",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ann A. Copestake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Copestake (1990, 1992) describes a program capable of extracting the genus term of a definition from an LDOCE definition, resolving the sense of such terms, and constructing hierarchical taxonomies of the resulting word senses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15007513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f365cd5d3d677b0f739481444cf66b0ea271523",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This abstract describes an approach to extracting taxonomies from machine readable dictionaries and using them to structure a lexical knowledge base which incorporates default inheritance. Taxonomy construction is based on an intuitive notion of the organisation of the substantial quantities of data in machine readable dictionaries which were developed for quite independent purposes. Our intention is to investigate how this aaects, and is aaected by, the formal semantics of the knowledge representation for the lexical knowledge base which we are attempting to create, especially with respect to inheritance."
            },
            "slug": "An-Approach-to-Building-the-Hierarchical-Element-of-Copestake",
            "title": {
                "fragments": [],
                "text": "An Approach to Building the Hierarchical Element of a Lexical Knowledge Base From a Machine Readable"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This abstract describes an approach to extracting taxonomies from machine readable dictionaries and using them to structure a lexical knowledge base which incorporates default inheritance, especially with respect to inheritance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3261291"
                        ],
                        "name": "J. Kipps",
                        "slug": "J.-Kipps",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kipps",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kipps"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56824959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb81f333c9193be61a26299c94d522a573127561",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A variation on Tomita\u2019s algorithm is analyzed in regards to its time and space complexity. It is shown to have a general time bound of 0(n^{\\tilde{\\rho}+1}), where n is the length of the input string and \\rho is the length of the longest production. A modified algorithm is presented in which the time bound is reduced to 0(n^3). The space complexity of Tomita\u2019s algorithm is shown to be proportional to n^2 in the worst case and is changed by at most a constant factor with the modification. Empirical results are used to illustrate the trade off between time and space on a simple example. A discussion of two subclasses of context-free grammars that can be recognized in 0(n^2) and O(n) is also included."
            },
            "slug": "Analysis-of-Tomita\u2019s-Algorithm-for-General-Parsing-Kipps",
            "title": {
                "fragments": [],
                "text": "Analysis of Tomita\u2019s Algorithm for General Context-Free Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A variation on Tomita\u2019s algorithm is analyzed in regards to its time and space complexity and two subclasses of context-free grammars that can be recognized in 0(n^2) and O(n) are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144741427"
                        ],
                        "name": "C. Pollard",
                        "slug": "C.-Pollard",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pollard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393013"
                        ],
                        "name": "I. Sag",
                        "slug": "I.-Sag",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Sag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g. Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 188
                            }
                        ],
                        "text": "To date, we have not attempted to compute CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out of the categories concerned would be necessary for an LR parser to work effectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17735681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "706943e308cca585d991af5bc340063b8e9be97a",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "ity over the technical or implementational point of view. In this regard, it cannot be criticized as designed in a too rigid way from the implementational viewpoint and not adaptable to new situations and unforeseen phenomena. The separation of data structures from the procedures and the modularity of the system are features that are essential to the extendability of the system to other domains. In general, the work is a good example of: 1. the necessity of creating extensive lexicons, where \"extensive\" must be intended both in breadth (i.e., in quantitative terms) and in depth (i.e., from aqualitative viewpoint, as to the types of information associated with the entries); 2. the necessity of working with large textual corpora, both for obtaining linguistic data and for testing systems. This is encouraging for a trend that is in recent years showing up, and having, for example, in Europe, great success also in projects sponsored by national and international organizations."
            },
            "slug": "Information-Based-Syntax-and-Semantics:-Volume-1,-Pollard-Sag",
            "title": {
                "fragments": [],
                "text": "Information-Based Syntax and Semantics: Volume 1, Fundamentals"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "In general, the work is a good example of: the necessity of creating extensive lexicons, where \"extensive\" must be intended both in breadth and in depth, and the separation of data structures from the procedures and the modularity of the system are features that are essential to the extendability of theSystem to other domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692491"
                        ],
                        "name": "S. Shieber",
                        "slug": "S.-Shieber",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Shieber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shieber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 91
                            }
                        ],
                        "text": "Grammars written in other, more low-level unification grammar formalisms, such as PATR-I1 (Shieber 1984), commonly employ treatments of the type just described to deal with phenomena such as gapping, coordination, and compounding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215768278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "decd25cc661adb7c0769588a2c0bf243caacb49b",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate."
            },
            "slug": "The-Design-of-a-Computer-Language-for-Linguistic-Shieber",
            "title": {
                "fragments": [],
                "text": "The Design of a Computer Language for Linguistic Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper attempts to explicate the design decisions of the PATR group in terms of a set of properties that effective computer languages should incorporate."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177145"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 352,
                                "start": 110
                            }
                        ],
                        "text": "However , for large real-world NL grammars such as the ANLT, the table size is still quite manageable despite Johnson's (1989) worst-case complexity result of the number of LR(0) states being exponential on g rammar size (leading to a parser with exponential ly bad time performance). We have, therefore, not found it necessary to use Schabes' (1991a) LR-like tables (with number of states guaranteed to be polynomial even in the worst case)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 110
                            }
                        ],
                        "text": "However , for large real-world NL grammars such as the ANLT, the table size is still quite manageable despite Johnson's (1989) worst-case complexity result of the number of LR(0) states being exponential on g rammar size (leading to a parser with exponential ly bad time performance)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 141
                            }
                        ],
                        "text": "Our use of local ambiguity packing does not in practice seem to result in exponentially bad performance with respect to sentence length (cf. Johnson 1989) since we have been able to generate packed parse forests for sentences of over 30 words having many thousands of parses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59638266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7833de8289b2fa55fafbf52b53877dabf5a8eed6",
            "isKey": true,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Tomita parsing algorithm adapts Knuth\u2019s (1967) well-known parsing algo\u00ad rithm for LR()t) grammars to non-LR grammars, including ambiguous gram\u00ad mars. Knuth\u2019s algorithm is provably efficient: it requires at most 0 (n |G |) units of time, where |G| is the size of (i.e. the number of symbols in) G and n is the length of the string to be parsed. This is often significantly better than the 0 (n 3|G |2) worst case time required by standard parsing algorithms such as the Earley algorithm. Since the Tomita algorithm is closely related to K nuth\u2019s algorithm, one might expect that it too is provably more efficient than the Ear\u00ad ley algorithm, especially as actual computational implementations of Tom ita\u2019s algorithm outperform implementations of the Earley algorithm (Tomita 1986, 1987). This paper shows that this is not the case. Two main results are presented in this paper. First, for any m there is a grammar Lm such that Tomita\u2019s algorithm performs Q(nm) operations to parse a string of length n. Second, there is a sequence of grammars G m such that Tomita\u2019s algorithm performs f2(nc1Gm') operations to parse a string of length n. Thus it is not the case that the Tomita algorithm is always more efficient than Earley\u2019s algorithm; rather there are grammars for which it is exponentially slower. This result is forshadowed in Tomita (1986, p. 72), where the author remarks that Tomita\u2019s algorithm can require time proportional to more than the cube of the input length. The result showing that the Tomita parser can require time proportional to an exponential function of the grammar size is new, as fair as I can tell."
            },
            "slug": "The-Computational-Complexity-of-Tomita\u2019s-Algorithm-Johnson",
            "title": {
                "fragments": [],
                "text": "The Computational Complexity of Tomita\u2019s Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is not the case that the Tomita algorithm is always more efficient than Earley\u2019s algorithm; rather there are grammars for which it is exponentially slower, and two main results are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416786"
                        ],
                        "name": "Mats Rooth",
                        "slug": "Mats-Rooth",
                        "structuredName": {
                            "firstName": "Mats",
                            "lastName": "Rooth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mats Rooth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Hindle and Rooth (1991) report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightfor- wardly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5410054,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "bdaf232c561f1f50e88b1d24097e214890b37e8b",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning."
            },
            "slug": "Structural-Ambiguity-and-Lexical-Relations-Hindle-Rooth",
            "title": {
                "fragments": [],
                "text": "Structural Ambiguity and Lexical Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is proposed that many ambiguous prepositional phrase attachments can be resolved on the based of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731375"
                        ],
                        "name": "M. Brent",
                        "slug": "M.-Brent",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brent",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3086368"
                        ],
                        "name": "R. Berwick",
                        "slug": "R.-Berwick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Berwick",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Berwick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6782079,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "b18ac643377521bbd7e56521ef169ccafcd1b774",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an implemented program that takes a tagged text corpus and generates a partial list of the subcategorization frames in which each verb occurs. The completeness of the output list increases monotonically with the total occurrences of each verb in the training corpus. False positive rates are one to three percent. Five subcategorization frames are currently detected and we foresee no impediment to detecting many more. Ultimately, we expect to provide a large subcategorization dictionary to the NLP community and to train dictionaries for specific corpora."
            },
            "slug": "Automatic-Acquisition-of-Subcategorization-Frames-Brent-Berwick",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of Subcategorization Frames from Tagged Text"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An implemented program that takes a tagged text corpus and generates a partial list of the subcategorization frames in which each verb occurs and expects to provide a large subc categorization dictionary to the NLP community and to train dictionaries for specific corpora."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 129
                            }
                        ],
                        "text": "In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified (e.g. Church and Hanks 1989), because these apply to adjacent words in unanalyzed text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": false,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 99
                            }
                        ],
                        "text": "This would make the statistical data concerning trigrams of head-preposition-head less sparse (cf. Gale and Church 1990) and easier to gather from a corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10164826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d30aa623fd96da99a16c0c3bde73f50c92a5c42",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "It is difficult to estimate the probability of a word's context because of sparse data problems. If appropriate care is taken, we find that it is possible to make useful estimates of contextual probabilities that improve performance in a spelling correction application. In contrast, less careful estimates are found to be useless. Specifically, we will show that the Good-Turing method makes the use of contextual information practical for a spelling corrector, while attempts to use the maximum likelihood estimator (MLE) or expected likelihood estimator (ELE) fail. Spelling correction was selected as an application domain because it is analogous to many important recognition applications based on a noisy channel model (such as speech recognition), though somewhat simpler and therefore possibly more amenable to detailed statistical analysis."
            },
            "slug": "Poor-Estimates-of-Context-are-Worse-than-None-Gale-Church",
            "title": {
                "fragments": [],
                "text": "Poor Estimates of Context are Worse than None"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It is found that it is possible to make useful estimates of contextual probabilities that improve performance in a spelling correction application, and it is shown that the Good-Turing method makes the use of contextual information practical for a spelling corrector."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 190
                            }
                        ],
                        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14999150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "493b63168e2778d64934f9027a3dc74f8f9f746f",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We report here on our experiments with POST (Part of Speech Tagger) to address problems of ambiguity and of understanding unknown words. Part of speech tagging, perse, is a well understood problem. Our paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag. We describe the algorithms that we used and the specific results of our experiments on Wall Street Journal articles and on MUC terrorist messages."
            },
            "slug": "POST:-Using-Probabilities-in-Language-Processing-Meteer-Schwartz",
            "title": {
                "fragments": [],
                "text": "POST: Using Probabilities in Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10213745"
                        ],
                        "name": "M. Pollack",
                        "slug": "M.-Pollack",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Pollack",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pollack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 177
                            }
                        ],
                        "text": "In addition, it involves annotating each grammar rule about what should be relaxed and requires that semantic interpretation can be extended to 'fitted' or partial parses (e.g. Pollack and Pereira 1988)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7531745,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1ef8981c314ce27d05ec4f01c887ceba3a5e8ecd",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on a mechanism for semantic and pragmatic interpretation that has been designed to take advantage of the generally compositional nature of semantic analysis, without unduly constraining the order in which pragmatic decisions are made. To achieve this goal, we introduce the idea of a conditional interpretation: one that depends upon a set of assumptions about subsequent pragmatic processing. Conditional interpretations are constructed compositionally according to a set of declaratively specified interpretation rules. The mechanism can handle a wide range of pragmatic phenomena and their interactions."
            },
            "slug": "An-Integrated-Framework-for-Semantic-and-Pragmatic-Pollack-Pereira",
            "title": {
                "fragments": [],
                "text": "An Integrated Framework for Semantic and Pragmatic Interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A mechanism for semantic and pragmatic interpretation is reported on that has been designed to take advantage of the generally compositional nature of semantic analysis, without unduly constraining the order in which pragmatic decisions are made."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705097"
                        ],
                        "name": "F. Pereira",
                        "slug": "F.-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "Carlos",
                                "Neves"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692491"
                        ],
                        "name": "S. Shieber",
                        "slug": "S.-Shieber",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Shieber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shieber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 81
                            }
                        ],
                        "text": "We have also experimented with a smaller grammar employing 'gap threading' (e.g. Pereira and Shieber 1987), an alternative treatment of UBCs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1343,
                                "start": 0
                            }
                        ],
                        "text": "Pereira and Shieber 1987), an alternative treatment of UBCs. We were able to use the same techniques for expanding out and inference on the values of the (in this case atomic) features used for threading the gaps to produce a backbone grammar (and parse table) that had the same constraining power with respect to gaps as the original grammar. To date, we have not attempted to compute CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out of the categories concerned would be necessary for an LR parser to work effectively. This and other areas of complexity in unification-based formalisms need further investigation before we can claim to have developed a system capable of producing a useful LR parse table for any unificationbased grammar. In particular, declaring certain category-valued features so that they cannot take variable values may lead to nontermination in the backbone construction for some grammars. However, it should be possible to restrict the set of features that are considered in category-valued features in an analogous way to Shieber's (1985) restrictors for Earley's (1970) algorithm, so that a parse table can still be constructed."
                    },
                    "intents": []
                }
            ],
            "corpusId": 19956410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "547d483ed1e80066693af561f63daa30ffa8e9fa",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nA concise and practical introduction to logic programming and the logic-programming language Prolog, both as vehicles for understanding elementary computational linguistics and as tools for implementing the basic components of natural-language-processing systems."
            },
            "slug": "Prolog-and-Natural-Language-Analysis-Pereira-Shieber",
            "title": {
                "fragments": [],
                "text": "Prolog and Natural-Language Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A concise and practical introduction to logic programming and the logic-programming language Prolog, both as vehicles for understanding elementary computational linguistics and as tools for implementing the basic components of natural-language-processing systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766182"
                        ],
                        "name": "B. B. Kristensen",
                        "slug": "B.-B.-Kristensen",
                        "structuredName": {
                            "firstName": "Bent",
                            "lastName": "Kristensen",
                            "middleNames": [
                                "Bruun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. B. Kristensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35115420"
                        ],
                        "name": "O. Madsen",
                        "slug": "O.-Madsen",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Madsen",
                            "middleNames": [
                                "Lehrmann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Madsen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 50
                            }
                        ],
                        "text": "Instead, we have moved to an algorithm devised by Kristensen and Madsen (1981), which avoids performing the LR(1) closure operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16785177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ecbd443095b2b6edc695b76aa13fbfdd74bd4eb",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods for constructing LALR(k) parsers are discussed. Algorithms for computing LALR(k)-lookahead are presented together with the necessary theory to prove their correctness. Firstly a special algorithm for the LALR(1) case is presented. Secondly a general LALR(k)-algorithm with k >= 1 is presented. Given an item and a state the algorithms compute their corresponding LALR-lookahead during a recursive traversal of the LR(0)-machine. Finally the LALR(k)-algorithm is generalised to compute LALR(k)-lookahead for all items and states visited during the recursive traversal performed by the former algorithms."
            },
            "slug": "Methods-for-Computing-LALR(k)-Lookahead-Kristensen-Madsen",
            "title": {
                "fragments": [],
                "text": "Methods for Computing LALR(k) Lookahead"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The LALR(k)-algorithm is generalised to compute LALS(k)-lookahead for all items and states visited during the recursive traversal performed by the former algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "TOPL"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133664"
                        ],
                        "name": "D. Cutting",
                        "slug": "D.-Cutting",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Cutting",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cutting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314752"
                        ],
                        "name": "Penelope Sibun",
                        "slug": "Penelope-Sibun",
                        "structuredName": {
                            "firstName": "Penelope",
                            "lastName": "Sibun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Penelope Sibun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 229
                            }
                        ],
                        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7617879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c2263ceef50530996f4807da8d9a0e835905e8",
            "isKey": false,
            "numCitedBy": 785,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment."
            },
            "slug": "A-Practical-Part-of-Speech-Tagger-Cutting-Kupiec",
            "title": {
                "fragments": [],
                "text": "A Practical Part-of-Speech Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An implementation of a part-of-speech tagger based on a hidden Markov model that enables robust and accurate tagging with few resource requirements and accuracy exceeds 96%."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715952"
                        ],
                        "name": "A. Aho",
                        "slug": "A.-Aho",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Aho",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144281449"
                        ],
                        "name": "R. Sethi",
                        "slug": "R.-Sethi",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Sethi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 56
                            }
                        ],
                        "text": "We therefore broadly follow the alternative approach of Aho, Sethi, and Ullman (1986), but with a number of optimizations: . . . Constructing the LR(0) sets of items: we compute LR(0) states containing only kernel items (the item IS' --> S], where S' is the start symbol, and all items that have a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 367,
                                "start": 96
                            }
                        ],
                        "text": "However, in an initial implementation we found that the LR(1) closure operation as described by Aho et al. was too expensive to be practicable for the number and size of LR(0) states we deal with, even with schemes for caching the closures of nonkernel items once they had been computed. Instead, we have moved to an algorithm devised by Kristensen and Madsen (1981), which avoids performing the LR(1) closure operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42981739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7f33d55d94e75a554251fe7dc07f1d7b4db8e1a",
            "isKey": false,
            "numCitedBy": 9130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction 1.1 Language Processors 1.2 The Structure of a Compiler 1.3 The Evolution of Programming Languages 1.4 The Science of Building a Compiler 1.5 Applications of Compiler Technology 1.6 Programming Language Basics 1.7 Summary of Chapter 1 1.8 References for Chapter 1 2 A Simple Syntax-Directed Translator 2.1 Introduction 2.2 Syntax Definition 2.3 Syntax-Directed Translation 2.4 Parsing 2.5 A Translator for Simple Expressions 2.6 Lexical Analysis 2.7 Symbol Tables 2.8 Intermediate Code Generation 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer 3.2 Input Buffering 3.3 Specification of Tokens 3.4 Recognition of Tokens 3.5 The Lexical-Analyzer Generator Lex 3.6 Finite Automata 3.7 From Regular Expressions to Automata 3.8 Design of a Lexical-Analyzer Generator 3.9 Optimization of DFA-Based Pattern Matchers 3.10 Summary of Chapter 3 3.11 References for Chapter 3 4 Syntax Analysis 4.1 Introduction 4.2 Context-Free Grammars 4.3 Writing a Grammar 4.4 Top-Down Parsing 4.5 Bottom-Up Parsing 4.6 Introduction to LR Parsing: Simple LR 4.7 More Powerful LR Parsers 4.8 Using Ambiguous Grammars 4.9 Parser Generators 4.10 Summary of Chapter 4 4.11 References for Chapter 4 5 Syntax-Directed Translation 5.1 Syntax-Directed Definitions 5.2 Evaluation Orders for SDD's 5.3 Applications of Syntax-Directed Translation 5.4 Syntax-Directed Translation Schemes 5.5 Implementing L-Attributed SDD's 5.6 Summary of Chapter 5 5.7 References for Chapter 5 6 Intermediate-Code Generation 6.1 Variants of Syntax Trees 6.2 Three-Address Code 6.3 Types and Declarations 6.4 Translation of Expressions 6.5 Type Checking 6.6 Control Flow 6.7 Backpatching 6.8 Switch-Statements 6.9 Intermediate Code for Procedures 6.10 Summary of Chapter 6 6.11 References for Chapter 6 7 Run-Time Environments 7.1 Storage Organization 7.2 Stack Allocation of Space 7.3 Access to Nonlocal Data on the Stack 7.4 Heap Management 7.5 Introduction to Garbage Collection 7.6 Introduction to Trace-Based Collection 7.7 Short-Pause Garbage Collection 7.8 Advanced Topics in Garbage Collection 7.9 Summary of Chapter 7 7.10 References for Chapter 7 8 Code Generation 8.1 Issues in the Design of a Code Generator 8.2 The Target Language 8.3 Addresses in the Target Code 8.4 Basic Blocks and Flow Graphs 8.5 Optimization of Basic Blocks 8.6 A Simple Code Generator 8.7 Peephole Optimization 8.8 Register Allocation and Assignment 8.9 Instruction Selection by Tree Rewriting 8.10 Optimal Code Generation for Expressions 8.11 Dynamic Programming Code-Generation 8.12 Summary of Chapter 8 8.13 References for Chapter 8 9 Machine-Independent Optimizations 9.1 The Principal Sources of Optimization 9.2 Introduction to Data-Flow Analysis 9.3 Foundations of Data-Flow Analysis 9.4 Constant Propagation 9.5 Partial-Redundancy Elimination 9.6 Loops in Flow Graphs 9.7 Region-Based Analysis 9.8 Symbolic Analysis 9.9 Summary of Chapter 9 9.10 References for Chapter 9 10 Instruction-Level Parallelism 10.1 Processor Architectures 10.2 Code-Scheduling Constraints 10.3 Basic-Block Scheduling 10.4 Global Code Scheduling 10.5 Software Pipelining 10.6 Summary of Chapter 10 10.7 References for Chapter 10 11 Optimizing for Parallelism and Locality 11.1 Basic Concepts 11.2 Matrix Multiply: An In-Depth Example 11.3 Iteration Spaces 11.4 Affine Array Indexes 11.5 Data Reuse 11.6 Array Data-Dependence Analysis 11.7 Finding Synchronization-Free Parallelism 11.8 Synchronization Between Parallel Loops 11.9 Pipelining 11.10 Locality Optimizations 11.11 Other Uses of Affine Transforms 11.12 Summary of Chapter 11 11.13 References for Chapter 11 12 Interprocedural Analysis 12.1 Basic Concepts 12.2 Why Interprocedural Analysis? 12.3 A Logical Representation of Data Flow 12.4 A Simple Pointer-Analysis Algorithm 12.5 Context-Insensitive Interprocedural Analysis 12.6 Context-Sensitive Pointer Analysis 12.7 Datalog Implementation by BDD's 12.8 Summary of Chapter 12 12.9 References for Chapter 12 A A Complete Front End A.1 The Source Language A.2 Main A.3 Lexical Analyzer A.4 Symbol Tables and Types A.5 Intermediate Code for Expressions A.6 Jumping Code for Boolean Expressions A.7 Intermediate Code for Statements A.8 Parser A.9 Creating the Front End B Finding Linearly Independent Solutions Index"
            },
            "slug": "Compilers:-Principles,-Techniques,-and-Tools-Aho-Sethi",
            "title": {
                "fragments": [],
                "text": "Compilers: Principles, Techniques, and Tools"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This book discusses the design of a Code Generator, the role of the Lexical Analyzer, and other topics related to code generation and optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley series in computer science / World student series edition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46191146"
                        ],
                        "name": "C. Chapelle",
                        "slug": "C.-Chapelle",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Chapelle",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chapelle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 142
                            }
                        ],
                        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 207
                            }
                        ],
                        "text": "\" Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 144116247,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "91b52959c7731830f1518ed35e5ab1dabadf79ec",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "specifically on the difficulty of recognizing learning versus language difficulties, that is, how to identify a nonnative-speaking child's need for special education services. They propose a model that administrators can employ that minimizes bias. In a similar vein, the next chapter, by Jeffery Braden and Sandra Fradd, suggests ways that administrators can anticipate difficulties and intervene before such referrals are necessary. William Tikunoff, in the eighth chapter, focuses on instructional leadership. He discusses the characteristics of an effective principal and targets specific areas, such as effective time management. The final chapter, by Beatrice Ward, addresses the greatest resource of any educational institution: the teachers. She describes the clinical approach to teacher development and how it can be implemented. Overall, this book fills a need for basic, factual information about legal requirements, program types, and effective instructional and leadership strategies with respect to the LEP population. Furthermore, it provides guidance on the complex issue of special education for LEP students, particularly the referral process. An additional chapter exploring different models for assessment and program design for these students would have provided depth and balance. Although there is necessarily some overlap between the chapters, it is reinforcing, not repetitive. In addition to being extremely useful to administrators, this book would be of value to school personnel such as psychologists, special education consultants, LEP consultants, instructors-in short, for anyone committed to the design and delivery of effective instructional programs for LEP students."
            },
            "slug": "The-Computational-Analysis-of-English\u2014A-Approach-Chapelle",
            "title": {
                "fragments": [],
                "text": "The Computational Analysis of English\u2014A Corpus\u2010Based Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "The two main algorithms utilized are the Viterbi (1967) algorithm and the Baum-Welch algorithm (Baum 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145140888"
                        ],
                        "name": "J. Holmes",
                        "slug": "J.-Holmes",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holmes",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47531272"
                        ],
                        "name": "W. Holmes",
                        "slug": "W.-Holmes",
                        "structuredName": {
                            "firstName": "Wendy",
                            "lastName": "Holmes",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Holmes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "In the field of speech recognition, statistical techniques based on hidden Markov modeling are well established (see e.g. Holmes 1988:129f for an introduction)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195610924,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "c646ccfeaee60487ba1a75f195ce17b8a4ff29bd",
            "isKey": false,
            "numCitedBy": 375,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nSpeech Synthesis and Recognition is an easy to read introduction to the subjects of generating and interpreting speech for those who have no experience and wish to specialise in the area, and also for professionals in related fields."
            },
            "slug": "Speech-Synthesis-and-Recognition-Holmes-Holmes",
            "title": {
                "fragments": [],
                "text": "Speech Synthesis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32771628"
                        ],
                        "name": "Paul Procter",
                        "slug": "Paul-Procter",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Procter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Procter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 104
                            }
                        ],
                        "text": "Copestake (1990, 1992) describes a program capable of extracting the genus term of a definition from an LDOCE definition, resolving the sense of such terms, and constructing hierarchical taxonomies of the resulting word senses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 33
                            }
                        ],
                        "text": "Chart and LR parse times for the LDOCE definition the state of being away or of not being present with the ANLT grammar (in CPU seconds on a DEC 3100)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 130
                            }
                        ],
                        "text": "In order to test the techniques and ideas described in previous sections, we undertook a preliminary experiment using a subset of LDOCE noun definitions as our test corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 272
                            }
                        ],
                        "text": "The feasibility and usefulness of our approach has been investigated in a preliminary way by analyzing a ~small corpus of Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing noun definitions drawn from the Longman Dictionary of Contemporary English (LDOCE) (Procter 1978)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 105
                            }
                        ],
                        "text": "To date, the largest number of interactions we have observed for a single phrase is 55 for the (30-word) LDOCE definition for youth hostel: a hostel for usu young people walking around country areas on holiday for which they pay small amounts of money to the youth hostels association or the international yha."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "We also parsed a further 55 LDOCE noun definitions not drawn from the training corpus, each containing up to 10 words (mean length 5.7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "Section 8 describes and presents the results of our first experiment parsing LDOCE noun definitions, and Section 9 draws some preliminary conclusions and outlines ways in which the work described should be modified and extended."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 67
                            }
                        ],
                        "text": "A corpus of approximately 32,000 noun definitions was created from LDOCE by extracting the definition fields and normalizing the definitions to remove punctuation, font control information, and so forth, s A lexicon was created for this corpus by extracting the appropriate lemmas and matching these against entries in the ANLT lexicon."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 113
                            }
                        ],
                        "text": "Figure 8 summarizes the amount of interaction required in the experiment reported below for parsing a set of 150 LDOCE noun definitions with the ANLT grammar."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62730943,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "32364b07ffde874288e4d24f46b67f330b4ebf59",
            "isKey": true,
            "numCitedBy": 1410,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Longman dictionary of contemporary English , Longman dictionary of contemporary English , \u0645\u0631\u06a9\u0632 \u0641\u0646\u0627\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u0648 \u0627\u0637\u0644\u0627\u0639 \u0631\u0633\u0627\u0646\u06cc \u06a9\u0634\u0627\u0648\u0631\u0632\u06cc"
            },
            "slug": "Longman-Dictionary-of-Contemporary-English-Procter",
            "title": {
                "fragments": [],
                "text": "Longman Dictionary of Contemporary English"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Longman dictionary of contemporary English is a collection ofverbs, idioms andverbs used in English since the mid-19th century that reflect the changing nature of the language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66434770"
                        ],
                        "name": "E. Briscoe",
                        "slug": "E.-Briscoe",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Briscoe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Briscoe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 0
                            }
                        ],
                        "text": "Briscoe and Carroll (1991) report an earlier version of this experiment using different versions of the grammar and parser in which results differed in minor ways. Carroll and Briscoe (1992) report a third version of the experiment in which results were improved slightly through the use of a better normalization and parse forest unpacking technique."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 15
                            }
                        ],
                        "text": "In Carroll and Briscoe (1992) we present a more motivated technique for normalizing the probability of competing sub-analyses in the parse forest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 280
                            }
                        ],
                        "text": "\u2026the parser is as predictive as the backbone grammar and LR technique allow, and the LALR(1) parse table allows one word lookahead to resolve some ambiguities (although, of course, the resolution of a local ambiguity may potentially involve an unlimited amount of lookahead; e.g. Briscoe 1987:125ff)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1005,
                                "start": 20
                            }
                        ],
                        "text": "Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96.8% of a corpus of 10,000 noun phrases extracted (without regard for their internal form) from a variety of corpora. However, although Taylor, Grover, and Briscoe show that the ANLT grammar has very wide coverage, they abstract away from issues of lexical idiosyncrasy by formimg equivalence classes of noun phrases and parsing a single token of each class, and they do not address the issues of 1) tuning a grammar to a particular corpus or sublanguage 2) selecting the correct analysis from the set licensed by the grammar and 3) providing reliable analyses of input outside the coverage of the grammar. Firstly, it is clear that vocabulary, idiom, and conventionalized constructions used in, say, legal language and dictionary definitions, will differ both in terms of the range and frequency of words and constructions deployed. Secondly, Church and Patil (1982) demonstrate that for a realistic grammar parsing realistic input, the set of possible analyses licensed by the grammar can be in the thousands."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 20
                            }
                        ],
                        "text": "Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Briscoe and Carroll (1991) report an earlier version of this experiment using different versions of the grammar and parser in which results differed in minor ways."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Briscoe (1987) demonstrates that the structure of the search space in parse derivations makes a left-to-right, incremental mode of parse selection most efficient."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61112516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56312ee3851282fcf16ffbdef0570efa507deb54",
            "isKey": true,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modelling-Human-Speech-Comprehension:-A-approach-Briscoe",
            "title": {
                "fragments": [],
                "text": "Modelling Human Speech Comprehension: A computational approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114595835"
                        ],
                        "name": "H. Vaughan",
                        "slug": "H.-Vaughan",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Vaughan",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Vaughan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 93
                            }
                        ],
                        "text": "Nevertheless, if there are systematic structural tendencies evident in corpora (for example, Frazier's [1988] parsing strategies predict a preference for left-branching analyses of such compounds), then the probabilistic model is sensitive enough to discriminate them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2207,
                                "start": 93
                            }
                        ],
                        "text": "Nevertheless, if there are systematic structural tendencies evident in corpora (for example, Frazier's [1988] parsing strategies predict a preference for left-branching analyses of such compounds), then the probabilistic model is sensitive enough to discriminate them. 6 In practice, we take the geometric mean of the probabilities rather than their product to rank parse derivations. Otherwise, it would be difficult to prevent the system from always developing a bias in favor of analyses involving fewer rules or equivalently 'smaller' trees, almost regardless of the training material. Of course, the need for this step reflects the fact that, although the model is more context-dependent than probabilistic CFG, it is by no means a perfect probabilistic model of NL. 7 For example, the stochastic nature of the model and the fact that the entire left context of a parse derivation is not encoded in LR state information means that the probabilistic model cannot take account of, say, the pattern of resolution of earlier conflicts in the current derivation. Another respect in which the model is approximate is that we are associating probabilities with the context-free backbone of the unification grammar. Successful unification of features at parse time does not affect the probability of a (partial) analysis, while unification failure, in effect, sets the probability of any such analysis to zero. As long as we only use the probabilistic model to rank successful analyses, this is not particularly problematic. However, parser control regimes that attempt some form of best-first search using probabilistic information associated with transitions might not yield the desired result given this property. For example, it is not possible to use Viterbi-style optimization of search for the maximally probable parse because this derivation may contain a sub-analysis that will be pruned locally before a subsequent unification failure renders the current most probable analysis impossible. In general, the current breadth-first probabilistic parser is more efficient than its nonprobabilistic counterpart described in the previous section. In contrast to the parser described by Ng and Tomita (1991), our probabilistic parser is able to merge (state and stack) configurations and in all cases still maintain a full record of all the probabilities computed up to that point, since it associates probabilities with partial analyses of the input so far rather than with nodes in the graph-structured stack."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 0
                            }
                        ],
                        "text": "Frazier 1988). Fujisaki et al. (1989) propose a rather inelegant solution for the noun compound case, which involves creating 5582 instances of 4 morphosyntactically identical rules for classes of word forms with distinct bracketing behavior in noun-noun compounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 243
                            }
                        ],
                        "text": "We want to keep these structural configurations probabilistically distinct in case there are structurally conditioned differences in their frequency of occurrence ; as would be predicted, for example, by the theory of parsing strategies (e.g. Frazier 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 5
                            }
                        ],
                        "text": "than Fujisaki et al.'s unsupervised training experiment discussed above, despite the use of supervised training and a more sophisticated grammatical model. It is likely that these differences derive from the corpus material used for training and testing, and that the results reported by Fujisaki et al. will not be achieved with all corpora. Pereira and Schabes (1992) report an experiment using Baum-Welch re-estimation to infer a grammar and associated rule probabilities from a category set containing 15 nonterminals and 48 terminals, corresponding to the Penn Treebank lexical tagset (Santorini 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 65088917,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "65803e34e33f15819b048c247cfa8ea90b5a7977",
            "isKey": true,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\"Grammar\"-and-the-Language-Vaughan",
            "title": {
                "fragments": [],
                "text": "\"Grammar\" and the Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1941
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2553,
                                "start": 108
                            }
                        ],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992). This approach has the advantage that the resulting grammar defines a well-defined set of analyses for which rules of compositional interpretation might be developed. However, the technique is limited in several ways; firstly, such grammars are restricted to small (maximum about 15 nonterminal) CNF CFGs because of the computational cost of iterative re-estimation with an algorithm polynomial in sentence length and nonterminal category size; and secondly, because some form of supervised training will be essential if the analyses assigned by the grammar are to be linguistically motivated. Immediate prospects for applying such techniques to realistic NL grammars do not seem promising--the ANLT backbone grammar discussed in Section 4 contains almost 500 categories. However, Briscoe and Waegner (1992) describe an experiment in which, firstly, Baum-Welch re-estimation was used in conjunction with other more linguistically motivated constraints on the class of grammars that could be inferred, such as 'headedness'; and secondly, initial probabilities were heavily biased in favor of manually coded, linguistically highly plausible rules. This approach resulted in a simple tag sequence grammar often able to assign coherent and semantically/pragmatically plausible analyses to tag sequences drawn from the Spoken English Corpus. By combining such techniques and relaxing the CNF constraint, for example, by adopting the trellis algorithm version of Baum-Welch re-estimation (Kupiec 1991), it might be possible to create a computationally tractable system operating with a realistic NL grammar that would only infer a new rule from a finite space of linguistically motivated possibilities in the face of parse failure or improbability. In the shorter term, such techniques combined with simple tag sequence grammars might yield robust phrase-level 'skeleton' parsers that could be used as corpus analysis tools. The utility of the system reported here would be considerably improved by a more tractable approach to probabilistically unpacking the packed parse forest than exhaustive search. Finding the n-best analyses would allow us to recover analyses for longer sentences where a parse forest is constructed and would make the approach generally more efficient. Carroll and Briscoe (1992) present a heuristic algorithm for parse forest unpacking that interleaves normalization of competing sub-analyses with best-first extraction of the n most probable analyses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Baum (1972) proves that Baum-Welch re-estimation will converge to a local optimum in the sense that the initial probabilities will be modified to increase the likelihood of the corpus given the grammar and 'stabilize' within some threshold after a number of iterations over the training corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 108
                            }
                        ],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992). This approach has the advantage that the resulting grammar defines a well-defined set of analyses for which rules of compositional interpretation might be developed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2809,
                                "start": 0
                            }
                        ],
                        "text": "Baum (1972) proves that Baum-Welch re-estimation will converge to a local optimum in the sense that the initial probabilities will be modified to increase the likelihood of the corpus given the grammar and 'stabilize' within some threshold after a number of iterations over the training corpus. However, there is no guarantee that the global optimum will be found, and the a priori initial probabilities chosen are critical for convergence on useful probabilities (e.g. Lari and Young 1990). The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992). Fujisaki et al. (1989) describe a corpus analysis experiment using a probabilistic CNF CFG containing 7550 rules on a corpus of 4206 sentences (with an average sentence length of approximately 11 words). The unsupervised training process involved automatically assigning probabilities to each CF rule on the basis of their frequency of occurrence in all possible analyses of each sentence of the corpus. These probabilities were iteratively re-estimated using a variant of the Baum-Welch algorithm, and the Viterbi algorithm was used in conjunction with the CYK parsing algorithm to efficiently select the most probable analysis after training. Thus the model was restricted in that many of the possible parameters (rules) defined over the (non-)terminal category set were initially set to zero and training was used only to estimate new probabilities for a set of predefined rules. Fujisaki et al. suggest that the stable probabilities will model semantic and pragmatic constraints in the corpus, but this will only be so if these correlate with the frequency of rules in correct analyses, and also if the 'noise' in the training data created by the incorrect parses is effectively factored out. Whether this is so will depend on the number of 'false positive' examples with only incorrect analyses, the degree of heterogeneity in the training corpus, and so forth. Fujisaki et al. report some results based on testing the parser on the corpus used for training. In 72 out of 84 sentences examined, the most probable analysis was also the correct analysis. Of the remainder, 6 were false positives and did not receive a correct parse, while the other 6 did but it was not the most probable. A success rate (per sentence) of 85% is apparently impressive, but it is difficult to evaluate properly in the absence of full details concerning the nature of the corpus. For example, if the corpus contains many simple and similar constructions, unsupervised training is more likely to converge quickly on a useful set of probabilities. Sharman, Jelinek, and Mercer (1990) conducted a similar experiment with a grammar in ID/LP format (Gazdar et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 766,
                                "start": 0
                            }
                        ],
                        "text": "Baum (1972) proves that Baum-Welch re-estimation will converge to a local optimum in the sense that the initial probabilities will be modified to increase the likelihood of the corpus given the grammar and 'stabilize' within some threshold after a number of iterations over the training corpus. However, there is no guarantee that the global optimum will be found, and the a priori initial probabilities chosen are critical for convergence on useful probabilities (e.g. Lari and Young 1990). The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992). Fujisaki et al. (1989) describe a corpus analysis experiment using a probabilistic CNF CFG containing 7550 rules on a corpus of 4206 sentences (with an average sentence length of approximately 11 words)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "The two main algorithms utilized are the Viterbi (1967) algorithm and the Baum-Welch algorithm (Baum 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1062,
                                "start": 108
                            }
                        ],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992). This approach has the advantage that the resulting grammar defines a well-defined set of analyses for which rules of compositional interpretation might be developed. However, the technique is limited in several ways; firstly, such grammars are restricted to small (maximum about 15 nonterminal) CNF CFGs because of the computational cost of iterative re-estimation with an algorithm polynomial in sentence length and nonterminal category size; and secondly, because some form of supervised training will be essential if the analyses assigned by the grammar are to be linguistically motivated. Immediate prospects for applying such techniques to realistic NL grammars do not seem promising--the ANLT backbone grammar discussed in Section 4 contains almost 500 categories. However, Briscoe and Waegner (1992) describe an experiment in which, firstly, Baum-Welch re-estimation was used in conjunction with other more linguistically motivated constraints on the class of grammars that could be inferred, such as 'headedness'; and secondly, initial probabilities were heavily biased in favor of manually coded, linguistically highly plausible rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 108
                            }
                        ],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": true,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Fujisaki et al. (1989) demonstrate that the Viterbi algorithm can be used in conjunction with the CYK parsing algorithm and a CFG in CNF to efficiently select the most probable derivation of a given input."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Fujisaki et al. (1989) describe a corpus analysis experiment using a probabilistic CNF CFG containing 7550 rules on a corpus of 4206 sentences (with an average sentence length of approximately 11 words)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Fujisaki et al. (1989) propose a rather inelegant solution for the noun compound case, which involves creating 5582 instances of 4 morphosyntactically identical rules for classes of word forms with distinct bracketing behavior in noun-noun compounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Results comparable to those obtained by Fujisaki et al. (1989) and Sharman, Jelinek, and Mercer (1990) are possible on the basis of a quite modest amount of manual effort and a very much smaller training corpus, because the parse histories contain little 'noise' and usefully reflect the semantically and pragmatically appropriate analysis in the training corpus, and because the number of failures of coverage were reduced to some extent by adding the rules specifically motivated by the training corpus."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 40
                            }
                        ],
                        "text": "Results comparable to those obtained by Fujisaki et al. (1989) and Sharman, Jelinek, and Mercer (1990) are possible on the basis of a quite modest amount of manual effort and a very much smaller training corpus, because the parse histories contain little 'noise' and usefully reflect the\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A probabilistic method for sentence disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144803172"
                        ],
                        "name": "L. Frazier",
                        "slug": "L.-Frazier",
                        "structuredName": {
                            "firstName": "Lyn",
                            "lastName": "Frazier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Frazier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 243
                            }
                        ],
                        "text": "We want to keep these structural configurations probabilistically distinct in case there are structurally conditioned differences in their frequency of occurrence ; as would be predicted, for example, by the theory of parsing strategies (e.g. Frazier 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 140826115,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "221f632ed0bd656854956190712ce2b1b441159b",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Linguistics:-The-Cambridge-Survey:-Grammar-and-Frazier",
            "title": {
                "fragments": [],
                "text": "Linguistics: The Cambridge Survey: Grammar and language processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2370699"
                        ],
                        "name": "M. Wood",
                        "slug": "M.-Wood",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Wood",
                            "middleNames": [
                                "McGee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 107
                            }
                        ],
                        "text": "Firstly, although CFG is an adequate model of the majority of constructions occurring in natural language (Gazdar and Mellish 1989), it is clear that wide-coverage CFGs will need to be very large indeed, and this will lead to difficulties of (manual) development of consistent grammars and,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 106
                            }
                        ],
                        "text": "Firstly, although CFG is an adequate model of the majority of constructions occurring in natural language (Gazdar and Mellish 1989), it is clear that wide-coverage CFGs will need to be very large indeed, and this will lead to difficulties of (manual) development of consistent grammars and, possibly, to computational intractability at parse time (particularly during the already computationally expensive training phase)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67342029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "638f0aea448afab6379e154580f90cede1eb428c",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Natural-language-processing-in-LISP-Wood",
            "title": {
                "fragments": [],
                "text": "Natural language processing in LISP"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391514108"
                        ],
                        "name": "G. Gazdar",
                        "slug": "G.-Gazdar",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Gazdar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gazdar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801917"
                        ],
                        "name": "C. Mellish",
                        "slug": "C.-Mellish",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Mellish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mellish"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 107
                            }
                        ],
                        "text": "Firstly, although CFG is an adequate model of the majority of constructions occurring in natural language (Gazdar and Mellish 1989), it is clear that wide-coverage CFGs will need to be very large indeed, and this will lead to difficulties of (manual) development of consistent grammars and,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202791076,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "e66cad9a00fe4a6c3f5c4137dc1a534d87132e67",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Natural-Language-Processing-in-Pop-11:-An-to-Gazdar-Mellish",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing in Pop-11: An Introduction to Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402050148"
                        ],
                        "name": "P. Saint-Dizier",
                        "slug": "P.-Saint-Dizier",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Saint-Dizier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Saint-Dizier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60541696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "777570c140f6f995385f4d5a3b6bd902dfab3f5d",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Review-of-Prolog-and-natural-language-analysis:-10-Saint-Dizier",
            "title": {
                "fragments": [],
                "text": "Review of Prolog and natural-language analysis: CSLI lecture notes 10 by Fernando C. N. Pereira and Stuart M. Shieber. Center for the Study of Language and Information 1987."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122761969"
                        ],
                        "name": "J. Carroll",
                        "slug": "J.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003169"
                        ],
                        "name": "Claire Grover",
                        "slug": "Claire-Grover",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Grover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Grover"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 69
                            }
                        ],
                        "text": "For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 124
                            }
                        ],
                        "text": "Following our experience of constructing a substantial lexicon for the ANLT grammar from unreliable and indeterminate data (Carroll and Grover 1989), we decided to construct the disambiguated training corpus semi-automatically, restricting manual interaction to selection between alternatives\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 123
                            }
                        ],
                        "text": "Following our experience of constructing a substantial lexicon for the ANLT grammar from unreliable and indeterminate data (Carroll and Grover 1989), we decided to construct the disambiguated training corpus semi-automatically, restricting manual interaction to selection between alternatives defined by the ANLT grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60512070,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4507ea38c873c8d269aa5de50aa02156c6ae9e20",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-derivation-of-a-large-computational-lexicon-for-Carroll-Grover",
            "title": {
                "fragments": [],
                "text": "The derivation of a large computational lexicon for English from LDOCE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142223"
                        ],
                        "name": "M. Lesk",
                        "slug": "M.-Lesk",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lesk",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lesk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60958348,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "5e63a4e7e572bf4d3231a5290d7d5c1e1beeddbf",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Review-of-The-computational-analysis-of-English:-a-Lesk",
            "title": {
                "fragments": [],
                "text": "Review of The computational analysis of English: a corpus-based approach by Roger Garside, Geoffrey Leech, and Geoffrey Sampson. Longman 1987."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998726"
                        ],
                        "name": "G. Leech",
                        "slug": "G.-Leech",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Leech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153733790"
                        ],
                        "name": "R. Garside",
                        "slug": "R.-Garside",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Garside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Garside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Leech and Garside (1991) discuss the problems that arise in manual parsing of corpora concerning accuracy and consistency of analyses across time and analyst, the labor-intensive nature of producing detailed analyses, and so forth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59784284,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "274e83db1aa17f64eb994256cfb4de3d078da493",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Running-a-grammar-factory:-The-production-of-or-Leech-Garside",
            "title": {
                "fragments": [],
                "text": "Running a grammar factory: The production of syntactically analysed corpora or \u201ctreebanks\u201d"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144003169"
                        ],
                        "name": "Claire Grover",
                        "slug": "Claire-Grover",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Grover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Grover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713574"
                        ],
                        "name": "B. Boguraev",
                        "slug": "B.-Boguraev",
                        "structuredName": {
                            "firstName": "Branimir",
                            "lastName": "Boguraev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boguraev"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 0
                            }
                        ],
                        "text": "Holmes 1988:129f for an introduction). The two main algorithms utilized are the Viterbi (1967) algorithm and the Baum-Welch algorithm (Baum 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 49
                            }
                        ],
                        "text": "For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56796940,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "290175a4a8e54e9256a97a4529e85ed5f1900837",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Alvey-natural-language-tools-grammar-(2nd-Grover-Briscoe",
            "title": {
                "fragments": [],
                "text": "The Alvey natural language tools grammar (2nd Release)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15379653"
                        ],
                        "name": "Ann A. Copestake",
                        "slug": "Ann-A.-Copestake",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Copestake",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ann A. Copestake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Copestake (1990, 1992) describes a program capable of extracting the genus term of a definition from an LDOCE definition, resolving the sense of such terms, and constructing hierarchical taxonomies of the resulting word senses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54097841,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "67621fc7a9f6a83262722e4f7f3603567ca99a68",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Defaults-in-lexical-representation-Copestake",
            "title": {
                "fragments": [],
                "text": "Defaults in lexical representation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144566672"
                        ],
                        "name": "N. Chapman",
                        "slug": "N.-Chapman",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Chapman",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chapman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6929909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b6e9627b570353b0b27d91869cd4b0ccce4653d",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LR-parsing-theory-and-practice-Chapman",
            "title": {
                "fragments": [],
                "text": "LR parsing - theory and practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344567"
                        ],
                        "name": "F. DeRemer",
                        "slug": "F.-DeRemer",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "DeRemer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. DeRemer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2366802"
                        ],
                        "name": "T. Pennello",
                        "slug": "T.-Pennello",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Pennello",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pennello"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "One approach is graph-based (DeRemer and Pennello 1982), transforming the parse table construction problem to a set of wellknown directed graph problems, which in turn are solvable by efficient algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "Unfortunately this approach does not work for grammars that are not LR(k) for any k (DeRemer and Pennello 1982:633), for example, ambiguous grammars."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52833742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4337e7504e0d43a0c21802d5301fdbb1c3950d0f",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Two relations that capture the essential structure of the problem of computing LALR(1) look-ahead sets are defined, and an efficient algorithm is presented to compute the sets in time linear in the size of the relations. In particular, for a PASCAL grammar, the algorithm performs fewer than 15 percent of the set unions performed by the popular compiler-compiler YACC. When a grammar is not LALR(1), the relations, represented explicitly, provide for printing useroriented error messages that specifically indicate how the look-ahead problem arose. In addition, certain loops in the digraphs induced by these relations indicate that the grammar is not LR(k) for any k. Finally, an oft-discovered and used but incorrect look-ahead set algorithm is similarly based on two other relations defined for the fwst time here. The formal presentation of this algorithm should help prevent its rediscovery."
            },
            "slug": "Efficient-Computation-of-LALR(1)-Look-Ahead-Sets-DeRemer-Pennello",
            "title": {
                "fragments": [],
                "text": "Efficient Computation of LALR(1) Look-Ahead Sets"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two relations that capture the essential structure of the problem of computing LALR(1) look-ahead sets are defined, and an efficient algorithm is presented to compute the sets in time linear in the size of the relations."
            },
            "venue": {
                "fragments": [],
                "text": "TOPL"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143793703"
                        ],
                        "name": "Z. Shang",
                        "slug": "Z.-Shang",
                        "structuredName": {
                            "firstName": "Zhigang",
                            "lastName": "Shang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Shang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49989446"
                        ],
                        "name": "V. E. Isaac",
                        "slug": "V.-E.-Isaac",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Isaac",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. E. Isaac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115355312"
                        ],
                        "name": "Haicheng Li",
                        "slug": "Haicheng-Li",
                        "structuredName": {
                            "firstName": "Haicheng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haicheng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50146077"
                        ],
                        "name": "L. Patel",
                        "slug": "L.-Patel",
                        "structuredName": {
                            "firstName": "Lekha",
                            "lastName": "Patel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Patel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5290747"
                        ],
                        "name": "K. Catron",
                        "slug": "K.-Catron",
                        "structuredName": {
                            "firstName": "Katrina",
                            "lastName": "Catron",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Catron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054753783"
                        ],
                        "name": "T. Curran",
                        "slug": "T.-Curran",
                        "structuredName": {
                            "firstName": "T",
                            "lastName": "Curran",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1896827"
                        ],
                        "name": "G. Montelione",
                        "slug": "G.-Montelione",
                        "structuredName": {
                            "firstName": "Gaetano",
                            "lastName": "Montelione",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Montelione"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143883217"
                        ],
                        "name": "C. Abate",
                        "slug": "C.-Abate",
                        "structuredName": {
                            "firstName": "Cory",
                            "lastName": "Abate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Abate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 91
                            }
                        ],
                        "text": "Grammars written in other, more low-level unification grammar formalisms, such as PATR-I1 (Shieber 1984), commonly employ treatments of the type just described to deal with phenomena such as gapping, coordination, and compounding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 91406203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d8c2dece5fc48dceefba4b676b642f1cb286af8",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Design-of-a-Shang-Isaac",
            "title": {
                "fragments": [],
                "text": "Design of a"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731375"
                        ],
                        "name": "M. Brent",
                        "slug": "M.-Brent",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brent",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 156
                            }
                        ],
                        "text": "It seems likely that automatic acquisition of such information must await successful techniques for robust parsing of, at least, phrases in corpora (though Brent [1991] claims to be able to recognize some subcategorization patterns using large quantities of untagged text)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61284815,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "2ffde8e3e02edca7b3638297c6a04b0994569f45",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur. Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980). The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus. False positive rates are one to three percent of observations. Five SFs are currently detected and more are planned. Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora."
            },
            "slug": "Automatic-Acquisition-of-Subcategorization-Frames-Brent",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of Subcategorization Frames from Untagged Text"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "An implemented program that takes a raw, untagged text corpus as its only input and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705097"
                        ],
                        "name": "F. Pereira",
                        "slug": "F.-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "Carlos",
                                "Neves"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Pereira and Schabes (1992) report an experiment using Baum-Welch re-estimation to infer a grammar and associated rule probabilities from a category set containing 15 nonterminals and 48 terminals, corresponding to the Penn Treebank lexical tagset (Santorini 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 227
                            }
                        ],
                        "text": "A more promising approach with similar potential robustness would be to infer a probabilistic grammar using Baum-Welch re-estimation from a given training corpus and predefined category set, following Lari and Young (1990) and Pereira and Schabes (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63967455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15e4843e2c55843b5c5b429f89dad3d99e801f02",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of handparsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided."
            },
            "slug": "Inside-Outside-Reestimation-From-Partially-Corpora-Pereira-Schabes",
            "title": {
                "fragments": [],
                "text": "Inside-Outside Reestimation From Partially Bracketed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus to achieve faster convergence and better modeling of hierarchical structure than the original one."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 19
                            }
                        ],
                        "text": "2 Figures given by Klein and Martin (1989). 3 Grammar from Spector (1983) with optionality expanded out; statistics taken from a parse table constructed by the second author."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 215
                            }
                        ],
                        "text": "For the grammars we have investigated, this representat ion achieves a similar order of space saving to the comb vector representat ion suggested by Aho, Sethi, and Ullman (1986:244ff) for unambiguous grammars (see Klein and Martin [1989] for a survey of representat ion techniques)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The parser generating system PGSU.\" Software-Practice and Experience"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "The two main algorithms utilized are the Viterbi (1967) algorithm and the Baum-Welch algorithm (Baum 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Error bounds for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\" The grammar development environment : a user guide"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic parsing for general context-free grammars."
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings, 2nd International Workshop on Parsing Technologies, Cancun,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 162
                            }
                        ],
                        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 227
                            }
                        ],
                        "text": "\" Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\" The grammatical database and parsing scheme"
            },
            "venue": {
                "fragments": [],
                "text": "The Computational Analysis of English : A Corpus - Based Approach , edited by"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexing and parsing modula-2.\" SIGPLANNotices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\" The parser generating system PGSU"
            },
            "venue": {
                "fragments": [],
                "text": "Software - - Practice and Experience"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 243
                            }
                        ],
                        "text": "We want to keep these structural configurations probabilistically distinct in case there are structurally conditioned differences in their frequency of occurrence ; as would be predicted, for example, by the theory of parsing strategies (e.g. Frazier 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\" Grammar and language processing"
            },
            "venue": {
                "fragments": [],
                "text": "Linguistics : The Cambridge Survey , Volume 2 , edited by F . Newmeyer , 1445"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 0
                            }
                        ],
                        "text": "Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF). Fujisaki et al. (1989) demonstrate that the Viterbi algorithm can be used in conjunction with the CYK parsing algorithm and a CFG in CNF to efficiently select the most probable derivation of a given input. Kupiec (1991) extends Baum-Welch re-estimation to arbitrary (nonCNF) CFGs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 0
                            }
                        ],
                        "text": "Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF). Fujisaki et al. (1989) demonstrate that the Viterbi algorithm can be used in conjunction with the CYK parsing algorithm and a CFG in CNF to efficiently select the most probable derivation of a given input."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Trainable grammars for speech recognition.\" Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, 547-550"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech Communication Papers for the 97th Meeting of the"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 243
                            }
                        ],
                        "text": "We want to keep these structural configurations probabilistically distinct in case there are structurally conditioned differences in their frequency of occurrence ; as would be predicted, for example, by the theory of parsing strategies (e.g. Frazier 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grammar and language processing In Linguistics: The Cambridge Survey"
            },
            "venue": {
                "fragments": [],
                "text": "Grammar and language processing In Linguistics: The Cambridge Survey"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 30
                            }
                        ],
                        "text": "Our parser is thus similar to Tomita's (1987), except that it performs unifications rather than invoking CF rule augmentations; however, the main difference between our approach and Tomita's is the way in which the CF grammar that drives the parser comes into being."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parsers for natural languages."
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings, lOth International Conference on Computational Linguistics,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 251
                            }
                        ],
                        "text": "This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g. Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 145
                            }
                        ],
                        "text": "\u2026in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 215
                            }
                        ],
                        "text": "To date, we have not attempted to compute CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out of the categories concerned would be necessary for an LR parser to work effectively."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unification categorial grammar. In Edinburgh Working Papers in Cognitive Science, 1: Categorial Grammar, Unification Grammar, and Parsing"
            },
            "venue": {
                "fragments": [],
                "text": "Unification categorial grammar. In Edinburgh Working Papers in Cognitive Science, 1: Categorial Grammar, Unification Grammar, and Parsing"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 100
                            }
                        ],
                        "text": "Several researchers (Wright and Wrigley 1989; Wright 1990; Ng and Tomita 1991; Wright, Wrigley, and Sharman 1991) have proposed using LR parsers as a practical method of parsing with a probabilistic context-free grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 21
                            }
                        ],
                        "text": "Wright, Wrigley, and Sharman (1991) describe a Viterbi-like algorithm for unpacking parse forests containing probabilities of (sub-)analyses to find the n-best analyses, but this approach does not generalize (except in a heuristic way) to our approach in which unification failure on the different\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to language modelling Unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "An introduction to language modelling Unpublished manuscript"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "Sharman, Jelinek, and Mercer (1990) conducted a similar experiment with a grammar in ID/LP format (Gazdar et al. 1985; Sharman 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic ID/LP grammars for English"
            },
            "venue": {
                "fragments": [],
                "text": "IBM UK Scientific Centre"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 277
                            }
                        ],
                        "text": "\u2026to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions ; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Hindle and Rooth (1991) report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 285
                            }
                        ],
                        "text": "Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LAR processing for natural language grammars."
            },
            "venue": {
                "fragments": [],
                "text": "Masters dissertation,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 91
                            }
                        ],
                        "text": "Grammars written in other, more low-level unification grammar formalisms, such as PATR-I1 (Shieber 1984), commonly employ treatments of the type just described to deal with phenomena such as gapping, coordination, and compounding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The design of a 58"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 152
                            }
                        ],
                        "text": "Its perplexity (PP) measures based on bigram and trigram word models and an estimate of an infinite model were PP(2) = 104, PP(3) = 41, and PP(inf) = 8 (Sharman 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 100
                            }
                        ],
                        "text": "Several researchers (Wright and Wrigley 1989; Wright 1990; Ng and Tomita 1991; Wright, Wrigley, and Sharman 1991) have proposed using LR parsers as a practical method of parsing with a probabilistic context-free grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 21
                            }
                        ],
                        "text": "Wright, Wrigley, and Sharman (1991) describe a Viterbi-like algorithm for unpacking parse forests containing probabilities of (sub-)analyses to find the n-best analyses, but this approach does not generalize (except in a heuristic way) to our approach in which unification failure on the different\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to language modelling."
            },
            "venue": {
                "fragments": [],
                "text": "Unpublished manuscript, IBM UK Scientific Centre,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "This parser is integrated with the Grammar Development Environment (GDE; Carroll et al. 1988) in the ANLT system, and provided as an alternative parser for use with stable grammars for batch parsing of large bodies of text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The grammar development environment: a user guide"
            },
            "venue": {
                "fragments": [],
                "text": "The grammar development environment: a user guide"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 162
                            }
                        ],
                        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 227
                            }
                        ],
                        "text": "\" Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The grammatical database and parsing scheme.\" In The Computational Analysis of English: A Corpus-Based Approach, edited by R"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Baker (1982) demonstrates that Baum-Welch re-estimation can be extended to context-free grammars (CFGs) in Chomsky Normal Form (CNF)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Trainable grammars for speech recognition Speech Communication Papers for the 97th Meeting of the"
            },
            "venue": {
                "fragments": [],
                "text": "Trainable grammars for speech recognition Speech Communication Papers for the 97th Meeting of the"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Copestake (1990, 1992) describes a program capable of extracting the genus term of a definition from an LDOCE definition, resolving the sense of such terms, and constructing hierarchical taxonomies of the resulting word senses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Defaults in lexical representation In Default Inheritance in Unification-based Approaches to the Lexicon"
            },
            "venue": {
                "fragments": [],
                "text": "Defaults in lexical representation In Default Inheritance in Unification-based Approaches to the Lexicon"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 248
                            }
                        ],
                        "text": "Pereira and Schabes (1992) report an experiment using Baum-Welch re-estimation to infer a grammar and associated rule probabilities from a category set containing 15 nonterminals and 48 terminals, corresponding to the Penn Treebank lexical tagset (Santorini 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Penn treebank tagging and parsing manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 209
                            }
                        ],
                        "text": "In this case, the LR parse table would be based on complex categories, with unification of complex categories taking the place of equality of atomic ones in the standard LR parse table construction algorithm (Osborne 1990; Nakazawa 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Corpus parsing"
            },
            "venue": {
                "fragments": [],
                "text": "Corpus parsing"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 162
                            }
                        ],
                        "text": "The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g. Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The grammatical database and parsing scheme In The Computational Analysis of English: A Corpus-Based Approach"
            },
            "venue": {
                "fragments": [],
                "text": "The grammatical database and parsing scheme In The Computational Analysis of English: A Corpus-Based Approach"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 251
                            }
                        ],
                        "text": "This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles (e.g. Gazdar et al. 1985; Pollard and Sag 1987; Zeevat, Calder, and Klein 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 145
                            }
                        ],
                        "text": "\u2026in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 215
                            }
                        ],
                        "text": "To date, we have not attempted to compute CF backbones for grammars written in formalisms with minimal phrase structure components and (almost) completely general categories, such as HPSG (Pollard and Sag 1987) and UCG (Zeevat, Calder, and Klein 1987); more extensive inference on patterns of possible unification within nested categories and appropriate expanding-out of the categories concerned would be necessary for an LR parser to work effectively."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unification categorial grammar"
            },
            "venue": {
                "fragments": [],
                "text": "Edinburgh Working Papers in Cognitive Science, 1: Categorial Grammar, Unification Grammar, and Parsing, edited by"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Probabilistic LR Parsing computer language for linguistic information"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings, l Oth International Conference on Computational Linguistics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LR(k) grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "Sharman, Jelinek, and Mercer (1990) conducted a similar experiment with a grammar in ID/LP format (Gazdar et al. 1985; Sharman 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 53
                            }
                        ],
                        "text": "1 Creating LR Parse Tables from Unification Grammars Tomita (1987) describes a system for nondeterministic LR parsing of context-free grammars consisting of atomic categories, in which each CF production may be augmented with a set of tests (which perform similar types of operations to those available in a unification grammar)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic ID/LP grammars for English.\" Report 217"
            },
            "venue": {
                "fragments": [],
                "text": "IBM UK Scientific Centre,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 248
                            }
                        ],
                        "text": "Pereira and Schabes (1992) report an experiment using Baum-Welch re-estimation to infer a grammar and associated rule probabilities from a category set containing 15 nonterminals and 48 terminals, corresponding to the Penn Treebank lexical tagset (Santorini 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Penn treebank tagging and parsing manual.\" University of Pennsylvania, CIS Dept"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 44,
            "methodology": 60,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 106,
        "totalPages": 11
    },
    "page_url": "https://www.semanticscholar.org/paper/Generalized-Probabilistic-LR-Parsing-of-Natural-Briscoe-Carroll/8ad8e98574a275930bf04a477ce3532fd13c503c?sort=total-citations"
}