{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398746441"
                        ],
                        "name": "Eric Thibodeau-Laufer",
                        "slug": "Eric-Thibodeau-Laufer",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Thibodeau-Laufer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Thibodeau-Laufer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815021"
                        ],
                        "name": "Guillaume Alain",
                        "slug": "Guillaume-Alain",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Alain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Alain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965424"
                        ],
                        "name": "J. Yosinski",
                        "slug": "J.-Yosinski",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Yosinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yosinski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "Other related methods for training probabilistic models include generative stochastic networks [3, 45] which directly train a Markov kernel to match its equilibrium distribution to the data distribution, neural autoregressive distribution estimators [24] (and their recurrent [42] and deep [43]\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9494295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining."
            },
            "slug": "Deep-Generative-Stochastic-Networks-Trainable-by-Bengio-Thibodeau-Laufer",
            "title": {
                "fragments": [],
                "text": "Deep Generative Stochastic Networks Trainable by Backprop"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders are provided and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "t descent on K. The variance 1 of the \ufb01rst step is \ufb01xed to a small constant to prevent over\ufb01tting. The dependence of samples from q x(1 T)jx(0) on 1 (Tis made explicit by using \u2018frozen noise\u2019 \u2013 as in [22] the noise is treated as an additional auxiliary variable, and held constant while computing partial derivatives of Kwith respect to the parameters. For binomial diffusion, the discrete state space ma"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "abilistic models against each other. This approach remained largely unexplored for nearly two decades, with some exceptions [35,21]. There has been a recent explosion of work developing this idea. In [22,12,34,32] variational learning and inference algorithms were developed which allow a \ufb02exible generative model and posterior distribution over latent variables to be directly trained against each other. The var"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 216078090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "isKey": false,
            "numCitedBy": 16788,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
            },
            "slug": "Auto-Encoding-Variational-Bayes-Kingma-Welling",
            "title": {
                "fragments": [],
                "text": "Auto-Encoding Variational Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1407546424"
                        ],
                        "name": "J. Sohl-Dickstein",
                        "slug": "J.-Sohl-Dickstein",
                        "structuredName": {
                            "firstName": "Jascha",
                            "lastName": "Sohl-Dickstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sohl-Dickstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3341676"
                        ],
                        "name": "Peter Battaglino",
                        "slug": "Peter-Battaglino",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Battaglino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Battaglino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49488190"
                        ],
                        "name": "M. DeWeese",
                        "slug": "M.-DeWeese",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "DeWeese",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. DeWeese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 238
                            }
                        ],
                        "text": "A variety of analytic approximations exist which ameliorate, but do not remove, this tradeoff \u2013 for instance mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy belief propagation [29], and many, many more."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2595478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35e57c040ddf95eca2a9bd6e1c532a08147b1f29",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Fitting probabilistic models to data is often difficult, due to the general intractability of the partition function and its derivatives. Here we propose a new parameter estimation technique that does not require computing an intractable normalization factor or sampling from the equilibrium distribution of the model. This is achieved by establishing dynamics that would transform the observed data distribution into the model distribution, and then setting as the objective the minimization of the KL divergence between the data distribution and the distribution produced by running the dynamics for an infinitesimal time. Score matching, minimum velocity learning, and certain forms of contrastive divergence are shown to be special cases of this learning technique. We demonstrate parameter estimation in Ising models, deep belief networks and an independent component analysis model of natural scenes. In the Ising model case, current state of the art techniques are outperformed by at least an order of magnitude in learning time, with lower error in recovered coupling parameters."
            },
            "slug": "Minimum-Probability-Flow-Learning-Sohl-Dickstein-Battaglino",
            "title": {
                "fragments": [],
                "text": "Minimum Probability Flow Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work proposes a new parameter estimation technique that does not require computing an intractable normalization factor or sampling from the equilibrium distribution of the model, and demonstrates parameter estimation in Ising models, deep belief networks and an independent component analysis model of natural scenes."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 25
                            }
                        ],
                        "text": "In [19, 10], followed by [31, 30, 28], variational learning and inference algorithms were developed which allow a flexible generative model and posterior distribution over latent variables to be directly trained against each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16935709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f87247fb37f6b48da0757d7a1acf38da44510cdb",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic backpropagation \u2013 rules for back-propagation through stochastic variables \u2013 and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation."
            },
            "slug": "Stochastic-Back-propagation-and-Variational-in-Deep-Rezende-Mohamed",
            "title": {
                "fragments": [],
                "text": "Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning that introduces a recognition model to represent approximate posterior distributions and that acts as a stochastic encoder of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16895865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "484ad17c926292fbe0d5211540832a8c8a8e958b",
            "isKey": false,
            "numCitedBy": 3903,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation."
            },
            "slug": "Stochastic-Backpropagation-and-Approximate-in-Deep-Rezende-Mohamed",
            "title": {
                "fragments": [],
                "text": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work marries ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning that introduces a recognition model to represent approximate posterior distributions and that acts as a stochastic encoder of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757324"
                        ],
                        "name": "Oren Rippel",
                        "slug": "Oren-Rippel",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Rippel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Rippel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722180"
                        ],
                        "name": "Ryan P. Adams",
                        "slug": "Ryan-P.-Adams",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Adams",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan P. Adams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "\u2026the Jarzynski equality [16] (known in machine learning as annealed importance sampling [30]), which uses a Markov chain which converts one distribution into another to compute the ratio of normalizing constants between the two distributions; Langevin dynamics [23] which show how to define a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17083936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "638f454d6ccb6fb3e5cea263dafdd5ec51aaa536",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the fundamental problems in machine learning is the estimation of a probability distribution from data. Many techniques have been proposed to study the structure of data, most often building around the assumption that observations lie on a lower-dimensional manifold of high probability. It has been more difficult, however, to exploit this insight to build explicit, tractable density models for high-dimensional data. In this paper, we introduce the deep density model (DDM), a new approach to density estimation. We exploit insights from deep learning to construct a bijective map to a representation space, under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities. The simplicity of the latent distribution under the model allows us to feasibly explore it, and the invertibility of the map to characterize contraction of measure across it. This enables us to compute normalized densities for out-of-sample data. This combination of tractability and flexibility allows us to tackle a variety of probabilistic tasks on high-dimensional datasets, including: rapid computation of normalized densities at test-time without evaluating a partition function; generation of samples without MCMC; and characterization of the joint entropy of the data."
            },
            "slug": "High-Dimensional-Probability-Estimation-with-Deep-Rippel-Adams",
            "title": {
                "fragments": [],
                "text": "High-Dimensional Probability Estimation with Deep Density Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The deep density model (DDM) is introduced, a new approach to density estimation that exploits insights from deep learning to construct a bijective map to a representation space, under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2825051"
                        ],
                        "name": "B. Uria",
                        "slug": "B.-Uria",
                        "structuredName": {
                            "firstName": "Benigno",
                            "lastName": "Uria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Uria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "\u2026its equilibrium distribution to the data distribution, neural autoregressive distribution estimators [24] (and their recurrent [42] and deep [43] extensions) which decompose the joint distribution into a sequence of tractable conditional distributions over each dimension, adversarial networks\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13147238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "705fd4febe2fff810d2f72f48dcda20826eca77a",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance."
            },
            "slug": "A-Deep-and-Tractable-Density-Estimator-Uria-Murray",
            "title": {
                "fragments": [],
                "text": "A Deep and Tractable Density Estimator"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717963"
                        ],
                        "name": "Karol Gregor",
                        "slug": "Karol-Gregor",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Gregor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Gregor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723876"
                        ],
                        "name": "C. Blundell",
                        "slug": "C.-Blundell",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Blundell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Blundell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "We build a Markov chain which converts a simple known distribution (e.g. a Gaussian) into a target (data) distribution using a diffusion process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14576846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games."
            },
            "slug": "Deep-AutoRegressive-Networks-Gregor-Danihelka",
            "title": {
                "fragments": [],
                "text": "Deep AutoRegressive Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An efficient approximate parameter estimation method based on the minimum description length (MDL) principle is derived, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403025868"
                        ],
                        "name": "Jean Pouget-Abadie",
                        "slug": "Jean-Pouget-Abadie",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Pouget-Abadie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Pouget-Abadie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113742925"
                        ],
                        "name": "Bing Xu",
                        "slug": "Bing-Xu",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955694"
                        ],
                        "name": "Sherjil Ozair",
                        "slug": "Sherjil-Ozair",
                        "structuredName": {
                            "firstName": "Sherjil",
                            "lastName": "Ozair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherjil Ozair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "MNIST log likelihoods were estimated using the Parzen-window code from (Goodfellow et al., 2014), and show that our performance is comparable to other recent techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For this comparison we therefore estimate MNIST log likelihood using the Parzen-window code released with (Goodfellow et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The relative log likelihoods are given in Table 2 to a variety of techniques (Bengio et al., 2012; Bengio & Thibodeau-Laufer, 2013; Goodfellow et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Adversarial networks (Goodfellow et al., 2014) train a generative model against a classifier which attempts to distinguish generated samples from true data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1033682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54e325aee6b2d476bbbb88615ac15e251c6e8214",
            "isKey": true,
            "numCitedBy": 29672,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
            },
            "slug": "Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie",
            "title": {
                "fragments": [],
                "text": "Generative Adversarial Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new framework for estimating generative models via an adversarial process, in which two models are simultaneously train: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "The wake-sleep algorithm [14, 6] introduced the idea of training inference and generative probabilistic models against each other."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1890561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "isKey": false,
            "numCitedBy": 1173,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "slug": "The-Helmholtz-Machine-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "The Helmholtz Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations is described, viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46573521"
                        ],
                        "name": "Laurent Dinh",
                        "slug": "Laurent-Dinh",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Dinh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Dinh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145055042"
                        ],
                        "name": "David Krueger",
                        "slug": "David-Krueger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Krueger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Krueger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13995862,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "dc8301b67f98accbb331190dd7bd987952a692af",
            "isKey": false,
            "numCitedBy": 1169,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting."
            },
            "slug": "NICE:-Non-linear-Independent-Components-Estimation-Dinh-Krueger",
            "title": {
                "fragments": [],
                "text": "NICE: Non-linear Independent Components Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE) is proposed, based on the idea that a good representation is one in which the data has a distribution that is easy to model."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2214496"
                        ],
                        "name": "Andreas Stuhlm\u00fcller",
                        "slug": "Andreas-Stuhlm\u00fcller",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stuhlm\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Stuhlm\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144364160"
                        ],
                        "name": "Jessica Taylor",
                        "slug": "Jessica-Taylor",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jessica Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In (Stuhlm\u00fcller et al., 2013) stochastic inverses are learned for Bayesian networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10112543,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f887b89684157c2c842010ed63f12bea7787745b",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model's joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efficiency of this sampler for a variety of parameter regimes and Bayes nets."
            },
            "slug": "Learning-Stochastic-Inverses-Stuhlm\u00fcller-Taylor",
            "title": {
                "fragments": [],
                "text": "Learning Stochastic Inverses"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Inverse MCMC algorithm is described, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler, and the efficiency of this sampler for a variety of parameter regimes and Bayes nets is explored."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145095579"
                        ],
                        "name": "L. Yao",
                        "slug": "L.-Yao",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955694"
                        ],
                        "name": "Sherjil Ozair",
                        "slug": "Sherjil-Ozair",
                        "structuredName": {
                            "firstName": "Sherjil",
                            "lastName": "Ozair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherjil Ozair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "Other related methods for training probabilistic models include generative stochastic networks [3, 45] which directly train a Markov kernel to match its equilibrium distribution to the data distribution, neural autoregressive distribution estimators [24] (and their recurrent [42] and deep [43]\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10718578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa1213960a755fcc5efab9e07d57dcccc0dc6edc",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Autoregressive Distribution Estimators (NADEs) have recently been shown as successful alternatives for modeling high dimensional multimodal distributions. One issue associated with NADEs is that they rely on a particular order of factorization for P(x). This issue has been recently addressed by a variant of NADE called Orderless NADEs and its deeper version, Deep Orderless NADE. Orderless NADEs are trained based on a criterion that stochastically maximizes P(x) with all possible orders of factorizations. Unfortunately, ancestral sampling from deep NADE is very expensive, corresponding to running through a neural net separately predicting each of the visible variables given some others. This work makes a connection between this criterion and the training criterion for Generative Stochastic Networks (GSNs). It shows that training NADEs in this way also trains a GSN, which defines a Markov chain associated with the NADE model. Based on this connection, we show an alternative way to sample from a trained Orderless NADE that allows to trade-off computing time and quality of the samples: a 3 to 10-fold speedup (taking into account the waste due to correlations between consecutive samples of the chain) can be obtained without noticeably reducing the quality of the samples. This is achieved using a novel sampling procedure for GSNs called annealed GSN sampling, similar to tempering methods that combines fast mixing (obtained thanks to steps at high noise levels) with accurate samples (obtained thanks to steps at low noise levels)."
            },
            "slug": "On-the-Equivalence-between-Deep-NADE-and-Generative-Yao-Ozair",
            "title": {
                "fragments": [],
                "text": "On the Equivalence between Deep NADE and Generative Stochastic Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows an alternative way to sample from a trained Orderless NADE that allows to trade-off computing time and quality of the samples, and makes a connection between this criterion and the training criterion for Generative Stochastic Networks (GSNs)."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "\u2026directly train a Markov kernel to match its equilibrium distribution to the data distribution, neural autoregressive distribution estimators [24] (and their recurrent [42] and deep [43] extensions) which decompose the joint distribution into a sequence of tractable conditional distributions over\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13975441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32f078a7478d1ec2169599500a4507aceaccdda7",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this diculty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM."
            },
            "slug": "The-Neural-Autoregressive-Distribution-Estimator-Larochelle-Murray",
            "title": {
                "fragments": [],
                "text": "The Neural Autoregressive Distribution Estimator"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new approach for modeling the distribution of high-dimensional vectors of discrete variables inspired by the restricted Boltzmann machine, which outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744439"
                        ],
                        "name": "J. Suykens",
                        "slug": "J.-Suykens",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Suykens",
                            "middleNames": [
                                "A.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suykens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704135"
                        ],
                        "name": "J. Vandewalle",
                        "slug": "J.-Vandewalle",
                        "structuredName": {
                            "firstName": "Joos",
                            "lastName": "Vandewalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vandewalle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11137581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9170d193fd72c3a3b42f7d56153487b72626e424",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A new algorithm for nonconvex optimization by means of a so-called Fokker-Planck Learning Machine is proposed in this paper. This is done by considering the Fokker-Planck (FP) equation related to continuous simulated annealing, which has been proven to convergence to the global optimum under certain conditions. An approximate solution to the FP equation is sought by parametrizing the transition density by means of Gaussian sum approximations (Radial Basis Function networks). Like in genetic algorithms a population of points is considered. At each generation the points are generated based upon the transition density and the density is updated according to the FP equation."
            },
            "slug": "Nonconvex-optimization-using-a-Fokker-Planck-Suykens-Vandewalle",
            "title": {
                "fragments": [],
                "text": "Nonconvex optimization using a Fokker-Planck learning machine"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A new algorithm for nonconvex optimization by means of a so-called Fokker-Planck Learning Machine is proposed in this paper, which considers the Fokkers Planck equation related to continuous simulated annealing to find an approximate solution to the FP equation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785346"
                        ],
                        "name": "Roger B. Grosse",
                        "slug": "Roger-B.-Grosse",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Grosse",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger B. Grosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2772217"
                        ],
                        "name": "Chris J. Maddison",
                        "slug": "Chris-J.-Maddison",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Maddison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris J. Maddison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ward trajectory is important for the performance of the trained model. In AIS, the right schedule of intermediate distributions can greatly improve the accuracy of the log partition function estimate [13]. In thermodynamics the schedule taken when moving between equilibrium distributions determines how much free energy is lost [39,18]. In the case of Gaussian diffusion, we learn the forward diffusion "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2669960,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f1422fa2b097b68ab9c4baf6b54a2184ae868299",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many powerful Monte Carlo techniques for estimating partition functions, such as annealed importance sampling (AIS), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and the intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families defined by averaging the moments of the initial and target distributions. We analyze the asymptotic performance of both the geometric and moment averages paths and derive an asymptotically optimal piecewise linear schedule. AIS with moment averaging performs well empirically at estimating partition functions of restricted Boltzmann machines (RBMs), which form the building blocks of many deep learning models."
            },
            "slug": "Annealing-between-distributions-by-averaging-Grosse-Maddison",
            "title": {
                "fragments": [],
                "text": "Annealing between distributions by averaging moments"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel sequence of intermediate distributions for exponential families defined by averaging the moments of the initial and target distributions is presented and an asymptotically optimal piecewise linear schedule is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "(46) Using Bayes\u2019 rule we can rewrite this in terms of a posterior and marginals from the forward trajectory,\nK = T\u2211 t=2 \u222b dx(0\u00b7\u00b7\u00b7T )q ( x(0\u00b7\u00b7\u00b7T ) ) log [ p ( x(t\u22121)|x(t) ) q ( x(t\u22121)|x(t),x(0) ) q (x(t\u22121)|x(0)) q ( x(t)|x(0) ) ]\u2212Hp (X(T )) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "A variety of analytic approximations exist which ameliorate, but do not remove, this tradeoff \u2013 for instance mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy belief propagation [29], and many, many more."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "By Bayes\u2019 rule the forward chain presented in section 2.1 satisfies\nq ( x(t)|x(t\u22121) ) q ( x(t\u22121) ) = q ( x(t\u22121)|x(t) ) q ( x(t) ) ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2073260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6120cc252bc74239012f11b8b075cb7cb16bee26",
            "isKey": true,
            "numCitedBy": 2947,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case."
            },
            "slug": "An-Introduction-to-Variational-Methods-for-Models-Jordan-Ghahramani",
            "title": {
                "fragments": [],
                "text": "An Introduction to Variational Methods for Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields), and describes a general framework for generating variational transformations based on convex duality."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794837"
                        ],
                        "name": "Siwei Lyu",
                        "slug": "Siwei-Lyu",
                        "structuredName": {
                            "firstName": "Siwei",
                            "lastName": "Lyu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siwei Lyu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "\u2026mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy belief propagation [29], and many, many more."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7417159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ffbf8b2caa2237850d651d7074d771728c1480",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "When used to learn high dimensional parametric probabilistic models, the classical maximum likelihood (ML) learning often suffers from computational intractability, which motivates the active developments of non-ML learning methods. Yet, because of their divergent motivations and forms, the objective functions of many non-ML learning methods are seemingly unrelated, and there lacks a unified framework to understand them. In this work, based on an information geometric view of parametric learning, we introduce a general non-ML learning principle termed as minimum KL contraction, where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learning methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score matching [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be unified under the minimum KL contraction framework with different choices of the KL contraction operators."
            },
            "slug": "Unifying-Non-Maximum-Likelihood-Learning-Objectives-Lyu",
            "title": {
                "fragments": [],
                "text": "Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces a general non-ML learning principle termed as minimum KL contraction, where it seeks optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1407546424"
                        ],
                        "name": "J. Sohl-Dickstein",
                        "slug": "J.-Sohl-Dickstein",
                        "structuredName": {
                            "firstName": "Jascha",
                            "lastName": "Sohl-Dickstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sohl-Dickstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16443937"
                        ],
                        "name": "Ben Poole",
                        "slug": "Ben-Poole",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Poole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Poole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25769960"
                        ],
                        "name": "S. Ganguli",
                        "slug": "S.-Ganguli",
                        "structuredName": {
                            "firstName": "Surya",
                            "lastName": "Ganguli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ganguli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "In all cases the objective function and gradient were computed using Theano [4], and model training was with SFO [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10978620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af0ee019dcc1fe7eab918e3c670a6c47e48d17f6",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability and limit memory requirements even for high dimensional optimization problems by storing and manipulating these quadratic approximations in a shared, time evolving, low dimensional subspace. This algorithm contrasts with earlier stochastic second order techniques that treat the Hessian of each contributing function as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Each update step requires only a single contributing function or minibatch evaluation (as in SGD), and each step is scaled using an approximate inverse Hessian and little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods). We experimentally demonstrate improved convergence on seven diverse optimization problems. The algorithm is released as open source Python and MATLAB packages."
            },
            "slug": "Fast-large-scale-optimization-by-unifying-gradient-Sohl-Dickstein-Poole",
            "title": {
                "fragments": [],
                "text": "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent with the second order curvature information leveraged by quasi-Newton methods is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073063"
                        ],
                        "name": "Lucas Theis",
                        "slug": "Lucas-Theis",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Theis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucas Theis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422336"
                        ],
                        "name": "A\u00e4ron van den Oord",
                        "slug": "A\u00e4ron-van-den-Oord",
                        "structuredName": {
                            "firstName": "A\u00e4ron",
                            "lastName": "Oord",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A\u00e4ron van den Oord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731199"
                        ],
                        "name": "M. Bethge",
                        "slug": "M.-Bethge",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Bethge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bethge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 161
                            }
                        ],
                        "text": "The log likelihood bounds reported here are instead for data that has been pre-processed by adding uniform noise to remove pixel quantization, as recommended in (Theis et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2187805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39e0c341351f8f4a39ac890b96217c7f4bde5369",
            "isKey": false,
            "numCitedBy": 827,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided."
            },
            "slug": "A-note-on-the-evaluation-of-generative-models-Theis-Oord",
            "title": {
                "fragments": [],
                "text": "A note on the evaluation of generative models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models and shows that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144908539"
                        ],
                        "name": "J. Bornschein",
                        "slug": "J.-Bornschein",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Bornschein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bornschein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10872458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a24ec97e7f2881e245d20c46a56cbbfc734a4ff",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufficiently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models."
            },
            "slug": "Reweighted-Wake-Sleep-Bornschein-Bengio",
            "title": {
                "fragments": [],
                "text": "Reweighted Wake-Sleep"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel interpretation of the wake-sleep algorithm is proposed which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network, and using a more powerful layer model, such as NADE, yields substantially better generative models."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2825051"
                        ],
                        "name": "B. Uria",
                        "slug": "B.-Uria",
                        "structuredName": {
                            "firstName": "Benigno",
                            "lastName": "Uria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Uria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "\u2026to match its equilibrium distribution to the data distribution, neural autoregressive distribution estimators [24] (and their recurrent [42] and deep [43] extensions) which decompose the joint distribution into a sequence of tractable conditional distributions over each dimension,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14249325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "309494da0769345cb35ca0b7b0aae8143eee85a2",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case."
            },
            "slug": "RNADE:-The-real-valued-neural-autoregressive-Uria-Murray",
            "title": {
                "fragments": [],
                "text": "RNADE: The real-valued neural autoregressive density-estimator"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work introduces RNADE, a new model for joint density estimation of real-valued vectors that calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145876882"
                        ],
                        "name": "Toshiyuki TANAKA",
                        "slug": "Toshiyuki-TANAKA",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "TANAKA",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki TANAKA"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "\u2026of analytic approximations exist which ameliorate, but do not remove, this tradeoff \u2013 for instance mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123126013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d90e4b3f68044727973d62a17ca8a6dec6e9c5",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "I present a mean-field theory for Boltzmann machine learning, derived by employing Thouless-Anderson-Palmer free energy formalism to a full extent. Using the Plefka expansion an extended theory that takes higher-order correction to mean-field free energy formalism into consideration is presented, from which the mean-field approximation of general orders, along with the linear response correction, are derived by truncating the Plefka expansion up to desired orders. A theoretical foundation for an effective trick of using ``diagonal weights,'' introduced by Kappen and Rodr\\'{\\i}guez, is also given. Because of the finite system size and a lack of scaling assumptions on interaction coefficients, the truncated free energy formalism cannot provide an exact description in the case of Boltzmann machines. Accuracies of mean-field approximations of several orders are compared by computer simulations."
            },
            "slug": "Mean-field-theory-of-Boltzmann-machine-learning-TANAKA",
            "title": {
                "fragments": [],
                "text": "Mean-field theory of Boltzmann machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A mean-field theory for Boltzmann machine learning is presented, derived by employing Thouless-Anderson-Palmer free energy formalism to a full extent and a theoretical foundation for an effective trick of using ``diagonal weights,'' introduced by Kappen and Rodr\\'{\\i}guez is given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Since q ( x(t)|x(t\u22121) ) is a Gaussian (bi-\nnomial) distribution, and if \u03b2t is small, then q ( x(t\u22121)|x(t) )\nwill also be a Gaussian (binomial) distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18268744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "isKey": false,
            "numCitedBy": 17113,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images."
            },
            "slug": "Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Layers of Features from Tiny Images"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "\u2026and MCGSMs.\nRelated ideas from physics include the Jarzynski equality [16] (known in machine learning as annealed importance sampling [30]), which uses a Markov chain which converts one distribution into another to compute the ratio of normalizing constants between the two distributions;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "Our method uses a Markov chain to gradually convert one distribution into another, an idea used in non-equilibrium statistical physics [16] and sequential Monte Carlo [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11112994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f59406cce55c7bb9a78521bd14755a0db0aee7d",
            "isKey": false,
            "numCitedBy": 1212,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Simulated annealing\u2014moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions\u2014has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers."
            },
            "slug": "Annealed-importance-sampling-Neal",
            "title": {
                "fragments": [],
                "text": "Annealed importance sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler, which can be seen as a generalization of a recently-proposed variant of sequential importance sampling."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1407546424"
                        ],
                        "name": "J. Sohl-Dickstein",
                        "slug": "J.-Sohl-Dickstein",
                        "structuredName": {
                            "firstName": "Jascha",
                            "lastName": "Sohl-Dickstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sohl-Dickstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3341676"
                        ],
                        "name": "Peter Battaglino",
                        "slug": "Peter-Battaglino",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Battaglino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Battaglino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49488190"
                        ],
                        "name": "M. DeWeese",
                        "slug": "M.-DeWeese",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "DeWeese",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. DeWeese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "\u2026instance mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy belief propagation [29], and many, many\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15550051,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f028d43bd69c7eccd5c95c0df8b189c3f491ad2",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Fitting probabilistic models to data is often difficult, due to the general intractability of the partition function. We propose a new parameter fitting method, minimum probability flow (MPF), which is applicable to any parametric model. We demonstrate parameter estimation using MPF in two cases: a continuous state space model, and an Ising spin glass. In the latter case, MPF outperforms current techniques by at least an order of magnitude in convergence time with lower error in the recovered coupling parameters."
            },
            "slug": "A-new-method-for-parameter-estimation-in-models:-Sohl-Dickstein-Battaglino",
            "title": {
                "fragments": [],
                "text": "A new method for parameter estimation in probabilistic models: Minimum probability flow"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work proposes a new parameter fitting method, minimum probability flow (MPF), which is applicable to any parametric model and demonstrates parameter estimation using MPF in two cases: a continuous state space model, and an Ising spin glass."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar objective in (Schmidhuber, 1992) learns a two-way mapping to a representation with marginally independent units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42023620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac91740ae76ed9dbd853bddd6d1d9dc43bc55179",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose a novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns. The principle is based on two opposing forces. For each representational unit there is an adaptive predictor, which tries to predict the unit from the remaining units. In turn, each unit tries to react to the environment such that it minimizes its predictability. This encourages each unit to filter \"abstract concepts\" out of the environmental input such that these concepts are statistically independent of those on which the other units focus. I discuss various simple yet potentially powerful implementations of the principle that aim at finding binary factorial codes (Barlow et al. 1989), i.e., codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols. Such codes are potentially relevant for (1) segmentation tasks, (2) speeding up supervised learning, and (3) novelty detection. Methods for finding factorial codes automatically implement Occam's razor for finding codes using a minimal number of units. Unlike previous methods the novel principle has a potential for removing not only linear but also nonlinear output redundancy. Illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible. The final part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences."
            },
            "slug": "Learning-Factorial-Codes-by-Predictability-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Factorial Codes by Predictability Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns based on two opposing forces that has a potential for removing not only linear but also nonlinear output redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1935910"
                        ],
                        "name": "G. Mesnil",
                        "slug": "G.-Mesnil",
                        "structuredName": {
                            "firstName": "Gr\u00e9goire",
                            "lastName": "Mesnil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mesnil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2921469"
                        ],
                        "name": "Yann Dauphin",
                        "slug": "Yann-Dauphin",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "Dauphin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann Dauphin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425018"
                        ],
                        "name": "S. Rifai",
                        "slug": "S.-Rifai",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Rifai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rifai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "6 bits DBN [2] 138\u00b1 2 bits Deep GSN [3] 214\u00b1 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "489 bits/pixel MNIST Stacked CAE [2] 121\u00b1 1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1334653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0965d8f9842f2db960b36b528107ca362c00d1a",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples."
            },
            "slug": "Better-Mixing-via-Deep-Representations-Bengio-Mesnil",
            "title": {
                "fragments": [],
                "text": "Better Mixing via Deep Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proposed that the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels, and mixing between modes would be more efficient at higher Levels of representation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "\u2026but do not remove, this tradeoff \u2013 for instance mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18600461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b5bea7b4d40003a6887794652ea07196a97134",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new learning algorithm for Mean Field Boltzmann Machines based on the contrastive divergence optimization criterion. In addition to minimizing the divergence between the data distribution and the equilibrium distribution, we maximize the divergence between one-step reconstructions of the data and the equilibrium distribution. This eliminates the need to estimate equilibrium statistics, so we do not need to approximate the multimodal probability distribution of the free network with the unimodal mean field distribution. We test the learning algorithm on the classification of digits."
            },
            "slug": "A-New-Learning-Algorithm-for-Mean-Field-Boltzmann-Welling-Hinton",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Mean Field Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A new learning algorithm for Mean Field Boltzmann Machines based on the contrastive divergence optimization criterion that eliminates the need to estimate equilibrium statistics, so it does not need to approximate the multimodal probability distribution of the free network with the unimodal mean field distribution."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "\u2026do not remove, this tradeoff \u2013 for instance mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4572,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49994357"
                        ],
                        "name": "R. Spinney",
                        "slug": "R.-Spinney",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Spinney",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Spinney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34784713"
                        ],
                        "name": "I. Ford",
                        "slug": "I.-Ford",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Ford",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ly a single sample from q x(1 T)jx(0) is required to exactly evaluate the above integral, as can be seen by substitution. This corresponds to the case of a quasi-static process in statistical physics [36,16]. 2.4. Training Training amounts to maximizing the model log likelihood, L= Z d x(0)q  (0)  logp  (0)  (10) = Z dx (0)q  x  log 2 4 R dx(1 T)q x(1 T)jx(0) p x(T) Q T t=1 p(x(t 1)jx(t)) q( x(t)j "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "an greatly improve the accuracy of the log partition function estimate [11]. In thermodynamics the schedule taken when moving between equilibrium distributions determines how much free energy is lost [36,16]. In the case of Gaussian diffusion, we learn the forward diffusion schedule 1 T by gradient descent on K. The dependence of samples from q x(1 T)jx(0) on 1 T is made explicit by using \u2018frozen noise\u2019 "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55884912,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7ecb1bde128ddfdaeb97088238ab9cf5bbd02e9c",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The uctuation relations have received considerable attention since their emergence and development in the 1990s. As an aid to newcomers to the eld, we present a summary of the main results and suggest ways to interpret this material. Starting with a consideration of the under-determined time evolution of a simple open system, formulated using continuous Markovian stochastic dynamics, an expression for the entropy generated with time is proposed in terms of the probability of observing a trajectory associated with a prescribed driving protocol, and the probability of its time-reverse. This forms the basis for a more complete theoretical description of non-equilibrium thermodynamic processes. Having established a connection between entropy production and an inequivalence in probability for forward and time-reversed events, we proceed in the manner of Sekimoto and Seifert, in particular, to derive results in stochastic thermodynamics: a description of the evolution of a system between equilibrium states that ties in with well-established thermodynamic expectations. We derive uctuation relations, state conditions for their validity, and illustrate their operation in some simple cases, thereby providing some introductory insight into the various celebrated symmetry relations that have emerged in this eld."
            },
            "slug": "Fluctuation-Relations:-A-Pedagogical-Overview-Spinney-Ford",
            "title": {
                "fragments": [],
                "text": "Fluctuation Relations: A Pedagogical Overview"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073063"
                        ],
                        "name": "Lucas Theis",
                        "slug": "Lucas-Theis",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Theis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucas Theis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34584608"
                        ],
                        "name": "Reshad Hosseini",
                        "slug": "Reshad-Hosseini",
                        "structuredName": {
                            "firstName": "Reshad",
                            "lastName": "Hosseini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reshad Hosseini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731199"
                        ],
                        "name": "M. Bethge",
                        "slug": "M.-Bethge",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Bethge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bethge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "\u2026train a generative model against a classifier which attempts to distinguish generated samples from true data, and mixtures of conditional Gaussian scale mixtures (MCGSMs) [41] which describe a dataset using Gaussian scale mixtures, with parameters which depend on a sequence of causal neighborhoods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7078763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c4cff24f76f00911ef5e4e8220a87eacc527c69",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a probabilistic model for natural images that is based on mixtures of Gaussian scale mixtures and a simple multiscale representation. We show that it is able to generate images with interesting higher-order correlations when trained on natural images or samples from an occlusion-based model. More importantly, our multiscale model allows for a principled evaluation. While it is easy to generate visually appealing images, we demonstrate that our model also yields the best performance reported to date when evaluated with respect to the cross-entropy rate, a measure tightly linked to the average log-likelihood. The ability to quantitatively evaluate our model differentiates it from other multiscale models, for which evaluation of these kinds of measures is usually intractable."
            },
            "slug": "Mixtures-of-Conditional-Gaussian-Scale-Mixtures-to-Theis-Hosseini",
            "title": {
                "fragments": [],
                "text": "Mixtures of Conditional Gaussian Scale Mixtures Applied to Multiscale Image Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "It is demonstrated that this probabilistic model based on mixtures of Gaussian scale mixtures and a simple multiscale representation yields the best performance reported to date when evaluated with respect to the cross-entropy rate, a measure tightly linked to the average log-likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "PloS one"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662552"
                        ],
                        "name": "M. Svens\u00e9n",
                        "slug": "M.-Svens\u00e9n",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Svens\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Svens\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There is additionally significant work learning flexible generative mappings from simple latent distributions to data distributions \u2013 early examples including (MacKay, 1995) where neural networks are introduced as generative models, and (Bishop et al., 1998) where a stochastic manifold mapping is learned from a latent space to the data space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207605229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2639515c248f220c73d44688c0097a99b01e1474",
            "isKey": false,
            "numCitedBy": 1456,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis, which is based on a linear transformation between the latent space and the data space. In this article, we introduce a form of nonlinear latent variable model called the generative topographic mapping, for which the parameters of the model can be determined using the expectation-maximization algorithm. GTM provides a principled alternative to the widely used self-organizing map (SOM) of Kohonen (1982) and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multiphase oil pipeline."
            },
            "slug": "GTM:-The-Generative-Topographic-Mapping-Bishop-Svens\u00e9n",
            "title": {
                "fragments": [],
                "text": "GTM: The Generative Topographic Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A form of nonlinear latent variable model called the generative topographic mapping, for which the parameters of the model can be determined using the expectation-maximization algorithm, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3197309"
                        ],
                        "name": "A. Kanaujia",
                        "slug": "A.-Kanaujia",
                        "structuredName": {
                            "firstName": "Atul",
                            "lastName": "Kanaujia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kanaujia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711560"
                        ],
                        "name": "Dimitris N. Metaxas",
                        "slug": "Dimitris-N.-Metaxas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Metaxas",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitris N. Metaxas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15133267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1af67fb5c15470c9e8954cae66702293dd66bae8",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for jointly learning a consistent bidirectional generative-recognition model that combines top-down and bottom-up processing for monocular 3d human motion reconstruction. Learning progresses in alternative stages of self-training that optimize the probability of the image evidence: the recognition model is tunned using samples from the generative model and the generative model is optimized to produce inferences close to the ones predicted by the current recognition model. At equilibrium, the two models are consistent. During on-line inference, we scan the image at multiple locations and predict 3d human poses using the recognition model. But this implicitly includes one-shot generative consistency feedback. The framework provides a uniform treatment of human detection, 3d initialization and 3d recovery from transient failure. Our experimental results show that this procedure is promising for the automatic reconstruction of human motion in more natural scene settings with background clutter and occlusion."
            },
            "slug": "Learning-Joint-Top-Down-and-Bottom-up-Processes-for-Sminchisescu-Kanaujia",
            "title": {
                "fragments": [],
                "text": "Learning Joint Top-Down and Bottom-up Processes for 3D Visual Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "An algorithm for jointly learning a consistent bidirectional generative-recognition model that combines top-down and bottom-up processing for monocular 3d human motion reconstruction that is promising for the automatic reconstruction of human motion in more natural scene settings with background clutter and occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955694"
                        ],
                        "name": "Sherjil Ozair",
                        "slug": "Sherjil-Ozair",
                        "structuredName": {
                            "firstName": "Sherjil",
                            "lastName": "Ozair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherjil Ozair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "In [21, 11, 33, 31] variational learning and inference algorithms were developed which allow a flexible generative model and posterior distribution over latent variables to be directly trained against each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5519738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3ccb7fff54b2b328945fcbe465931193ceecf62",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "For discrete data, the likelihood $P(x)$ can be rewritten exactly and parametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$ has enough capacity to put no probability mass on any $x'$ for which $f(x')\\neq f(x)$, where $f(\\cdot)$ is a deterministic discrete function. The log of the first factor gives rise to the log-likelihood reconstruction error of an autoencoder with $f(\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic) decoder. The log of the second term can be seen as a regularizer on the encoded activations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder can be represented by a deep neural network and trained to maximize the average of the optimal log-likelihood $\\log p(x)$. The objective is to learn an encoder $f(\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than $X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates probability mass in a smaller number of (relevant) dimensions over which the distribution factorizes. Generating samples from the model is straightforward using ancestral sampling. One challenge is that regular back-propagation cannot be used to obtain the gradient on the parameters of the encoder, but we find that using the straight-through estimator works well here. We also find that although optimizing a single level of such architecture may be difficult, much better results can be obtained by pre-training and stacking them, gradually transforming the data distribution into one that is more easily captured by a simple parametric model."
            },
            "slug": "Deep-Directed-Generative-Autoencoders-Ozair-Bengio",
            "title": {
                "fragments": [],
                "text": "Deep Directed Generative Autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "Although optimizing a single level of such architecture may be difficult, much better results can be obtained by pre-training and stacking them, gradually transforming the data distribution into one that is more easily captured by a simple parametric model."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1152227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9966e890f2eedb4577e11b9d5a66380a4d9341fe",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data."
            },
            "slug": "Estimation-of-Non-Normalized-Statistical-Models-by-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Estimation of Non-Normalized Statistical Models by Score Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, it is proved a surprising result that gives a simple formula that simplifies to a sample average of a sum of some derivatives of the log- density given by the model."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "There is additionally significant work learning flexible generative mappings from simple latent distributions to data distributions \u2013 early examples including (MacKay, 1995) where neural networks are introduced as generative models, and (Bishop et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122200499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2861a5e59de4d61bcd8a9ae4785978ac11fc9c1",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-neural-networks-and-density-networks-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian neural networks and density networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080409"
                        ],
                        "name": "Yuri Burda",
                        "slug": "Yuri-Burda",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Burda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Burda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785346"
                        ],
                        "name": "Roger B. Grosse",
                        "slug": "Roger-B.-Grosse",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Grosse",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger B. Grosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 3
                            }
                        ],
                        "text": "In (Burda et al., 2014) it is shown that AIS can also be performed using the reverse rather than forward trajectory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14063602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80a63f7e42166c64e330934339d18a72c330ae35",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random fields (MRFs) are difficult to evaluate as generative models because computing the test log-probabilities requires the intractable partition function. Annealed importance sampling (AIS) is widely used to estimate MRF partition functions, and often yields quite accurate results. However, AIS is prone to overestimate the log-likelihood with little indication that anything is wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower bound on the log-likelihood of an approximation to the original MRF model. RAISE requires only the same MCMC transition operators as standard AIS. Experimental results indicate that RAISE agrees closely with AIS log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the side of underestimating, rather than overestimating, the log-likelihood."
            },
            "slug": "Accurate-and-conservative-estimates-of-MRF-using-Burda-Grosse",
            "title": {
                "fragments": [],
                "text": "Accurate and conservative estimates of MRF log-likelihood using reverse annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results indicate that RAISE agrees closely with AIS log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the side of underestimating, rather than overestimating, the log-likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16462148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19908640236767427ebf0524dc3a4bb09d65145e",
            "isKey": false,
            "numCitedBy": 1774,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, researchers have demonstrated that \"loopy belief propagation\" -- the use of Pearl's polytree algorithm in a Bayesian network with loops -- can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of \"Turbo Codes\" -- codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. \n \nIn this paper we ask: is there something special about the error-correcting code context, or does loopy propagation work as an approximate inference scheme in a more general setting? We compare the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy beliefs oscillated and had no obvious relationship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some simple methods of preventing them lead to the wrong results."
            },
            "slug": "Loopy-Belief-Propagation-for-Approximate-Inference:-Murphy-Weiss",
            "title": {
                "fragments": [],
                "text": "Loopy Belief Propagation for Approximate Inference: An Empirical Study"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper compares the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR, and finds that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sions [37,38], variational Bayes [17], contrastive divergence [40,12], minimum probability \ufb02ow [34,33], minimum KL contraction [25], proper scoring rules [8,29], score matching [14], pseudolikelihood [4], loopy belief propagation [26], and many, many more. Nonparametric methods [7] can also be very effective1. 1.1. Diffusion probabilistic models We present a way to de\ufb01ne probabilistic models that all"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116757950,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1406b6d771c270aff4dcb1c96e4f5c62c02c00a5",
            "isKey": true,
            "numCitedBy": 1657,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In rather formal terms, the situation with which this paper is concerned may be described as follows. We are given a fixed system of n sites, labelled by the first n positive integers, and an associated vector x of observations, Xi, . . ., Xn, which, in turn, is presumed to be a realization of a vector X of (dependent) random variables, Xi, . . ., X.. In practice, the sites may represent points or regions in space and the random variables may be either continuous or discrete. The main statistical objectives are the following: firstly, to provide a means of using the available concomitant information, particularly the configuration of the sites, to attach a plausible probability distribution to the random vector X; secondly, to estimate any unknown parameters in the distribution from the realization x; thirdly, where possible, to quantify the extent of disagreement between hypothesis and observation."
            },
            "slug": "Statistical-Analysis-of-Non-Lattice-Data-Besag",
            "title": {
                "fragments": [],
                "text": "Statistical Analysis of Non-Lattice Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831199"
                        ],
                        "name": "S. Gershman",
                        "slug": "S.-Gershman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Gershman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gershman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16074636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57df462188c1b97bd3898f54161ba85f474116b6",
            "isKey": false,
            "numCitedBy": 488,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Tutorial-on-Bayesian-Nonparametric-Models-Gershman-Blei",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Bayesian Nonparametric Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848757"
                        ],
                        "name": "C. Jarzynski",
                        "slug": "C.-Jarzynski",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Jarzynski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jarzynski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "This corresponds to the case of a quasi-static process in statistical physics [38, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123151672,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e2ba7bf26dc2cbc8e9faa5cb21bdce5244521e51",
            "isKey": false,
            "numCitedBy": 696,
            "numCiting": 142,
            "paperAbstract": {
                "fragments": [],
                "text": "The reason we never observe violations of the second law of thermodynamics is in part a matter of statistics: When \u223c1023 degrees of freedom are involved, the odds are overwhelmingly stacked against the possibility of seeing significant deviations away from the mean behavior. As we turn our attention to smaller systems, however, statistical fluctuations become more prominent. In recent years it has become apparent that the fluctuations of systems far from thermal equilibrium are not mere background noise, but satisfy strong, useful, and unexpected properties. In particular, a proper accounting of fluctuations allows us to rewrite familiar inequalities of macroscopic thermodynamics as equalities. This review describes some of this progress, and argues that it has refined our understanding of irreversibility and the second law."
            },
            "slug": "Equalities-and-Inequalities:-Irreversibility-and-of-Jarzynski",
            "title": {
                "fragments": [],
                "text": "Equalities and Inequalities: Irreversibility and the Second Law of Thermodynamics at the Nanoscale"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572884"
                        ],
                        "name": "R. Neal",
                        "slug": "R.-Neal",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Neal",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "a twodimensional swiss roll, binary sequence, handwritten digit (MNIST), and several natural image (CIFAR-10, bark, and dead leaves) datasets. 1.2. Relationship to other work The wake-sleep algorithm [15,7] introduced the idea of training inference and generative probabilistic models against each other. This approach remained largely unexplored for nearly two decades, with some exceptions [35,21]. There"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 871473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
            "isKey": true,
            "numCitedBy": 1001,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
            },
            "slug": "The-\"wake-sleep\"-algorithm-for-unsupervised-neural-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "The \"wake-sleep\" algorithm for unsupervised neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described, where bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representations in the layer above."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3504356"
                        ],
                        "name": "W. Feller",
                        "slug": "W.-Feller",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Feller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Feller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 192
                            }
                        ],
                        "text": "\u2026Langevin dynamics [23] which show how to define a Gaussian diffusion process which has any target distribution as its equilibrium; and the Kolmogorov forward and backward equations [7] which show that forward and reverse diffusion processes can be described using the same functional form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "(5)\nFor both Gaussian and binomial diffusion, for continuous diffusion (limit of small step size \u03b2) the reversal of the diffusion process has the identical functional form as the forward process [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121027442,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4cdcf495232f3ec44183dc74cd8eca4b44c2de64",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since Kolmogoroff\u2019s famous paper of 1931, \u201cOn Analytical Methods in the Theory of Probability,\u201d the theory of stochastic processess has been developed and it has been shown that it can successfully be applied to practical problems and used to describe empirial phenomena. However, the theory is new and the most appropriate methematical techniques have yet to be discovered."
            },
            "slug": "On-the-Theory-of-Stochastic-Processes,-with-to-Feller",
            "title": {
                "fragments": [],
                "text": "On the Theory of Stochastic Processes, with Particular Reference to Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "This approach remained largely unexplored for nearly two decades, with some exceptions [34, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5931210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks."
            },
            "slug": "Fast-Inference-in-Sparse-Coding-Algorithms-with-to-Kavukcuoglu-Ranzato",
            "title": {
                "fragments": [],
                "text": "Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a simple and efficient algorithm to learn basis functions, which provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848757"
                        ],
                        "name": "C. Jarzynski",
                        "slug": "C.-Jarzynski",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Jarzynski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jarzynski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "We compare experimentally against adversarial networks and MCGSMs.\nRelated ideas from physics include the Jarzynski equality [16] (known in machine learning as annealed importance sampling [30]), which uses a Markov chain which converts one distribution into another to compute the ratio of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "Our method uses a Markov chain to gradually convert one distribution into another, an idea used in non-equilibrium statistical physics [16] and sequential Monte Carlo [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119101580,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ecf951301536a73b8c4a00d8f75dc68d6af8d283",
            "isKey": false,
            "numCitedBy": 955,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "It has recently been shown that the Helmholtz free energy difference between two equilibrium configurations of a system may be obtained from an ensemble of finite-time (nonequilibrium) measurements of the work performed in switching an external parameter of the system. Here this result is established, as an identity, within the master equation formalism. Examples are discussed and numerical illustrations provided."
            },
            "slug": "Equilibrium-free-energy-differences-from-A-approach-Jarzynski",
            "title": {
                "fragments": [],
                "text": "Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065306202"
                        ],
                        "name": "Jonathan T. Barron",
                        "slug": "Jonathan-T.-Barron",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Barron",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan T. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080407"
                        ],
                        "name": "M. Biggin",
                        "slug": "M.-Biggin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Biggin",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Biggin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2837110"
                        ],
                        "name": "D. Knowles",
                        "slug": "D.-Knowles",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Knowles",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Knowles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38822331"
                        ],
                        "name": "S. Ker\u00e4nen",
                        "slug": "S.-Ker\u00e4nen",
                        "structuredName": {
                            "firstName": "Soile",
                            "lastName": "Ker\u00e4nen",
                            "middleNames": [
                                "V.",
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ker\u00e4nen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "This method of achieving multiscale convolution was described in [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2842233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c5616a24c80915f52bac2321dbb6bb296375095",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for the per-voxel semantic segmentation of a three-dimensional volume. At the core of our algorithm is a novel \"pyramid context\" feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient. This feature not only allows for efficient semantic segmentation but enables other aspects of our algorithm, such as novel learned features and a stacked architecture that can reason about self-consistency. We demonstrate our technique on 3D fluorescence microscopy data of Drosophila embryos for which we are able to produce extremely accurate semantic segmentations in a matter of minutes, and for which other algorithms fail due to the size and high-dimensionality of the data, or due to the difficulty of the task."
            },
            "slug": "Volumetric-Semantic-Segmentation-Using-Pyramid-Barron-Biggin",
            "title": {
                "fragments": [],
                "text": "Volumetric Semantic Segmentation Using Pyramid Context Features"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The core of this algorithm is a novel \"pyramid context\" feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient, which allows for efficient semantic segmentation of a three-dimensional volume."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967465"
                        ],
                        "name": "Olivier Breuleux",
                        "slug": "Olivier-Breuleux",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Breuleux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Breuleux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227028"
                        ],
                        "name": "Fr\u00e9d\u00e9ric Bastien",
                        "slug": "Fr\u00e9d\u00e9ric-Bastien",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bastien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fr\u00e9d\u00e9ric Bastien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2755582"
                        ],
                        "name": "Guillaume Desjardins",
                        "slug": "Guillaume-Desjardins",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Desjardins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Desjardins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3183121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63936fa32f9e75ab2a864daae6791ce02112183d",
            "isKey": false,
            "numCitedBy": 825,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy's syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy's syntax and semantics, while being statically typed and functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learn- ing algorithms implemented with Theano are from 1:6 to 7:5 faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the CPU and between 6:5 and 44 faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design."
            },
            "slug": "Theano:-A-CPU-and-GPU-Math-Compiler-in-Python-Bergstra-Breuleux",
            "title": {
                "fragments": [],
                "text": "Theano: A CPU and GPU Math Compiler in Python"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1839473"
                        ],
                        "name": "T. Gneiting",
                        "slug": "T.-Gneiting",
                        "structuredName": {
                            "firstName": "Tilmann",
                            "lastName": "Gneiting",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gneiting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804386"
                        ],
                        "name": "A. Raftery",
                        "slug": "A.-Raftery",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Raftery",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Raftery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1878582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bbd8fc39487249bd1b6886e4dc763550877b758",
            "isKey": false,
            "numCitedBy": 3730,
            "numCiting": 175,
            "paperAbstract": {
                "fragments": [],
                "text": "Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G \u2260 F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage."
            },
            "slug": "Strictly-Proper-Scoring-Rules,-Prediction,-and-Gneiting-Raftery",
            "title": {
                "fragments": [],
                "text": "Strictly Proper Scoring Rules, Prediction, and Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The theory of proper scoring rules on general probability spaces is reviewed and developed, and the intuitively appealing interval score is proposed as a utility function in interval estimation that addresses width as well as coverage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832448"
                        ],
                        "name": "Ann B. Lee",
                        "slug": "Ann-B.-Lee",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Lee",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ann B. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145738882"
                        ],
                        "name": "Jinggang Huang",
                        "slug": "Jinggang-Huang",
                        "structuredName": {
                            "firstName": "Jinggang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinggang Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 34
                            }
                        ],
                        "text": "Dead Leaf Images Dead leaf images [18, 27] consist of layered occluding circles, drawn from a power law distribution over scales."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "leaf images [27], and bark texture images [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 51
                            }
                        ],
                        "text": "The proposed framework trained on dead leaf images [18, 27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13343075,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "556767b5a36ed8f7f8183882bde33399b5199328",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a scale-invariant version of Matheron's \u201cdead leaves model\u201d for the statistics of natural images. The model takes occlusions into account and resembles the image formation process by randomly adding independent elementary shapes, such as disks, in layers. We compare the empirical statistics of two large databases of natural images with the statistics of the occlusion model, and find an excellent qualitative, and good quantitative agreement. At this point, this is the only image model which comes close to duplicating the simplest, elementary statistics of natural images\u2014such as, the scale invariance property of marginal distributions of filter responses, the full co-occurrence statistics of two pixels, and the joint statistics of pairs of Haar wavelet responses."
            },
            "slug": "Occlusion-Models-for-Natural-Images:-A-Statistical-Lee-Mumford",
            "title": {
                "fragments": [],
                "text": "Occlusion Models for Natural Images: A Statistical Study of a Scale-Invariant Dead Leaves Model"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A scale-invariant version of Matheron's \u201cdead leaves model\u201d for the statistics of natural images that takes occlusions into account and resembles the image formation process by randomly adding independent elementary shapes, such as disks, in layers."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10151581"
                        ],
                        "name": "T. Plefka",
                        "slug": "T.-Plefka",
                        "structuredName": {
                            "firstName": "T",
                            "lastName": "Plefka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plefka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123236717,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "753761f39e455948779771e9a6b25ade1a4456f1",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that the power expansion of the Gibbs potential of the SK model up to second order in the exchange couplings leads to the TAP equation. This result remains valid for the general (including a ferromagnetic exchange) SK model. Theorems of power expansions and resolvent techniques are employed to solve the convergence problem. The convergence condition is presented for the whole temperature range and for general distributions of the local magnetisations."
            },
            "slug": "Convergence-condition-of-the-TAP-equation-for-the-Plefka",
            "title": {
                "fragments": [],
                "text": "Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1437976615"
                        ],
                        "name": "M. Parry",
                        "slug": "M.-Parry",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Parry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Parry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078496605"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "\u2026mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy belief propagation [29], and many, many more."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39122874,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2d1b584ea292ab6619ac558607a0b07805227634",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate proper scoring rules for continuous distributions on the real line. It is known that the log score is the only such rule that depends on the quoted density only through its value at the outcome that materializes. Here we allow further dependence on a finite number $m$ of derivatives of the density at the outcome, and describe a large class of such $m$-local proper scoring rules: these exist for all even $m$ but no odd $m$. We further show that for $m\\geq2$ all such $m$-local rules can be computed without knowledge of the normalizing constant of the distribution."
            },
            "slug": "Proper-local-scoring-rules-Parry-Dawid",
            "title": {
                "fragments": [],
                "text": "Proper local scoring rules"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47380634"
                        ],
                        "name": "D. Lemons",
                        "slug": "D.-Lemons",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Lemons",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lemons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "133603110"
                        ],
                        "name": "Anthony P. Gythiel",
                        "slug": "Anthony-P.-Gythiel",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Gythiel",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony P. Gythiel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122676963,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bb32bc8bc76ead611110fb02ed580954a316c192",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a translation of Paul Langevin\u2019s landmark paper. In it Langevin successfully applied Newtonian dynamics to a Brownian particle and so invented an analytical approach to random processes which has remained useful to this day."
            },
            "slug": "Paul-Langevin\u2019s-1908-paper-\u201cOn-the-Theory-of-[\u201cSur-Lemons-Gythiel",
            "title": {
                "fragments": [],
                "text": "Paul Langevin\u2019s 1908 paper \u201cOn the Theory of Brownian Motion\u201d [\u201cSur la th\u00e9orie du mouvement brownien,\u201d C. R. Acad. Sci. (Paris) 146, 530\u2013533 (1908)]"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Bark Texture Images A probabilistic model was trained on bark texture images (T01-T04) from (Lazebnik et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(a) A bark image from (Lazebnik et al., 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206763997,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "72bf4b2ce534b95bc24118491dbc4f8d550734a2",
            "isKey": false,
            "numCitedBy": 1158,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a texture representation suitable for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and nonrigid deformations. At the feature extraction stage, a sparse set of affine Harris and Laplacian regions is found in the image. Each of these regions can be thought of as a texture element having a characteristic elliptic shape and a distinctive appearance pattern. This pattern is captured in an affine-invariant fashion via a process of shape normalization followed by the computation of two novel descriptors, the spin image and the RIFT descriptor. When affine invariance is not required, the original elliptical shape serves as an additional discriminative feature for texture recognition. The proposed approach is evaluated in retrieval and classification tasks using the entire Brodatz database and a publicly available collection of 1,000 photographs of textured surfaces taken from different viewpoints."
            },
            "slug": "A-sparse-texture-representation-using-local-affine-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "A sparse texture representation using local affine regions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a publicly available collection of 1,000 photographs of textured surfaces taken from different viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "Samples from a diffusion probabilistic model trained on MNIST digits."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 175
                            }
                        ],
                        "text": "We demonstrate the utility of these diffusion probabilistic models by training high log likelihood models for a twodimensional swiss roll, binary sequence, handwritten digit (MNIST), and several natural image (CIFAR-10, bark, and dead leaves) datasets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "Samples from the MNIST model are given in Figure App.1 in the Appendix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "MNIST In order to allow a direct comparison against previous work on a simple dataset, we trained on MNIST digits [26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 43
                            }
                        ],
                        "text": "However, most previous reported results on MNIST log likelihood rely on Parzen-window based estimates computed from model samples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 142
                            }
                        ],
                        "text": "We train diffusion based probabilistic models on several continuous and binary datasets, specifically swiss roll data, binary \u201cheartbeat\u201d data, MNIST [26], CIFAR-10 [22], dead\nleaf images [27], and bark texture images [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 4
                            }
                        ],
                        "text": "For MNIST, the dense pathway was used to the exclusion of the multi-scale convolutional pathway.\nlikelihood."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "DATASETS MNIST In order to allow a direct comparison against previous work on a simple dataset, we trained on MNIST digits [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 42
                            }
                        ],
                        "text": "For this comparison we therefore estimate MNIST log likelihood using the Parzen-window code released with [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "We train diffusion based probabilistic models on several continuous and binary datasets, specifically swiss roll data, binary \u201cheartbeat\u201d data, MNIST [26], CIFAR-10 [22], dead Dataset K K \u2212 Lnull Swiss Roll 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60282629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2",
            "isKey": true,
            "numCitedBy": 4404,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough."
            },
            "slug": "The-mnist-database-of-handwritten-digits-LeCun-Cortes",
            "title": {
                "fragments": [],
                "text": "The mnist database of handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An improved articulated bar flail having shearing edges for efficiently shredding materials and an improved shredder cylinder with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft are disclosed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116882963"
                        ],
                        "name": "K. Perez",
                        "slug": "K.-Perez",
                        "structuredName": {
                            "firstName": "Kerstin",
                            "lastName": "Perez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Perez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Langevin dynamics [23] which show how to define a Gaussian diffusion process which has any target distribution as its equilibrium; and the Kolmogorov forward and backward equations [7] which show that forward and reverse diffusion processes can be described using the same functional form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125588372,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8e86507c92f98c4aa2e5f213d431776c10c6e1b0",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "ING AND INDEXING"
            },
            "slug": "Nuclear-Instruments-and-Methods-in-Physics-Research-Perez",
            "title": {
                "fragments": [],
                "text": "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84569456"
                        ],
                        "name": "Bart van Merri\u00ebnboer",
                        "slug": "Bart-van-Merri\u00ebnboer",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Merri\u00ebnboer",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bart van Merri\u00ebnboer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2292403"
                        ],
                        "name": "J. Chorowski",
                        "slug": "J.-Chorowski",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Chorowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Chorowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1862138"
                        ],
                        "name": "Dmitriy Serdyuk",
                        "slug": "Dmitriy-Serdyuk",
                        "structuredName": {
                            "firstName": "Dmitriy",
                            "lastName": "Serdyuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitriy Serdyuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93244959"
                        ],
                        "name": "D. Bogdanov",
                        "slug": "D.-Bogdanov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Bogdanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bogdanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3074927"
                        ],
                        "name": "Vincent Dumoulin",
                        "slug": "Vincent-Dumoulin",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Dumoulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Dumoulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923596"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 112492977,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e70dc6ad191734e60c0b2d98e434a13e807ccbc6",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blocks-and-Fuel-Merri\u00ebnboer-Chorowski",
            "title": {
                "fragments": [],
                "text": "Blocks and Fuel"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 284
                            }
                        ],
                        "text": "\u2026train a generative model against a classifier which attempts to distinguish generated samples from true data, and mixtures of conditional Gaussian scale mixtures (MCGSMs) [41] which describe a dataset using Gaussian scale mixtures, with parameters which depend on a sequence of causal neighborhoods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generative Adversarial Nets. Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Generative Adversarial Nets. Advances in Neural Information Processing Systems"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "In all cases the objective function and gradient were computed using Theano [4], and model training was with SFO [37]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 28
                            }
                        ],
                        "text": "Model training was with SFO (Sohl-Dickstein et al., 2014), except for the CIFAR-10 results in Figure 3 which used RMSprop."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast largescale optimization by unifying stochastic gradient and quasi-Newton methods"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 31st International Conference on Machine Learning"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "In [21, 11, 33, 31] variational learning and inference algorithms were developed which allow a flexible generative model and posterior distribution over latent variables to be directly trained against each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Andriy Mnih, Charles Blundell, and Daan Wierstra . Deep AutoRegressive Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Andriy Mnih, Charles Blundell, and Daan Wierstra . Deep AutoRegressive Networks"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Nonparametric methods [8] can also be very effective1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proper local scoring rules. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Proper local scoring rules. The Annals of Statistics"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "In all cases the objective function and gradient were computed using Theano [4], and model training was with SFO [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theano: a CPU and GPU math expression compiler"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Python for Scientific Computing Conference (SciPy)"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "\u2026which decompose the joint distribution into a sequence of tractable conditional distributions over each dimension, adversarial networks [10] which train a generative model against a classifier which attempts to distinguish generated samples from true data, and mixtures of conditional Gaussian\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. Advances in Neural Information Processing Systems"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 295
                            }
                        ],
                        "text": "Related ideas from physics include the Jarzynski equality [15] (known in machine learning as annealed importance sampling [27]), which uses a Markov chain which converts one distribution into another to compute the ratio of normalizing constants between the two distributions; Langevin dynamics [21] which show how to define a Gaussian diffusion process which has any target distribution as its equilibrium; and the Kolmogorov forward and backward equations [6] which show that forward and reverse diffusion processes can be described using the same functional form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sur la th{\u00e9}orie du mouvement brownien"
            },
            "venue": {
                "fragments": [],
                "text": "CR Acad. Sci. Paris,"
            },
            "year": 1908
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "In all cases the objective function and gradient were computed using Theano [4], and model training was with SFO [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theano: a CPU and GPU math expression com-  piler"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Python for Scientific Computing Conference (SciPy),"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "\u2026which converts one distribution into another to compute the ratio of normalizing constants between the two distributions; Langevin dynamics [23] which show how to define a Gaussian diffusion process which has any target distribution as its equilibrium; and the Kolmogorov forward and backward\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sur la th\u00e9orie du mouvement brownien"
            },
            "venue": {
                "fragments": [],
                "text": "CR Acad. Sci. Paris"
            },
            "year": 1908
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We demonstrate the utility of these diffusion probabilistic models by training high log likelihood models for a twodimensional swiss roll, binary sequence, handwritten digit (MNIST), and several natural image (CIFAR-10, bark, and dead leaves) datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reweighted Wake-Sleep. International Conference on Learning Representations"
            },
            "venue": {
                "fragments": [],
                "text": "Reweighted Wake-Sleep. International Conference on Learning Representations"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 83
                            }
                        ],
                        "text": "In [21, 11, 33, 31] variational learning and inference algorithms were developed which allow a flexible generative model and posterior distribution over latent variables to be directly trained against each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Auto-Encoding Variational Bayes. International Conference on Learning Representations"
            },
            "venue": {
                "fragments": [],
                "text": "Auto-Encoding Variational Bayes. International Conference on Learning Representations"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "\u2026\u2013 for instance mean field theory and its expansions [39, 40], variational Bayes [19], contrastive divergence [44, 13], minimum probability flow [36, 35], minimum KL contraction [28], proper scoring rules [9, 32], score matching [15], pseudolikelihood [5], loopy belief propagation [29], and many,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minimum Probability Flow Learning. International Conference on Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Minimum Probability Flow Learning. International Conference on Machine Learning"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yann Dauphin, and Salah Rifai. Better Mixing via Deep Representations"
            },
            "venue": {
                "fragments": [],
                "text": "Yann Dauphin, and Salah Rifai. Better Mixing via Deep Representations"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "This corresponds to the case of a quasi-static process in statistical physics [38, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fluctuation Relations : A Pedagogical Overview. arXiv preprint arXiv:1201"
            },
            "venue": {
                "fragments": [],
                "text": "Fluctuation Relations : A Pedagogical Overview. arXiv preprint arXiv:1201"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 3
                            }
                        ],
                        "text": "In (Rippel & Adams, 2013; Dinh et al., 2014) bijective deterministic maps are learned to a latent representation with a simple factorial density function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NICE: Non-linear Independent Components"
            },
            "venue": {
                "fragments": [],
                "text": "Estimation. arXiv:1410.8516,"
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 42,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 72,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-Unsupervised-Learning-using-Nonequilibrium-Sohl-Dickstein-Weiss/2dcef55a07f8607a819c21fe84131ea269cc2e3c?sort=total-citations"
}