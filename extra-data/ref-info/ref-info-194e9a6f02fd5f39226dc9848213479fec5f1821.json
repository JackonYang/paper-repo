{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717629"
                        ],
                        "name": "Yansong Feng",
                        "slug": "Yansong-Feng",
                        "structuredName": {
                            "firstName": "Yansong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yansong Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2022 Chapter 6 expends the paper \u201cHow Many Words Is a Picture Worth? Automatic Caption Generation for News Images\u201d (Feng and Lapata, 2010a), where we propose both extractive and abstractive models to render the image content (extracted by our probabilistic annotation model introduced in Chapter 5) into human-readable sentences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18650536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods."
            },
            "slug": "How-Many-Words-Is-a-Picture-Worth-Automatic-Caption-Feng-Lapata",
            "title": {
                "fragments": [],
                "text": "How Many Words Is a Picture Worth? Automatic Caption Generation for News Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results show that an abstractive model defined over phrases is superior to extractive methods."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717629"
                        ],
                        "name": "Yansong Feng",
                        "slug": "Yansong-Feng",
                        "structuredName": {
                            "firstName": "Yansong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yansong Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2022 Chapter 5 elaborates on the paper \u201cTopic Models for Image Annotation and Text Illustration\u201d (Feng and Lapata, 2010b), where we demonstrate how a classic topic model can be extended to perform the automatic image annotation task in our news dataset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12547672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60be767a255fd13f73ed4e64d9901b30cf6081e8",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Image annotation, the task of automatically generating description words for a picture, is a key component in various image search and retrieval applications. Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections. In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement. We describe a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. We show that this model outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of our dataset."
            },
            "slug": "Topic-Models-for-Image-Annotation-and-Text-Feng-Lapata",
            "title": {
                "fragments": [],
                "text": "Topic Models for Image Annotation and Text Illustration"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics is described, which outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of the dataset."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824057"
                        ],
                        "name": "Florent Monay",
                        "slug": "Florent-Monay",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Monay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florent Monay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403029865"
                        ],
                        "name": "D. G\u00e1tica-P\u00e9rez",
                        "slug": "D.-G\u00e1tica-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "G\u00e1tica-P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G\u00e1tica-P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Regions are then described by a standard set of features, including color, texture, and shape, and subsequently treated as continuous vectors (e.g., [5], [42]) or in quantized form (e.g., [3], [22])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "0162-8828/13/$31.00 2013 IEEE Published by the IEEE Computer Society\nor grammars for producing textual output."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15084283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c79b3086598da24bb26f7da043741666b03d9b9",
            "isKey": true,
            "numCitedBy": 207,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "To go beyond the query-by-example paradigm in image retrieval, there is a need for semantic indexing of large image collections for intuitive text-based image search. Different models have been proposed to learn the dependencies between the visual content of an image set and the associated text captions, then allowing for the automatic creation of semantic indexes for unannotated images. The task, however, remains unsolved. In this paper, we present three alternatives to learn a probabilistic latent semantic analysis (PLSA) model for annotated images and evaluate their respective performance for automatic image indexing. Under the PLSA assumptions, an image is modeled as a mixture of latent aspects that generates both image features and text captions, and we investigate three ways to learn the mixture of aspects. We also propose a more discriminative image representation than the traditional Blob histogram, concatenating quantized local color information and quantized local texture descriptors. The first learning procedure of a PLSA model for annotated images is a standard expectation-maximization (EM) algorithm, which implicitly assumes that the visual and the textual modalities can be treated equivalently. The other two models are based on an asymmetric PLSA learning, allowing to constrain the definition of the latent space on the visual or on the textual modality. We demonstrate that the textual modality is more appropriate to learn a semantically meaningful latent space, which translates into improved annotation performance. A comparison of our learning algorithms with respect to recent methods on a standard data set is presented, and a detailed evaluation of the performance shows the validity of our framework."
            },
            "slug": "Modeling-Semantic-Aspects-for-Cross-Media-Image-Monay-G\u00e1tica-P\u00e9rez",
            "title": {
                "fragments": [],
                "text": "Modeling Semantic Aspects for Cross-Media Image Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents three alternatives to learn a probabilistic latent semantic analysis (PLSA) model for annotated images and evaluates their respective performance for automatic image indexing, and proposes a more discriminative image representation than the traditional Blob histogram."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "228029e7533e32a025071e31e3f4f08d2bea5f5a",
            "isKey": false,
            "numCitedBy": 1301,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Libraries have traditionally used manual image annotation for indexing and then later retrieving their image collections. However, manual image annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content. Here, we propose an automatic approach to annotating and retrieving images based on a training set of images. We assume that regions in an image can be described using a small vocabulary of blobs. Blobs are generated from image features using clustering. Given a training set of images with annotations, we show that probabilistic models allow us to predict the probability of generating a word given the blobs in an image. This may be used to automatically annotate and retrieve images given a word as a query. We show that relevance models allow us to derive these probabilities in a natural way. Experiments show that the annotation performance of this cross-media relevance model is almost six times as good (in terms of mean precision) than a model based on word-blob co-occurrence model and twice as good as a state of the art model derived from machine translation. Our approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval."
            },
            "slug": "Automatic-image-annotation-and-retrieval-using-Jeon-Lavrenko",
            "title": {
                "fragments": [],
                "text": "Automatic image annotation and retrieval using cross-media relevance models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval by assuming that regions in an image can be described using a small vocabulary of blobs."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9197086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6eb3a15108dfdec25b46522ed94b866aeb156de9",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a semi-supervised model which segments and annotates images using very few labeled images and a large unaligned text corpus to relate image regions to text labels. Given photos of a sports event, all that is necessary to provide a pixel-level labeling of objects and background is a set of newspaper articles about this sport and one to five labeled images. Our model is motivated by the observation that words in text corpora share certain context and feature similarities with visual objects. We describe images using visual words, a new region-based representation. The proposed model is based on kernelized canonical correlation analysis which finds a mapping between visual and textual words by projecting them into a latent meaning space. Kernels are derived from context and adjective features inside the respective visual and textual domains. We apply our method to a challenging dataset and rely on articles of the New York Times for textual features. Our model outperforms the state-of-the-art in annotation. In segmentation it compares favorably with other methods that use significantly more labeled training data."
            },
            "slug": "Connecting-modalities:-Semi-supervised-segmentation-Socher-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A semi-supervised model which segments and annotates images using very few labeled images and a large unaligned text corpus to relate image regions to text labels and outperforms the state-of-the-art in annotation."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717629"
                        ],
                        "name": "Yansong Feng",
                        "slug": "Yansong-Feng",
                        "structuredName": {
                            "firstName": "Yansong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yansong Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16408584,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6ffc5d4e234c0f4f37b00492ae33cb5dfe65765",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The availability of databases of images labeled with keywords is necessary for developing and evaluating image annotation models. Dataset collection is however a costly and time consuming task. In this paper we exploit the vast resource of images available on the web. We create a database of pictures that are naturally embedded into news articles and propose to use their captions as a proxy for annotation keywords. Experimental results show that an image annotation model can be developed on this dataset alone without the overhead of manual annotation. We also demonstrate that the news article associated with the picture can be used to boost image annotation performance."
            },
            "slug": "Automatic-Image-Annotation-Using-Auxiliary-Text-Feng-Lapata",
            "title": {
                "fragments": [],
                "text": "Automatic Image Annotation Using Auxiliary Text Information"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper creates a database of pictures that are naturally embedded into news articles and proposes to use their captions as a proxy for annotation keywords, showing that an image annotation model can be developed on this dataset alone without the overhead of manual annotation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857558"
                        ],
                        "name": "Shaolei Feng",
                        "slug": "Shaolei-Feng",
                        "structuredName": {
                            "firstName": "Shaolei",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolei Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Developing dictionaries that specify exhaustively image-to-text correspondences is a difficult and time-consuming task that must be repeated for new domains and languages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3829888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba4e1089e2c5a1c12e9f6c2686e9c8d1870c718e",
            "isKey": false,
            "numCitedBy": 912,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Retrieving images in response to textual queries requires some knowledge of the semantics of the picture. Here, we show how we can do both automatic image annotation and retrieval (using one word queries) from images and videos using a multiple Bernoulli relevance model. The model assumes that a training set of images or videos along with keyword annotations is provided. Multiple keywords are provided for an image and the specific correspondence between a keyword and an image is not provided. Each image is partitioned into a set of rectangular regions and a real-valued feature vector is computed over these regions. The relevance model is a joint probability distribution of the word annotations and the image feature vectors and is computed using the training set. The word probabilities are estimated using a multiple Bernoulli model and the image feature probabilities using a non-parametric kernel density estimate. The model is then used to annotate images in a test set. We show experiments on both images from a standard Corel data set and a set of video key frames from NIST's video tree. Comparative experiments show that the model performs better than a model based on estimating word probabilities using the popular multinomial distribution. The results also show that our model significantly outperforms previously reported results on the task of image and video annotation."
            },
            "slug": "Multiple-Bernoulli-relevance-models-for-image-and-Feng-Manmatha",
            "title": {
                "fragments": [],
                "text": "Multiple Bernoulli relevance models for image and video annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work shows how it can do both automatic image annotation and retrieval (using one word queries) from images and videos using a multiple Bernoulli relevance model, which significantly outperforms previously reported results on the task of image and video annotation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2159982"
                        ],
                        "name": "A. Makadia",
                        "slug": "A.-Makadia",
                        "structuredName": {
                            "firstName": "Ameesh",
                            "lastName": "Makadia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Makadia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144658464"
                        ],
                        "name": "V. Pavlovic",
                        "slug": "V.-Pavlovic",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Pavlovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Pavlovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152663162"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 151206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f773aa2936dde24327704e1d8542702bdd5b9c1c",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatically assigning keywords to images is of great interest as it allows one to retrieve, index, organize and understand large collections of image data. Many techniques have been proposed for image annotation in the last decade that give reasonable performance on standard datasets. However, most of these works fail to compare their methods with simple baseline techniques to justify the need for complex models and subsequent training. In this work, we introduce a new and simple baseline technique for image annotation that treats annotation as a retrieval problem. The proposed technique utilizes global low-level image features and a simple combination of basic distance measures to find nearest neighbors of a given image. The keywords are then assigned using a greedy label transfer mechanism. The proposed baseline method outperforms the current state-of-the-art methods on two standard and one large Web dataset. We believe that such a baseline measure will provide a strong platform to compare and better understand future annotation techniques."
            },
            "slug": "Baselines-for-Image-Annotation-Makadia-Pavlovic",
            "title": {
                "fragments": [],
                "text": "Baselines for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a new and simple baseline technique for image annotation that treats annotation as a retrieval problem and outperforms the current state-of-the-art methods on two standard and one large Web dataset."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Due to polysemy and synonymy, many words in this vocabulary will refer to the same underlying concept."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Developing dictionaries that specify exhaustively image-to-text correspondences is a difficult and time-consuming task that must be repeated for new domains and languages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 575890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18f8820e2a5ca6273a39123c27c0745870cda057",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval."
            },
            "slug": "A-Model-for-Learning-the-Semantics-of-Pictures-Lavrenko-Manmatha",
            "title": {
                "fragments": [],
                "text": "A Model for Learning the Semantics of Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries using a formalism that models the generation of annotated images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2813866"
                        ],
                        "name": "M. \u00d6zcan",
                        "slug": "M.-\u00d6zcan",
                        "structuredName": {
                            "firstName": "Mert",
                            "lastName": "\u00d6zcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. \u00d6zcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144586159"
                        ],
                        "name": "Jie Luo",
                        "slug": "Jie-Luo",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033284"
                        ],
                        "name": "B. Caputo",
                        "slug": "B.-Caputo",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Caputo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caputo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Our approach leverages the vast resource of pictures available on the web and the fact that many of them naturally co-occur with topically related documents and are captioned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2313241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6adf1fb96a6a2399d7a6b816d3939e94fa86ae5b",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a large scale database of images and captions, designed for supporting research on how to use captioned images from the Web for training visual classifiers. It consists of more than 125,000 images of celebrities from different fields downloaded from the Web. Each image is associated to its original text caption, extracted from the html page the image comes from. We coin it FAN-Large, for Face And Names Large scale database. Its size and deliberate high level of noise makes it to our knowledge the largest and most realistic database supporting this type of research. The dataset and its annotations are publicly available and can be obtained from http://www.vision. ee.ethz.ch/~calvin/fanlarge/. We report results on a thorough assessment of FAN-Large using several existing approaches for name-face association, and present and evaluate new contextual features derived from the caption. Our findings provide important cues on the strengths and limitations of existing approaches."
            },
            "slug": "A-Large-Scale-Database-of-Images-and-Captions-for-\u00d6zcan-Luo",
            "title": {
                "fragments": [],
                "text": "A Large-Scale Database of Images and Captions for Automatic Face Naming"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A thorough assessment of FAN-Large is reported, using several existing approaches for name-face association, and present and evaluate new contextual features derived from the caption."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723884"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707259"
                        ],
                        "name": "J. Chai",
                        "slug": "J.-Chai",
                        "structuredName": {
                            "firstName": "Joyce",
                            "lastName": "Chai",
                            "middleNames": [
                                "Yue"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145388187"
                        ],
                        "name": "Luo Si",
                        "slug": "Luo-Si",
                        "structuredName": {
                            "firstName": "Luo",
                            "lastName": "Si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luo Si"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To render image regions more word-like, some existing algorithms (Mori et al., 1999; Duygulu et al., 2002; Barnard et al., 2002; Jeon et al., 2003; Jeon and Manmatha, 2004; Pan et al., 2004; Jin et al., 2004) cluster their features into a certain number of blobs (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Active learning has also been employed to reduce the size of the training database (Jin et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Normalized cuts (Shi and Malik, 2000) are widely used to segment images as they produce acceptable output for a wide range of occasions and non-surprisingly are popular in image annotation research (Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Jeon et al., 2003; Pan et al., 2004; Jin et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Many generative models that have been successfully applied in speech recognition, machine translation and information retrieval, have been ported to image annotation (Mori et al., 1999; Duygulu et al., 2002; Barnard and Forsyth, 2001; Wang and Li, 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Feng et al., 2004; Pan et al., 2004; Jin et al., 2004; Li and Wang, 2006, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In more recent work (Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Jeon et al., 2003; Lavrenko et al., 2004; Pan et al., 2004; Jin et al., 2004), frequently used visual features computed for each image region include region size, position, convexity, moment, average RGB value, average Lab value, standard deviation of RGB and Lab value, and oriented energy of different directions using Gabor filtering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9904485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c678c5626f9bc264b8d7626bb08630c76f77fa08",
            "isKey": true,
            "numCitedBy": 149,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Image annotations allow users to access a large image database with textual queries. There have been several studies on automatic image annotation utilizing machine learning techniques, which automatically learn statistical models from annotated images and apply them to generate annotations for unseen images. One common problem shared by most previous learning approaches for automatic image annotation is that each annotated word is predicated for an image independently from other annotated words. In this paper, we proposed a coherent language model for automatic image annotation that takes into account the word-to-word correlation by estimating a coherent language model for an image. This new approach has two important advantages: 1) it is able to automatically determine the annotation length to improve the accuracy of retrieval results, and 2) it can be used with active learning to significantly reduce the required number of annotated image examples. Empirical studies with Corel dataset are presented to show the effectiveness of the coherent language model for automatic image annotation."
            },
            "slug": "Effective-automatic-image-annotation-via-a-coherent-Jin-Chai",
            "title": {
                "fragments": [],
                "text": "Effective automatic image annotation via a coherent language model and active learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A coherent language model for automatic image annotation is proposed that takes into account the word-to-word correlation by estimating a coherent language models for an image to significantly reduce the required number of annotated image examples."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1888731"
                        ],
                        "name": "Mohsen Hejrati",
                        "slug": "Mohsen-Hejrati",
                        "structuredName": {
                            "firstName": "Mohsen",
                            "lastName": "Hejrati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohsen Hejrati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21160985"
                        ],
                        "name": "M. Sadeghi",
                        "slug": "M.-Sadeghi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Sadeghi",
                            "middleNames": [
                                "Amin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sadeghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052690705"
                        ],
                        "name": "Peter Young",
                        "slug": "Peter-Young",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125805"
                        ],
                        "name": "Cyrus Rashtchian",
                        "slug": "Cyrus-Rashtchian",
                        "structuredName": {
                            "firstName": "Cyrus",
                            "lastName": "Rashtchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cyrus Rashtchian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "They do this by relying on general world knowledge and expertise in current affairs that goes beyond what is described in the article or shown in the picture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "More generally, the ability to link images with textual descriptions would facilitate the retrieval and management of multimedia data (e.g., video and image collections, graphics) as well as increase the accessibility of the web for visually impaired (blind and partially sighted) users who cannot\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13272863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us-ingdata. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche."
            },
            "slug": "Every-Picture-Tells-a-Story:-Generating-Sentences-Farhadi-Hejrati",
            "title": {
                "fragments": [],
                "text": "Every Picture Tells a Story: Generating Sentences from Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A system that can compute a score linking an image to a sentence, which can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our annotation model takes these topic distributions into account while finding the most likely keywords for an image and its associated document."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 254
                            }
                        ],
                        "text": "\u2026and 2) obtaining the tight lower bound through minimizing the Kullback-Leibler (KL) divergence between the introduced variational distribution and the true posterior distribution (we refer interested readers to Blei et al. [47] and Blei [4] for more details on their inference procedure)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "This approach can create sentences of high quality that are both meaningful and fluent; however, the reliance on manually created resources largely limits the deployment of existing methods to real-world applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63473035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2dac26f6dacf5679a155f138b44a51081400f25",
            "isKey": true,
            "numCitedBy": 80,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Managing large and growing collections of information is a central goal of modern computer science. Data repositories of texts, images, sounds, and genetic information have become widely accessible, thus necessitating good methods of retrieval, organization, and exploration. In this thesis, we describe a suite of probabilistic models of information collections for which the above problems can be cast as statistical queries. \nWe use directed graphical models as a flexible, modular framework for describing appropriate modeling assumptions about the data. Fast approximate posterior inference algorithms based on variational methods free us from having to specify tractable models, and further allow us to take the Bayesian perspective, even in the face of large datasets. \nWith this framework in hand, we describe latent Dirichlet allocation (LDA), a graphical model particularly suited to analyzing text collections. LDA posits a finite index of hidden topics which describe the underlying documents. New documents are situated into the collection via approximate posterior inference of their associated index terms. Extensions to LDA can index a set of images, or multimedia collections of interrelated text and images. \nFinally, we describe nonparametric Bayesian methods for relaxing the assumption of a fixed number of topics, and develop models based on the natural assumption that the size of the index can grow with the collection. This idea is extended to trees, and to models which represent the hidden structure and content of a topic hierarchy that underlies a collection."
            },
            "slug": "Probabilistic-models-of-text-and-images-Blei-Jordan",
            "title": {
                "fragments": [],
                "text": "Probabilistic models of text and images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A suite of probabilistic models of information collections for which the above problems can be cast as statistical queries are described, and directed graphical models are used as a flexible, modular framework for describing appropriate modeling assumptions about the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145575177"
                        ],
                        "name": "G. Carneiro",
                        "slug": "G.-Carneiro",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Carneiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carneiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699559"
                        ],
                        "name": "N. Vasconcelos",
                        "slug": "N.-Vasconcelos",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Vasconcelos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vasconcelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, this \u201cone vs all\u201d model has a potential shortcoming as it needs strongly labeled training data, which means all objects appearing in the image must be annotated explicitly, since images with missing labels will negatively affect the accuracy of the classifiers that are trained for these missing labels (Carneiro and Vasconcelos, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative is to directly build an image annotation model on the extracted continuous visual features and use a Gaussian kernel distribution to model the image regions which improves the overall performance (Lavrenko et al., 2003; Wang and Li, 2002; Blei and Jordan, 2003; Feng et al., 2004; Lavrenko et al., 2004; Li and Wang, 2003; Carneiro and Vasconcelos, 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The use of MIL greatly reduces the negative effect of weak labeling (Maron and Ratan, 1998; Carneiro and Vasconcelos, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In other work (Feng et al., 2004; Jeon and Manmatha, 2004; Carneiro and Vasconcelos, 2005; Wang and Li, 2002; Li and Wang, 2003, 2006) rectangular blocks are used as a proxy for the image regions, and only the RGB, Lab features, and texture energy (responses of Gabor filtering, DCT filtering or wavelet transformation) are considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6876487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e12fc90004ae8b0fc83eba38f101d8b5c54740",
            "isKey": true,
            "numCitedBy": 180,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new method to automatically annotate and retrieve images using a vocabulary of image semantics. The novel contributions include a discriminant formulation of the problem, a multiple instance learning solution that enables the estimation of concept probability distributions without prior image segmentation, and a hierarchical description of the density of each image class that enables very efficient training. Compared to current methods of image annotation and retrieval, the one now proposed has significantly smaller time complexity and better recognition performance. Specifically, its recognition complexity is O(C/spl times/R), where C is the number of classes (or image annotations) and R is the number of image regions, while the best results in the literature have complexity O(T/spl times/R), where T is the number of training images. Since the number of classes grows substantially slower than that of training images, the proposed method scales better during training, and processes test images faster This is illustrated through comparisons in terms of complexity, time, and recognition performance with current state-of-the-art methods."
            },
            "slug": "Formulating-semantic-image-annotation-as-a-learning-Carneiro-Vasconcelos",
            "title": {
                "fragments": [],
                "text": "Formulating semantic image annotation as a supervised learning problem"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A new method to automatically annotate and retrieve images using a vocabulary of image semantics that has significantly smaller time complexity and better recognition performance than current methods of image annotation and retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "Regions are then described by a standard set of features, including color, texture, and shape, and subsequently treated as continuous vectors (e.g., [5], [42]) or in quantized form (e.g., [3], [22])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 165
                            }
                        ],
                        "text": "This approach can create sentences of high quality that are both meaningful and fluent; however, the reliance on manually created resources largely limits the deployment of existing methods to real-world applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 868535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e",
            "isKey": true,
            "numCitedBy": 1760,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
            },
            "slug": "Matching-Words-and-Pictures-Barnard-Sahin",
            "title": {
                "fragments": [],
                "text": "Matching Words and Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text, is presented, and a number of models for the joint distribution of image regions and words are developed, including several which explicitly learn the correspondence between regions and Words."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2170746"
                        ],
                        "name": "M. Hodosh",
                        "slug": "M.-Hodosh",
                        "structuredName": {
                            "firstName": "Micah",
                            "lastName": "Hodosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hodosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052690705"
                        ],
                        "name": "Peter Young",
                        "slug": "Peter-Young",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125805"
                        ],
                        "name": "Cyrus Rashtchian",
                        "slug": "Cyrus-Rashtchian",
                        "structuredName": {
                            "firstName": "Cyrus",
                            "lastName": "Rashtchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cyrus Rashtchian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3196382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01915181692c821cc5a0a703047bd5b07c1f9af5",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image 'understanding' would also require identifying their attributes, relations and activities. Since this information cannot be conveyed by simple keywords, we have collected a corpus of \"action\" photos each associated with five descriptive captions. In order to obtain a consistent semantic representation for each image, we need to first identify which NPs refer to the same entities. We present three hierarchical Bayesian models for cross-caption coreference resolution. We have also created a simple ontology of entity classes that appear in images and evaluate how well these can be recovered."
            },
            "slug": "Cross-Caption-Coreference-Resolution-for-Automatic-Hodosh-Young",
            "title": {
                "fragments": [],
                "text": "Cross-Caption Coreference Resolution for Automatic Image Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A corpus of \"action\" photos each associated with five descriptive captions is collected and a simple ontology of entity classes that appear in images is created to evaluate how well these can be recovered."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35788904"
                        ],
                        "name": "Sung Ju Hwang",
                        "slug": "Sung-Ju-Hwang",
                        "structuredName": {
                            "firstName": "Sung",
                            "lastName": "Hwang",
                            "middleNames": [
                                "Ju"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sung Ju Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Our approach leverages the vast resource of pictures available on the web and the fact that many of them naturally co-occur with topically related documents and are captioned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16736499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c10a15e52c85654db9c9343ae1dd892a2ac4a279",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an approach to image retrieval and auto-tagging that leverages the implicit information about object importance conveyed by the list of keyword tags a person supplies for an image. We propose an unsupervised learning procedure based on Kernel Canonical Correlation Analysis that discovers the relationship between how humans tag images (e.g., the order in which words are mentioned) and the relative importance of objects and their layout in the scene. Using this discovered connection, we show how to boost accuracy for novel queries, such that the search results better preserve the aspects a human may find most worth mentioning. We evaluate our approach on three datasets using either keyword tags or natural language descriptions, and quantify results with both ground truth parameters as well as direct tests with human subjects. Our results show clear improvements over approaches that either rely on image features alone, or that use words and image features but ignore the implied importance cues. Overall, our work provides a novel way to incorporate high-level human perception of scenes into visual representations for enhanced image search."
            },
            "slug": "Learning-the-Relative-Importance-of-Objects-from-Hwang-Grauman",
            "title": {
                "fragments": [],
                "text": "Learning the Relative Importance of Objects from Tagged Images for Retrieval and Cross-Modal Search"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An unsupervised learning procedure based on Kernel Canonical Correlation Analysis is proposed that discovers the relationship between how humans tag images and the relative importance of objects and their layout in the scene."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629707"
                        ],
                        "name": "Amr Ahmed",
                        "slug": "Amr-Ahmed",
                        "structuredName": {
                            "firstName": "Amr",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amr Ahmed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34088891"
                        ],
                        "name": "R. Murphy",
                        "slug": "R.-Murphy",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Murphy",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7262932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57fec656119e82b5e70b1a654f6d87d8c1137ef4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A major source of information (often the most crucial and informative part) in scholarly articles from scientific journals, proceedings and books are the figures that directly provide images and other graphical illustrations of key experimental results and other scientific contents. In biological articles, a typical figure often comprises multiple panels, accompanied by either scoped or global captioned text. Moreover, the text in the caption contains important semantic entities such as protein names, gene ontology, tissues labels, etc., relevant to the images in the figure. Due to the avalanche of biological literature in recent years, and increasing popularity of various bio-imaging techniques, automatic retrieval and summarization of biological information from literature figures has emerged as a major unsolved challenge in computational knowledge extraction and management in the life science. We present a new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figures, and we derive an efficient inference algorithm based on collapsed Gibbs sampling for information retrieval and visualization. The resulting program constitutes one of the key IR engines in our SLIF system that has recently entered the final round (4 out 70 competing systems) of the Elsevier Grand Challenge on Knowledge Enhancement in the Life Science. Here we present various evaluations on a number of data mining tasks to illustrate our method."
            },
            "slug": "Structured-correspondence-topic-models-for-mining-Ahmed-Xing",
            "title": {
                "fragments": [],
                "text": "Structured correspondence topic models for mining captioned figures in biological literature"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figures is presented, and an efficient inference algorithm based on collapsed Gibbs sampling for information retrieval and visualization is derived."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145970060"
                        ],
                        "name": "Ahmet Aker",
                        "slug": "Ahmet-Aker",
                        "structuredName": {
                            "firstName": "Ahmet",
                            "lastName": "Aker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmet Aker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718590"
                        ],
                        "name": "R. Gaizauskas",
                        "slug": "R.-Gaizauskas",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gaizauskas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gaizauskas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "Our innovation is to exploit this implicit information and treat the surrounding document and caption words as labels for the image, thus reducing the need for human supervision."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Due to their prominence, journalists are given explicit instructions on how to write good captions.2 The latter must be succinct and informative, clearly identify the subject of the picture, establish its relevance to the article, and provide some context for the picture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5223711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8dbc756ea246f599250c09e3efd9bba9909a842",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple web-documents that contain information related to an image's location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns."
            },
            "slug": "Generating-Image-Descriptions-Using-Dependency-Aker-Gaizauskas",
            "title": {
                "fragments": [],
                "text": "Generating Image Descriptions Using Dependency Relational Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2003), or averagely cut rectangles (Feng et al., 2004; Li and Wang, 2003; Wang and Li, 2002; Jeon and Manmatha, 2004))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To render image regions more word-like, some existing algorithms (Mori et al., 1999; Duygulu et al., 2002; Barnard et al., 2002; Jeon et al., 2003; Jeon and Manmatha, 2004; Pan et al., 2004; Jin et al., 2004) cluster their features into a certain number of blobs (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In other work (Feng et al., 2004; Jeon and Manmatha, 2004; Carneiro and Vasconcelos, 2005; Wang and Li, 2002; Li and Wang, 2003, 2006) rectangular blocks are used as a proxy for the image regions, and only the RGB, Lab features, and texture energy (responses of Gabor filtering, DCT filtering or wavelet transformation) are considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8522062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125477f471e0a8fef582c6c491c2a0b6b8334a50",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose the use of the Maximum Entropy approach for the task of automatic image annotation. Given labeled training data, Maximum Entropy is a statistical technique which allows one to predict the probability of a label given test data. The techniques allow for relationships between features to be effectively captured. and has been successfully applied to a number of language tasks including machine translation. In our case, we view the image annotation task as one where a training data set of images labeled with keywords is provided and we need to automatically label the test images with keywords. To do this, we first represent the image using a language of visterms and then predict the probability of seeing an English word given the set of visterms forming the image. Maximum Entropy allows us to compute the probability and in addition allows for the relationships between visterms to be incorporated. The experimental results show that Maximum Entropy outperforms one of the classical translation models that has been applied to this task and the Cross Media Relevance Model. Since the Maximum Entropy model allows for the use of a large number of predicates to possibly increase performance even further, Maximum Entropy model is a promising model for the task of automatic image annotation."
            },
            "slug": "Using-Maximum-Entropy-for-Automatic-Image-Jeon-Manmatha",
            "title": {
                "fragments": [],
                "text": "Using Maximum Entropy for Automatic Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The experimental results show that Maximum Entropy outperforms one of the classical translation models that has been applied to this task and the Cross Media Relevance Model."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2004053"
                        ],
                        "name": "Vicente Ordonez",
                        "slug": "Vicente-Ordonez",
                        "structuredName": {
                            "firstName": "Vicente",
                            "lastName": "Ordonez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vicente Ordonez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564333"
                        ],
                        "name": "Girish Kulkarni",
                        "slug": "Girish-Kulkarni",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Kulkarni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "The former stage analyzes the content of the image and identifies \u201cwhat to say\u201d (i.e., which events or objects are worth talking about), whereas the second stage determines \u201chow to say it\u201d (i.e., how to render the selected content into natural language text)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14579301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e080b98efbe65c02a116439205ca2344b9f7cd4",
            "isKey": false,
            "numCitedBy": 735,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset \u2013 performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning."
            },
            "slug": "Im2Text:-Describing-Images-Using-1-Million-Ordonez-Kulkarni",
            "title": {
                "fragments": [],
                "text": "Im2Text: Describing Images Using 1 Million Captioned Photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new objective performance measure for image captioning is introduced and methods incorporating many state of the art, but fairly noisy, estimates of image content are developed to produce even more pleasing results."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47713710"
                        ],
                        "name": "Benjamin Z. Yao",
                        "slug": "Benjamin-Z.-Yao",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Yao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Z. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112063737"
                        ],
                        "name": "Xiong Yang",
                        "slug": "Xiong-Yang",
                        "structuredName": {
                            "firstName": "Xiong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110901865"
                        ],
                        "name": "Liang Lin",
                        "slug": "Liang-Lin",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "The literature is littered with various attempts to learn the associations between image features and words using supervised\nclassification [1], [2], instantiations of the noisy-channel model [3], latent variable models [4], [5], [6], and models inspired by information retrieval [7], [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Caption generation, image annotation, summarization, topic models\n\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6023198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05e074abddd3fe987b9bebd46f6cf4bf8465c37e",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an image parsing to text description (I2T) framework that generates text descriptions of image and video content based on image understanding. The proposed I2T framework follows three steps: 1) input images (or video frames) are decomposed into their constituent visual patterns by an image parsing engine, in a spirit similar to parsing sentences in natural language; 2) the image parsing results are converted into semantic representation in the form of Web ontology language (OWL), which enables seamless integration with general knowledge bases; and 3) a text generation engine converts the results from previous steps into semantically meaningful, human readable, and query-able text reports. The centerpiece of the I2T framework is an and-or graph (AoG) visual knowledge representation, which provides a graphical representation serving as prior knowledge for representing diverse visual patterns and provides top-down hypotheses during the image parsing. The AoG embodies vocabularies of visual elements including primitives, parts, objects, scenes as well as a stochastic image grammar that specifies syntactic relations (i.e., compositional) and semantic relations (e.g., categorical, spatial, temporal, and functional) between these visual elements. Therefore, the AoG is a unified model of both categorical and symbolic representations of visual knowledge. The proposed I2T framework has two objectives. First, we use semiautomatic method to parse images from the Internet in order to build an AoG for visual knowledge representation. Our goal is to make the parsing process more and more automatic using the learned AoG model. Second, we use automatic methods to parse image/video in specific domains and generate text reports that are useful for real-world applications. In the case studies at the end of this paper, we demonstrate two automatic I2T systems: a maritime and urban scene video surveillance system and a real-time automatic driving scene understanding system."
            },
            "slug": "I2T:-Image-Parsing-to-Text-Description-Yao-Yang",
            "title": {
                "fragments": [],
                "text": "I2T: Image Parsing to Text Description"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An image parsing to text description (I2T) framework that generates text descriptions of image and video content based on image understanding and uses automatic methods to parse image/video in specific domains and generate text reports that are useful for real-world applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302320"
                        ],
                        "name": "Florian Schroff",
                        "slug": "Florian-Schroff",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Schroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Schroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9680304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd32111096e78055e935344142a9ac66daa9a55f",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this work is to automatically generate a large number of images for a specified object class (for example, penguin). A multi-modal approach employing both text, meta data and visual features is used to gather many, high-quality images from the Web. Candidate images are obtained by a text based Web search querying on the object identifier (the word penguin). The Web pages and the images they contain are downloaded. The task is then to remove irrelevant images and re-rank the remainder. First, the images are re-ranked using a Bayes posterior estimator trained on the text surrounding the image and meta data features (such as the image alternative tag, image title tag, and image filename). No visual information is used at this stage. Second, the top-ranked images are used as (noisy) training data and a SVM visual classifier is learnt to improve the ranking further. The principal novelty is in combining text/meta-data and visual features in order to achieve a completely automatic ranking of the images. Examples are given for a selection of animals (e.g. camels, sharks, penguins), vehicles (cars, airplanes, bikes) and other classes (guitar, wristwatch), totalling 18 classes. The results are assessed by precision/recall curves on ground truth annotated data and by comparison to previous approaches including those of Berg et al. (on an additional six classes) and Fergus et al."
            },
            "slug": "Harvesting-Image-Databases-from-the-Web-Schroff-Criminisi",
            "title": {
                "fragments": [],
                "text": "Harvesting Image Databases from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A multi-modal approach employing both text, meta data and visual features is used to gather many, high-quality images from the Web to automatically generate a large number of images for a specified object class."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1943594"
                        ],
                        "name": "Jia-Yu Pan",
                        "slug": "Jia-Yu-Pan",
                        "structuredName": {
                            "firstName": "Jia-Yu",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia-Yu Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97598888"
                        ],
                        "name": "Hyung-Jeong Yang",
                        "slug": "Hyung-Jeong-Yang",
                        "structuredName": {
                            "firstName": "Hyung-Jeong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyung-Jeong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702392"
                        ],
                        "name": "C. Faloutsos",
                        "slug": "C.-Faloutsos",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Faloutsos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Faloutsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly, standard latent semantic analysis (LSA) and its probabilistic variant (PLSA), are applied to address the multimodal relations from a multimodal space consisting of image features combined with the corresponding keywords (Pan et al., 2004; Monay and Gatica-Perez, 2003, 2007; Fergus et al., 2005; Sivic et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To render image regions more word-like, some existing algorithms (Mori et al., 1999; Duygulu et al., 2002; Barnard et al., 2002; Jeon et al., 2003; Jeon and Manmatha, 2004; Pan et al., 2004; Jin et al., 2004) cluster their features into a certain number of blobs (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Normalized cuts (Shi and Malik, 2000) are widely used to segment images as they produce acceptable output for a wide range of occasions and non-surprisingly are popular in image annotation research (Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Jeon et al., 2003; Pan et al., 2004; Jin et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "These models span from the standard latent semantic analysis (LSA, Zhao and Grosky 2003; Pan et al. 2004) and its probabilistic variant (PLSA, Monay and Gatica-Perez 2003, 2007; Fergus et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Many generative models that have been successfully applied in speech recognition, machine translation and information retrieval, have been ported to image annotation (Mori et al., 1999; Duygulu et al., 2002; Barnard and Forsyth, 2001; Wang and Li, 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Feng et al., 2004; Pan et al., 2004; Jin et al., 2004; Li and Wang, 2006, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In more recent work (Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Jeon et al., 2003; Lavrenko et al., 2004; Pan et al., 2004; Jin et al., 2004), frequently used visual features computed for each image region include region size, position, convexity, moment, average RGB value, average Lab value, standard deviation of RGB and Lab value, and oriented energy of different directions using Gabor filtering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2592435,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "id": "cc58597d33d5ff5a51d7810512e2b90b056840ca",
            "isKey": true,
            "numCitedBy": 120,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine the problem of automatic image captioning. Given a training set of captioned images, we want to discover correlations between image features and keywords, so that we can automatically find good keywords for a new image. We experiment thoroughly with multiple design alternatives on large datasets of various content styles, and our proposed methods achieve up to a 45% relative improvement on captioning accuracy over the state of the art."
            },
            "slug": "Automatic-image-captioning-Pan-Yang",
            "title": {
                "fragments": [],
                "text": "Automatic image captioning"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This work examines the problem of automatic image captioning and proposes methods to discover correlations between image features and keywords, so that it can automatically find good keywords for a new image."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27426,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In other work (Feng et al., 2004; Jeon and Manmatha, 2004; Carneiro and Vasconcelos, 2005; Wang and Li, 2002; Li and Wang, 2003, 2006) rectangular blocks are used as a proxy for the image regions, and only the RGB, Lab features, and texture energy (responses of Gabor filtering, DCT filtering or wavelet transformation) are considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2003), or averagely cut rectangles (Feng et al., 2004; Li and Wang, 2003; Wang and Li, 2002; Jeon and Manmatha, 2004))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One solution is to cluster semantically related words into categories (Duygulu et al., 2002; Wang and Li, 2002; Li and Wang, 2003, 2006; Barnard et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Many generative models that have been successfully applied in speech recognition, machine translation and information retrieval, have been ported to image annotation (Mori et al., 1999; Duygulu et al., 2002; Barnard and Forsyth, 2001; Wang and Li, 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Feng et al., 2004; Pan et al., 2004; Jin et al., 2004; Li and Wang, 2006, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative is to directly build an image annotation model on the extracted continuous visual features and use a Gaussian kernel distribution to model the image regions which improves the overall performance (Lavrenko et al., 2003; Wang and Li, 2002; Blei and Jordan, 2003; Feng et al., 2004; Lavrenko et al., 2004; Li and Wang, 2003; Carneiro and Vasconcelos, 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3715563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7ce4bb8d5a700ec96d5e87bb31a20b3627fd9b6",
            "isKey": true,
            "numCitedBy": 68,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic linguistic indexing of pictures is an important but highly challenging problem for researchers in computer vision and content-based image retrieval. In this paper, we introduce a statistical modeling approach to this problem. Categorized images are used to train a dictionary of hundreds of concepts automatically based on statistical modeling. Images of any given concept category are regarded as instances of a stochastic process that characterizes the category. To measure the extent of association between an image and the textual description of a category of images, the likelihood of the occurrence of the image based on the stochastic process derived from the category is computed. A high likelihood indicates a strong association. In our experimental implementation, the ALIP (Automatic Linguistic Indexing of Pictures) system, we focus on a particular group of stochastic processes for describing images, that is, the two-dimensional multiresolution hidden Markov models (2-D MHMMs). We implemented and tested the system on a photographic image database of 600 different semantic cat- egories, each with about 40 training images. Tested using 3,000 images outside the training database, the system has demonstrated good accuracy and high potential in linguistic indexing of these test images."
            },
            "slug": "Learning-based-linguistic-indexing-of-pictures-with-Wang-Li",
            "title": {
                "fragments": [],
                "text": "Learning-based linguistic indexing of pictures with 2--d MHMMs"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A statistical modeling approach to automatic linguistic indexing of pictures by focusing on a particular group of stochastic processes for describing images, that is, the two-dimensional multiresolution hidden Markov models (2-D MHMMs)."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824057"
                        ],
                        "name": "Florent Monay",
                        "slug": "Florent-Monay",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Monay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florent Monay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403029865"
                        ],
                        "name": "D. G\u00e1tica-P\u00e9rez",
                        "slug": "D.-G\u00e1tica-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "G\u00e1tica-P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G\u00e1tica-P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the Corel dataset, the average annotation length is less than 5 words while Monay and Gatica-Perez (2007) extract averagely 240 visual terms per image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Mori et al. (1999) simply count the most frequent co-occurring words for each image region."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Mean Average Precision is the mean of the Average Precision scores for a group of queries (more details can be found in Monay and Gatica-Perez (2007))"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While Monay and Gatica-Perez (2007) report large performance differences among the three models on the Corel dataset, we find that when taking accompanying documents into account, these models yield similar results on our dataset."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1007967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c253729d6170b31972ded6bfec1ea502f3ff86e",
            "isKey": true,
            "numCitedBy": 281,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Image auto-annotation, i.e., the association of words to whole images, has attracted considerable attention. In particular, unsupervised, probabilistic latent variable models of text and image features have shown encouraging results, but their performance with respect to other approaches remains unknown. In this paper, we apply and compare two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA). Annotation strategies for each model are discussed. Remarkably, we found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods. Furthermore, non-probabilistic methods (LSA and direct image matching) outperformed PLSA on the same dataset."
            },
            "slug": "On-image-auto-annotation-with-latent-space-models-Monay-G\u00e1tica-P\u00e9rez",
            "title": {
                "fragments": [],
                "text": "On image auto-annotation with latent space models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper applies and compares two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA), and found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly, standard latent semantic analysis (LSA) and its probabilistic variant (PLSA), are applied to address the multimodal relations from a multimodal space consisting of image features combined with the corresponding keywords (Pan et al., 2004; Monay and Gatica-Perez, 2003, 2007; Fergus et al., 2005; Sivic et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2004) and its probabilistic variant (PLSA, Monay and Gatica-Perez 2003, 2007; Fergus et al. 2005; Sivic et al. 2005; Bosch 2007), latent dirichlet allocation and its variants (LDA, Blei and Jordan 2003; Barnard et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2359343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97c2c77cd76ed176683fac79c115729ad45482bc",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].1 1This work was sponsored in part by the EU Project CogViSys, the University of Oxford, Shell Oil, and the National Geospatial-Intelligence Agency."
            },
            "slug": "Discovering-object-categories-in-image-collections-Sivic-Russell",
            "title": {
                "fragments": [],
                "text": "Discovering object categories in image collections"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Given a set of images containing multiple object categories, this work seeks to discover those categories and their image locations without supervision using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746841"
                        ],
                        "name": "Nicolas Usunier",
                        "slug": "Nicolas-Usunier",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Usunier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Usunier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Given a query, search engines usually retrieve relevant pictures by analyzing the image caption (if it exists), textual descriptions found adjacent to the image, and other text-related factors such as the file name of the image and clickthrough data (Weston et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7587705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aea0f946e8dcddb65cc2e907456c42453f246a50",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced \u201csibling\u201d precision metric, where our method also obtains excellent results."
            },
            "slug": "Large-scale-image-annotation:-learning\u00a0to\u00a0rank-Weston-Bengio",
            "title": {
                "fragments": [],
                "text": "Large scale image annotation: learning\u00a0to\u00a0rank with\u00a0joint word-image embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work proposes a strongly performing method that scales to image annotation datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This task, on its own, is of significant importance for many image-based applications, such as image retrieval, picture browsing support, and story picturing (Lavrenko et al., 2003; Jeon et al., 2003; Blei and Jordan, 2003; Joshi et al., 2006; Li and Wang, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09932c4c63a2b15987baec125eb70440aaa9c9d6",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Developing effective methods for automated annotation of digital pictures continues to challenge computer scientists. The capability of annotating pictures by computers can lead to breakthroughs in a wide range of applications, including Web image search, online picture-sharing communities, and scientific experiments. In this work, the authors developed new optimization and estimation techniques to address two fundamental problems in machine learning. These new techniques serve as the basis for the automatic linguistic indexing of pictures - real time (ALIPR) system of fully automatic and high-speed annotation for online pictures. In particular, the D2-clustering method, in the same spirit as K-Means for vectors, is developed to group objects represented by bags of weighted vectors. Moreover, a generalized mixture modeling technique (kernel smoothing as a special case) for nonvector data is developed using the novel concept of hypothetical local mapping (HLM). ALIPR has been tested by thousands of pictures from an Internet photo-sharing site, unrelated to the source of those pictures used in the training process. Its performance has also been studied at an online demonstration site, where arbitrary users provide pictures of their choices and indicate the correctness of each annotation word. The experimental results show that a single computer processor can suggest annotation terms in real time and with good accuracy."
            },
            "slug": "Real-Time-Computerized-Annotation-of-Pictures-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Real-Time Computerized Annotation of Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "New optimization and estimation techniques to address two fundamental problems in machine learning are developed that serve as the basis for the automatic linguistic indexing of pictures - real time (ALIPR) system of fully automatic and high-speed annotation for online pictures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47852085"
                        ],
                        "name": "S. Moran",
                        "slug": "S.-Moran",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Moran",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Moran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similar improvements include introducing beamsearch during the iterations over the whole vocabulary and then discarding impossible candidate words according to already selected keywords (Moran, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64444466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f17686ade65e7dcaf9e8e90b1842762a8fdc0a2",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic Image Tagging seeks to assign relevant words (e.g. \u201cjungle\u201d, \u201cboat\u201d, \u201ctrees\u201d) to images that describe the actual content found in the images without intermediate manual labelling. Current approaches are largely based on categorization, and treat the tags independently, so an annotation (jungle,trees) is just as plausible as (jungle,snow). The goal of this dissertation was to develop a probabilistic model (the Continuous Relevance Model) to take into account the dependencies between keywords so as to provide more precise annotations. The main findings suggest that, under certain conditions, taking into account keyword correlation, coupled with an efficient method (beam search) to search over sets of tags is an effective method to increase annotation accuracy."
            },
            "slug": "Automatic-Image-Tagging-Moran",
            "title": {
                "fragments": [],
                "text": "Automatic Image Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main findings suggest that, under certain conditions, taking into account keyword correlation, coupled with an efficient method (beam search) to search over sets of tags is an effective method to increase annotation accuracy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 221
                            }
                        ],
                        "text": "Although these approaches may achieve better results all things being equal, they highly depend on strongly labeled training data, and correspondingly their accuracy is closely related to the quality of the training data (Holub, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 143
                            }
                        ],
                        "text": ", partially labeled data, and can relatively easily deal with changes of dataset and keyword vocabulary, compared to discriminative approaches (Ulusoy and Bishop, 2005; Holub, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59723731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce198cbb4dd0486597ee643b2467a16b4bc72d6a",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The number and variety of online news sources makes it difficult for people to track the news concerning even a single event. Redundancy causes such tracking to be extremely time-consuming: multiple news feeds on the same event tend to contain similar information. A summary of such news feeds can present important information in one short text, dramatically reducing reading time. The focus of this thesis is information fusion, a technique which, given multiple documents, identifies redundant information and synthesizes a coherent summary. This technique is embodied in MultiGen, a system that I have designed, implemented and evaluated over the course of my Ph.D. Unlike previous work in the area, MultiGen is a domain-independent system: it generates news summaries on a variety of topics in any domain. Another contribution to the state of the art is that the system generates the summary by reusing and altering phrases from the input articles, creating a more fluent and cohesive text. This is in contrast with other existing systems, which simply extract sentences from input articles and concatenate them together, leading to fluency problems. Currently MultiGen operates as part of Columbia's Newsblaster system. Everyday, Newsblaster downloads all news articles from a variety of sources, clusters articles by topic, and generates a cohesive, readable automatic summary of each document cluster. \nOne key challenge in multidocument summarization is eliminating redundant information in the produced summaries. Articles about the same event often contain descriptions of the same fact using different wording. To address this issue, we need a method to identify paraphrases\u2014fragments of text that convey similar meaning even if they are not identical in wording. Automatic identification of paraphrases was not addressed in previous research, although it is necessary for many applications, including question-answering, information extraction and natural language generation. This thesis presents unsupervised learning techniques to identify paraphrases given a corpus of multiple parallel texts. This type of corpus provides many instances of paraphrasing, because these texts preserve the meaning of the original source, but may use different words to convey the meaning. Both the data and the method are departures from past approaches to corpus based techniques. Our evaluation experiments show that the algorithm extracts paraphrases with high accuracy and significantly outperforms a state of the art algorithm developed for related tasks in machine translation."
            },
            "slug": "Information-fusion-for-multidocument-summarization:-McKeown-Barzilay",
            "title": {
                "fragments": [],
                "text": "Information fusion for multidocument summarization: paraphrasing and generation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This thesis presents unsupervised learning techniques to identify paraphrasing given a corpus of multiple parallel texts, and shows that the algorithm extracts paraphrases with high accuracy and significantly outperforms a state of the art algorithm developed for related tasks in machine translation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059257793"
                        ],
                        "name": "Jo\u00e3o Freitas",
                        "slug": "Jo\u00e3o-Freitas",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Freitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 188
                            }
                        ],
                        "text": "Regions are then described by a standard set of features, including color, texture, and shape, and subsequently treated as continuous vectors (e.g., [5], [42]) or in quantized form (e.g., [3], [22])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12561212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d9f55b445f36578802e7eef4393cfa914b11620",
            "isKey": false,
            "numCitedBy": 1765,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach."
            },
            "slug": "Object-Recognition-as-Machine-Translation:-Learning-Sahin-Barnard",
            "title": {
                "fragments": [],
                "text": "Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows how to cluster words that individually are difficult to predict into clusters that can be predicted well, and cannot predict the distinction between train and locomotive using the current set of features, but can predict the underlying concept."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797588"
                        ],
                        "name": "K. Deschacht",
                        "slug": "K.-Deschacht",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Deschacht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Deschacht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145446752"
                        ],
                        "name": "Marie-Francine Moens",
                        "slug": "Marie-Francine-Moens",
                        "structuredName": {
                            "firstName": "Marie-Francine",
                            "lastName": "Moens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marie-Francine Moens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10201935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d994a18440983067612ecc47a71b0f6bef808fe",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to automatically annotate images using associated text. We detect and classify all entities (persons and objects) in the text after which we determine the salience (the importance of an entity in a text) and visualness (the extent to which an entity can be perceived visually) of these entities. We combine these measures to compute the probability that an entity is present in the image. The suitability of our approach was successfully tested on 50 image-text pairs of Yahoo! News."
            },
            "slug": "Text-Analysis-for-Automatic-Image-Annotation-Deschacht-Moens",
            "title": {
                "fragments": [],
                "text": "Text Analysis for Automatic Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel approach to automatically annotate images using associated text that detects and classify all entities in the text and combines these measures to compute the probability that an entity is present in the image."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118343423"
                        ],
                        "name": "Sonal Gupta",
                        "slug": "Sonal-Gupta",
                        "structuredName": {
                            "firstName": "Sonal",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sonal Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2130350426"
                        ],
                        "name": "Joohyun Kim",
                        "slug": "Joohyun-Kim",
                        "structuredName": {
                            "firstName": "Joohyun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joohyun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 212623440,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01c23e9c697f209cd1fa9a645d50059305a07161",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing visual scenes and activities is challenging: often visual cues alone are ambiguous, and it is expensive to obtain manually labeled examples from which to learn. To cope with these constraints, we propose to leverage the text that often accompanies visual data to learn robust models of scenes and actions from partially labeled collections. Our approach uses co-training, a semi-supervised learning method that accommodates multi-modal views of data. To classify images, our method learns from captioned images of natural scenes; and to recognize human actions, it learns from videos of athletic events with commentary. We show that by exploiting both multi-modal representations and unlabeled data our approach learns more accurate image and video classifiers than standard baseline algorithms."
            },
            "slug": "Watch,-Listen-&-Learn:-Co-training-on-Captioned-and-Gupta-Kim",
            "title": {
                "fragments": [],
                "text": "Watch, Listen & Learn: Co-training on Captioned Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to leverage the text that often accompanies visual data to learn robust models of scenes and actions from partially labeled collections, and shows that by exploiting both multi-modal representations and unlabeled data this approach learns more accurate image and video classifiers than standard baseline algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108881999"
                        ],
                        "name": "Chong Wang",
                        "slug": "Chong-Wang",
                        "structuredName": {
                            "firstName": "Chong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "This approach can create sentences of high quality that are both meaningful and fluent; however, the reliance on manually created resources largely limits the deployment of existing methods to real-world applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "Due to polysemy and synonymy, many words in this vocabulary will refer to the same underlying concept."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14362511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fdf31d5ebdd293b3027e6555e256a936ff5515a",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Image classification and annotation are important problems in computer vision, but rarely considered together. Intuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely annotated with words \u201croad,\u201d \u201ccar,\u201d and \u201ctraffic\u201d than words \u201cfish,\u201d \u201cboat,\u201d and \u201cscuba.\u201d In this paper, we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation performance, and superior classification performance."
            },
            "slug": "Simultaneous-image-classification-and-annotation-Wang-Blei",
            "title": {
                "fragments": [],
                "text": "Simultaneous image classification and annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new probabilistic model for jointly modeling the image, its class label, and its annotations is developed, which derives an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758740"
                        ],
                        "name": "T. Westerveld",
                        "slug": "T.-Westerveld",
                        "structuredName": {
                            "firstName": "Thijs",
                            "lastName": "Westerveld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Westerveld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52630096"
                        ],
                        "name": "A. DeVries",
                        "slug": "A.-DeVries",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "DeVries",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. DeVries"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14337671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5b9e5204d68d3c0f111e8fbd0a29b238abf4881",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "textabstractWe present evaluation results of a generative probabilistic image retrieval model using `easy data'. Previous research into our model's retrieval effectiveness has used the test collection developed at TREC's Video Track, but as discussed in detail in [WeVr:SIGIR:03], its search task has been too difficult to measure actual performance of the retrieval model. The `easy data' experiments presented here evaluate our model under varying model parameters on the Corel set. The Corel data set is relatively easy because images are nicely grouped into coherent themes, the within theme similarity is high and the across theme similarity relatively low. These properties make Corel a nice vehicle for testing, presenting or selling new content based retrieval techniques and models. In contrast to the TREC data, the Corel collection gives statistically significant differences between varying experimental conditions, so we get more insight in the model's behaviour. We then discuss at length the limitations of the results obtained using this data set, comparing the experiments performed here to those on the TREC data."
            },
            "slug": "Experimental-evaluation-of-a-generative-image-model-Westerveld-DeVries",
            "title": {
                "fragments": [],
                "text": "Experimental evaluation of a generative probabilistic image retrieval model on 'easy' data"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Evaluation results of a generative probabilistic image retrieval model using `easy data' are presented, and the limitations of the results obtained using this data set are discussed, comparing the experiments performed here to those on the TREC data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857558"
                        ],
                        "name": "Shaolei Feng",
                        "slug": "Shaolei-Feng",
                        "structuredName": {
                            "firstName": "Shaolei",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolei Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative is to directly build an image annotation model on the extracted continuous visual features and use a Gaussian kernel distribution to model the image regions which improves the overall performance (Lavrenko et al., 2003; Wang and Li, 2002; Blei and Jordan, 2003; Feng et al., 2004; Lavrenko et al., 2004; Li and Wang, 2003; Carneiro and Vasconcelos, 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In more recent work (Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Jeon et al., 2003; Lavrenko et al., 2004; Pan et al., 2004; Jin et al., 2004), frequently used visual features computed for each image region include region size, position, convexity, moment, average RGB value, average Lab value, standard deviation of RGB and Lab value, and oriented energy of different directions using Gabor filtering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3847457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fad43474e4fa71eb2527ef30adfa7bb7870baa83",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply a continuous relevance model (CRM) to the problem of directly retrieving the visual content of videos using text queries. The model computes a joint probability model for image features and words using a training set of annotated images. The model may then be used to annotate unseen test images. The probabilistic annotations are used for retrieval using text queries. We also propose a modified model - the normalized CRM - which substantially improves performance on a subset of the TREC video dataset."
            },
            "slug": "Statistical-models-for-automatic-video-annotation-Lavrenko-Feng",
            "title": {
                "fragments": [],
                "text": "Statistical models for automatic video annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A continuous relevance model is applied to the problem of directly retrieving the visual content of videos using text queries and a modified model is proposed - the normalized CRM - which substantially improves performance on a subset of the TREC video dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5868929"
                        ],
                        "name": "J. Tang",
                        "slug": "J.-Tang",
                        "structuredName": {
                            "firstName": "Jiayu",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773066"
                        ],
                        "name": "P. Lewis",
                        "slug": "P.-Lewis",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Lewis",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 132
                            }
                        ],
                        "text": "It is therefore relatively easy to learn the associations between images and keywords and do well on annotation and retrieval tasks (Tang and Lewis, 2007; Westerveld and de Vries, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "5), thus allowing models to learn image-keyword associations reliably (Tang and Lewis, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "It is therefore relatively easy to learn the associations between images and keywords and do well on annotation and retrieval tasks (Tang and Lewis, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18243979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44cadceb7fa1959c44af0f4ae6a9be7f3d265316",
            "isKey": true,
            "numCitedBy": 50,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The Corel Image set is widely used for image annotation performance evaluation although it has been claimed that Corel images are relatively easy to annotate. The aim of this paper is to demonstrate some of the disadvantages of datasets like the Corel set for effective auto-annotation evaluation. We first compare the performance of several annotation algorithms using the Corel set and find that simple near neighbor propagation techniques perform fairly well. A support vector machine (SVM)-based annotation method achieves even better results, almost as good as the best found in the literature. We then build a new image collection using the Yahoo Image Search engine and query-by-single-word searches to create a more challenging annotated set automatically. Then, using three very different image annotation methods, we demonstrate some of the problems of annotation using the Corel set compared with the Yahoo-based training set. In both cases the training sets are used to create a set of annotations for the Corel test set"
            },
            "slug": "A-Study-of-Quality-Issues-for-Image-Auto-Annotation-Tang-Lewis",
            "title": {
                "fragments": [],
                "text": "A Study of Quality Issues for Image Auto-Annotation With the Corel Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper compares the performance of several annotation algorithms using the Corel set and finds that simple near neighbor propagation techniques perform fairly well and a support vector machine (SVM)-based annotation method achieves even better results, almost as good as the best found in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2003), or averagely cut rectangles (Feng et al., 2004; Li and Wang, 2003; Wang and Li, 2002; Jeon and Manmatha, 2004))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative is to directly build an image annotation model on the extracted continuous visual features and use a Gaussian kernel distribution to model the image regions which improves the overall performance (Lavrenko et al., 2003; Wang and Li, 2002; Blei and Jordan, 2003; Feng et al., 2004; Lavrenko et al., 2004; Li and Wang, 2003; Carneiro and Vasconcelos, 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3028284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f39d3e88cce063ccd3ca01100efd44dcabc9d3b4",
            "isKey": false,
            "numCitedBy": 1187,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic linguistic indexing of pictures is an important but highly challenging problem for researchers in computer vision and content-based image retrieval. In this paper, we introduce a statistical modeling approach to this problem. Categorized images are used to train a dictionary of hundreds of statistical models each representing a concept. Images of any given concept are regarded as instances of a stochastic process that characterizes the concept. To measure the extent of association between an image and the textual description of a concept, the likelihood of the occurrence of the image based on the characterizing stochastic process is computed. A high likelihood indicates a strong association. In our experimental implementation, we focus on a particular group of stochastic processes, that is, the two-dimensional multiresolution hidden Markov models (2D MHMMs). We implemented and tested our ALIP (Automatic Linguistic Indexing of Pictures) system on a photographic image database of 600 different concepts, each with about 40 training images. The system is evaluated quantitatively using more than 4,600 images outside the training database and compared with a random annotation scheme. Experiments have demonstrated the good accuracy of the system and its high potential in linguistic indexing of photographic images."
            },
            "slug": "Automatic-Linguistic-Indexing-of-Pictures-by-a-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Automatic Linguistic Indexing of Pictures by a Statistical Modeling Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper implemented and tested the ALIP (Automatic Linguistic Indexing of Pictures) system on a photographic image database of 600 different concepts, each with about 40 training images and demonstrated the good accuracy of the system and its high potential in linguistic indexing of photographic images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33421444"
                        ],
                        "name": "Quanfu Fan",
                        "slug": "Quanfu-Fan",
                        "structuredName": {
                            "firstName": "Quanfu",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quanfu Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3246962"
                        ],
                        "name": "R. Swaminathan",
                        "slug": "R.-Swaminathan",
                        "structuredName": {
                            "firstName": "Ranjini",
                            "lastName": "Swaminathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Swaminathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642913"
                        ],
                        "name": "A. Hoogs",
                        "slug": "A.-Hoogs",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Hoogs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hoogs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34265270"
                        ],
                        "name": "Roderic Collins",
                        "slug": "Roderic-Collins",
                        "structuredName": {
                            "firstName": "Roderic",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roderic Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073488753"
                        ],
                        "name": "Pascale Rondot",
                        "slug": "Pascale-Rondot",
                        "structuredName": {
                            "firstName": "Pascale",
                            "lastName": "Rondot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascale Rondot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781137"
                        ],
                        "name": "J. Kaufhold",
                        "slug": "J.-Kaufhold",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kaufhold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kaufhold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 100
                            }
                        ],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14110947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be4ca1d090363b6b641e01b3d895d6c380fb0603",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe present a new data set of 1014 images with manual segmentations and semantic labels for each segment, together with a methodology for using this kind of data for recognition evaluation. The images and segmentations are from the UCB segmentation benchmark database (Martin et\u00a0al., in International conference on computer vision, vol.\u00a0II, pp.\u00a0416\u2013421, 2001). The database is extended by manually labeling each segment with its most specific semantic concept in WordNet (Miller et\u00a0al., in Int. J. Lexicogr. 3(4):235\u2013244, 1990). The evaluation methodology establishes protocols for mapping algorithm specific localization (e.g., segmentations) to our data, handling synonyms, scoring matches at different levels of specificity, dealing with vocabularies with sense ambiguity (the usual case), and handling ground truth regions with multiple labels. Given these protocols, we develop two evaluation approaches. The first measures the range of semantics that an algorithm can recognize, and the second measures the frequency that an algorithm recognizes semantics correctly. The data, the image labeling tool, and programs implementing our evaluation strategy are all available on-line (kobus.ca//research/data/IJCV_2007).\n\nWe apply this infrastructure to evaluate four algorithms which learn to label image regions from weakly labeled data. The algorithms tested include two variants of multiple instance learning (MIL), and two generative multi-modal mixture models. These experiments are on a significantly larger scale than previously reported, especially in the case of MIL methods. More specifically, we used training data sets up to 37,000 images and training vocabularies of up to 650 words.\n\nWe found that one of the mixture models performed best on image annotation and the frequency correct measure, and that variants of MIL gave the best semantic range performance. We were able to substantively improve the performance of MIL methods on the other tasks (image annotation and frequency correct region labeling) by providing an appropriate prior.\n"
            },
            "slug": "Evaluation-of-Localized-Semantics:-Data,-and-Barnard-Fan",
            "title": {
                "fragments": [],
                "text": "Evaluation of Localized Semantics: Data, Methodology, and Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new data set of 1014 images with manual segmentations and semantic labels for each segment is presented, together with a methodology for using this kind of data for recognition evaluation, and four algorithms which learn to label image regions from weakly labeled data are evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747647"
                        ],
                        "name": "S. Santini",
                        "slug": "S.-Santini",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Santini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Santini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722619"
                        ],
                        "name": "Amarnath Gupta",
                        "slug": "Amarnath-Gupta",
                        "structuredName": {
                            "firstName": "Amarnath",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amarnath Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938740"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "Experimental results show that it is viable to generate captions that are pertinent to the specific content of an image and its associated article, while permitting creativity in the description."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2827898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b7c4096ed697696a5f4fc8f3a6a750dc0cdecfe",
            "isKey": false,
            "numCitedBy": 6727,
            "numCiting": 410,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap."
            },
            "slug": "Content-Based-Image-Retrieval-at-the-End-of-the-Smeulders-Worring",
            "title": {
                "fragments": [],
                "text": "Content-Based Image Retrieval at the End of the Early Years"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap are discussed, as well as aspects of system engineering: databases, system architecture, and evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5113463"
                        ],
                        "name": "D. Joshi",
                        "slug": "D.-Joshi",
                        "structuredName": {
                            "firstName": "Dhiraj",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we will show how the proposed image annotation framework can be applied to text illustration (Joshi et al., 2006), a task which has received less attention in the literature, but is routinely performed by news writers who often have to search large image collections in order to find suitable pictures for their text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This task, on its own, is of significant importance for many image-based applications, such as image retrieval, picture browsing support, and story picturing (Lavrenko et al., 2003; Jeon et al., 2003; Blei and Jordan, 2003; Joshi et al., 2006; Li and Wang, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The majority of these are based on visual similarity, instance-based learning (Barnard and Forsyth, 2001) and typically use visual rankingbased schemes (Joshi et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 404001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7d6366c90fe8f1fe87c3ad3abbed5945bf9a979",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented."
            },
            "slug": "The-Story-Picturing-Engine---a-system-for-automatic-Joshi-Wang",
            "title": {
                "fragments": [],
                "text": "The Story Picturing Engine---a system for automatic text illustration"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An unsupervised approach to automated story picturing by extracting semantic keywords from the story, an annotated image database is searched and a novel image ranking scheme automatically determines the importance of each image."
            },
            "venue": {
                "fragments": [],
                "text": "TOMCCAP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761880"
                        ],
                        "name": "A. Haghighi",
                        "slug": "A.-Haghighi",
                        "structuredName": {
                            "firstName": "Aria",
                            "lastName": "Haghighi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Haghighi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909300"
                        ],
                        "name": "Lucy Vanderwende",
                        "slug": "Lucy-Vanderwende",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vanderwende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vanderwende"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 161
                            }
                        ],
                        "text": ", 2007; Haghighi and Vanderwende, 2009), syntactic features (such as clause information or dependencies) (Barzilay and McKeown, 2005), and topic representations (Daum\u00e9 III and Marcu, 2006; Haghighi and Vanderwende, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 77
                            }
                        ],
                        "text": "Existing document content representations include simple unigram frequencies (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007; Haghighi and Vanderwende, 2009), syntactic features (such as clause information or dependencies) (Barzilay and McKeown, 2005), and topic representations (Daum\u00e9 III and Marcu, 2006; Haghighi and Vanderwende, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 678258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9065d03256bcc962938f81eb795e70db214c459c",
            "isKey": false,
            "numCitedBy": 441,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HierSum yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system. We also explore HierSum's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation."
            },
            "slug": "Exploring-Content-Models-for-Multi-Document-Haghighi-Vanderwende",
            "title": {
                "fragments": [],
                "text": "Exploring Content Models for Multi-Document Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions and yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state of theart discriminative system."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152732685"
                        ],
                        "name": "Jianping Fan",
                        "slug": "Jianping-Fan",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026151"
                        ],
                        "name": "Yuli Gao",
                        "slug": "Yuli-Gao",
                        "structuredName": {
                            "firstName": "Yuli",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuli Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704919"
                        ],
                        "name": "Hangzai Luo",
                        "slug": "Hangzai-Luo",
                        "structuredName": {
                            "firstName": "Hangzai",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hangzai Luo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another solution is to construct a hierarchical structure to describe the relations among the words using WordNet (Miller, 1995) or other structured lexicon databases (Fan et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", sheep is a sub-category of animal, human can drive a car, etc (Fan et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16398174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad6b8a25007c3339e99e908d994710cfb4f88a34",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a hierarchical classification framework has been proposed for bridging the semantic gap effectively and achieving multi-level image annotation automatically. First, the semantic gap between the low-level computable visual features and users' real information needs is partitioned into four smaller gaps, and multiple approachesallare proposed to bridge these smaller gaps more effectively. To learn more reliable contextual relationships between the atomic image concepts and the co-appearances of salient objects, a multi-modal boosting algorithm is proposed. To enable hierarchical image classification and avoid inter-level error transmission, a hierarchical boosting algorithm is proposed by incorporating concept ontology and multi-task learning to achieve hierarchical image classifier training with automatic error recovery. To bridge the gap between the computable image concepts and the users' real information needs, a novel hyperbolic visualization framework is seamlessly incorporated to enable intuitive query specification and evaluation by acquainting the users with a good global view of large-scale image collections. Our experiments on large-scale image databases have also obtained very positive results."
            },
            "slug": "Hierarchical-classification-for-automatic-image-Fan-Gao",
            "title": {
                "fragments": [],
                "text": "Hierarchical classification for automatic image annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel hyperbolic visualization framework is seamlessly incorporated to enable intuitive query specification and evaluation by acquainting the users with a good global view of large-scale image collections."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115414"
                        ],
                        "name": "A. Nenkova",
                        "slug": "A.-Nenkova",
                        "structuredName": {
                            "firstName": "Ani",
                            "lastName": "Nenkova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nenkova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2824779"
                        ],
                        "name": "S. Maskey",
                        "slug": "S.-Maskey",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Maskey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maskey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1614038854"
                        ],
                        "name": "Yang Liu",
                        "slug": "Yang-Liu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 47179013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbd66cbe4014c24c6a5223017dbe79186358636e",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 344,
            "paperAbstract": {
                "fragments": [],
                "text": "It has now been 50 years since the publication of Luhn\u2019s seminal paper on automatic summarization. During these years the practical need for automatic summarization has become increasingly urgent and numerous papers have been published on the topic. As a result, it has become harder to find a single reference that gives an overview of past efforts or a complete view of summarization tasks and necessary system components. This article attempts to fill this void by providing a comprehensive overview of research in summarization, including the more traditional efforts in sentence extraction as well as the most novel recent approaches for determining important content, for domain and genre specific summarization and for evaluation of summarization. We also discuss the challenges that remain open, in particular the need for language generation and deeper semantic understanding of language that would be necessary for future advances in the field. We would like to thank the anonymous reviewers, our students and Noemie Elhadad, Hongyan Jing, Julia Hirschberg, Annie Louis, Smaranda Muresan and Dragomir Radev for their helpful feedback. This paper was supported in part by the U.S. National Science Foundation (NSF) under IIS-05-34871 and CAREER 09-53445. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. Full text available at: http://dx.doi.org/10.1561/1500000015"
            },
            "slug": "Automatic-Summarization-Nenkova-Maskey",
            "title": {
                "fragments": [],
                "text": "Automatic Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The challenges that remain open, in particular the need for language generation and deeper semantic understanding of language that would be necessary for future advances in the field are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40490812"
                        ],
                        "name": "R. Datta",
                        "slug": "R.-Datta",
                        "structuredName": {
                            "firstName": "Ritendra",
                            "lastName": "Datta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Datta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5113463"
                        ],
                        "name": "D. Joshi",
                        "slug": "D.-Joshi",
                        "structuredName": {
                            "firstName": "Dhiraj",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the literature, normalized cuts is reported to achieve better segmentation results in many applications compared to other methods (Datta et al., 2008) ."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Generally speaking, most previous contentbased image retrieval systems extract both global features such as a color histogram, and local features including object shape, size, texture, position and so on (Datta et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Daum\u00e9 III and Marcu (2006), also Haghighi and Vanderwende (2009), propose models based on the idea that the document\u2019s content can be represented by the distribution of topics corresponding to coarse-gained categories such as education, sports, and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "More recent research has placed emphasis on a relatively simpler approach, namely automatic image annotation, which can be considered as an approximation of the full image understanding problem by addressing the main objects or events instead of every objects in the image (Datta et al., 2008) (see Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7060187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dfa5679a15d0125ecec8539b79e8ba0babb8f73",
            "isKey": true,
            "numCitedBy": 3618,
            "numCiting": 325,
            "paperAbstract": {
                "fragments": [],
                "text": "We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research."
            },
            "slug": "Image-retrieval:-Ideas,-influences,-and-trends-of-Datta-Joshi",
            "title": {
                "fragments": [],
                "text": "Image retrieval: Ideas, influences, and trends of the new age"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation are surveyed, and the spawning of related subfields are discussed, to discuss the adaptation of existing image retrieval techniques to build systems that can be useful in the real world."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40490812"
                        ],
                        "name": "R. Datta",
                        "slug": "R.-Datta",
                        "structuredName": {
                            "firstName": "Ritendra",
                            "lastName": "Datta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Datta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5964689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24b0d89b1df469be8aa12fb797081be87251727a",
            "isKey": false,
            "numCitedBy": 531,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": "The last decade has witnessed great interest in research on content-based image retrieval. This has paved the way for a large number of new techniques and systems, and a growing interest in associated fields to support such systems. Likewise, digital imagery has expanded its horizon in many directions, resulting in an explosion in the volume of image data required to be organized. In this paper, we discuss some of the key contributions in the current decade related to image retrieval and automated image annotation, spanning 120 references. We also discuss some of the key challenges involved in the adaptation of existing image retrieval techniques to build useful systems that can handle real-world data. We conclude with a study on the trends in volume and impact of publications in the field with respect to venues/journals and sub-topics."
            },
            "slug": "Content-based-image-retrieval:-approaches-and-of-Datta-Li",
            "title": {
                "fragments": [],
                "text": "Content-based image retrieval: approaches and trends of the new age"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Some of the key contributions in the current decade related to image retrieval and automated image annotation are discussed, spanning 120 references, and a study on the trends in volume and impact of publications in the field with respect to venues/journals and sub-topics is concluded."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564333"
                        ],
                        "name": "Girish Kulkarni",
                        "slug": "Girish-Kulkarni",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Kulkarni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128210"
                        ],
                        "name": "Visruth Premraj",
                        "slug": "Visruth-Premraj",
                        "structuredName": {
                            "firstName": "Visruth",
                            "lastName": "Premraj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Visruth Premraj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985883"
                        ],
                        "name": "S. Dhar",
                        "slug": "S.-Dhar",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Dhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50341924"
                        ],
                        "name": "Siming Li",
                        "slug": "Siming-Li",
                        "structuredName": {
                            "firstName": "Siming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Furthermore, image descriptions tend to be concise, focusing on the most important depicted objects or events."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The aim is to create news-worthy text that draws the reader into the accompanying article rather than enumerating the objects in the picture and how they relate to each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18124397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d896605fbf93315b68d4ee03be0770077f84e40",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work."
            },
            "slug": "Baby-Talk-:-Understanding-and-Generating-Image-Kulkarni-Premraj",
            "title": {
                "fragments": [],
                "text": "Baby Talk : Understanding and Generating Image Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision that is very effective at producing relevant sentences for images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144244342"
                        ],
                        "name": "Xiaojun Qi",
                        "slug": "Xiaojun-Qi",
                        "structuredName": {
                            "firstName": "Xiaojun",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojun Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687343"
                        ],
                        "name": "Yutao Han",
                        "slug": "Yutao-Han",
                        "structuredName": {
                            "firstName": "Yutao",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yutao Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some methods adopt a \u201done vs all\u201d model (Vailaya et al., 1999; Maron and Ratan, 1998; Qi and Han, 2007; Chai and Hung, 2008; Gupta et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18399921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8fe23ea0225daa42c1bb9690b6602345ee144e3",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Incorporating-multiple-SVMs-for-automatic-image-Qi-Han",
            "title": {
                "fragments": [],
                "text": "Incorporating multiple SVMs for automatic image annotation"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564333"
                        ],
                        "name": "Girish Kulkarni",
                        "slug": "Girish-Kulkarni",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Kulkarni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128210"
                        ],
                        "name": "Visruth Premraj",
                        "slug": "Visruth-Premraj",
                        "structuredName": {
                            "firstName": "Visruth",
                            "lastName": "Premraj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Visruth Premraj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985883"
                        ],
                        "name": "S. Dhar",
                        "slug": "S.-Dhar",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Dhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50341924"
                        ],
                        "name": "Siming Li",
                        "slug": "Siming-Li",
                        "structuredName": {
                            "firstName": "Siming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10116609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "169b847e69c35cfd475eb4dcc561a24de11762ca",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work."
            },
            "slug": "Baby-talk:-Understanding-and-generating-simple-Kulkarni-Premraj",
            "title": {
                "fragments": [],
                "text": "Baby talk: Understanding and generating simple image descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision that is very effective at producing relevant sentences for images."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113355660"
                        ],
                        "name": "M. Corio",
                        "slug": "M.-Corio",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Corio",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Corio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806207"
                        ],
                        "name": "G. Lapalme",
                        "slug": "G.-Lapalme",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Lapalme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lapalme"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "We focus on captioned images embedded in news articles, and learn both models of content selection and surface realization from data without requiring expensive manual annotation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14723110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6885c80317cb7b134baec0cc8063aac6caa08e4e",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describeSelTex a text generation system for producing short texts and captions to accomp any information graphics that are generated according to the writer\u2019s intentio ns.SelTex uses rules that were extracted from a corpus study of more than 400 text excerpts. This corp us study shows that text and graphics play complementary roles in transmitting information from th e writer to the reader. We then derive some observations for the automatic generation of texts associat ed with graphics many of which were implemented inSelTex. For the past few years, we have studied the automatic generat ion of graphics from statistical data in the context of thePostGraphe system [5, 6].PostGraphe is given data in tabular form as might be found in a spreadsheet; also input is a declaration of the types of val ues in the columns of the table. The user then indicates the intentions to be conveyed in the graphics (e.g . compare two variables or show the evolution of a set of variables) and the system generates a report in L ATEX with the appropriate PostScript graphic files. PostGraphe also generates an accompanying text for only a few simple tex t schemas. Although PostGraphe has shown the potential of graphic generation, its text gene ration capabilities are well below its level of graphic competence. So we decided to redesign th e text generation module to better capture the subtle nature of the interaction between text and graphics. Before designingSelTex, the new text module to be integrated inPostGraphe, we did a corpus study of more than 400 texts associated with g raphics. The preliminary findings were presented in [3]. They will be r eviewed here briefly before describing the text generation rules that were deduced from this corpus stu dy. We will focus on the selection criteria for relevant facts that are expressed in the text that accompani es graphic. 1 Overview ofPostGraphe Many sophisticated tools can be used to create presentation s usi g statistical graphs. However, most of them focus on producing professional-looking graphics without trying to help the user to organize the presentation. To help in this aspect, we have built PostGraphe which generates a report integrating graphics and text from a set of writer\u2019s intentions. In figure 1, the writer\u2019s intentions have an important effect on the way information is presented. Indeed, the figure shows the same data presented in two different ways according to what the writer wants to convey. One part of the example shows how to present the evolution of t he data and the other part shows how to compare its various elements. The writer\u2019s intentions can be classified according to two ba sic criteria: structural differences and contents differences. Figure 1 illustrates the structural dif ference between evolution and comparison. We refer 1971 1972 1973 1974 1975 1976 0 6 12 18"
            },
            "slug": "Generation-of-texts-for-information-graphics-Corio-Lapalme",
            "title": {
                "fragments": [],
                "text": "Generation of texts for information graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "SelTex a text generation system for producing short texts and captions to accomp any information graphics that are generated according to the writer\u2019s intentio ns is described and it is shown that text and graphics play complementary roles in transmitting information from th e writer to the reader."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Other approaches include sentence compression which helps make the summary more concentrate on the gist hence avoid redundancies (Knight and Marcu, 2002; Clarke, 2008), and sentence fusion which provides a flexible framework for both single- and multi- document summarization by taking syntactic information into account (Barzilay and McKeown, 2005; Filippova, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7793213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa07fa673d8c908e91d22c4566572a72548ccee9",
            "isKey": false,
            "numCitedBy": 520,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Summarization-beyond-sentence-extraction:-A-to-Knight-Marcu",
            "title": {
                "fragments": [],
                "text": "Summarization beyond sentence extraction: A probabilistic approach to sentence compression"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2635321"
                        ],
                        "name": "Josiah Wang",
                        "slug": "Josiah-Wang",
                        "structuredName": {
                            "firstName": "Josiah",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josiah Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686341"
                        ],
                        "name": "K. Markert",
                        "slug": "K.-Markert",
                        "structuredName": {
                            "firstName": "Katja",
                            "lastName": "Markert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Markert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Our approach leverages the vast resource of pictures available on the web and the fact that many of them naturally co-occur with topically related documents and are captioned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1801271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a251dac6589a83e0bbcf9bef9a80c21222aeecbb",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the task of learning models for visual object recognition from natural language descriptions alone. The approach contributes to the recognition of fine-grain object categories, such as animal and plant species, where it may be difficult to collect many images for training, but where textual descriptions of visual attributes are readily available. As an example we tackle recognition of butterfly species, learning models from descriptions in an online nature guide. We propose natural language processing methods for extracting salient visual attributes from these descriptions to use as \u2018templates\u2019 for the object categories, and apply vision methods to extract corresponding attributes from test images. A generative model is used to connect textual terms in the learnt templates to visual attributes. We report experiments comparing the performance of humans and the proposed method on a dataset of ten butterfly categories."
            },
            "slug": "Learning-Models-for-Object-Recognition-from-Natural-Wang-Markert",
            "title": {
                "fragments": [],
                "text": "Learning Models for Object Recognition from Natural Language Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes natural language processing methods for extracting salient visual attributes from natural language descriptions to use as \u2018templates\u2019 for the object categories, and applies vision methods to extract corresponding attributes from test images."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "When estimating P(VI|s), the probability of image regions given the current entry, Lavrenko et al. (2003) reasonably assume Gaussian kernel distributions for the generation of image regions:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Lavrenko et al. (2003) adapt the idea of estimating the conditional probability P(w|RelevantDocuments) through query words to the scenario of image annotation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ContRel: our second baseline is the continuous relevance model introduced in Lavrenko et al. (2003). This model is trained solely on image-caption pairs and uses the same settings described in Chapter 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although these results are not strictly comparable with ours due to the different nature of the dataset (in addition, we output 10 annotation words, whereas Lavrenko et al. (2003) output 5), they give some indication of the decrease in performance incurred when using a more challenging dataset."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Generative approaches for modeling relevance have been actively investigated in information retrieval (Lavrenko, 2004; Lavrenko and Croft, 2001; Metzler et al., 2004), and applied to the image annotation task (Lavrenko et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1487333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24fdbaa40e00ab39660abda6acb82a5c453fca29",
            "isKey": true,
            "numCitedBy": 75,
            "numCiting": 219,
            "paperAbstract": {
                "fragments": [],
                "text": "A modern information retrieval system must have the capability to find, organize and present very different manifestations of information such as text, pictures, videos or database records any of which may be of relevance to the user. However, the concept of relevance, while seemingly intuitive, is actually hard to define, and it's even harder to model in a formal way. Lavrenko does not attempt to bring forth a new definition of relevance, nor provide arguments as to why any particular definition might be theoretically superior or more complete. Instead, he takes a widely accepted, albeit somewhat conservative definition, makes several assumptions, and from them develops a new probabilistic model that explicitly captures that notion of relevance. With this book, he makes two major contributions to the field of information retrieval: first, a new way to look at topical relevance, complementing the two dominant models, i.e., the classical probabilistic model and the language modeling approach, and which explicitly combines documents, queries, and relevance in a single formalism; second, a new method for modeling exchangeable sequences of discrete random variables which does not make any structural assumptions about the data and which can also handle rare events. Thus his book is of major interest to researchers and graduate students in information retrieval who specialize in relevance modeling, ranking algorithms, and language modeling."
            },
            "slug": "A-Generative-Theory-of-Relevance-Lavrenko",
            "title": {
                "fragments": [],
                "text": "A Generative Theory of Relevance"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book makes two major contributions to the field of information retrieval: first, a new way to look at topical relevance, complementing the two dominant models, i.e., the classical probabilistic model and the language modeling approach, and which explicitly combines documents, queries, and relevance in a single formalism."
            },
            "venue": {
                "fragments": [],
                "text": "The Information Retrieval Series"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21160985"
                        ],
                        "name": "M. Sadeghi",
                        "slug": "M.-Sadeghi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Sadeghi",
                            "middleNames": [
                                "Amin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sadeghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15433626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec97294c1e5974c6b827f8fda67f2e96cf1d8339",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce visual phrases, complex visual composites like \u201ca person riding a horse\u201d. Visual phrases often display significantly reduced visual complexity compared to their component objects, because the appearance of those objects can change profoundly when they participate in relations. We introduce a dataset suitable for phrasal recognition that uses familiar PASCAL object categories, and demonstrate significant experimental gains resulting from exploiting visual phrases. We show that a visual phrase detector significantly outperforms a baseline which detects component objects and reasons about relations, even though visual phrase training sets tend to be smaller than those for objects. We argue that any multi-class detection system must decode detector outputs to produce final results; this is usually done with non-maximum suppression. We describe a novel decoding procedure that can account accurately for local context without solving difficult inference problems. We show this decoding procedure outperforms the state of the art. Finally, we show that decoding a combination of phrasal and object detectors produces real improvements in detector results."
            },
            "slug": "Recognition-using-visual-phrases-Sadeghi-Farhadi",
            "title": {
                "fragments": [],
                "text": "Recognition using visual phrases"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a visual phrase detector significantly outperforms a baseline which detects component objects and reasons about relations, even though visual phrase training sets tend to be smaller than those for objects."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144586159"
                        ],
                        "name": "Jie Luo",
                        "slug": "Jie-Luo",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033284"
                        ],
                        "name": "B. Caputo",
                        "slug": "B.-Caputo",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Caputo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caputo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Our approach leverages the vast resource of pictures available on the web and the fact that many of them naturally co-occur with topically related documents and are captioned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5110570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a2b5d227f70780c24ca379fadda2644dbc39b94",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a corpus of news items consisting of images accompanied by text captions, we want to find out \"who's doing what\", i.e. associate names and action verbs in the captions to the face and body pose of the persons in the images. We present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus. These models can then be used to recognize people and actions in novel images without captions. We demonstrate experimentally that our joint 'face and pose' model solves the correspondence problem better than earlier models covering only the face, and that it can perform recognition of new uncaptioned images."
            },
            "slug": "Who's-Doing-What:-Joint-Modeling-of-Names-and-Verbs-Luo-Caputo",
            "title": {
                "fragments": [],
                "text": "Who's Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus can be used to recognize people and actions in novel images without captions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490678"
                        ],
                        "name": "P. H\u00e8de",
                        "slug": "P.-H\u00e8de",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "H\u00e8de",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. H\u00e8de"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3070099"
                        ],
                        "name": "Pierre-Alain Mo\u00ebllic",
                        "slug": "Pierre-Alain-Mo\u00ebllic",
                        "structuredName": {
                            "firstName": "Pierre-Alain",
                            "lastName": "Mo\u00ebllic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Alain Mo\u00ebllic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086126207"
                        ],
                        "name": "Jo\u00ebl Bourgeoys",
                        "slug": "Jo\u00ebl-Bourgeoys",
                        "structuredName": {
                            "firstName": "Jo\u00ebl",
                            "lastName": "Bourgeoys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00ebl Bourgeoys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285512"
                        ],
                        "name": "M. Joint",
                        "slug": "M.-Joint",
                        "structuredName": {
                            "firstName": "Magali",
                            "lastName": "Joint",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Joint"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115821781"
                        ],
                        "name": "Corinne Thomas",
                        "slug": "Corinne-Thomas",
                        "structuredName": {
                            "firstName": "Corinne",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinne Thomas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Many of the search engines deployed on the web retrieve images without analyzing their content, simply by matching user queries against collocated textual information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Caption generation, image annotation, summarization, topic models\n\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15063863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "832d7e11abe08644922018dd00c31004a0744fc8",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Image annotation is frequently used in image base management. Unfortunately, manual keyword indexing is costly and not exempt of errors for large image bases. In this article, we present a method for automatic image description in natural language for images without problems of occlusion. This method relies on a double expertise in image indexation and natural language processing and generation."
            },
            "slug": "Automatic-generation-of-natural-language-for-images-H\u00e8de-Mo\u00ebllic",
            "title": {
                "fragments": [],
                "text": "Automatic generation of natural language description for images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article presents a method for automatic image description in natural language for images without problems of occlusion that relies on a double expertise in image indexation and natural language processing and generation."
            },
            "venue": {
                "fragments": [],
                "text": "RIAO"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Regions are then described by a standard set of features, including color, texture, and shape, and subsequently treated as continuous vectors (e.g., [5], [42]) or in quantized form (e.g., [3], [22])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207561477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "473f4b7f8ae2b03dda2593f54b316ff7d55db26b",
            "isKey": false,
            "numCitedBy": 1214,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval."
            },
            "slug": "Modeling-annotated-data-Blei-Jordan",
            "title": {
                "fragments": [],
                "text": "Modeling annotated data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Three hierarchical probabilistic mixture models which aim to describe annotated data with multiple types, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3017324"
                        ],
                        "name": "Katja Filippova",
                        "slug": "Katja-Filippova",
                        "structuredName": {
                            "firstName": "Katja",
                            "lastName": "Filippova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katja Filippova"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Other approaches include sentence compression which helps make the summary more concentrate on the gist hence avoid redundancies (Knight and Marcu, 2002; Clarke, 2008), and sentence fusion which provides a flexible framework for both single- and multi- document summarization by taking syntactic information into account (Barzilay and McKeown, 2005; Filippova, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62082882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f77f6927a6b3d3805e2fc585bf267c2dc7265d5f",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 142,
            "paperAbstract": {
                "fragments": [],
                "text": "The popularity of text summarization (TS) in the NLP community has been steadily increasing in recent years. This is not surprising given its practical utility: e.g., multi-document summarization systems would be of great use given the enormous amount of news published daily online. Although TS methods vary considerably, most of them share one important property: they are extractive, and the most common extraction unit is the sentence - that is, most TS systems build summaries from extracted sentences. The extractive strategy has a well-recognized drawback which is related to the fact that sentences pulled from different documents may overlap but also complement each other. As a consequence, extractive systems are often unable to produce summaries which are complete and non-redundant at the same time. Sentence fusion is a text-to-text generation technique which addresses exactly this problem. Sentence fusion systems take a set of related documents as input and output sentences ``fused'' from dependency structures of similar sentences. In this thesis we present a novel sentence fusion system which advances TS towards abstractive summarization by building a global representation of input sentences and generating a new sentence from this representation. The sentence fusion process includes two main tasks - dependency tree construction and dependency tree linearization, both of which we solve in a novel and effective way. Our tree construction method is largely unsupervised and generates grammatical sentences by taking syntactic and semantic knowledge into account without reliance on hand-crafted rules. Tree linearization is accomplished with a method that extends previous approaches but requires little overgeneration in comparison with them. Our method is also significantly more accurate than the previous ones because it utilizes features from several levels of linguistic organization (syntax, semantics, information structure). We test our system on a corpus of comparable biographies in German and obtain good readability results in an evaluation with native speakers. We also apply the same method to sentence compression (i.e., the task of producing a summary of a single sentence) in English and German and obtain results comparable to those reported by recent systems designed exclusively for this task."
            },
            "slug": "Dependency-Graph-Based-Sentence-Fusion-and-Filippova",
            "title": {
                "fragments": [],
                "text": "Dependency Graph Based Sentence Fusion and Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This thesis presents a novel sentence fusion system which advances TS towards abstractive summarization by building a global representation of input sentences and generating a new sentence from this representation in a novel and effective way."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784365"
                        ],
                        "name": "Laura A. Dabbish",
                        "slug": "Laura-A.-Dabbish",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Dabbish",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura A. Dabbish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 338469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d4a6e4900ec0f096c87bb2b1272eeceaa584a6",
            "isKey": false,
            "numCitedBy": 2386,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained."
            },
            "slug": "Labeling-images-with-a-computer-game-Ahn-Dabbish",
            "title": {
                "fragments": [],
                "text": "Labeling images with a computer game"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new interactive system: a game that is fun and can be used to create valuable output that addresses the image-labeling problem and encourages people to do the work by taking advantage of their desire to be entertained."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, Fei-Fei and Perona (2005) propose a variant of LDA for learning natural scene categories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6387937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2252ccce2b65abc3759149b5c06587cc318e2f",
            "isKey": false,
            "numCitedBy": 3886,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes."
            },
            "slug": "A-Bayesian-hierarchical-model-for-learning-natural-Fei-Fei-Perona",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchical model for learning natural scene categories"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work proposes a novel approach to learn and recognize natural scene categories by representing the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909300"
                        ],
                        "name": "Lucy Vanderwende",
                        "slug": "Lucy-Vanderwende",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vanderwende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vanderwende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111436972"
                        ],
                        "name": "Hisami Suzuki",
                        "slug": "Hisami-Suzuki",
                        "structuredName": {
                            "firstName": "Hisami",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hisami Suzuki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125776"
                        ],
                        "name": "Chris Brockett",
                        "slug": "Chris-Brockett",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Brockett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Brockett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115414"
                        ],
                        "name": "A. Nenkova",
                        "slug": "A.-Nenkova",
                        "structuredName": {
                            "firstName": "Ani",
                            "lastName": "Nenkova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nenkova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Existing document content representations include simple unigram frequencies (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007; Haghighi and Vanderwende, 2009), syntactic features (such as clause information or dependencies) (Barzilay and McKeown, 2005), and topic representations (Daum\u00e9 III and Marcu, 2006; Haghighi and Vanderwende, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10496513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3827c54958e653b5ce23e19a347f1f52e25592b8",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-SumBasic:-Task-focused-summarization-with-Vanderwende-Suzuki",
            "title": {
                "fragments": [],
                "text": "Beyond SumBasic: Task-focused summarization with sentence simplification and lexical expansion"
            },
            "venue": {
                "fragments": [],
                "text": "Information Processing & Management"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13121800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e36d141e2964817c3d926c380793e404a3a3367",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
            },
            "slug": "Learning-the-semantics-of-words-and-pictures-Barnard-Forsyth",
            "title": {
                "fragments": [],
                "text": "Learning the semantics of words and pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features, and can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Other approaches include sentence compression which helps make the summary more concentrate on the gist hence avoid redundancies (Knight and Marcu, 2002; Clarke, 2008), and sentence fusion which provides a flexible framework for both single- and multi- document summarization by taking syntactic information into account (Barzilay and McKeown, 2005; Filippova, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2007; Haghighi and Vanderwende, 2009), syntactic features (such as clause information or dependencies) (Barzilay and McKeown, 2005), and topic representations (Daum\u00e9 III and Marcu, 2006; Haghighi and Vanderwende, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16188305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21a7b165b2e09cfc166bbbc69fef0732d6d99d5c",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources."
            },
            "slug": "Sentence-Fusion-for-Multidocument-News-Barzilay-McKeown",
            "title": {
                "fragments": [],
                "text": "Sentence Fusion for Multidocument News Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This article introduces sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents that moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly, standard latent semantic analysis (LSA) and its probabilistic variant (PLSA), are applied to address the multimodal relations from a multimodal space consisting of image features combined with the corresponding keywords (Pan et al., 2004; Monay and Gatica-Perez, 2003, 2007; Fergus et al., 2005; Sivic et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(2005) and Fergus et al. (2005) for more details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2004) and its probabilistic variant (PLSA, Monay and Gatica-Perez 2003, 2007; Fergus et al. 2005; Sivic et al. 2005; Bosch 2007), latent dirichlet allocation and its variants (LDA, Blei and Jordan 2003; Barnard et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A prototype system, iGraph proposed by Ferres et al. (2006), is designed to help people with visual impairment to better access graphical information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7005884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c6f0c1917bb0f7e23c4c35b553045fa39663211",
            "isKey": true,
            "numCitedBy": 830,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate tire models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets"
            },
            "slug": "Learning-object-categories-from-Google's-image-Fergus-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "Learning object categories from Google's image search"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new model, TSI-pLSA, is developed, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner, and can handle the high intra-class variability and large proportion of unrelated images returned by search engines."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068300013"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1db1bcb9edf9d166a566beb01bb8af83998c0add",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Title generation is a complex task involving both natural language understanding and natural language synthesis. In this paper, we propose a new probabilistic model for title generation. Different from the previous statistical models for title generation, which treat title generation as a generation process that converts the 'document representation' of information directly into a 'title representation' of the same information, this model introduces a hidden state called 'information source' and divides title generation into two steps, namely the step of distilling the 'information source' from the observation of a document and the step of generating a title from the estimated 'information source'. In our experiment, the new probabilistic model outperforms the previous model for title generation in terms of both automatic evaluations and human judgments."
            },
            "slug": "A-New-Probabilistic-Model-for-Title-Generation-Jin-Hauptmann",
            "title": {
                "fragments": [],
                "text": "A New Probabilistic Model for Title Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new probabilistic model is proposed for title generation that introduces a hidden state called 'information source' and divides title generation into two steps, namely the step of distilling the ' information source' from the observation of a document and the stepof generating a title from the estimated 'information sources'."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068300013"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Zhou and Hovy (2003), first, extract keywords according to word frequency, position feature and the conditional probability of a word appearing in the headline given that the same word appears in the document (the ones used by Jin and Hauptmann (2002)), then glue each keyword with its left and right neighbors in original text and further define a phrase as a word window that covers a series of keyword chunks in the document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7465678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "637e0f53e4ffd4f672145db190435cb7cf0586e4",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we implemented a set of title generation methods using training set of 21190 news stories and evaluated them on an independent test corpus of 1006 broadcast news documents, comparing the results over manual transcription to the results over automatically recognized speech. We use both F1 and the average number of correct title words in the correct order as metric. Overall, the results show that title generation for speech recognized news documents is possible at a level approaching the accuracy of titles generated for perfect text transcriptions."
            },
            "slug": "Automatic-Title-Generation-for-Spoken-Broadcast-Jin-Hauptmann",
            "title": {
                "fragments": [],
                "text": "Automatic Title Generation for Spoken Broadcast News"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that title generation for speech recognized news documents is possible at a level approaching the accuracy of titles generated for perfect text transcriptions."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49693392"
                        ],
                        "name": "A. Kojima",
                        "slug": "A.-Kojima",
                        "structuredName": {
                            "firstName": "Atsuhiro",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kojima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2023734"
                        ],
                        "name": "M. Izumi",
                        "slug": "M.-Izumi",
                        "structuredName": {
                            "firstName": "Masao",
                            "lastName": "Izumi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Izumi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114145871"
                        ],
                        "name": "Takeshi Tamura",
                        "slug": "Takeshi-Tamura",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeshi Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950023"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Kunio",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", Kojima et al. (2002) construct a knowledge base that maps the moving trajectory of the human head into abstract actions, such as enter,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For instance, in an office-scene video surveillance application (Kojima et al., 2000), a human action concept ontology is manually constructed to map a sequence of human positions and postures into abstract actions (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27525133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e62d7dd5ffb078e4244fef096a4a868462eb6d5e",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In visual surveillance applications, it is becoming popular to perceive video images and to interpret them using natural language concepts. We propose an approach to generating a natural language description of human behavior appearing in real video images. First, a head region of a human, on behalf of the whole body, is extracted from each frame. Using a model based method, three dimensional pose and position of the head are estimated. Next, the trajectory of these parameters is divided into segments of monotonous motions. For each segment, we evaluate conceptual features such as degree of change of pose and position and that of relative distance to some objects in the surroundings, and so on. By calculating the product of these feature values, a most suitable verb is selected and other syntactic elements are supplied. Finally natural language text is generated using a technique of machine translation."
            },
            "slug": "Generating-natural-language-description-of-human-Kojima-Izumi",
            "title": {
                "fragments": [],
                "text": "Generating natural language description of human behavior from video images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes an approach to generating a natural language description of human behavior appearing in real video images using a model based method and a technique of machine translation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277853"
                        ],
                        "name": "A. Vailaya",
                        "slug": "A.-Vailaya",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Vailaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vailaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34659351"
                        ],
                        "name": "M\u00e1rio A. T. Figueiredo",
                        "slug": "M\u00e1rio-A.-T.-Figueiredo",
                        "structuredName": {
                            "firstName": "M\u00e1rio",
                            "lastName": "Figueiredo",
                            "middleNames": [
                                "A.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rio A. T. Figueiredo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "Experimental results show that it is viable to generate captions that are pertinent to the specific content of an image and its associated article, while permitting creativity in the description."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9140319,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "142f056a365dccd029c0897fcfa7833aecf2212f",
            "isKey": false,
            "numCitedBy": 868,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Grouping images into (semantically) meaningful categories using low-level visual features is a challenging and important problem in content-based image retrieval. Using binary Bayesian classifiers, we attempt to capture high-level concepts from low-level image features under the constraint that the test image does belong to one of the classes. Specifically, we consider the hierarchical classification of vacation images; at the highest level, images are classified as indoor or outdoor; outdoor images are further classified as city or landscape; finally, a subset of landscape images is classified into sunset, forest, and mountain classes. We demonstrate that a small vector quantizer (whose optimal size is selected using a modified MDL criterion) can be used to model the class-conditional densities of the features, required by the Bayesian methodology. The classifiers have been designed and evaluated on a database of 6931 vacation photographs. Our system achieved a classification accuracy of 90.5% for indoor/outdoor, 95.3% for city/landscape, 96.6% for sunset/forest and mountain, and 96% for forest/mountain classification problems. We further develop a learning method to incrementally train the classifiers as additional data become available. We also show preliminary results for feature reduction using clustering techniques. Our goal is to combine multiple two-class classifiers into a single hierarchical classifier."
            },
            "slug": "Image-classification-for-content-based-indexing-Vailaya-Figueiredo",
            "title": {
                "fragments": [],
                "text": "Image classification for content-based indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The goal is to combine multiple two-class classifiers into a single hierarchical classifier, and it is demonstrated that a small vector quantizer can be used to model the class-conditional densities of the features, required by the Bayesian methodology."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49693392"
                        ],
                        "name": "A. Kojima",
                        "slug": "A.-Kojima",
                        "structuredName": {
                            "firstName": "Atsuhiro",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kojima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114145871"
                        ],
                        "name": "Takeshi Tamura",
                        "slug": "Takeshi-Tamura",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeshi Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950023"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Kunio",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "As this limits the applicability of search engines (images that do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Caption generation, image annotation, summarization, topic models\n\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16139212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d53a97a3dd7760b193c0d9a5293b60feff239059",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for describing human activities from video images based on concept hierarchies of actions. Major difficulty in transforming video images into textual descriptions is how to bridge a semantic gap between them, which is also known as inverse Hollywood problem. In general, the concepts of events or actions of human can be classified by semantic primitives. By associating these concepts with the semantic features extracted from video images, appropriate syntactic components such as verbs, objects, etc. are determined and then translated into natural language sentences. We also demonstrate the performance of the proposed method by several experiments."
            },
            "slug": "Natural-Language-Description-of-Human-Activities-on-Kojima-Tamura",
            "title": {
                "fragments": [],
                "text": "Natural Language Description of Human Activities from Video Images Based on Concept Hierarchy of Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A method for describing human activities from video images based on concept hierarchies of actions based on semantic primitives, which demonstrates the performance of the proposed method by several experiments."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729172"
                        ],
                        "name": "I. Mani",
                        "slug": "I.-Mani",
                        "structuredName": {
                            "firstName": "Inderjeet",
                            "lastName": "Mani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701063"
                        ],
                        "name": "M. Maybury",
                        "slug": "M.-Maybury",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Maybury",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maybury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "say in a limited space, while keeping sentences grammatical, and making sure coherence is preserved when multi-sentence output needed (Mani, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5393989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e53dbd9f57937931a124aed93d744fe50e031af0",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an automatic speech summarization technique for English. In our proposed method, a set of words maximizing a summarization score indicating appropriateness of summarization is extracted from automatically transcribed speech and concatenated to create a summary. The extraction process is performed using a Dynamic Programming (DP) technique according to a target compression ratio. In this paper, English broadcast news speech transcribed using a speech recognizer is automatically summarized. In order to apply our method, originally proposed for Japanese, to English, the model of estimating word concatenation probabilities based on a dependency structure in the original speech given by a Stochastic Dependency Context Free Grammar (SDCFG) is modified. A summarization method for multiple utterances using twolevel DP technique is also proposed. The automatically summarized sentences are evaluated by a summarization accuracy based on the comparison with the manual summarization of correctly transcribed speech by human subjects. Experimental results show that our proposed method effectively extracts relatively important information and remove redundant and irrelevant information from English news speech."
            },
            "slug": "Automatic-Summarization-Mani-Maybury",
            "title": {
                "fragments": [],
                "text": "Automatic Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Experimental results show that the proposed automatic speech summarization technique for English effectively extracts relatively important information and remove redundant and irrelevant information from English news speech."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751139"
                        ],
                        "name": "V. Mittal",
                        "slug": "V.-Mittal",
                        "structuredName": {
                            "firstName": "Vibhu",
                            "lastName": "Mittal",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mittal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47147237"
                        ],
                        "name": "Johanna D. Moore",
                        "slug": "Johanna-D.-Moore",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Moore",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna D. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825424"
                        ],
                        "name": "G. Carenini",
                        "slug": "G.-Carenini",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Carenini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carenini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697331"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Roth",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6375093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f569c8bcddfbfb57ab03b5a74c794fb95fa16f9b",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical presentations can be used to communicate information in relational data sets succinctly and effectively. However, novel graphical presentations that represent many attributes and relationships are often difficult to understand completely until explained. Automatically generated graphical presentations must therefore either be limited to generating simple, conventionalized graphical presentations, or risk incomprehensibility. A possible solution to this problem would be to extend automatic graphical presentation systems to generate explanatory captions in natural language, to enable users to understand the information expressed in the graphic. This paper presents a system to do so. It uses a text planner to determine the content and structure of the captions based on: (1) a representation of the structure of the graphical presentation and its mapping to the data it depicts, (2) a framework for identifying the perceptual complexity of graphical elements, and (3) the structure of the data expressed in the graphic. The output of the planner is further processed regarding issues such as ordering, aggregation, centering, generating referring expressions, and lexical choice. We discuss the architecture of our system and its strengths and limitations. Our implementation is currently limited to 2-D charts and maps, but, except for lexical information, it is completely domain independent. We illustrate our discussion with figures and generated captions about housing sales in Pittsburgh."
            },
            "slug": "Describing-Complex-Charts-in-Natural-Language:-A-Mittal-Moore",
            "title": {
                "fragments": [],
                "text": "Describing Complex Charts in Natural Language: A Caption Generation System"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A system to extend automatic graphical presentation systems to generate explanatory captions in natural language, to enable users to understand the information expressed in the graphic, and is currently limited to 2-D charts and maps."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751139"
                        ],
                        "name": "V. Mittal",
                        "slug": "V.-Mittal",
                        "structuredName": {
                            "firstName": "Vibhu",
                            "lastName": "Mittal",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mittal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2819135"
                        ],
                        "name": "M. Witbrock",
                        "slug": "M.-Witbrock",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Witbrock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Witbrock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Along with the title, the lead, and section headings, captions are the most commonly read words in a news article."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9952653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d08213ede54c4e205d18b4400288831af918ec8",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required. An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding. A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation. The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language. This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus."
            },
            "slug": "Headline-Generation-Based-on-Statistical-Banko-Mittal",
            "title": {
                "fragments": [],
                "text": "Headline Generation Based on Statistical Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054535159"
                        ],
                        "name": "D. Z. R. Schwartz",
                        "slug": "D.-Z.-R.-Schwartz",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "Zajic",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Z. R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118092738"
                        ],
                        "name": "B. Door",
                        "slug": "B.-Door",
                        "structuredName": {
                            "firstName": "Blanche",
                            "lastName": "Door",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Door"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 103
                            }
                        ],
                        "text": "Headline generation is such a task that generates a very short title-like summary for a given document (Witbrock and Mittal, 1999; Banko et al., 2000; Jin and Hauptmann, 2001a,b, 2002; Zajic et al., 2002; Zhou and Hovy, 2003; Dorr et al., 2003; Bonnie et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12252356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1328dcfa7b84e453e3236f7b01fa35cf6375ab2",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel application of Hidden Markov Models to automatic generation of informative headlines for English texts. We propose four decoding parameters to make the headlines appear more like Headlinese, the language of informative newspaper headlines. We also allow for morphological variation in words between headline and story English. Informal and formal evaluations indicate that our approach produces informative headlines, mimicking a Headlinese style generated by humans."
            },
            "slug": "Automatic-Headline-Generation-for-Newspaper-Stories-Schwartz-Door",
            "title": {
                "fragments": [],
                "text": "Automatic Headline Generation for Newspaper Stories"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Informal and formal evaluations indicate that the proposed novel application of Hidden Markov Models to automatic generation of informative headlines for English texts produces informative headlines mimicking a Headlinese style generated by humans."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32278254"
                        ],
                        "name": "Matthew G. Snover",
                        "slug": "Matthew-G.-Snover",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Snover",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew G. Snover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3333779"
                        ],
                        "name": "L. Micciulla",
                        "slug": "L.-Micciulla",
                        "structuredName": {
                            "firstName": "Linnea",
                            "lastName": "Micciulla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Micciulla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our automatic evaluation was based on Translation Edit Rate (TER, [62]), a measure commonly used to evaluate the quality of machine translation output."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14226,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a5899f1ec92af7d01f35225161430116a6eabbea",
            "isKey": false,
            "numCitedBy": 1633,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a new, intuitive measure for evaluating machine translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Error Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We also compute a human-targeted TER (or HTER), where the minimum TER of the translation is computed against a human \u2018targeted reference\u2019 that preserves the meaning (provided by the reference translations) and is fluent, but is chosen to minimize the TER score for a particular system output. We show that: (1) The single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU; (2) The human-targeted HTER yields a 33% error-rate reduction and is shown to be very well correlated with human judgments; (3) The four-reference variant of TER and the single-reference variant of HTER yield higher correlations with human judgments than BLEU; (4) HTER yields higher correlations with human judgments than METEOR or its human-targeted variant (HMETEOR); and (5) The four-reference variant of TER correlates as well with a single human judgment as a second human judgment does, while HTER, HBLEU, and HMETEOR correlate significantly better with a human judgment than a second human judgment does."
            },
            "slug": "A-STUDY-OF-TRANSLATION-ERROR-RATE-WITH-TARGETED-Snover-Dorr",
            "title": {
                "fragments": [],
                "text": "A STUDY OF TRANSLATION ERROR RATE WITH TARGETED HUMAN ANNOTATION"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A new, intuitive measure for evaluating machine translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments is defined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876441"
                        ],
                        "name": "Xing Wei",
                        "slug": "Xing-Wei",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Zhou and Hovy (2003) first identify keywords according to frequency, position, and probabilistic features (the ones used by Jin and Hauptmann (2001a)), then expand these keywords with their near bigram neighbors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2003) and ad-hoc information retrieval (Wei and Croft, 2006) with good results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3343003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94a8ace25d5112e22f7235bbba26570b008a73e9",
            "isKey": false,
            "numCitedBy": 1145,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency."
            },
            "slug": "LDA-based-document-models-for-ad-hoc-retrieval-Wei-Croft",
            "title": {
                "fragments": [],
                "text": "LDA-based document models for ad-hoc retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an LDA-based document model within the language modeling framework, and evaluates it on several TREC collections, and shows that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2 Compared to LDA, PLSA is not a fully generative model (Griffiths et al., 2007)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5715561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "509a2ca90a85c62d66a16b37e0de28715dd4e89f",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 140,
            "paperAbstract": {
                "fragments": [],
                "text": "Processing language requires the retrieval of concepts from memory in response to an ongoing stream of information. This retrieval is facilitated if one can infer the gist of a sentence, conversation, or document and use that gist to predict related concepts and disambiguate words. This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language-processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure."
            },
            "slug": "Topics-in-semantic-representation.-Griffiths-Steyvers",
            "title": {
                "fragments": [],
                "text": "Topics in semantic representation."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference that leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2156851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aedb8df8f953429ec5a6df99fda5c5d71dbee4ff",
            "isKey": false,
            "numCitedBy": 2326,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Generative-Visual-Models-from-Few-Training-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1943594"
                        ],
                        "name": "Jia-Yu Pan",
                        "slug": "Jia-Yu-Pan",
                        "structuredName": {
                            "firstName": "Jia-Yu",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia-Yu Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97598888"
                        ],
                        "name": "Hyung-Jeong Yang",
                        "slug": "Hyung-Jeong-Yang",
                        "structuredName": {
                            "firstName": "Hyung-Jeong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyung-Jeong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702392"
                        ],
                        "name": "C. Faloutsos",
                        "slug": "C.-Faloutsos",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Faloutsos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Faloutsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "0162-8828/13/$31.00 2013 IEEE Published by the IEEE Computer Society\nor grammars for producing textual output."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2316797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b378d652102f4f6d89533eec043f67afb51fd0a6",
            "isKey": true,
            "numCitedBy": 28,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose multi-modal story-oriented video summarization (MMSS) which, unlike previous works that use fine-tuned, domain-specific heuristics, provides a domain-independent, graph-based framework. MMSS uncovers correlation between information of different modalities which gives meaningful story-oriented news video summaries. MMSS can also be applied for video retrieval, giving performance that matches the best traditional retrieval techniques (OKAPI and LSI), with no fine-tuned heuristics such as tf/idf."
            },
            "slug": "MMSS:-multi-modal-story-oriented-video-Pan-Yang",
            "title": {
                "fragments": [],
                "text": "MMSS: multi-modal story-oriented video summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Multi-modal story-oriented video summarization (MMSS) is proposed, which, unlike previous works that use fine-tuned, domain-specific heuristics, provides a domain-independent, graph-based framework that uncovers correlation between information of different modalities."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth IEEE International Conference on Data Mining (ICDM'04)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "0162-8828/13/$31.00 2013 IEEE Published by the IEEE Computer Society\nor grammars for producing textual output."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7605995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6dd83b2aa34c806596fc619ff3fbccf5f9830ab",
            "isKey": true,
            "numCitedBy": 2499,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis."
            },
            "slug": "Unsupervised-Learning-by-Probabilistic-Latent-Hofmann",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning by Probabilistic Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice, and results in a more principled approach with a solid foundation in statistical inference."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751139"
                        ],
                        "name": "V. Mittal",
                        "slug": "V.-Mittal",
                        "structuredName": {
                            "firstName": "Vibhu",
                            "lastName": "Mittal",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mittal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697331"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Roth",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47147237"
                        ],
                        "name": "Johanna D. Moore",
                        "slug": "Johanna-D.-Moore",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Moore",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna D. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057824649"
                        ],
                        "name": "J. Mattis",
                        "slug": "J.-Mattis",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Mattis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mattis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825424"
                        ],
                        "name": "G. Carenini",
                        "slug": "G.-Carenini",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Carenini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carenini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3182946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "929a86761fc9f65e5c7b94b998fab933fdc46406",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical presentations can he used to communicate information in relational data sets suffciently and effectively However novel graphical presentations about numerous attributes and their relationships are often difficult to understand completely until explained Automatically generated graphical presenlations must therefore either be limited to simple conventional ones or risk incornprehensibility One way of alleviating this problem is to design graphical presentation systems that can work in conjunction with a natural language generator to produce explanatory captions This paper presents three strategies for generating explanatory captions to accompany information graphics based on (1) a representation of the structure of the graphical presentation (2) a framework for identifying the perceptual complexity of graphical elements and (3) the structure of the data expressed in the graphic We describe an implemented system and illustrate how it is used to generate explanatory captions for a range of graphics from a data set about real estate transactions in Pittsburgh."
            },
            "slug": "Generating-Explanatory-Captions-for-Information-Mittal-Roth",
            "title": {
                "fragments": [],
                "text": "Generating Explanatory Captions for Information Graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents three strategies for generating explanatory captions to accompany information graphics based on a representation of the structure of the graphical presentation, a framework for identifying the perceptual complexity of graphical elements and theructure of the data expressed in the graphic."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068300013"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14732314,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "374b863e21af9a26e68ece6ba86573f703e5b4ed",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present and compare automatically generated titles for machine-translated documents using several different statistics-based methods. A Naive Bayesian, a K-Nearest Neighbour, a TF-IDF and an iterative Expectation-Maximization method for title generation were applied to 1000 original English news documents and again to the same documents translated from English into Portuguese, French or German and back to English using SYSTRAN. The AutoSummarization function of Microsoft Word was used as a base line. Results on several metrics show that the statistics-based methods of title generation for machine-translated documents are fairly language independent and title generation is possible at a level approaching the accuracy of titles generated for the original English documents."
            },
            "slug": "Title-Generation-for-Machine-Translated-Documents-Jin-Hauptmann",
            "title": {
                "fragments": [],
                "text": "Title Generation for Machine-Translated Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Results on several metrics show that the statistics-based methods of title generation for machine-translated documents are fairly language independent and title generation is possible at a level approaching the accuracy of titles generated for the original English documents."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848824"
                        ],
                        "name": "Karen Sp\u00e4rck Jones",
                        "slug": "Karen-Sp\u00e4rck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sp\u00e4rck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sp\u00e4rck Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12824751,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "802b58b789338b2325d4d7361044809f3e2ca949",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 173,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-summarising:-The-state-of-the-art-Jones",
            "title": {
                "fragments": [],
                "text": "Automatic summarising: The state of the art"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32278254"
                        ],
                        "name": "Matthew G. Snover",
                        "slug": "Matthew-G.-Snover",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Snover",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew G. Snover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152901366"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3333779"
                        ],
                        "name": "L. Micciulla",
                        "slug": "L.-Micciulla",
                        "structuredName": {
                            "firstName": "Linnea",
                            "lastName": "Micciulla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Micciulla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8938789,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "51951073580f6995e55be873db9a7f6a9736ca86",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU\u2014even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as\u2014or better than\u2014a second human judgment does."
            },
            "slug": "A-Study-of-Translation-Edit-Rate-with-Targeted-Snover-Dorr",
            "title": {
                "fragments": [],
                "text": "A Study of Translation Edit Rate with Targeted Human Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments is examined, which indicates that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate withhuman judgments as well as\u2014or better than\u2014a second human judgment does."
            },
            "venue": {
                "fragments": [],
                "text": "AMTA"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2333082"
                        ],
                        "name": "Y. Mori",
                        "slug": "Y.-Mori",
                        "structuredName": {
                            "firstName": "Yasuhide",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079483"
                        ],
                        "name": "Hironobu Takahashi",
                        "slug": "Hironobu-Takahashi",
                        "structuredName": {
                            "firstName": "Hironobu",
                            "lastName": "Takahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hironobu Takahashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776022"
                        ],
                        "name": "R. Oka",
                        "slug": "R.-Oka",
                        "structuredName": {
                            "firstName": "Ryu-ichi",
                            "lastName": "Oka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Oka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "This very naive model works well on an encyclopedia collection database (Mori et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 401,
                                "start": 166
                            }
                        ],
                        "text": "Many generative models that have been successfully applied in speech recognition, machine translation and information retrieval, have been ported to image annotation (Mori et al., 1999; Duygulu et al., 2002; Barnard and Forsyth, 2001; Wang and Li, 2002; Barnard et al., 2002; Lavrenko et al., 2003; Blei and Jordan, 2003; Feng et al., 2004; Pan et al., 2004; Jin et al., 2004; Li and Wang, 2006, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 65
                            }
                        ],
                        "text": "To render image regions more word-like, some existing algorithms (Mori et al., 1999; Duygulu et al., 2002; Barnard et al., 2002; Jeon et al., 2003; Jeon and Manmatha, 2004; Pan et al., 2004; Jin et al., 2004) cluster their features into a certain number of blobs (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18574318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b29ffb4207435540ddecf4b14a8a32106b33830",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to make a relationship between images and words. We adopt two processes in the method, one is a process to uniformly divide each image into sub-images with key words, and the other is a process to carry out vector quantization of the sub-images. These processes lead to results which show that each sub-image can be correlated to a set of words each of which is selected from words assigned to whole images. Original aspects of the method are, (1) all words assigned to a whole image are inherited to each divided sub-image, (2) the voting probability of each word for a set of divided images is estimated by the result of a vector quantization of the feature vector of sub-images. Some experiments show the e ectiveness of the proposed method."
            },
            "slug": "Image-to-word-transformation-based-on-dividing-Mori-Takahashi",
            "title": {
                "fragments": [],
                "text": "Image-to-word transformation based on dividing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "All words assigned to a whole image are inherited to each divided sub-image and the voting probability of each word for a set of divided images is estimated by the result of a vector quantization of the feature vector of sub-images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152732685"
                        ],
                        "name": "Jianping Fan",
                        "slug": "Jianping-Fan",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704919"
                        ],
                        "name": "Hangzai Luo",
                        "slug": "Hangzai-Luo",
                        "structuredName": {
                            "firstName": "Hangzai",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hangzai Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026151"
                        ],
                        "name": "Yuli Gao",
                        "slug": "Yuli-Gao",
                        "structuredName": {
                            "firstName": "Yuli",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuli Gao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2Another stream of work is cast in a semi-supervised learning setting, where each unlabeled sample is assumed to originate from one of the known classes (or concepts) which can be effectively learned from existing annotated training data, mainly through a classifier-based approach (Fan et al., 2005; Schroff et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6549076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af110d6aed8955ffd681bdfb96b892f21d02c574",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we have proposed a novel framework to achieve more effective classifier training by using unlabeled samples. By integrating concept hierarchy for semantic image concept organization, a hierarchical mixture model is proposed to enable multi-level image concept modeling and hierarchical classifier training. To effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy, we have proposed a novel adaptive EM algorithm to achieve more effective classifier training with higher prediction accuracy. To effectively learn the classifiers for the higher-level semantic image concepts, we have also proposed a novel technique for classifier combining by using hierarchical mixture model. The experimental results on two large-scale image databases are also provided."
            },
            "slug": "Learning-the-semantics-of-images-by-using-unlabeled-Fan-Luo",
            "title": {
                "fragments": [],
                "text": "Learning the semantics of images by using unlabeled samples"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A novel framework to achieve more effective classifier training by using unlabeled samples by integrating concept hierarchy for semantic image concept organization and a hierarchical mixture model is proposed to enable multi-level image concept modeling and hierarchical classifiers training."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2819135"
                        ],
                        "name": "M. Witbrock",
                        "slug": "M.-Witbrock",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Witbrock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Witbrock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751139"
                        ],
                        "name": "V. Mittal",
                        "slug": "V.-Mittal",
                        "structuredName": {
                            "firstName": "Vibhu",
                            "lastName": "Mittal",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mittal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8713002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b43d11ca01316f0e96c07f6481b6b3a04a78040",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Using current extractive summarization techniques, it is impossible to produce a coherent document summary shorter than a single sentence, or to produce a summary that conforms to particular stylistic constraints. Ideally, one would prefer to understand the document, and to generate an appropriate summary directly from the results of that understanding. Absent a comprehensive natural language understanding system, an approximation must be used. This paper presents an alternative statistical model of a summarization process, which jointly applies statistical models of the term selection and term ordering process to produce brief coherent summaries in a style learned from a training corpus."
            },
            "slug": "Ultra-summarization-(poster-abstract):-a-approach-Witbrock-Mittal",
            "title": {
                "fragments": [],
                "text": "Ultra-summarization (poster abstract): a statistical approach to generating highly condensed non-extractive summaries"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents an alternative statistical model of a summarization process, which jointly applies statistical models of the term selection and term ordering process to produce brief coherent summaries in a style learned from a training corpus."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Blei et al. [47] describe the generative process for a document d as follows:\n1. choose j Dir\u00f0 \u00de, 2. for n 2 1; 2; . . . ; N :\na. choose topic znj Mult\u00f0 \u00de, b. choose a word wnjzn; 1:K Mult\u00f0 zn\u00de,\nwhere each entry of 1:K is a distribution over words, indicating a topic definition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "We follow the convexity-based variational inference procedure described in Blei et al. [47], which involves two steps: 1) introducing variational parameters in order to find the tightest lower bound for the target posterior distribution, and 2) obtaining the tight lower bound through minimizing the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 75
                            }
                        ],
                        "text": "We follow the convexity-based variational inference procedure described in Blei et al. [47], which involves two steps: 1) introducing variational parameters in order to find the tightest lower bound for the target posterior distribution, and 2) obtaining the tight lower bound through minimizing the Kullback-Leibler (KL) divergence between the introduced variational distribution and the true posterior distribution (we refer interested readers to Blei et al. [47] and Blei [4] for more details on their inference procedure)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": true,
            "numCitedBy": 30948,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115414"
                        ],
                        "name": "A. Nenkova",
                        "slug": "A.-Nenkova",
                        "structuredName": {
                            "firstName": "Ani",
                            "lastName": "Nenkova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nenkova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909300"
                        ],
                        "name": "Lucy Vanderwende",
                        "slug": "Lucy-Vanderwende",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vanderwende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vanderwende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909300"
                        ],
                        "name": "Lucy Vanderwende",
                        "slug": "Lucy-Vanderwende",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vanderwende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vanderwende"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Existing document content representations include simple unigram frequencies (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007; Haghighi and Vanderwende, 2009), syntactic features (such as clause information or dependencies) (Barzilay and McKeown, 2005), and topic representations (Daum\u00e9 III and Marcu, 2006; Haghighi and Vanderwende, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14102322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "676b1549adae511164c1b5343f10260fd42035b4",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Most multi-document summarizers utilize term frequency related features to determine sentence importance. No empirical studies, however, have been carried out to isolate the contribution made by frequency information from that of other features. Here, we examine the impact of frequency on various aspects of summarization and the role of frequency in the design of a summarization system. We describe SumBasic, a summarization system that exploits frequency exclusively to create summaries. SumBasic outperforms many of the summarization systems in DUC 2004, and performs very well in the 2005 MSE evaluation, confirming that frequency alone is a powerful feature in summary creation. We also demonstrate how a frequency-based summarizer can incorporate context adjustment in a natural way, and show that this adjustment contributes to the good performance of the summarizer and is sufficient means for duplication removal in multi-document summarization."
            },
            "slug": "The-Impact-of-Frequency-on-Summarization-Nenkova-Vanderwende",
            "title": {
                "fragments": [],
                "text": "The Impact of Frequency on Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "SumBasic is described, a summarization system that exploits frequency exclusively to create summaries and it is demonstrated how a frequency-based summarizer can incorporate context adjustment in a natural way and show that this adjustment contributes to the good performance of the summarizer and is sufficient means for duplication removal in multi-document summarization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6241932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc8fe65096cc0971aebe45c50c64a173b94a36d5",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present BAYESUM (for \"Bayesian summarization\"), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework."
            },
            "slug": "Bayesian-Query-Focused-Summarization-Daum\u00e9-Marcu",
            "title": {
                "fragments": [],
                "text": "Bayesian Query-Focused Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system, and how B Bayesian summarization can be understood as a justified query expansion technique in the language modeling for IR framework."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717629"
                        ],
                        "name": "Yansong Feng",
                        "slug": "Yansong-Feng",
                        "structuredName": {
                            "firstName": "Yansong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yansong Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Humans can describe images effortlessly, probably because they have a common underlying representation for the two modalities (Feng and Lapata, 2010c)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14826028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d84082b1b86be38decd388f955cd9a884e0311f",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science. Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information. In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context. Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account."
            },
            "slug": "Visual-Information-in-Semantic-Representation-Feng-Lapata",
            "title": {
                "fragments": [],
                "text": "Visual Information in Semantic Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account and a model of multimodal meaning representation which is based on the linguistic and visual context is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389036863"
                        ],
                        "name": "Jordan L. Boyd-Graber",
                        "slug": "Jordan-L.-Boyd-Graber",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Boyd-Graber",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordan L. Boyd-Graber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215824810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "457628a1c232bb48acc2db8440571e289cc80e15",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree-specific syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents."
            },
            "slug": "Syntactic-Topic-Models-Boyd-Graber-Blei",
            "title": {
                "fragments": [],
                "text": "Syntactic Topic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The syntactic topic model (STM) is developed, a nonparametric Bayesian model of parsed documents that generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390051006"
                        ],
                        "name": "I. Langkilde-Geary",
                        "slug": "I.-Langkilde-Geary",
                        "structuredName": {
                            "firstName": "Irene",
                            "lastName": "Langkilde-Geary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Langkilde-Geary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 101
                            }
                        ],
                        "text": "Word morphology and necessary function words are often manually imported into an NLG system as rules (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14174813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "093b25b0ce5aaa65c9b7a5ddd1b2fcb8788ffc36",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine the practical s~'nergy between symbolic and statistical language processing in a generator called Nitrogen. The analysis provides insight into the kinds of linguistic decisions that bigram frequency statistics can make, and how it improves scalability.. We also discuss the limits of bigram statistical knowledge. We focus on specific examples of Nitrogen's output. 1 I n t r o d u c t i o n Langkilde and Knight (1998) introduced Nitrogen, a system that implements a new style of generation in which corpus-based ngram statistics are used in place of deep, extensive symbolic knowledge to provide Very large-scale generation (lexicons and knowledge bases on the order of 200,000 entities), and simultaneously simplify the input and improve robustness for sentence generation. Nitrogen's generation occurs in two stages, as shown in Figure 1. First the input is mapped tO a word lattice, a compact representation of multiple generation possibilities. Then, a statistical extractor selects the most fluent path through the lattice. The word lattice encodes alternative English expressions for the input when the symbolic knowledge is unavailable (whether from the input, or from the knowledge bases) for making realization decisions. The Nitrogen statistical extractor ranks these alternative s using bigram (adjacent word pairs) and unigram (single word) statistics Collected from two years of the Wall Street Journal. The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). meaning symbolic generator \u2022 [ <--lexicon ~-gralnmar $ word lattice of possible renderings I statistical extractor [ 4 English string I--corpus Figure h Combining Symbolic and Statistical Knowledge in a Natural Language Generator (Knight and Hatzivassil0glou, 1995). In essence, Nitrogen uses ngram statistics to robustly make a wide variety of decisions, from tense to word choice\u2022 to syntactic subcategorization, that traditionally are handled either with defaults (e.g., assume present tense, use the alphabetically-first synonyms, use nominal arguments), explicit input specification, or by using deep, detailed knowledge bases. However, in scaling up a generator system, these methods become unsatisfactory. Defaults are too rigid and limit quality; detailed input specs are difficult or complex to construct, or m~' be unavailable; and"
            },
            "slug": "The-Practical-Value-of-N-Grams-Is-in-Generation-Langkilde-Geary-Knight",
            "title": {
                "fragments": [],
                "text": "The Practical Value of N-Grams Is in Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The practical s~'nergy between symbolic and statistical language processing in a generator called Nitrogen is examined, providing insight into the kinds of linguistic decisions that bigram frequency statistics can make, and how it improves scalability."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277853"
                        ],
                        "name": "A. Vailaya",
                        "slug": "A.-Vailaya",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Vailaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vailaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34659351"
                        ],
                        "name": "M\u00e1rio A. T. Figueiredo",
                        "slug": "M\u00e1rio-A.-T.-Figueiredo",
                        "structuredName": {
                            "firstName": "M\u00e1rio",
                            "lastName": "Figueiredo",
                            "middleNames": [
                                "A.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rio A. T. Figueiredo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some methods adopt a \u201done vs all\u201d model (Vailaya et al., 1999; Maron and Ratan, 1998; Qi and Han, 2007; Chai and Hung, 2008; Gupta et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8262367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d01805e377a90563292322cdce31828ccde40ba5",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Grouping images into (semantically) meaningful categories using low level visual features is a challenging and important problem in content based image retrieval. Using binary Bayesian classifiers, we attempt to capture high level concepts from low level image features under the constraint that the test image does belong to one of the classes of interest. Specifically, we consider the hierarchical classification of vacation images; at the highest level, images are classified into indoor/outdoor classes, outdoor images are further classified into city/landscape classes, and finally, a subset of landscape images is classified into sunset, forest, and mountain classes. We demonstrate that a small codebook (the optimal size of codebook is selected using a modified MDL criterion) extracted from a vector quantizer can be used to estimate the class-conditional densities of the observed features needed for the Bayesian methodology. On a database of 6931 vacation photographs, our system achieved an accuracy of 90.5% for indoor vs. outdoor classification, 95.3% for city vs. landscape classification, 96.6% for sunset vs. forest and mountain classification, and 95.5% for forest vs. mountain classification. We further develop a learning paradigm to incrementally train the classifiers as additional training samples become available and also show preliminary results for feature size reduction using clustering techniques."
            },
            "slug": "Content-based-hierarchical-classification-of-images-Vailaya-Figueiredo",
            "title": {
                "fragments": [],
                "text": "Content-based hierarchical classification of vacation images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A learning paradigm to incrementally train the classifiers as additional training samples become available is developed and preliminary results for feature size reduction using clustering techniques are shown."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE International Conference on Multimedia Computing and Systems"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2332212"
                        ],
                        "name": "Massimo Fasciano",
                        "slug": "Massimo-Fasciano",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Fasciano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimo Fasciano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806207"
                        ],
                        "name": "G. Lapalme",
                        "slug": "G.-Lapalme",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Lapalme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lapalme"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", the author\u2019s goal to convey ostensible increase or decrease of a quantity of interest (Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Elzer et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", by choosing sentence patterns to describe the trend of a stock present in a financial graphics) (Mittal et al., 1998, 1995; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9535039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69d62978c66ef9c4b94ab86e97238e1087576db6",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. To use graphics efficiently in an automatic report generation system, one has to model messages and how they go from the writer (intention) to the reader (interpretation). This paper describes PostGraphe, a system which generates a report integrating graphics and text from a set of writer's intentions. The system is given the data in tabular form as might be found in a spreadsheet; also input is a declaration of the types of values in the columns of the table. The user then indicates the intentions to be conveyed in the graphics (e.g., compare two variables or show the evolution of a set of variables) and the system generates a report in LATEX with the appropriate PostScript graphic files. PostGraphe uses the same information to generate the accompanying text that helps the reader to focus on the important points of the graphics. We also describe how these ideas have been embedded to create a new Chart Wizard for Microsoft Excel."
            },
            "slug": "Intentions-in-the-Coordinated-Generation-of-and-Fasciano-Lapalme",
            "title": {
                "fragments": [],
                "text": "Intentions in the Coordinated Generation of Graphics and Text from Tabular Data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "PostGraphe, a system which generates a report integrating graphics and text from a set of writer's intentions, is described, which has been embedded to create a new Chart Wizard for Microsoft Excel."
            },
            "venue": {
                "fragments": [],
                "text": "Knowledge and Information Systems"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680617"
                        ],
                        "name": "Donald Metzler",
                        "slug": "Donald-Metzler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Metzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Metzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Generative approaches for modeling relevance have been actively investigated in information retrieval (Lavrenko, 2004; Lavrenko and Croft, 2001; Metzler et al., 2004), and applied to the image annotation task (Lavrenko et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 237457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffb8d330005ee961b0b59eb2dd712a58714ec633",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language modeling allows formal methods to be applied to information retrieval. As a result, such methods are preferred over their heuristic tf.idf -based counterparts. In language modeling, a statistical model is estimated for each document in the corpus. Documents are then scored by the likelihood the query was generated by the document\u2019s model. Typically, the underlying model is assumed to be of a specific parametric form. In the past, a number of different assumptions have been made about this distribution. In [1], documents were modeled by a multiple-Bernoulli distribution. However, the estimation and smoothing techniques used to estimate the model were non-standard and somewhat heuristic. The predominant modeling assumption used today, as described in [2], is to model documents by a multinomial distribution. Such models may be smoothed in a number of ways [4]. Among these is Bayesian (Dirichlet) smoothing that takes a formal, Bayesian approach to smoothing by assuming a Dirichlet prior over the document model. Unlike Ponte and Croft\u2019s multiple-Bernoulli estimation techniques, the multinomial assumption combined with Bayesian smoothing results in a completely formal statistical model. In this paper, we revisit the multiple-Bernoulli assumption and formalize it by taking a Bayesian approach to estimating smoothed document models."
            },
            "slug": "Formal-multiple-bernoulli-models-for-language-Metzler-Lavrenko",
            "title": {
                "fragments": [],
                "text": "Formal multiple-bernoulli models for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The multiple-Bernoulli assumption is revisited and formalized by taking a Bayesian approach to estimating smoothed document models, resulting in a completely formal statistical model."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326473"
                        ],
                        "name": "S. Carberry",
                        "slug": "S.-Carberry",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "Carberry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carberry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5127675"
                        ],
                        "name": "Stephanie Elzer",
                        "slug": "Stephanie-Elzer",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Elzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephanie Elzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143752493"
                        ],
                        "name": "N. Green",
                        "slug": "N.-Green",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Green",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734707"
                        ],
                        "name": "Kathleen F. McCoy",
                        "slug": "Kathleen-F.-McCoy",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McCoy",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kathleen F. McCoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021168"
                        ],
                        "name": "D. Chester",
                        "slug": "D.-Chester",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Chester",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, Carberry et al. (2004) address the importance of graphics description generation in document summarization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 102471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad372d7335e80bb83732db3210a403ae90ad7602",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Information graphics (non-pictorial graphics such as bar charts or line graphs) are an important component of multimedia documents. Often such graphics convey information that is not contained elsewhere in the document. Thus document summarization must be extended to include summarization of information graphics. This paper addresses our work on graphic summarization. It argues that the message that the graphic designer intended to convey must play a major role in determining the content of the summary, and it outlines our approach to identifying this intended message and using it to construct the summary."
            },
            "slug": "Extending-Document-Summarization-to-Information-Carberry-Elzer",
            "title": {
                "fragments": [],
                "text": "Extending Document Summarization to Information Graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that the message that the graphic designer intended to convey must play a major role in determining the content of the summary, and the approach to identifying this intended message and using it to construct the summary is outlined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "SIFT features have been shown to be superior to other descriptors [45] and are considered state of the art in object recognition [46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2572455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69401bfdafab7cde00bb8e5b2f6c28e9d72d8cfb",
            "isKey": false,
            "numCitedBy": 3666,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors."
            },
            "slug": "A-performance-evaluation-of-local-descriptors-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "A performance evaluation of local descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is observed that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best and Moments and steerable filters show the best performance among the low dimensional descriptors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 107
                            }
                        ],
                        "text": "Next, they learn the correspondence between the blobs and words, using the IBM machine translation model 2 (Brown et al. 1993), to capture the probability of translating images into words:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13259913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab7b5917515c460b90451e67852171a531671ab8",
            "isKey": false,
            "numCitedBy": 4745,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus."
            },
            "slug": "The-Mathematics-of-Statistical-Machine-Translation:-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "The Mathematics of Statistical Machine Translation: Parameter Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus, given a set of pairs of sentences that are translations of one another."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3192273"
                        ],
                        "name": "David M. Zajic",
                        "slug": "David-M.-Zajic",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zajic",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Zajic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Duygulu et al. (2002) segment images into regions and cluster the latter using Kmeans into 500 classes which they call blobs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Headline generation is such a task that generates a very short title-like summary for a given document (Witbrock and Mittal, 1999; Banko et al., 2000; Jin and Hauptmann, 2001a,b, 2002; Zajic et al., 2002; Zhou and Hovy, 2003; Dorr et al., 2003; Bonnie et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1729177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb56d57d9e64fb2c0af7f19120aae94485df59e2",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches."
            },
            "slug": "Hedge-Trimmer:-A-Parse-and-Trim-Approach-to-Dorr-Zajic",
            "title": {
                "fragments": [],
                "text": "Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "Hedge Trimmer is presented, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline."
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116539467"
                        ],
                        "name": "Liang Zhou",
                        "slug": "Liang-Zhou",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58975603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c075f5b9ee6109a9bb6baa2e6fa0d71663178ab",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Headlines are useful for users who only need information on the main topics of a story. We present a headline summarization system that is built at ISI for this purpose and is a top performer for DUC2003\u2019s task 1, generating very short summaries (10 words or less)."
            },
            "slug": "Headline-Summarization-at-ISI-Zhou",
            "title": {
                "fragments": [],
                "text": "Headline Summarization at ISI"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work presents a headline summarization system that is built at ISI and is a top performer for DUC2003\u2019s task 1, generating very short summaries (10 words or less)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848824"
                        ],
                        "name": "Karen Sp\u00e4rck Jones",
                        "slug": "Karen-Sp\u00e4rck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sp\u00e4rck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sp\u00e4rck Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Our extractive caption generator draws inspiration from previous work on automatic summarization, most of which focuses on sentence extraction (see [48] and [49] for comprehensive overviews)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5362882,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "103239acc7b83e493943c566bc951654f8d24650",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This position paper suggests that progress with automatic summarising demands a better research methodology and a carefully focussed research strategy. In order to develop effective procedures it is necessary to identify and respond to the context factors, i.e. input, purpose, and output factors, that bear on summarising and its evaluation. The paper analyses and illustrates these factors and their implications for evaluation. It then argues that this analysis, together with the state of the art and the intrinsic difficulty of summarising, imply a nearer-term strategy c on shallow, but not surface, text analysis and on indicative summarising. This is illustrated with current work, from which a potentially productive research programme can be developed."
            },
            "slug": "Automatic-summarising:-factors-and-directions-Jones",
            "title": {
                "fragments": [],
                "text": "Automatic summarising: factors and directions"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "It is suggested that progress with automatic summarising demands a better research methodology and a carefully focussed research strategy, and that a nearer-term strategy c on shallow, but not surface, text analysis and on indicative summarising is required."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752967"
                        ],
                        "name": "L. Ferres",
                        "slug": "L.-Ferres",
                        "structuredName": {
                            "firstName": "Leo",
                            "lastName": "Ferres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ferres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542085"
                        ],
                        "name": "A. Parush",
                        "slug": "A.-Parush",
                        "structuredName": {
                            "firstName": "Avi",
                            "lastName": "Parush",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Parush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145736297"
                        ],
                        "name": "Shelley Roberts",
                        "slug": "Shelley-Roberts",
                        "structuredName": {
                            "firstName": "Shelley",
                            "lastName": "Roberts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shelley Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796956"
                        ],
                        "name": "G. Lindgaard",
                        "slug": "G.-Lindgaard",
                        "structuredName": {
                            "firstName": "Gitte",
                            "lastName": "Lindgaard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lindgaard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Caption generation, image annotation, summarization, topic models\n\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16192929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80c8c6e1ca57cec1adc93040a6279b039ad1c42f",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Much numerical information is visualized in graphs. However, this is a medium that is problematic for people with visual impairments. We have developed a system called iGraph which provides short verbal descriptions of the information usually depicted in graphs. This system was used as a preliminary solution that was validated through a process of User Needs Analysis (UNA). This process provided some basic data on the needs of people with visual impairments in terms of the components and the language to be used for graph comprehension and also validated our initial approach. The UNA provided important directions for the further development of iGraph particularly in terms of interactive querying of graphs"
            },
            "slug": "Helping-People-with-Visual-Impairments-Gain-Access-Ferres-Parush",
            "title": {
                "fragments": [],
                "text": "Helping People with Visual Impairments Gain Access to Graphical Information Through Natural Language: The iGraph System"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "iGraph is a system which provides short verbal descriptions of the information usually depicted in graphs which was used as a preliminary solution that was validated through a process of User Needs Analysis (UNA)."
            },
            "venue": {
                "fragments": [],
                "text": "ICCHP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145150531"
                        ],
                        "name": "ilkay Ulusoy",
                        "slug": "ilkay-Ulusoy",
                        "structuredName": {
                            "firstName": "ilkay",
                            "lastName": "Ulusoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ilkay Ulusoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In sum, discriminative image annotation approaches are difficult to port across different databases with different concept sets (Ulusoy and Bishop, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", cluttered scenes and multiple labels, which can be better handled by generative models since they are not as sensitive to the quality of labeling as discriminative models are (Ulusoy and Bishop, 2005) (see Chapter 2 for an overview)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", partially labeled data, and can relatively easily deal with changes of dataset and keyword vocabulary, compared to discriminative approaches (Ulusoy and Bishop, 2005; Holub, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14271284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59be0bc6397f99386dd6a87b5966735e88948b54",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Many approaches to object recognition are founded on probability theory, and can be broadly characterized as either generative or discriminative according to whether or not the distribution of the image features is modelled. Generative and discriminative methods have very different characteristics, as well as complementary strengths and weaknesses. In this paper we introduce new generative and discriminative models for object detection and classification based on weakly labelled training data. We use these models to illustrate the relative merits of the two approaches in the context of a data set of widely varying images of non-rigid objects (animals). Our results support the assertion that neither approach alone will be sufficient for large scale object recognition, and we discuss techniques for combining them."
            },
            "slug": "Generative-versus-discriminative-methods-for-object-Ulusoy-Bishop",
            "title": {
                "fragments": [],
                "text": "Generative versus discriminative methods for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results support the assertion that neither generative or discriminative approach alone will be sufficient for large scale object recognition, and the techniques for combining them are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801516"
                        ],
                        "name": "D. McNamara",
                        "slug": "D.-McNamara",
                        "structuredName": {
                            "firstName": "Danielle",
                            "lastName": "McNamara",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McNamara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50582151"
                        ],
                        "name": "S. Dennis",
                        "slug": "S.-Dennis",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Dennis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58530979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e181167dc005da9e19f7ff16b4707c828d043dfb",
            "isKey": false,
            "numCitedBy": 1171,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: Part I: Introduction to LSA: Theory and Methods. T.K. Landauer, LSA as a Theory of Meaning. D. Martin, M. Berry, Mathematical Foundations Behind Latent Semantic Analysis. S. Dennis, How to Use the LSA Website. J. Quesada, Creating Your Own LSA Spaces. Part II: LSA in Cognitive Theory. W. Kintsch, Meaning in Context. M. Louwerse, Symbolic or Embodied Representations: A Case for Symbol Interdependency. M.W. Howard, K. Addis, B. Jing, M.K. Kahana, Semantic Structure and Episodic Memory. G. DenhiSre, B. Lemaire, C. Bellissens, S. Jhean-Larose, A Semantic Space for Modeling Children's Semantic Memory. P. Foltz, Discourse Coherence and LSA. J. Quesada, Spaces for Problem Solving. Part III: LSA in Educational Applications. K. Millis, J. Magliano, K. Wiemer-Hastings, S. Todaro, D.S. McNamara, Assessing and Improving Comprehension With Latent Semantic Analysis. D.S. McNamara, C. Boonthum, I. Levinstein, K. Millis, Evaluating Self-Explanations in iSTART: Comparing Word-Based and LSA Algorithms. A. Graesser, P. Penumatsa, M. Ventura, Z. Cai, X. Hu, Using LSA in AutoTutor: Learning Through Mixed-Initiative Dialog in Natural Language. E. Kintsch, D. Caccamise, M. Franzke, N. Johnson, S. Dooley, Summary Streetr: Computer-Guided Summary Writing. L. Streeter, K. Lochbaum, N. LaVoie, J.E. Psotka, Automated Tools for Collaborative Learning Environments. Part IV: Information Retrieval and HCI Applications of LSA. S.T. Dumais, LSA and Information Retrieval: Getting Back to Basics. P.K. Foltz, T.K. Landauer, Helping People Find and Learn From Documents: Exploiting Synergies Between Human and Computer Retrieval With SuperManual. M.H. Blackmon, M. Kitajima, D.R. Mandalia, P.G. Polson, Automating Usability Evaluation Cognitive Walkthrough for the Web Puts LSA to Work on Real-World HCI Design Problems. Part V: Extensions to LSA. D.S. McNamara, Z. Cai, M.M. Louwerse, Optimizing LSA Measures of Cohesion. X. Hu, Z. Cai, P. Wiemer-Hastings, A.C. Graesser, D.S. McNamara, Strength, Weakness, and Extensions of LSA. M. Steyvers, T. Griffiths, Probabilistic Topic Models. S. Dennis, Introducing Word Order: Within the LSA Framework. Part VI: Conclusion. W. Kintsch, D.S. McNamara, S. Dennis, T.K. Landauer, LSA and Meaning: In Theory and Application."
            },
            "slug": "Handbook-of-latent-semantic-analysis-Landauer-McNamara",
            "title": {
                "fragments": [],
                "text": "Handbook of latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This book discusses Latent Semantic Analysis as a Theory of Meaning, its application in Cognitive Theory, and its applications in Educational Applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "36% on the PennTree bank (Marcus et al. 1994)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8085545"
                        ],
                        "name": "Chi Tsai",
                        "slug": "Chi-Tsai",
                        "structuredName": {
                            "firstName": "Chi",
                            "lastName": "Tsai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi Tsai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMI-2011-10-0718."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62713841,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "603830cd53c3a7addc0c0151ff2270b2202d34d6",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatically-Annotating-Images-with-Keywords:-A-of-Tsai",
            "title": {
                "fragments": [],
                "text": "Automatically Annotating Images with Keywords: A Review of Image Annotation Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40544823"
                        ],
                        "name": "Hongyan Jing",
                        "slug": "Hongyan-Jing",
                        "structuredName": {
                            "firstName": "Hongyan",
                            "lastName": "Jing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongyan Jing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 800331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba0eac94ff6e5bc956852e21ba08df4827448f59",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resulting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer."
            },
            "slug": "Cut-and-Paste-Based-Text-Summarization-Jing-McKeown",
            "title": {
                "fragments": [],
                "text": "Cut and Paste Based Text Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which is used to develop the summarizer."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50649676"
                        ],
                        "name": "J. Clarke",
                        "slug": "J.-Clarke",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Clarke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3004447,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ec86811a79fb02a1c551b8f418314a00f5f5a99",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models."
            },
            "slug": "Global-inference-for-sentence-compression-:-an-Clarke-Lapata",
            "title": {
                "fragments": [],
                "text": "Global inference for sentence compression : an integer linear programming approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work shows how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints to infer globally optimal compressions in the presence of linguistically motivated constraints."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We chose to use TER over other metrics with similar properties such as BLEU (Papineni et al., 2002) since it can account for word reordering and be applied to individual sentences without any adjustments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": false,
            "numCitedBy": 16627,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49693392"
                        ],
                        "name": "A. Kojima",
                        "slug": "A.-Kojima",
                        "structuredName": {
                            "firstName": "Atsuhiro",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kojima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2899347"
                        ],
                        "name": "M. Takaya",
                        "slug": "M.-Takaya",
                        "structuredName": {
                            "firstName": "Mamoru",
                            "lastName": "Takaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Takaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50686092"
                        ],
                        "name": "S. Aoki",
                        "slug": "S.-Aoki",
                        "structuredName": {
                            "firstName": "Shigeki",
                            "lastName": "Aoki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Aoki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056934504"
                        ],
                        "name": "T. Miyamoto",
                        "slug": "T.-Miyamoto",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Miyamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Miyamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950023"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Kunio",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "As this limits the applicability of search engines (images that do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words for a picture automatically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Caption generation, image annotation, summarization, topic models\n\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11647837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7806ba91eade96c025bdb2cb8d0c0cdfddb8587",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method for recognizing human actions and objects and translating them into natural language text. First, 3D environmental map is constructed by accumulating range maps captured from a 3D range sensor mounted on a mobile robot. Then, pose of a person in the scene is estimated by fitting articulated cylindrical model and also object is recognized by matching 3D models. On condition that the person handles some objects, interaction with the object is classified. Finally, using conceptual model representing human actions and related objects, a natural language expression which is most suitable to explain the person's action is generated."
            },
            "slug": "Recognition-and-Textual-Description-of-Human-by-Kojima-Takaya",
            "title": {
                "fragments": [],
                "text": "Recognition and Textual Description of Human Activities by Mobile Robot"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "3D environmental map is constructed by accumulating range maps captured from a 3D range sensor mounted on a mobile robot, and conceptual model representing human actions and related objects are represented."
            },
            "venue": {
                "fragments": [],
                "text": "2008 3rd International Conference on Innovative Computing Information and Control"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082299938"
                        ],
                        "name": "D. Tal",
                        "slug": "D.-Tal",
                        "structuredName": {
                            "firstName": "Doron",
                            "lastName": "Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "During testing, we are given a document and an associated image for which we must generate a caption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1ed876196ec9733acb1daa6d65e35ff0414291",
            "isKey": false,
            "numCitedBy": 6039,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties."
            },
            "slug": "A-database-of-human-segmented-natural-images-and-to-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes is presented and an error measure is defined which quantifies the consistency between segmentations of differing granularities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60691216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54bcfca3fddc26b8889739a247a25e445818149",
            "isKey": false,
            "numCitedBy": 3827,
            "numCiting": 263,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing."
            },
            "slug": "Speech-and-language-processing-an-introduction-to-Jurafsky-Martin",
            "title": {
                "fragments": [],
                "text": "Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora, to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in artificial intelligence"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40293552"
                        ],
                        "name": "J. Peters",
                        "slug": "J.-Peters",
                        "structuredName": {
                            "firstName": "Jochen",
                            "lastName": "Peters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561225"
                        ],
                        "name": "D. Klakow",
                        "slug": "D.-Klakow",
                        "structuredName": {
                            "firstName": "Dietrich",
                            "lastName": "Klakow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klakow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3027407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2854e19e23a0987d0989cd8dfadebec3692e3a60",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method is presented to quickly adapt a given language model to local text characteristics. The basic approach is to choose the adaptive models as close as possible to the background estimates while constraining them to respect the locally estimated unigram probabilities. Several means are investigated to speed up the calculations. We measure both perplexity and word error rate to gauge the quality of our model."
            },
            "slug": "Language-model-adaptation-using-dynamic-marginals-Kneser-Peters",
            "title": {
                "fragments": [],
                "text": "Language model adaptation using dynamic marginals"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new method is presented to quickly adapt a given language model to local text characteristics by choosing the adaptive models as close as possible to the background estimates while constraining them to respect the locally estimated unigram probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2614094"
                        ],
                        "name": "Rebecca F. Bruce",
                        "slug": "Rebecca-F.-Bruce",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Bruce",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rebecca F. Bruce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2We use the term knowledge-lean to refer to approaches that minimize the denpendence on finegained training data, external rules or knowledge sources (Pedersen and Bruce, 1998), such as predefined grammars or sentence templates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6407161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0c30877ce30c3f02ed4c32dff413eb25f0f9f6f",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a corpus-based approach to word-sense disambiguation that only requires information that can be automatically extracted from untagged text. We use unsupervised techniques to estimate the parameters of a model describing the conditional distribution of the sense group given the known contextual features. Both the EM algorithm and Gibbs Sampling are evaluated to determine which is most appropriate for our data. We compare their disambiguation accuracy in an experiment with thirteen different words and three feature sets. Gibbs Sampling results in small but consistent improvement in disambiguation accuracy over the EM algorithm."
            },
            "slug": "Knowledge-Lean-Word-Sense-Disambiguation-Pedersen-Bruce",
            "title": {
                "fragments": [],
                "text": "Knowledge Lean Word-Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A corpus-based approach to word-sense disambiguation that only requires information that can be automatically extracted from untagged text and Gibbs Sampling results in small but consistent improvement in disambigsuation accuracy over the EM algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3192273"
                        ],
                        "name": "David M. Zajic",
                        "slug": "David-M.-Zajic",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zajic",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Zajic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 103
                            }
                        ],
                        "text": "Headline generation is such a task that generates a very short title-like summary for a given document (Witbrock and Mittal, 1999; Banko et al., 2000; Jin and Hauptmann, 2001a,b, 2002; Zajic et al., 2002; Zhou and Hovy, 2003; Dorr et al., 2003; Bonnie et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62613480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "462886d6616c358d627fc3690ed78990a94900e3",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports our results at DUC2004 and describes our approach, implemented in a system called Topiary. We will show that the combination of linguistically motivated sentence compression with statistically selected topic terms performs better than either alone, according to some automatic summary evaluation measures."
            },
            "slug": "BBN/UMD-at-DUC-2004:-Topiary-Zajic-Dorr",
            "title": {
                "fragments": [],
                "text": "BBN/UMD at DUC-2004: Topiary"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is shown that the combination of linguistically motivated sentence compression with statistically selected topic terms performs better than either alone or either alone, according to some automatic summary evaluation measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809403"
                        ],
                        "name": "S. Feiner",
                        "slug": "S.-Feiner",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Feiner",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Feiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", by choosing sentence patterns to describe the trend of a stock present in a financial graphics) (Mittal et al., 1998, 1995; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5496468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5458838502b70f00cb69cc5885d0ab3ab032a299",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "To generate multimedia explanations, a system must be able to coordinate the use of different media in a single explanation. In this paper, we present the architecture that we have developed for COMET (COordinated Multimedia Explanation Testbed), a system that generates directions for equipment maintenance and repair, and we show how it addresses the coordination problem. COMET includes a single content planner that produces a common content description used by multiple media-specific generators, and a media coordinator that performs a fine-grained division of information among media. Bidirectional interaction between media-specific generators allows influence across media. We describe COMET's current capabilities and provide an overview of our plans for extending the system."
            },
            "slug": "Coordinating-Text-and-Graphics-in-Explanation-Feiner-McKeown",
            "title": {
                "fragments": [],
                "text": "Coordinating Text and Graphics in Explanation Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The architecture that is developed for COMET (COordinated Multimedia Explanation Testbed), a system that generates directions for equipment maintenance and repair, is presented and it is shown how it addresses the coordination problem."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "We can overcome this by representing keywords and sentences in vector space [50] and computing the similarity between the two vectors representing the image keywords and document sentences, respectively."
                    },
                    "intents": []
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another solution is to construct a hierarchical structure to describe the relations among the words using WordNet (Miller, 1995) or other structured lexicon databases (Fan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This data collection project is ongoing and their ultimate goal is to created a visual version for WordNet (Miller, 1995), with the visual synsets corresponding to WordNet synsets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13891,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897400"
                        ],
                        "name": "S. Schwartz",
                        "slug": "S.-Schwartz",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "Elzer"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326473"
                        ],
                        "name": "S. Carberry",
                        "slug": "S.-Carberry",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "Carberry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carberry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48127057"
                        ],
                        "name": "Ingrid Zukerman",
                        "slug": "Ingrid-Zukerman",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Zukerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingrid Zukerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021168"
                        ],
                        "name": "D. Chester",
                        "slug": "D.-Chester",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Chester",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143752493"
                        ],
                        "name": "N. Green",
                        "slug": "N.-Green",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Green",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10109293"
                        ],
                        "name": "Seniz Demir",
                        "slug": "Seniz-Demir",
                        "structuredName": {
                            "firstName": "Seniz",
                            "lastName": "Demir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seniz Demir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "At training time, our models learn from images, their captions, and associated documents, while at test time they are given an image and the document it is embedded in and generate a caption."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2623140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5fc1848866e4c446f143ac54bba808dd883265e",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper extends language understanding and plan inference to information graphics. We identify the kinds of communicative signals that appear in information graphics, describe how we utilize them in a Bayesian network that hypothesizes the graphic's intended message, and discuss the performance of our implemented system. This work is part of a larger project aimed at making information graphics accessible to individuals with sight impairments."
            },
            "slug": "A-Probabilistic-Framework-for-Recognizing-Intention-Schwartz-Carberry",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Framework for Recognizing Intention in Information Graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The kinds of communicative signals that appear in information graphics are identified, how they are utilized in a Bayesian network that hypothesizes the graphic's intended message is described, and the performance of the implemented system is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2003) and several forms of Markov chain Monte Carlo (Jordan, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60578841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c9529259e180dea589447d9b7414a998286e1c2",
            "isKey": false,
            "numCitedBy": 1466,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams."
            },
            "slug": "Learning-in-Graphical-Models-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper presents an introduction to inference for Bayesian networks and a view of the EM algorithm that justifies incremental, sparse and other variants, as well as an information-theoretic analysis of hard and soft assignment methods for clustering."
            },
            "venue": {
                "fragments": [],
                "text": "NATO ASI Series"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "In order to do this we use the Scale Invariant Feature Transform (SIFT) algorithm [43], [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5258236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9f836d28f52ad260213d32224a6d227f8e8849a",
            "isKey": false,
            "numCitedBy": 16258,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "slug": "Object-recognition-from-local-scale-invariant-Lowe",
            "title": {
                "fragments": [],
                "text": "Object recognition from local scale-invariant features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737285"
                        ],
                        "name": "Radu Soricut",
                        "slug": "Radu-Soricut",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Soricut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Soricut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 934325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08e6c49c69f016308189fd2152bc31779c338f60",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation."
            },
            "slug": "Stochastic-Language-Generation-Using-and-its-in-and-Soricut-Marcu",
            "title": {
                "fragments": [],
                "text": "Stochastic Language Generation Using WIDL-Expressions and its Application in Machine Translation and Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work shows the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195306"
                        ],
                        "name": "H. Bay",
                        "slug": "H.-Bay",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Bay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433494"
                        ],
                        "name": "Andreas Ess",
                        "slug": "Andreas-Ess",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14777911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdbb606ae47c64049262dfbd3bb147d3f4ba8420",
            "isKey": false,
            "numCitedBy": 11654,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speeded-Up-Robust-Features-(SURF)-Bay-Ess",
            "title": {
                "fragments": [],
                "text": "Speeded-Up Robust Features (SURF)"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93707734"
                        ],
                        "name": "Helmut Schmidt",
                        "slug": "Helmut-Schmidt",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helmut Schmidt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17392458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd0bab6fc8cd43c0ce170ad2f4cb34181b31277d",
            "isKey": false,
            "numCitedBy": 2957,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data. In this tagging method, transition probabilities are estimated using a decision tree. Based on this method, a part-of-speech tagger (called TreeTagger) has been implemented which achieves 96.36 % accuracy on Penn-Treebank data which is better than that of a trigram tagger (96.06 %) on the same data."
            },
            "slug": "Probabilistic-part-of-speech-tagging-using-decision-Schmidt",
            "title": {
                "fragments": [],
                "text": "Probabilistic part-of-speech tagging using decision trees"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data, using a decision tree."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107761575"
                        ],
                        "name": "S. S. Jones",
                        "slug": "S.-S.-Jones",
                        "structuredName": {
                            "firstName": "S",
                            "lastName": "Jones",
                            "middleNames": [
                                "S"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. S. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107558564"
                        ],
                        "name": "L. Smith",
                        "slug": "L.-Smith",
                        "structuredName": {
                            "firstName": "L",
                            "lastName": "Smith",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559335"
                        ],
                        "name": "B. Landau",
                        "slug": "B.-Landau",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Landau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Landau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1998) and texture (Jones et al., 1991)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28631707,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d322c0e819361a3e42507ae261be31ab3005e684",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The ease with which young children learn object nouns suggests that they possess strategies to identify properties critical to lexical category membership. In previous work, young children used a same-shape criterion to extend new count nouns. The present research tested the generality of this shape bias. 2- and 3-year-olds were asked either to extend a novel count noun to new instances, or to choose unnamed objects to go together. The objects varied in shape, size, and texture. For half of the subjects, the objects had eyes--a property strongly associated with certain material kinds. If young children know this association, they should attend to texture as well as shape in classifying objects with eyes. With named objects only, both 2- and 3-year-old children classified eyeless objects by shape and objects with eyes by both shape and texture. The results suggest that very young children possess considerable knowledge about conditional relations between kinds of perceptual properties. Knowledge of such conditional relations may aid children in forming new categories and thus in discovering new word meanings."
            },
            "slug": "Object-properties-and-knowledge-in-early-lexical-Jones-Smith",
            "title": {
                "fragments": [],
                "text": "Object properties and knowledge in early lexical learning."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results suggest that very young children possess considerable knowledge about conditional relations between kinds of perceptual properties that may aid children in forming new categories and thus in discovering new word meanings."
            },
            "venue": {
                "fragments": [],
                "text": "Child development"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145730345"
                        ],
                        "name": "A. Abella",
                        "slug": "A.-Abella",
                        "structuredName": {
                            "firstName": "Alicia",
                            "lastName": "Abella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Abella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719062"
                        ],
                        "name": "J. Kender",
                        "slug": "J.-Kender",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kender",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kender"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690334"
                        ],
                        "name": "J. Starren",
                        "slug": "J.-Starren",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Starren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Starren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Caption generation, image annotation, summarization, topic models\n\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 45249140,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5bce81a52b81f434b00e7a9c633075ab74a7b0a1",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a system for describing renal stones found in radiographs. The system generates descriptions that adhere to those generated by radiologists. The descriptions are formulated by discovering the spatial relationships that exist between the major organs and the renal stones. The system consists of three major components. The first is the image processing component which is responsible for locating the stone. The second component is the inference network minimization component which determines which spatial relationships, of all those that exist between the stone and the organs, is the most descriptive. The third component is the natural language generation component which is responsible for translating the spatial relationships into appropriate medical terminology. We will illustrate all these components on several examples."
            },
            "slug": "Description-generation-of-abnormal-densities-found-Abella-Kender",
            "title": {
                "fragments": [],
                "text": "Description generation of abnormal densities found in radiographs."
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A system for describing renal stones found in radiographs that generates descriptions that adhere to those generated by radiologists, and consists of three major components."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Symposium on Computer Applications in Medical Care"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 398,
                                "start": 228
                            }
                        ],
                        "text": "Datasets used for object detection, recognition and image segmentation often require such detailed annotations since these applications need not only the object names, but also the accurate contours and locations of the objects (FeiFei et al., 2004; Griffin et al., 2007; Deng et al., 2009; Russell et al., 2008; Martin et al., 2001; Barnard et al., 2008; Yao et al., 2009; Everingham et al., 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 50
                            }
                        ],
                        "text": "ularies are, therefore, only a few hundred words4 (Fei-Fei et al., 2004; Griffin et al., 2007; Martin et al., 2001; Everingham et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "3, the PASCAL Visual Object Classes (VOC) Challenge 2007 dataset2 (Everingham et al., 2007) and ImageNet3 (Deng et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61615905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a2ed19ac684022aa3186887cd4893484ab8f80c",
            "isKey": true,
            "numCitedBy": 2169,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006). Details of the challenge, data, and evaluation are presented. Participants in the challenge submitted descriptions of their methods, and these have been included verbatim. This document should be considered preliminary, and subject to change."
            },
            "slug": "The-PASCAL-visual-object-classes-challenge-2006-Everingham-Zisserman",
            "title": {
                "fragments": [],
                "text": "The PASCAL visual object classes challenge 2006 (VOC2006) results"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11495042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
            },
            "slug": "Accurate-Unlexicalized-Parsing-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Accurate Unlexicalized Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is demonstrated that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146178376"
                        ],
                        "name": "Anna Bosch Ru\u00e9",
                        "slug": "Anna-Bosch-Ru\u00e9",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Ru\u00e9",
                            "middleNames": [
                                "Bosch"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Bosch Ru\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "SIFT features have been shown to be superior to other descriptors [45] and are considered state of the art in object recognition [46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 191331768,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "0958627da9704a908bcc233f3d413918379b2608",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 146,
            "paperAbstract": {
                "fragments": [],
                "text": "Lincrement de bases de dades que cada vegada contenen imatges mes dificils i amb un nombre mes elevat de categories, esta forcant el desenvolupament de tecniques de representacio dimatges que siguin discriminatives quan es vol treballar amb multiples classes i dalgorismes que siguin eficients en laprenentatge i classificacio. Aquesta tesi explora el problema de classificar les imatges segons lobjecte que contenen quan es disposa dun gran nombre de categories. Primerament sinvestiga com un sistema hibrid format per un model generatiu i un model discriminatiu pot beneficiar la tasca de classificacio dimatges on el nivell danotacio huma sigui minim. Per aquesta tasca introduim un nou vocabulari utilitzant una representacio densa de descriptors color-SIFT, i despres sinvestiga com els diferents parametres afecten la classificacio final. Tot seguit es proposa un metode par tal dincorporar informacio espacial amb el sistema hibrid, mostrant que la informacio de context es de gran ajuda per la classificacio dimatges. Despres introduim un nou descriptor de forma que representa la imatge segons la seva forma local i la seva forma espacial, tot junt amb un kernel que incorpora aquesta informacio espacial en forma piramidal. La forma es representada per un vector compacte obtenint un descriptor molt adequat per esser utilitzat amb algorismes daprenentatge amb kernels. Els experiments realitzats postren que aquesta informacio de forma te uns resultats semblants (i a vegades millors) als descriptors basats en aparenca. Tambe sinvestiga com diferents caracteristiques es poden combinar per esser utilitzades en la classificacio dimatges i es mostra com el descriptor de forma proposat juntament amb un descriptor daparenca millora substancialment la classificacio. Finalment es descriu un algoritme que detecta les regions dinteres automaticament durant lentrenament i la classificacio. Aixo proporciona un metode per inhibir el fons de la imatge i afegeix invarianca a la posicio dels objectes dins les imatges. Sensenya que la forma i laparenca sobre aquesta regio dinteres i utilitzant els classificadors random forests millora la classificacio i el temps computacional. Es comparen els postres resultats amb resultats de la literatura utilitzant les mateixes bases de dades que els autors Aixa com els mateixos protocols daprenentatge i classificacio. Es veu com totes les innovacions introduides incrementen la classificacio final de les imatges."
            },
            "slug": "Image-classification-for-a-large-number-of-object-Ru\u00e9",
            "title": {
                "fragments": [],
                "text": "Image classification for a large number of object categories"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559335"
                        ],
                        "name": "B. Landau",
                        "slug": "B.-Landau",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Landau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Landau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836466"
                        ],
                        "name": "Linda B. Smith",
                        "slug": "Linda-B.-Smith",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Smith",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda B. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48795139"
                        ],
                        "name": "Susan S. Jones",
                        "slug": "Susan-S.-Jones",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Jones",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan S. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", they generalize object names to new objects often on the basis of similarity in shape (Landau et al., 1998) and texture (Jones et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40906778,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cce8465738af258e6c0c3985bceafadf1890befa",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Object-perception-and-object-naming-in-early-Landau-Smith",
            "title": {
                "fragments": [],
                "text": "Object perception and object naming in early development"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902749"
                        ],
                        "name": "P. Quinn",
                        "slug": "P.-Quinn",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Quinn",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Quinn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047139"
                        ],
                        "name": "P. Eimas",
                        "slug": "P.-Eimas",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Eimas",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Eimas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5783404"
                        ],
                        "name": "Stacey L. Rosenkrantz",
                        "slug": "Stacey-L.-Rosenkrantz",
                        "structuredName": {
                            "firstName": "Stacey",
                            "lastName": "Rosenkrantz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stacey L. Rosenkrantz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, infants, from an early age, are able to form perceptually-based category representations (Quinn et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18937566,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "b08c7c062168489d65655daacde05a7c06ab30ef",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The paired-preference procedure was used in a series of experiments to explore the abilities of infants aged 3 and 4 months to categorize photographic exemplars from natural (adult-defined) basic-level categories. The question of whether the categorical representations that were evidenced excluded members of a related, perceptually similar category was also investigated. Experiments 1\u20133 revealed that infants could form categorical representations for dogs and cats that excluded birds. Experiment 4 showed that the representation for cats also excluded dogs, but that the representation for dogs did not exclude cats. However, a supplementary experiment showed that the representation for dogs did exclude cats when the variability of the dog exemplars was reduced to match that of the cat exemplars. The results are discussed in terms of abilities necessary for the formation of more complex categorical representations."
            },
            "slug": "Evidence-for-Representations-of-Perceptually-by-and-Quinn-Eimas",
            "title": {
                "fragments": [],
                "text": "Evidence for Representations of Perceptually Similar Natural Categories by 3-Month-Old and 4-Month-Old Infants"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The paired-preference procedure was used in a series of experiments to explore the abilities of infants aged 3 and 4 months to categorize photographic exemplars from natural (adult-defined) basic-level categories and revealed that infants could form categorical representations for dogs and cats that excluded birds."
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Word morphology and necessary function words are often manually imported into an NLG system as rules (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1060508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "925b40ae3aa7ed1bf642d78dc80fce1f573293e2",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to operate well even when pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable."
            },
            "slug": "Two-Level,-Many-Paths-Generation-Knight-Hatzivassiloglou",
            "title": {
                "fragments": [],
                "text": "Two-Level, Many-Paths Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A hybrid generator is built, in which gaps in symbolic knowledge are filled by statistical methods, to attack problems of large-scale natural language generation and to simplify current generators and enhance their portability."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060785901"
                        ],
                        "name": "F. Quimby",
                        "slug": "F.-Quimby",
                        "structuredName": {
                            "firstName": "Freddy",
                            "lastName": "Quimby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Quimby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Our approach leverages the vast resource of pictures available on the web and the fact that many of them naturally co-occur with topically related documents and are captioned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6082135,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "2788d0f1f7fbd44ebb5185e59c4aaf09aad97013",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\u2022 A picture in Radiance is a map of RGB radiance (or irradiance) values \u2022 The exposure of a Radiance picture may be adjusted without loss since it contains a dynamic range on the order of 10^77 \u2022 Individual radiance (or luminance) values may be displayed on demand by the X11 viewer, ximage \u2022 The falsecolor program may be used to convert an image to a numerically readable value map with legend \u2022 The glare program may be used to identify and analyze glare sources in a picture or scene \u2022 Other programs (principally rtrace) may be used to compute values that are not easily represented as a map"
            },
            "slug": "What's-in-a-picture-Quimby",
            "title": {
                "fragments": [],
                "text": "What's in a picture?"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The exposure of a Radiance picture may be adjusted without loss since it contains a dynamic range on the order of 10^77 and individual radiance values may be displayed on demand by the X11 viewer, ximage."
            },
            "venue": {
                "fragments": [],
                "text": "Laboratory animal science"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "In order to do this we use the Scale Invariant Feature Transform (SIFT) algorithm [43], [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25505,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34580624"
                        ],
                        "name": "M. Bornstein",
                        "slug": "M.-Bornstein",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Bornstein",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bornstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28305571"
                        ],
                        "name": "L. Cote",
                        "slug": "L.-Cote",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Cote",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cote"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49276662"
                        ],
                        "name": "S. Maital",
                        "slug": "S.-Maital",
                        "structuredName": {
                            "firstName": "Sharone",
                            "lastName": "Maital",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maital"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402448357"
                        ],
                        "name": "K. Painter",
                        "slug": "K.-Painter",
                        "structuredName": {
                            "firstName": "Kathleen M.",
                            "lastName": "Painter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Painter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46570158"
                        ],
                        "name": "Sung-Yun Park",
                        "slug": "Sung-Yun-Park",
                        "structuredName": {
                            "firstName": "Sung-Yun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sung-Yun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068389918"
                        ],
                        "name": "L. Pascual",
                        "slug": "L.-Pascual",
                        "structuredName": {
                            "firstName": "Liliana",
                            "lastName": "Pascual",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pascual"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114836574"
                        ],
                        "name": "M. P\u00eacheux",
                        "slug": "M.-P\u00eacheux",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "P\u00eacheux",
                            "middleNames": [
                                "Germaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P\u00eacheux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152571081"
                        ],
                        "name": "J. Ruel",
                        "slug": "J.-Ruel",
                        "structuredName": {
                            "firstName": "Josette",
                            "lastName": "Ruel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ruel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415806"
                        ],
                        "name": "P. Venuti",
                        "slug": "P.-Venuti",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Venuti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Venuti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7306447"
                        ],
                        "name": "A. Vyt",
                        "slug": "A.-Vyt",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Vyt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vyt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Perhaps unsurprisingly, words that refer to concrete entities and actions are among the first words being learned as these are directly observable in the environment (Bornstein et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1999; Carneiro and Vasconcelos, 2005) directly divide the image evenly into rectangular regions. Specifically, Wang and Li (2002) adopt the 2-D Multi-resolution Hidden Markov Model, which has been successfully applied in image segmentation research and can explore the statistical dependence among image rectangles across multiple resolutions, thereby avoiding the reliance on image segmentations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31573227,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "802f3d6623e706915907f80ebfadddd5c4a5aa49",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "The composition of young children's vocabularies in 7 contrasting linguistic communities was investigated. Mothers of 269 twenty-month-olds in Argentina, Belgium, France, Israel, Italy, the Republic of Korea, and the United States completed comparable vocabulary checklists for their children. In each language and vocabulary size grouping (except for children just learning to talk), children's vocabularies contained relatively greater proportions of nouns than other word classes. Each word class was consistently positively correlated with every other class in each language and for children with smaller and larger vocabularies. Noun prevalence in the vocabularies of young children and the merits of several theories that may account for this pattern are discussed."
            },
            "slug": "Cross-linguistic-analysis-of-vocabulary-in-young-Bornstein-Cote",
            "title": {
                "fragments": [],
                "text": "Cross-linguistic analysis of vocabulary in young children: spanish, dutch, French, hebrew, italian, korean, and american english."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Noun prevalence in the vocabularies of young children and the merits of several theories that may account for this pattern are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Child development"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3613636"
                        ],
                        "name": "S. Gunasekharan",
                        "slug": "S.-Gunasekharan",
                        "structuredName": {
                            "firstName": "Subahshini",
                            "lastName": "Gunasekharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gunasekharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32498452"
                        ],
                        "name": "N. Mayo",
                        "slug": "N.-Mayo",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Mayo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Mayo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996123"
                        ],
                        "name": "M. Corley",
                        "slug": "M.-Corley",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Corley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Corley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "Second, we can exploit the rich linguistic information inherent in the text and address caption generation with methods akin to text summarization without extensive knowledge engineering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17047232,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8db3dc5390a0fc7bd4e605aa493eaf5e42abaea6",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Internet-based experiments are gaining in popularity, most studies rely on directly evaluating participants\u2019 responses rather than response times. In the present article, we present two experiments that demonstrate the feasibility of collecting response latency data over the World-Wide Web using WebExp\u2014a software package designed to run psychological experiments over the Internet. Experiment 1 uses WebExp to collect measurements for known time intervals (generated using keyboard repetition). The resulting measurements are found to be accurate across platforms and load conditions. In Experiment 2, we use WebExp to replicate a lab-based self-paced reading study from the psycholinguistic literature. The data of the Web-based replication correlate significantly with those of the original study and show the same main effects and interactions. We conclude that WebExp can be used to obtain reliable response time data, at least for the self-paced reading paradigm."
            },
            "slug": "Timing-accuracy-of-Web-experiments:-A-case-study-Keller-Gunasekharan",
            "title": {
                "fragments": [],
                "text": "Timing accuracy of Web experiments: A case study using the WebExp software package"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Two experiments demonstrate the feasibility of collecting response latency data over the World-Wide Web using WebExp\u2014a software package designed to run psychological experiments over the Internet and conclude that WebExp can be used to obtain reliable response time data, at least for the self-paced reading paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144729741"
                        ],
                        "name": "K. J. Evans",
                        "slug": "K.-J.-Evans",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Evans",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. J. Evans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62768333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8be229a32b27556efb32d0b9b352838dddafce6",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Intensive-Methods-for-Testing-Hypotheses:-Evans",
            "title": {
                "fragments": [],
                "text": "Computer Intensive Methods for Testing Hypotheses: An Introduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40584063"
                        ],
                        "name": "Binoy Pinto",
                        "slug": "Binoy-Pinto",
                        "structuredName": {
                            "firstName": "Binoy",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binoy Pinto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124673168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc4f344af0057dd35bf69c5924bc71d2751a69d",
            "isKey": false,
            "numCitedBy": 706,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speeded-Up-Robust-Features-Pinto",
            "title": {
                "fragments": [],
                "text": "Speeded Up Robust Features"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We refer the interested reader to Monay and GaticaPerez [22] and Buckley and Voorhees [59] for more details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67700063,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24080e2499eed3204f0d171b6c1cd099d6d6bf21",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Retrieval-System-Evaluation-Buckley-Voorhees",
            "title": {
                "fragments": [],
                "text": "Retrieval System Evaluation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8147976"
                        ],
                        "name": "G. Chowdhury",
                        "slug": "G.-Chowdhury",
                        "structuredName": {
                            "firstName": "Gobinda",
                            "lastName": "Chowdhury",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Chowdhury"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60715646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0656f27ca3bc8dbe1f57bf8e6e35cb5afcbf004b",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "TREC:-Experiment-and-Evaluation-in-Information-Chowdhury",
            "title": {
                "fragments": [],
                "text": "TREC: Experiment and Evaluation in Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sentence Selection based on Topic Distribution In probabilistic topic models, the similarity between two documents can be measured by the extent to which they share common topics (Steyvers and Griffiths, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56964528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5881edb7593d4c1f2745066f687935192a643da4",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Topic-Models-Steyvers-Griffiths",
            "title": {
                "fragments": [],
                "text": "Probabilistic Topic Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62540156,
            "fieldsOfStudy": [],
            "id": "51966f024c49544669450819a595774a99de7369",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331580"
                        ],
                        "name": "O. Maron",
                        "slug": "O.-Maron",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Maron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Maron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578809"
                        ],
                        "name": "A. L. Ratan",
                        "slug": "A.-L.-Ratan",
                        "structuredName": {
                            "firstName": "Aparna",
                            "lastName": "Ratan",
                            "middleNames": [
                                "Lakshmi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. L. Ratan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some methods adopt a \u201done vs all\u201d model (Vailaya et al., 1999; Maron and Ratan, 1998; Qi and Han, 2007; Chai and Hung, 2008; Gupta et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The use of MIL greatly reduces the negative effect of weak labeling (Maron and Ratan, 1998; Carneiro and Vasconcelos, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39240439,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "6e0616e65727c7ab9c185c92e15ccd405cfc2a0b",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multiple-Instance-Learning-for-Natural-Scene-Maron-Ratan",
            "title": {
                "fragments": [],
                "text": "Multiple-Instance Learning for Natural Scene Classification"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582677"
                        ],
                        "name": "Michael Witbrock",
                        "slug": "Michael-Witbrock",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Witbrock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Witbrock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751139"
                        ],
                        "name": "V. Mittal",
                        "slug": "V.-Mittal",
                        "structuredName": {
                            "firstName": "Vibhu",
                            "lastName": "Mittal",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mittal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208926737,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "cc7371cb1fe7c020aa13610d384cf869062700d6",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A game machine which can be played by two or more players includes an elongated box-like housing in which a game ball can be inserted, the housing including elongated side members with transparent windows therein and opposed end members with openings in their upper portions through which the game ball can be projected, the housing also including two projector elements in respective opposite ends thereof which are capable of utilization by competing players to project the game ball towards the opposite end member, and at least two activator elements in the housing between the two projector elements which are capable of utilization by competing players to contact and move a game ball in the desired fashion. A flooring structure inside the housing forms a contoured playing deck surface above the bottom of the housing and provides multiple, uniform and equally spaced-apart spaces which extend from one end member of the housing to the other. The projector elements and the activator elements include portions which can move within these spaces from a positioning below the playing deck surface to varying positionings above the playing deck surface so as to cause suitable manipulations of the game ball, including dribbling, when contacted by the noted projector element and activator element portions."
            },
            "slug": "Ultra-Summarization:-A-Statistical-Approach-to-Witbrock-Mittal",
            "title": {
                "fragments": [],
                "text": "Ultra-Summarization: A Statistical Approach to Generating Highly Condensed Non-Extractive Summaries (poster abstract)."
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A game machine which can be played by two or more players includes an elongated box-like housing in which a game ball can be inserted and portions which can move within these spaces so as to cause suitable manipulations of the game ball, including dribbling, when contacted by the noted projector element and activator element portions."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR 1999"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121140068"
                        ],
                        "name": "R. Zhao",
                        "slug": "R.-Zhao",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160149"
                        ],
                        "name": "W. Grosky",
                        "slug": "W.-Grosky",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Grosky",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grosky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6728387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbd39b6bfe08939e5b82a5d34e852cf93b979de8",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "42-VIDEO-SHOT-DETECTION-USING-COLOR-ANGLOGRAM-AND-:-Zhao-Grosky",
            "title": {
                "fragments": [],
                "text": "42 VIDEO SHOT DETECTION USING COLOR ANGLOGRAM AND LATENT SEMANTIC INDEXING : FROM CONTENTS TO SEMANTICS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 110
                            }
                        ],
                        "text": "Natural language generation (NLG) is the task of producing natural language output according to certain input (Jurafsky and Martin, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 159
                            }
                        ],
                        "text": "With respect to grammaticality, most traditional NLG systems either adopt sentence-templates or rely on predefined grammars to create human-readable sentences (Jurafsky and Martin, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition (Prentice Hall Series in Artificial Intelligence)"
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall, 1 edition. neue Auflage kommt im Frhjahr 2008."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and representing topic"
            },
            "venue": {
                "fragments": [],
                "text": "A hierarchical mixture model for word occurrences in document databases. In Proceedings of the Conference for Automated Learning and Discovery, pages 408\u2013415, Pittsburgh, PA."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistically Generated Summary Sentences: A Preliminary Evaluation of Verisimilitude Using Precision of Dependency Relations"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Workshop Using Corpora for Natural Language Generation"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sd !\njWI !k Sd !j : \u00f013\u00de"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic Topic Models A Handbook of Latent Semantic Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic Topic Models A Handbook of Latent Semantic Analysis"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Our extractive caption generator draws inspiration from previous work on automatic summarization, most of which focuses on sentence extraction (see [48] and [49] for comprehensive overviews)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Summarizing: Factors and Directions Advances in Automatic Text Summarization"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic Summarizing: Factors and Directions Advances in Automatic Text Summarization"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 77,
            "methodology": 44,
            "result": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 155,
        "totalPages": 16
    },
    "page_url": "https://www.semanticscholar.org/paper/Automatic-Caption-Generation-for-News-Images-Feng-Lapata/194e9a6f02fd5f39226dc9848213479fec5f1821?sort=total-citations"
}