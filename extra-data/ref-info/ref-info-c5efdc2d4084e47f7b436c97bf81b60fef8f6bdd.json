{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3493665"
                        ],
                        "name": "Dafang He",
                        "slug": "Dafang-He",
                        "structuredName": {
                            "firstName": "Dafang",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dafang He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110765504"
                        ],
                        "name": "Yeqing Li",
                        "slug": "Yeqing-Li",
                        "structuredName": {
                            "firstName": "Yeqing",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeqing Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116230588"
                        ],
                        "name": "Alexander N. Gorban",
                        "slug": "Alexander-N.-Gorban",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gorban",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander N. Gorban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39099960"
                        ],
                        "name": "Derrall Heath",
                        "slug": "Derrall-Heath",
                        "structuredName": {
                            "firstName": "Derrall",
                            "lastName": "Heath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derrall Heath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46920727"
                        ],
                        "name": "Julian Ibarz",
                        "slug": "Julian-Ibarz",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Ibarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Ibarz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087411164"
                        ],
                        "name": "Qian Yu",
                        "slug": "Qian-Yu",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[21] verified the existence of a certain text string in an image with spatial attention mechanism."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195346653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9953824b3d4cd2be77ecbc5db3f7dec3dfa031e",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Many tasks are related to determining if a particular text string exists in an image. In this work, we propose a new framework that learns this task in an end-to-end way. The framework takes an image and a text string as input and then outputs the probability of the text string being present in the image. This is the first end-to-end framework that learns such relationships between text and images in scene text area. The framework does not require explicit scene text detection or recognition and thus no bounding box annotations are needed for it. It is also the first work in scene text area that tackles suh a weakly labeled problem. Based on this framework, we developed a model called Guided Attention. Our designed model achieves much better results than several state-of-the-art scene text reading based solutions for a challenging Street View Business Matching task. The task tries to find correct business names for storefront images and the dataset we collected for it is substantially larger, and more challenging than existing scene text dataset. This new real-world task provides a new perspective for studying scene text related problems. We also demonstrate the uniqueness of our task via a comparison between our problem and a typical Visual Question Answering problem."
            },
            "slug": "Guided-Attention-for-Large-Scale-Scene-Text-He-Li",
            "title": {
                "fragments": [],
                "text": "Guided Attention for Large Scale Scene Text Verification"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work proposes a new framework that learns this task in an end-to-end way and developed a model called Guided Attention, which achieves much better results than several state-of-the-art scene text reading based solutions for a challenging Street View Business Matching task."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108564789"
                        ],
                        "name": "Yipeng Sun",
                        "slug": "Yipeng-Sun",
                        "structuredName": {
                            "firstName": "Yipeng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yipeng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979323"
                        ],
                        "name": "Chengquan Zhang",
                        "slug": "Chengquan-Zhang",
                        "structuredName": {
                            "firstName": "Chengquan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengquan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3377311"
                        ],
                        "name": "Zuming Huang",
                        "slug": "Zuming-Huang",
                        "structuredName": {
                            "firstName": "Zuming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zuming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1962338347"
                        ],
                        "name": "Jiaming Liu",
                        "slug": "Jiaming-Liu",
                        "structuredName": {
                            "firstName": "Jiaming",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaming Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912505"
                        ],
                        "name": "Junyu Han",
                        "slug": "Junyu-Han",
                        "structuredName": {
                            "firstName": "Junyu",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyu Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12081764"
                        ],
                        "name": "Errui Ding",
                        "slug": "Errui-Ding",
                        "structuredName": {
                            "firstName": "Errui",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Errui Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Recently, detection and recognition branches are merged into an end-to-end framework, and jointly trained simultaneously [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56895325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65c7ed8b4dfe838396c4cc6935151c8c5442410e",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Reading text from images remains challenging due to multi-orientation, perspective distortion and especially the curved nature of irregular text. Most of existing approaches attempt to solve the problem in two or multiple stages, which is considered to be the bottleneck to optimize the overall performance. To address this issue, we propose an end-to-end trainable network architecture, named TextNet, which is able to simultaneously localize and recognize irregular text from images. Specifically, we develop a scale-aware attention mechanism to learn multi-scale image features as a backbone network, sharing fully convolutional features and computation for localization and recognition. In text detection branch, we directly generate text proposals in quadrangles, covering oriented, perspective and curved text regions. To preserve text features for recognition, we introduce a perspective RoI transform layer, which can align quadrangle proposals into small feature maps. Furthermore, in order to extract effective features for recognition, we propose to encode the aligned RoI features by RNN into context information, combining spatial attention mechanism to generate text sequences. This overall pipeline is capable of handling both regular and irregular cases. Finally, text localization and recognition tasks can be jointly trained in an end-to-end fashion with designed multi-task loss. Experiments on standard benchmarks show that the proposed TextNet can achieve state-of-the-art performance, and outperform existing approaches on irregular datasets by a large margin."
            },
            "slug": "TextNet:-Irregular-Text-Reading-from-Images-with-an-Sun-Zhang",
            "title": {
                "fragments": [],
                "text": "TextNet: Irregular Text Reading from Images with an End-to-End Trainable Network"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An end-to-end trainable network architecture, named TextNet, is proposed, which is able to simultaneously localize and recognize irregular text from images, and can achieve state-of-the-art performance on irregular datasets by a large margin."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155494310"
                        ],
                        "name": "Hui Li",
                        "slug": "Hui-Li",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155300848"
                        ],
                        "name": "Peng Wang",
                        "slug": "Peng-Wang",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2130498030"
                        ],
                        "name": "Guyu Zhang",
                        "slug": "Guyu-Zhang",
                        "structuredName": {
                            "firstName": "Guyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guyu Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The first stream of scene text reading usually contains two modules, scene text detection [10] and scene text recognition [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "After we obtain the location of texts, many recognition algorithms, such as CRNN [12] and attentionbased methods [6], could be utilized to obtain the texts in the image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Typically, scene text reading [6]\u2013[9] falls into two categories, one stream is recognizing all the texts in the image, and the other one is merely recognizing the ToIs, which is also called entity extraction or structural information extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53301402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7e4ecbb3ac6f69688eed29451bd8b7bf4fdf316",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing irregular text in natural scene images is challenging due to the large variance in text appearance, such as curvature, orientation and distortion. Most existing approaches rely heavily on sophisticated model designs and/or extra fine-grained annotations, which, to some extent, increase the difficulty in algorithm implementation and data collection. In this work, we propose an easy-to-implement strong baseline for irregular scene text recognition, using offthe-shelf neural network components and only word-level annotations. It is composed of a 31-layer ResNet, an LSTMbased encoder-decoder framework and a 2-dimensional attention module. Despite its simplicity, the proposed method is robust. It achieves state-of-the-art performance on irregular text recognition benchmarks and comparable results on regular text datasets. The code will be released."
            },
            "slug": "Show,-Attend-and-Read:-A-Simple-and-Strong-Baseline-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes an easy-to-implement strong baseline for irregular scene text recognition, using off- the-shelf neural network components and only word-level annotations, and achieves state-of-the-art performance on both regular and irregular sceneText recognition benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823518756"
                        ],
                        "name": "Han Hu",
                        "slug": "Han-Hu",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979323"
                        ],
                        "name": "Chengquan Zhang",
                        "slug": "Chengquan-Zhang",
                        "structuredName": {
                            "firstName": "Chengquan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengquan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108803115"
                        ],
                        "name": "Yuxuan Luo",
                        "slug": "Yuxuan-Luo",
                        "structuredName": {
                            "firstName": "Yuxuan",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxuan Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108824048"
                        ],
                        "name": "Yuzhuo Wang",
                        "slug": "Yuzhuo-Wang",
                        "structuredName": {
                            "firstName": "Yuzhuo",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuzhuo Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912505"
                        ],
                        "name": "Junyu Han",
                        "slug": "Junyu-Han",
                        "structuredName": {
                            "firstName": "Junyu",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyu Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12081764"
                        ],
                        "name": "Errui Ding",
                        "slug": "Errui-Ding",
                        "structuredName": {
                            "firstName": "Errui",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Errui Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "The first stream of scene text reading usually contains two modules, scene text detection [10] and scene text recognition [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12318201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a51a13d35b67c495bb269ee3f4be4c42d12a0abe",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Imagery texts are usually organized as a hierarchy of several visual elements, i.e. characters, words, text lines and text blocks. Among these elements, character is the most basic one for various languages such as Western, Chinese, Japanese, mathematical expression and etc. It is natural and convenient to construct a common text detection engine based on character detectors. However, training character detectors requires a vast of location annotated characters, which are expensive to obtain. Actually, the existing real text datasets are mostly annotated in word or line level. To remedy this dilemma, we propose a weakly supervised framework that can utilize word annotations, either in tight quadrangles or the more loose bounding boxes, for character detector training. When applied in scene text detection, we are thus able to train a robust character detector by exploiting word annotations in the rich large-scale real scene text datasets, e.g. ICDAR15 [19] and COCO-text [39]. The character detector acts as a key role in the pipeline of our text detection engine. It achieves the state-of-the-art performance on several challenging scene text detection benchmarks. We also demonstrate the flexibility of our pipeline by various scenarios, including deformed text detection and math expression recognition."
            },
            "slug": "WordSup:-Exploiting-Word-Annotations-for-Character-Hu-Zhang",
            "title": {
                "fragments": [],
                "text": "WordSup: Exploiting Word Annotations for Character Based Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A weakly supervised framework that can utilize word annotations, either in tight quadrangles or the more loose bounding boxes, for character detector training is proposed, able to train a robust character detector by exploiting word annotations in the rich large-scale real scene text datasets, e.g. ICDAR15 and COCO-text."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40299478"
                        ],
                        "name": "T. Wilkinson",
                        "slug": "T.-Wilkinson",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Wilkinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wilkinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064932059"
                        ],
                        "name": "Jonas Lindstr\u00f6m",
                        "slug": "Jonas-Lindstr\u00f6m",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "Lindstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonas Lindstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2358229"
                        ],
                        "name": "Anders Brun",
                        "slug": "Anders-Brun",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Brun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders Brun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18433479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3d4da0da12249ef819eef42910900a96342aaca",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we approach the problem of segmentation-free query-by-string word spotting for handwritten documents. In other words, we use methods inspired from computer vision and machine learning to search for words in large collections of digitized manuscripts. In particular, we are interested in historical handwritten texts, which are often far more challenging than modern printed documents. This task is important, as it provides people with a way to quickly find what they are looking for in large collections that are tedious and difficult to read manually. To this end, we introduce an end-to-end trainable model based on deep neural networks that we call Ctrl-F-Net. Given a full manuscript page, the model simultaneously generates region proposals, and embeds these into a distributed word embedding space, where searches are performed. We evaluate the model on common benchmarks for handwritten word spotting, outperforming the previous state-of-the-art segmentation-free approaches by a large margin, and in some cases even segmentation-based approaches. One interesting real-life application of our approach is to help historians to find and count specific words in court records that are related to women's sustenance activities and division of labor. We provide promising preliminary experiments that validate our method on this task."
            },
            "slug": "Neural-Ctrl-F:-Segmentation-Free-Query-by-String-in-Wilkinson-Lindstr\u00f6m",
            "title": {
                "fragments": [],
                "text": "Neural Ctrl-F: Segmentation-Free Query-by-String Word Spotting in Handwritten Manuscript Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An end-to-end trainable model based on deep neural networks that is able to find and count specific words in court records that are related to women's sustenance activities and division of labor is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "In addition, many methods [18], [19] were presented to avoid using results of OCR technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207252329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5ae7436b5946bd37d17fc1ed26374389a86deff",
            "isKey": false,
            "numCitedBy": 887,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present an end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query."
            },
            "slug": "Reading-Text-in-the-Wild-with-Convolutional-Neural-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Reading Text in the Wild with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval and a real-world application to allow thousands of hours of news footage to be instantly searchable via a text query is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1962338347"
                        ],
                        "name": "Jiaming Liu",
                        "slug": "Jiaming-Liu",
                        "structuredName": {
                            "firstName": "Jiaming",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaming Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979323"
                        ],
                        "name": "Chengquan Zhang",
                        "slug": "Chengquan-Zhang",
                        "structuredName": {
                            "firstName": "Chengquan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengquan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108564789"
                        ],
                        "name": "Yipeng Sun",
                        "slug": "Yipeng-Sun",
                        "structuredName": {
                            "firstName": "Yipeng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yipeng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912505"
                        ],
                        "name": "Junyu Han",
                        "slug": "Junyu-Han",
                        "structuredName": {
                            "firstName": "Junyu",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyu Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12081764"
                        ],
                        "name": "Errui Ding",
                        "slug": "Errui-Ding",
                        "structuredName": {
                            "firstName": "Errui",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Errui Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "Typically, scene text reading [6]\u2013[9] falls into two categories, one stream is recognizing all the texts in the image, and the other one is merely recognizing the ToIs, which is also called entity extraction or structural information extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57373893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22f6360f4174515ef69904f2f0609d0021a084a1",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Most text detection methods hypothesize texts are horizontal or multi-oriented and thus define quadrangles as the basic detection unit. However, text in the wild is usually perspectively distorted or curved, which can not be easily tackled by existing approaches. In this paper, we propose a deep character embedding network (CENet) which simultaneously predicts the bounding boxes of characters and their embedding vectors, thus making text detection a simple clustering task in the character embedding space. The proposed method does not require strong assumptions of forming a straight line on general text detection, which provides flexibility on arbitrarily curved or perspectively distorted text. For character detection task, a dense prediction subnetwork is designed to obtain the confidence score and bounding boxes of characters. For character embedding task, a subnet is trained with contrastive loss to project detected characters into embedding space. The two tasks share a backbone CNN from which the multi-scale feature maps are extracted. The final text regions can be easily achieved by a thresholding process on character confidence and embedding distance of character pairs. We evaluated our method on ICDAR13, ICDAR15, MSRA-TD500, and Total-Text. The proposed method achieves state-of-the-art or comparable performance on all these datasets, and shows substantial improvement in the irregular-text datasets, i.e. Total-Text."
            },
            "slug": "Detecting-Text-in-the-Wild-with-Deep-Character-Liu-Zhang",
            "title": {
                "fragments": [],
                "text": "Detecting Text in the Wild with Deep Character Embedding Network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A deep character embedding network (CENet) is proposed which simultaneously predicts the bounding boxes of characters and their embedding vectors, thus making text detection a simple clustering task in thecharacter embedding space."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282833"
                        ],
                        "name": "Z. Wojna",
                        "slug": "Z.-Wojna",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Wojna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Wojna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116230588"
                        ],
                        "name": "Alexander N. Gorban",
                        "slug": "Alexander-N.-Gorban",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gorban",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander N. Gorban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24344368"
                        ],
                        "name": "Dar-Shyang Lee",
                        "slug": "Dar-Shyang-Lee",
                        "structuredName": {
                            "firstName": "Dar-Shyang",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dar-Shyang Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087411164"
                        ],
                        "name": "Qian Yu",
                        "slug": "Qian-Yu",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110765504"
                        ],
                        "name": "Yeqing Li",
                        "slug": "Yeqing-Li",
                        "structuredName": {
                            "firstName": "Yeqing",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeqing Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46920727"
                        ],
                        "name": "Julian Ibarz",
                        "slug": "Julian-Ibarz",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Ibarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Ibarz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "General OCR 1532ms 3000ms 1600ms Attention OCR [18] 335ms 260ms 428ms EATEN 221ms 242ms 357ms"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 14
                            }
                        ],
                        "text": "Compared with Attention OCR, which also use attention mechanism, EATEN shows its advantage (the last two rows)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 126
                            }
                        ],
                        "text": "In train ticket scenario, as we can see from the 4th column of Table I, EATEN shows significant improvement over General OCR, Attention OCR, and EATEN w/o state (16.6%, 5.8%, and 4.8%)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "In addition, many methods [18], [19] were presented to avoid using results of OCR technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "(2) Attention OCR [18]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4700850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e269982523d0a782030dc50a74f5d6fe3b974c5",
            "isKey": true,
            "numCitedBy": 105,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network model \u2014 based on Convolutional Neural Networks, Recurrent Neural Networks and a novel attention mechanism \u2014 which achieves 84.2% accuracy on the challenging French Street Name Signs (FSNS) dataset, significantly outperforming the previous state of the art (Smith'16), which achieved 72.46%. Furthermore, our new method is much simpler and more general than the previous approach. To demonstrate the generality of our model, we show that it also performs well on an even more challenging dataset derived from Google Street View, in which the goal is to extract business names from store fronts. Finally, we study the speed/accuracy tradeoff that results from using CNN feature extractors of different depths. Surprisingly, we find that deeper is not always better (in terms of accuracy, as well as speed). Our resulting model is simple, accurate and fast, allowing it to be used at scale on a variety of challenging real-world text extraction problems."
            },
            "slug": "Attention-Based-Extraction-of-Structured-from-View-Wojna-Gorban",
            "title": {
                "fragments": [],
                "text": "Attention-Based Extraction of Structured Information from Street View Imagery"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A neural network model is presented \u2014 based on Convolutional Neural Networks, Recurrent Neural Networks and a novel attention mechanism \u2014 which achieves 84.2% accuracy on the challenging French Street Name Signs dataset, significantly outperforming the previous state of the art (Smith'16), which achieved 72.46%."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148927556"
                        ],
                        "name": "Xinyu Zhou",
                        "slug": "Xinyu-Zhou",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075348632"
                        ],
                        "name": "He Wen",
                        "slug": "He-Wen",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47905836"
                        ],
                        "name": "Yuzhi Wang",
                        "slug": "Yuzhi-Wang",
                        "structuredName": {
                            "firstName": "Yuzhi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuzhi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132667"
                        ],
                        "name": "Shuchang Zhou",
                        "slug": "Shuchang-Zhou",
                        "structuredName": {
                            "firstName": "Shuchang",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchang Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416953"
                        ],
                        "name": "Weiran He",
                        "slug": "Weiran-He",
                        "structuredName": {
                            "firstName": "Weiran",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiran He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387852255"
                        ],
                        "name": "Jiajun Liang",
                        "slug": "Jiajun-Liang",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "A text line is described as a rectangle or a quadrilateral, or even a mask region by using regression or segmentation methods [8], [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 706860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f1630b4485027eb99ae59b745372ef1f3699c16",
            "isKey": false,
            "numCitedBy": 904,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution."
            },
            "slug": "EAST:-An-Efficient-and-Accurate-Scene-Text-Detector-Zhou-Yao",
            "title": {
                "fragments": [],
                "text": "EAST: An Efficient and Accurate Scene Text Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes, and significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "After we obtain the location of texts, many recognition algorithms, such as CRNN [12] and attentionbased methods [6], could be utilized to obtain the texts in the image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e9149ab00236d04db23394774e716c4f1d89231",
            "isKey": false,
            "numCitedBy": 1383,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it."
            },
            "slug": "An-End-to-End-Trainable-Neural-Network-for-Sequence-Shi-Bai",
            "title": {
                "fragments": [],
                "text": "An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed, which generates an effective yet much smaller model, which is more practical for real-world application scenarios."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110759501"
                        ],
                        "name": "Ankush Gupta",
                        "slug": "Ankush-Gupta",
                        "structuredName": {
                            "firstName": "Ankush",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ankush Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "Data synthesis is a way to bypass the privacy problem, and it also shows great help in scene text detection and scene text recognition [7], [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206593628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "400eb5386b13c32968fee796c71dec32aa754f1e",
            "isKey": false,
            "numCitedBy": 888,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU."
            },
            "slug": "Synthetic-Data-for-Text-Localisation-in-Natural-Gupta-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Synthetic Data for Text Localisation in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning, are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51231577"
                        ],
                        "name": "Llu\u00eds G\u00f3mez",
                        "slug": "Llu\u00eds-G\u00f3mez",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "G\u00f3mez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds G\u00f3mez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51238351"
                        ],
                        "name": "Andr\u00e9s Mafla",
                        "slug": "Andr\u00e9s-Mafla",
                        "structuredName": {
                            "firstName": "Andr\u00e9s",
                            "lastName": "Mafla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9s Mafla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143823474"
                        ],
                        "name": "Mar\u00e7al Rusi\u00f1ol",
                        "slug": "Mar\u00e7al-Rusi\u00f1ol",
                        "structuredName": {
                            "firstName": "Mar\u00e7al",
                            "lastName": "Rusi\u00f1ol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mar\u00e7al Rusi\u00f1ol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 231
                            }
                        ],
                        "text": "Recently, scene text detection and recognition, two fundamental tasks in the field of computer vision, have become increasingly popular due to their wide applications such as scene text understanding [1], image and video retrieval [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Word spotting based methods [2], [20] directly predicted both bounding boxes and a compact text representation of the words within them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52110548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d1da046da3626c7b12fb9091ce7ddbc64389b57",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual information found in scene images provides high level semantic information about the image and its context and it can be leveraged for better scene understanding. In this paper we address the problem of scene text retrieval: given a text query, the system must return all images containing the queried text. The novelty of the proposed model consists in the usage of a single shot CNN architecture that predicts at the same time bounding boxes and a compact text representation of the words in them. In this way, the text based image retrieval task can be casted as a simple nearest neighbor search of the query text representation over the outputs of the CNN over the entire image database. Our experiments demonstrate that the proposed architecture outperforms previous state-of-the-art while it offers a significant increase in processing speed."
            },
            "slug": "Single-Shot-Scene-Text-Retrieval-G\u00f3mez-Mafla",
            "title": {
                "fragments": [],
                "text": "Single Shot Scene Text Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper addresses the problem of scene text retrieval: given a text query, the system must return all images containing the queried text and proposes a single shot CNN architecture that predicts at the same time bounding boxes and a compact text representation of the words in them."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979323"
                        ],
                        "name": "Chengquan Zhang",
                        "slug": "Chengquan-Zhang",
                        "structuredName": {
                            "firstName": "Chengquan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengquan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46212059"
                        ],
                        "name": "Borong Liang",
                        "slug": "Borong-Liang",
                        "structuredName": {
                            "firstName": "Borong",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Borong Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3377311"
                        ],
                        "name": "Zuming Huang",
                        "slug": "Zuming-Huang",
                        "structuredName": {
                            "firstName": "Zuming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zuming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994902"
                        ],
                        "name": "Mengyi En",
                        "slug": "Mengyi-En",
                        "structuredName": {
                            "firstName": "Mengyi",
                            "lastName": "En",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengyi En"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912505"
                        ],
                        "name": "Junyu Han",
                        "slug": "Junyu-Han",
                        "structuredName": {
                            "firstName": "Junyu",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyu Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12081764"
                        ],
                        "name": "Errui Ding",
                        "slug": "Errui-Ding",
                        "structuredName": {
                            "firstName": "Errui",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Errui Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713947"
                        ],
                        "name": "Xinghao Ding",
                        "slug": "Xinghao-Ding",
                        "structuredName": {
                            "firstName": "Xinghao",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinghao Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "A text line is described as a rectangle or a quadrilateral, or even a mask region by using regression or segmentation methods [8], [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119297349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2225036fac528936bb0a5a0e22360e233a2e2934",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous scene text detection methods have progressed substantially over the past years. However, limited by the receptive field of CNNs and the simple representations like rectangle bounding box or quadrangle adopted to describe text, previous methods may fall short when dealing with more challenging text instances, such as extremely long text and arbitrarily shaped text. To address these two problems, we present a novel text detector namely LOMO, which localizes the text progressively for multiple times (or in other word, LOok More than Once). LOMO consists of a direct regressor (DR), an iterative refinement module (IRM) and a shape expression module (SEM). At first, text proposals in the form of quadrangle are generated by DR branch. Next, IRM progressively perceives the entire long text by iterative refinement based on the extracted feature blocks of preliminary proposals. Finally, a SEM is introduced to reconstruct more precise representation of irregular text by considering the geometry properties of text instance, including text region, text center line and border offsets. The state-of-the-art results on several public benchmarks including ICDAR2017-RCTW, SCUT-CTW1500, Total-Text, ICDAR2015 and ICDAR17-MLT confirm the striking robustness and effectiveness of LOMO."
            },
            "slug": "Look-More-Than-Once:-An-Accurate-Detector-for-Text-Zhang-Liang",
            "title": {
                "fragments": [],
                "text": "Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel text detector namely LOMO is presented, which localizes the text progressively for multiple times (or in other word, LOok More than Once), and the state-of-the-art results on several public benchmarks confirm the striking robustness and effectiveness of LomO."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9510649"
                        ],
                        "name": "Yongchao Xu",
                        "slug": "Yongchao-Xu",
                        "structuredName": {
                            "firstName": "Yongchao",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongchao Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115942506"
                        ],
                        "name": "Yukang Wang",
                        "slug": "Yukang-Wang",
                        "structuredName": {
                            "firstName": "Yukang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yukang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144666089"
                        ],
                        "name": "Wei Zhou",
                        "slug": "Wei-Zhou",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153709848"
                        ],
                        "name": "Yongpan Wang",
                        "slug": "Yongpan-Wang",
                        "structuredName": {
                            "firstName": "Yongpan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongpan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109432908"
                        ],
                        "name": "Zhibo Yang",
                        "slug": "Zhibo-Yang",
                        "structuredName": {
                            "firstName": "Zhibo",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhibo Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54446029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a80212fe263cad9760e83a34bacf7203f70816f8",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text detection is an important step in the scene text reading system. The main challenges lie in significantly varied sizes and aspect ratios, arbitrary orientations, and shapes. Driven by the recent progress in deep learning, impressive performances have been achieved for multi-oriented text detection. Yet, the performance drops dramatically in detecting the curved texts due to the limited text representation (e.g., horizontal bounding boxes, rotated rectangles, or quadrilaterals). It is of great interest to detect the curved texts, which are actually very common in natural scenes. In this paper, we present a novel text detector named TextField for detecting irregular scene texts. Specifically, we learn a direction field pointing away from the nearest text boundary to each text point. This direction field is represented by an image of 2D vectors and learned via a fully convolutional neural network. It encodes both binary text mask and direction information used to separate adjacent text instances, which is challenging for the classical segmentation-based approaches. Based on the learned direction field, we apply a simple yet effective morphological-based post-processing to achieve the final detection. The experimental results show that the proposed TextField outperforms the state-of-the-art methods by a large margin (28% and 8%) on two curved text datasets: Total-Text and SCUT-CTW1500, respectively; TextField also achieves very competitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500. Furthermore, TextField is robust in generalizing unseen datasets."
            },
            "slug": "TextField:-Learning-a-Deep-Direction-Field-for-Text-Xu-Wang",
            "title": {
                "fragments": [],
                "text": "TextField: Learning a Deep Direction Field for Irregular Scene Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel text detector named TextField, which outperforms the state-of-the-art methods by a large margin on two curved text datasets: Total-Text and SCUT-CTW1500, respectively; TextField also achieves very competitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Following the success of synthetic word engine [23], we propose a more efficient text rendering pipeline."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "Data synthesis is a way to bypass the privacy problem, and it also shows great help in scene text detection and scene text recognition [7], [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11072772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0b8aad30d8dfd08535f361864f064b2fbbc9a75",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one \"reading\" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs."
            },
            "slug": "Synthetic-Data-and-Artificial-Neural-Networks-for-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work presents a framework for the recognition of natural scene text that does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2603501"
                        ],
                        "name": "Wanchen Sui",
                        "slug": "Wanchen-Sui",
                        "structuredName": {
                            "firstName": "Wanchen",
                            "lastName": "Sui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanchen Sui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108056226"
                        ],
                        "name": "Qing Zhang",
                        "slug": "Qing-Zhang",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146158921"
                        ],
                        "name": "Jun Yang",
                        "slug": "Jun-Yang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057047808"
                        ],
                        "name": "Wei Chu",
                        "slug": "Wei-Chu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Chu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, detection and recognition branches are merged into an end-to-end framework, and jointly trained simultaneously [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53770149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90c483bbfe183604f76d84e6bd57f71792b23f80",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel integrated framework for learning both text detection and recognition. For most of the existing methods, detection and recognition are treated as two isolated tasks and trained separately, since parameters of detection and recognition models are different and two models target to optimize their own loss functions during individual training processes. In contrast to those methods, by sharing model parameters, we merge the detection model and recognition model into a single end-to-end trainable model and train the joint model for two tasks simultaneously. The shared parameters not only help effectively reduce the computational load in inference process, but also improve the end-to-end text detection-recognition accuracy. In addition, we design a simpler and faster sequence learning method for the recognition network based on a succession of stacked convolutional layers without any recurrent structure, this is proved feasible and dramatically improves inference speed. Extensive experiments on different datasets demonstrate that the proposed method achieves very promising results."
            },
            "slug": "A-Novel-Integrated-Framework-for-Learning-both-Text-Sui-Zhang",
            "title": {
                "fragments": [],
                "text": "A Novel Integrated Framework for Learning both Text Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A novel integrated framework for learning both text detection and recognition is proposed, which merge the detection model and recognition model into a single end-to-end trainable model and train the joint model for two tasks simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "2018 24th International Conference on Pattern Recognition (ICPR)"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134639223"
                        ],
                        "name": "Rinon Gal",
                        "slug": "Rinon-Gal",
                        "structuredName": {
                            "firstName": "Rinon",
                            "lastName": "Gal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rinon Gal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134522657"
                        ],
                        "name": "Nimrod Morag",
                        "slug": "Nimrod-Morag",
                        "structuredName": {
                            "firstName": "Nimrod",
                            "lastName": "Morag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nimrod Morag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2509354"
                        ],
                        "name": "Roy Shilkrot",
                        "slug": "Roy-Shilkrot",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Shilkrot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Shilkrot"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] created an embedding that merged spatial and linguistic features for extracting ToI information from images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 131763346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57554e3e70ca194b0095ae8930a5ea655714441c",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Receipts are crucial for many businesses\u2019 operation, where expenses are tracked meticulously. Receipt documents are often scanned into images, digitized and analyzed before the information is streamed into institutional financial applications. The precise extraction of expense data from receipt images is a difficult task owed to the high variance in fonts and layouts, the frailty of the print paper, unstructured scanning environments and an immeasurable amount of domains. We propose a method that combines visual and linguistic features for automatic information retrieval from receipt images using deep network architectures, which outperforms existing approaches. Our Skip-Rect Embedding (SRE) descriptor is demonstrated in two canonical applications for receipt information retrieval: field extraction and Optical Character Recognition (OCR) error enhancement."
            },
            "slug": "Visual-Linguistic-Methods-for-Receipt-Field-Gal-Morag",
            "title": {
                "fragments": [],
                "text": "Visual-Linguistic Methods for Receipt Field Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a method that combines visual and linguistic features for automatic information retrieval from receipt images using deep network architectures, which outperforms existing approaches and is demonstrated in two canonical applications for receipt information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1938475"
                        ],
                        "name": "E. Bart",
                        "slug": "E.-Bart",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Bart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35126187"
                        ],
                        "name": "Prateek Sarkar",
                        "slug": "Prateek-Sarkar",
                        "structuredName": {
                            "firstName": "Prateek",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prateek Sarkar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Spatial connections, segmentation and layout analysis methods [3], [15], [16] were employed to extract structural information of ToIs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10185814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05f216ed26beede85bdb578f6442fc360ca30e58",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Repetition of layout structure is prevalent in document images. In document design, such repetition conveys the underlying logical and functional structure of the data. For example, in invoices, the names, unit prices, quantities and other descriptors of every line item are laid out in a consistent spatial structure. We propose a general method for extracting such repeated structure from documents. After receiving a single example of the structure to be found, the proposed method localizes additional instances of this structure in the same document and in additional documents. A wide variety of perceptually motivated cues (such as alignment and saliency) is used for this purpose. These cues are combined in a probabilistic model, and a novel algorithm for exact inference in this model is proposed and used. We demonstrate that this method can cope with complex instances of repeated structure and generalizes successfully across a wide range of structure variations."
            },
            "slug": "Information-extraction-by-finding-repeated-Bart-Sarkar",
            "title": {
                "fragments": [],
                "text": "Information extraction by finding repeated structure"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a general method for extracting repeated structure from documents and demonstrates that this method can cope with complex instances of repeated structure and generalizes successfully across a wide range of structure variations."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40265241"
                        ],
                        "name": "Bill Janssen",
                        "slug": "Bill-Janssen",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Janssen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bill Janssen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734223"
                        ],
                        "name": "E. Bier",
                        "slug": "E.-Bier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Bier",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144445858"
                        ],
                        "name": "Patricia Wall",
                        "slug": "Patricia-Wall",
                        "structuredName": {
                            "firstName": "Patricia",
                            "lastName": "Wall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patricia Wall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7776699"
                        ],
                        "name": "M. Sprague",
                        "slug": "M.-Sprague",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Sprague",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sprague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Classical approaches are based on rules and templates [4], [14], which firstly recognizes all the texts in the image by OCR methods, and then extracts ToIs with projective transformation, handcrafted strategies and many post-processes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5985110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d36185006382dbd3c7fa0079c0c90508c4cbd24e",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The Receipts2Go system is about the world of one-page documents: cash register receipts, book covers, cereal boxes, price tags, train tickets, fire extinguisher tags. In that world, we're exploring techniques for extracting accurate information from documents for which we have no layout descriptions -- indeed no initial idea of what the document's genre is -- using photos taken with cell phone cameras by users who aren't skilled document capture technicians. This paper outlines the system and reports on some initial results, including the algorithms we've found useful for cleaning up those document images, and the techniques used to extract and organize relevant information from thousands of similar-but-different page layouts."
            },
            "slug": "Receipts2Go:-the-big-world-of-small-documents-Janssen-Saund",
            "title": {
                "fragments": [],
                "text": "Receipts2Go: the big world of small documents"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The Receipts2Go system is outlined, including the algorithms the authors've found useful for cleaning up those document images, and the techniques used to extract and organize relevant information from thousands of similar-but-different page layouts."
            },
            "venue": {
                "fragments": [],
                "text": "DocEng '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13993295"
                        ],
                        "name": "E. Aslan",
                        "slug": "E.-Aslan",
                        "structuredName": {
                            "firstName": "Enes",
                            "lastName": "Aslan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Aslan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3437247"
                        ],
                        "name": "T. Karakaya",
                        "slug": "T.-Karakaya",
                        "structuredName": {
                            "firstName": "Tugrul",
                            "lastName": "Karakaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Karakaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807274"
                        ],
                        "name": "Ethem Unver",
                        "slug": "Ethem-Unver",
                        "structuredName": {
                            "firstName": "Ethem",
                            "lastName": "Unver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ethem Unver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793947"
                        ],
                        "name": "Y. Akg\u00fcl",
                        "slug": "Y.-Akg\u00fcl",
                        "structuredName": {
                            "firstName": "Yusuf",
                            "lastName": "Akg\u00fcl",
                            "middleNames": [
                                "Sinan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Akg\u00fcl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Spatial connections, segmentation and layout analysis methods [3], [15], [16] were employed to extract structural information of ToIs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27993673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea939d72d55c095e57fedaaf2aa49f596002c196",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated invoice processing and information extraction has attracted remarkable interest from business and academic circles. Invoice processing is a very critical and costly operation for participation banks because credit authorization process must be linked with the real trade activity via invoices. The classical invoice processing systems first assign the invoices to an invoice class but any error in document class decision will cause the invoice parsing to be invalid. This paper proposes a new invoice class-free-parsing method that uses a two-phase structure. The first phase uses individual invoice part detectors and the second phase employs an efficient part-based modeling approach. At the first phase, we employ different methods such as SVM, maximum entropy and HOG to produce candidates for the various types of invoice parts. At the second phase, the basic idea is to parse an invoice by parts arranged in a deformable composition similar to face or human body detection from digital images. The main advantage of the part-based modeling (PBM) approach is that this system can handle any type of invoice, a crucial functionality for business processes at participation banks. The proposed system is tested with real invoices and experimental results confirm the effectiveness of the proposed approach."
            },
            "slug": "A-Part-based-Modeling-Approach-for-Invoice-Parsing-Aslan-Karakaya",
            "title": {
                "fragments": [],
                "text": "A Part based Modeling Approach for Invoice Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new invoice class-free-parsing method that uses a two-phase structure that can handle any type of invoice, a crucial functionality for business processes at participation banks is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "VISIGRAPP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282833"
                        ],
                        "name": "Z. Wojna",
                        "slug": "Z.-Wojna",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Wojna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Wojna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "We choose Inception v3 [22] as the backbone."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206593880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "isKey": false,
            "numCitedBy": 15548,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set."
            },
            "slug": "Rethinking-the-Inception-Architecture-for-Computer-Szegedy-Vanhoucke",
            "title": {
                "fragments": [],
                "text": "Rethinking the Inception Architecture for Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work is exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144086254"
                        ],
                        "name": "Frederick Schulz",
                        "slug": "Frederick-Schulz",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Schulz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederick Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2198984"
                        ],
                        "name": "Markus Ebbecke",
                        "slug": "Markus-Ebbecke",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Ebbecke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Ebbecke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2336967"
                        ],
                        "name": "M. Gillmann",
                        "slug": "M.-Gillmann",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gillmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746927"
                        ],
                        "name": "Benjamin Adrian",
                        "slug": "Benjamin-Adrian",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Adrian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Adrian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582412"
                        ],
                        "name": "S. Agne",
                        "slug": "S.-Agne",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Agne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "Classical approaches are based on rules and templates [4], [14], which firstly recognizes all the texts in the image by OCR methods, and then extracts ToIs with projective transformation, handcrafted strategies and many post-processes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2923833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48cd65169d520c8c9493f9321c65aa29b7aabb80",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with the transfer of knowledge on invoice document layout and extraction strategies, collected by users of the invoice recognition software smartFIX over several years of productive use, to other user's systems. The results of a project analyzing this 'treasure' of knowledge and putting it to use in the smartFIX system are presented. The evaluation shows that this transfer of knowledge using state-of-the-art techniques in transfer learning achieves significantly higher initial recognition rates than the unaugmented system, delivering instant economic advantages by reducing accountant personnel workload."
            },
            "slug": "Seizing-the-Treasure:-Transferring-Knowledge-in-Schulz-Ebbecke",
            "title": {
                "fragments": [],
                "text": "Seizing the Treasure: Transferring Knowledge in Invoice Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The evaluation shows that this transfer of knowledge using state-of-the-art techniques in transfer learning achieves significantly higher initial recognition rates than the unaugmented system, delivering instant economic advantages by reducing accountant personnel workload."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35431121"
                        ],
                        "name": "H. Ha",
                        "slug": "H.-Ha",
                        "structuredName": {
                            "firstName": "Hien",
                            "lastName": "Ha",
                            "middleNames": [
                                "Thi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3449002"
                        ],
                        "name": "Marek Medved",
                        "slug": "Marek-Medved",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Medved",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marek Medved"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2473995"
                        ],
                        "name": "Zuzana Neverilov\u00e1",
                        "slug": "Zuzana-Neverilov\u00e1",
                        "structuredName": {
                            "firstName": "Zuzana",
                            "lastName": "Neverilov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zuzana Neverilov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2333943"
                        ],
                        "name": "Ales Horak",
                        "slug": "Ales-Horak",
                        "structuredName": {
                            "firstName": "Ales",
                            "lastName": "Horak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ales Horak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Previous approaches [3]\u2013[5] mainly adopt two steps, in which text information is extracted firstly via OCR (Optical Character Recognition), and then ToIs are extracted by handcrafted rules or layout analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52184826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43876995c53d8a4dbf37889818bf3454d5fefd50",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatically cataloging of thousands of paper-based structured documents is a crucial fund-saving task for future document management systems. Current optical character recognition (OCR) systems process the tabular data with a sufficient level of character-level accuracy; however, the overall structure of the document metadata is still an open practical task."
            },
            "slug": "Recognition-of-OCR-Invoice-Metadata-Block-Types-Ha-Medved",
            "title": {
                "fragments": [],
                "text": "Recognition of OCR Invoice Metadata Block Types"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Current optical character recognition systems process the tabular data with a sufficient level of character-level accuracy; however, the overall structure of the document metadata is still an open practical task."
            },
            "venue": {
                "fragments": [],
                "text": "TSD"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860829"
                        ],
                        "name": "H. Hamza",
                        "slug": "H.-Hamza",
                        "structuredName": {
                            "firstName": "Hatem",
                            "lastName": "Hamza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hamza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2448729"
                        ],
                        "name": "Y. Bela\u00efd",
                        "slug": "Y.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Yolande",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Bela\u00efd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128453"
                        ],
                        "name": "A. Bela\u00efd",
                        "slug": "A.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Abdel",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bela\u00efd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Spatial connections, segmentation and layout analysis methods [3], [15], [16] were employed to extract structural information of ToIs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Previous approaches [3]\u2013[5] mainly adopt two steps, in which text information is extracted firstly via OCR (Optical Character Recognition), and then ToIs are extracted by handcrafted rules or layout analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15685065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "228772fea2cfed70eacdd525795c104370e85122",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces the approach CBRDIA (Case-based Reasoning for Document Invoice Analysis) which uses the principles of case-based reasoning to analyze, recognize and interpret invoices. Two CBR cycles are performed sequentially in CBRDIA. The first one consists in checking whether a similar document has already been processed, which makes the interpretation of the current one easy. The second cycle works if the first one fails. It processes the document by analyzing and interpreting its structuring elements (adresses, amounts, tables, etc) one by one. The CBR cycles allow processing documents from both knonwn or unknown classes. Applied on 923 invoices, CBRDIA reaches a recognition rate of 85,22% for documents of known classes and 74,90% for documents of unknown classes."
            },
            "slug": "Case-Based-Reasoning-for-Invoice-Analysis-and-Hamza-Bela\u00efd",
            "title": {
                "fragments": [],
                "text": "Case-Based Reasoning for Invoice Analysis and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper introduces the approach CBRDIA (Case-based Reasoning for Document Invoice Analysis) which uses the principles of case-based reasoning to analyze, recognize and interpret invoices."
            },
            "venue": {
                "fragments": [],
                "text": "ICCBR"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 200
                            }
                        ],
                        "text": "Recently, scene text detection and recognition, two fundamental tasks in the field of computer vision, have become increasingly popular due to their wide applications such as scene text understanding [1], image and video retrieval [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast and accurate scene text understanding with image binarization and offthe-shelf OCR"
            },
            "venue": {
                "fragments": [],
                "text": "IJDAR, vol. 18, no. 2, pp. 169\u2013182, 2015."
            },
            "year": 2015
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/EATEN:-Entity-Aware-Attention-for-Single-Shot-Text-Guo-Qin/c5efdc2d4084e47f7b436c97bf81b60fef8f6bdd?sort=total-citations"
}