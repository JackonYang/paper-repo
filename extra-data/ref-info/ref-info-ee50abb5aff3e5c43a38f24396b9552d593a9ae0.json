{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17831368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5001470e8808afe9887afbe48e2eaaf1a0395d10",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We are developing a phoneme based, speaker-dependent continuous speech recognition system embedding a Multilayer Perceptron (MLP) (i.e., a feedforward Artificial Neural Network), into a Hidden Markov Model (HMM) approach. In [Bourlard & Wellekens], it was shown that MLPs were approximating Maximum a Posteriori (MAP) probabilities and could thus be embedded as an emission probability estimator in HMMs. By using contextual information from a sliding window on the input frames, we have been able to improve frame or phoneme classification performance over the corresponding performance for Simple Maximum Likelihood (ML) or even MAP probabilities that are estimated without the benefit of context. However, recognition of words in continuous speech was not so simply improved by the use of an MLP, and several modifications of the original scheme were necessary for getting acceptable performance. It is shown here that word recognition performance for a simple discrete density HMM system appears to be somewhat better when MLP methods are used to estimate the emission probabilities."
            },
            "slug": "A-Continuous-Speech-Recognition-System-Embedding-Bourlard-Morgan",
            "title": {
                "fragments": [],
                "text": "A Continuous Speech Recognition System Embedding MLP into HMM"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown here that word recognition performance for a simple discrete density HMM system appears to be somewhat better when MLP methods are used to estimate the emission probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065228513"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213649"
                        ],
                        "name": "C. Wellekens",
                        "slug": "C.-Wellekens",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wellekens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wellekens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "L P(XIW) = L P(q,/,XIW) , (3) l=l where P(q,/, XIW) denotes thus the probability that X is produced by W while associating Xn with state ql."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "Probability (3) is then approximated by:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Maximization of (3) can be worked out by the classical forward-backward recurrences of the Baum-Welch algorithm [J elinek 1976, Bourlard et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "To make all possible paths apparent, (3) can also be rewritten as"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 263
                            }
                        ],
                        "text": "\u2026perceptron (MLP) is now a familiar and promising tool in connectionist approach for classification problems [Rumelhart et al., 1986; Lippmann, 1987} and has already been widely tested on speech recognition problems [Waibel et aI., 1988; Watrous & Shastri, 1987; Bourlard & Wellekens, 1989]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 231
                            }
                        ],
                        "text": "The same kind of architecture has also been proved successful in performing the classification of acoustic vector strings into phoneme strings, where each current vector was classified by taking account of its surrounding vectors [Bourlard & Wellekens, 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 37
                            }
                        ],
                        "text": "This fact is clearly illustrated in [Bourlard & Wellekens, 1989; Waibel et al., 1988] and several solutions have recently been proposed in [Bahl et al., 1986; Bourlard & VVellekens, 1989; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62025798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a55d31784aca11871985096644a025f036633569",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speech-pattern-discrimination-and-multilayer-Bourlard-Wellekens",
            "title": {
                "fragments": [],
                "text": "Speech pattern discrimination and multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 65
                            }
                        ],
                        "text": "This fact is clearly illustrated in [Bourlard & Wellekens, 1989; Waibel et al., 1988] and several solutions have recently been proposed in [Bahl et al., 1986; Bourlard & VVellekens, 1989; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 217
                            }
                        ],
                        "text": "\u2026perceptron (MLP) is now a familiar and promising tool in connectionist approach for classification problems [Rumelhart et al., 1986; Lippmann, 1987} and has already been widely tested on speech recognition problems [Waibel et aI., 1988; Watrous & Shastri, 1987; Bourlard & Wellekens, 1989]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2786,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48462607"
                        ],
                        "name": "L. Shastri",
                        "slug": "L.-Shastri",
                        "structuredName": {
                            "firstName": "Lokendra",
                            "lastName": "Shastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 127
                            }
                        ],
                        "text": "The main advantage of this topology, when compared with other recurrent models proposed for sequential processing [Elman 1988, Watrous, 1987], over and above the possible interpretation in terms of HMM, is the control of the information fed back during the training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 238
                            }
                        ],
                        "text": "\u2026perceptron (MLP) is now a familiar and promising tool in connectionist approach for classification problems [Rumelhart et al., 1986; Lippmann, 1987} and has already been widely tested on speech recognition problems [Waibel et aI., 1988; Watrous & Shastri, 1987; Bourlard & Wellekens, 1989]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12357500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1e40283ecd4633c36c70fbc8dbb14e9a4afb37f",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for learning phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. The network uses hidden units with recurrent links to capture spectral/temporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. \n \nA simple connectionist network with recurrent links was trained on a single instance of the word pair \"no\" and \"go\", and successful learned a discriminatory mechanism. The trained network also correctly discriminated 98% of 25 other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items."
            },
            "slug": "Learning-Phonetic-Features-Using-Connectionist-Watrous-Shastri",
            "title": {
                "fragments": [],
                "text": "Learning Phonetic Features Using Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A method for learning phonetic features from speech data using connectionist networks is described and a supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60968219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3fa9a9a9652ab70dec27b0f4d333bdbbe9a31b1",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of maximum-mutual-information (MMI) training to hidden Markov models (HMMs) is studied for phonetic recognition. MMI training has been proposed as an alternative to standard maximum-likelihood (ML) training. In practice, MMI training performs better (produces models that are more accurate) than ML training. The fundamental notions of HMM, ML and MMI training are reviewed, and it is shown how MMI training can be applied easily to the case of phonetic models and phonetic recognition. Some computational heuristics are proposed to implement these computations practically. Some experiments (training and recognition) are detailed that show that the phonetic error rate decreases significantly when MMI training is used, as compared with ML training.<<ETX>>"
            },
            "slug": "Phonetic-recognition-using-hidden-Markov-models-and-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Phonetic recognition using hidden Markov models and maximum mutual information training"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The application of maximum-mutual-information (MMI) training to hidden Markov models (HMMs) is studied for phonetic recognition and shows that the phonetic error rate decreases significantly when MMI training is used, as compared with ML training."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102981024"
                        ],
                        "name": "A. Poritz",
                        "slug": "A.-Poritz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Poritz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Poritz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62479678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6627d8efde3ed55e34ccee059eb6cdac99bb2fe",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov modeling is a probabilistic technique for the study of time series. Hidden Markov theory permits modeling with any of the classical probability distributions. The costs of implementation are linear in the length of data. Models can be nested to reflect hierarchical sources of knowledge. These and other desirable features have made hidden Markov methods increasingly attractive for problems in language, speech and signal processing. The basic ideas are introduced by elementary examples in the spirit of the Polya urn models. The main tool in hidden Markov modeling is the Baum-Welch (or forward-backward) algorithm for maximum likelihood estimation of the model parameters. This iterative algorithm is discussed both from an intuitive point of view as an exercise in the art of counting and from a formal point of view via the information-theoretic Q-function. Selected examples drawn from the literature illustrate how the Baum-Welch technique places a rich variety of computational models at the disposal of the researcher.<<ETX>>"
            },
            "slug": "Hidden-Markov-models:-a-guided-tour-Poritz",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models: a guided tour"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main tool in hidden Markov modeling is the Baum-Welch algorithm for maximum likelihood estimation of the model parameters, which is discussed both from an intuitive point of view as an exercise in the art of counting and from a formalpoint of view via the information-theoretic Q-function."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 65
                            }
                        ],
                        "text": "This fact is clearly illustrated in [Bourlard & Wellekens, 1989; Waibel et al., 1988] and several solutions have recently been proposed in [Bahl et al., 1986; Bourlard & VVellekens, 1989; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121696597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09526c3ff288be85e51508f4270c920899389181",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A time\u2010delay neural network (TDNN) approach is presented to speech recognition that is characterized by two important properties: (1) Using multilayer arrangements of simple computing units, a TDNN can represent arbitrary nonlinear classification decision surfaces that are learned automatically using error back propagation. (2) The time\u2010delay arrangement enables the network to discover acoustic\u2010phonetic features and the temporal relationships between them independent of position in time and, hence, not blurred by temporal shifts in the input. The TDNNs are compared with the currently most popular technique in speech recognition, hidden Markov models (HMM). Extensive performance evaluation shows that the TDNN recognizes voiced stops extracted from varying phonetic contexts at an error rate four times lower (1.5% vs 6.3%) than the best of our HMMs. To perform this task, the TDNN \u201cinvented\u201d well\u2010known acoustic\u2010phonetic features (e.g., F2 rise, F2 fall, vowel onset) as useful abstractions. It also developed a..."
            },
            "slug": "Speech-recognition-using-time\u2010delay-neural-networks-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Speech recognition using time\u2010delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A time\u2010delay neural network approach to speech recognition that \u201cinvented\u201d well\u2010known acoustic\u2010phonetic features as useful abstractions and recognizes voiced stops extracted from varying phonetic contexts at an error rate four times lower than the best of the authors' HMMs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068002150"
                        ],
                        "name": "T. D. Harrison",
                        "slug": "T.-D.-Harrison",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Harrison",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62211969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87d79c0c5255bce9dacaf4dab07d00c682200f2e",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Boltzmann-machines-for-speech-recognition-Prager-Harrison",
            "title": {
                "fragments": [],
                "text": "Boltzmann machines for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 194
                            }
                        ],
                        "text": "This maximization with respect to the whole parameter space has been shown equivalent to the maximization of Mutual Information (l\\fMI) between a model and a vector sequence [Bahl et al., 1986; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 188
                            }
                        ],
                        "text": "This fact is clearly illustrated in [Bourlard & Wellekens, 1989; Waibel et al., 1988] and several solutions have recently been proposed in [Bahl et al., 1986; Bourlard & VVellekens, 1989; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60769407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This thesis examines the acoustic-modeling problem in automatic speech recognition from an information-theoretic point of view. This problem is to design a speech-recognition system which can extract from the speech waveform as much information as possible about the corresponding word sequence. The information extraction process is broken down into two steps: a signal processing step which converts a speech waveform into a sequence of information bearing acoustic feature vectors, and a step which models such a sequence. This thesis is primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space such as R sub N. It explores the trade-off between packing a lot of information into such sequences and being able to model them accurately. The difficulty of developing accurate models of continuous parameter sequences is addressed by investigating a method of parameter estimation which is specifically designed to cope with inaccurate modeling assumptions."
            },
            "slug": "The-acoustic-modeling-problem-in-automatic-speech-Brown",
            "title": {
                "fragments": [],
                "text": "The acoustic-modeling problem in automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis is primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space such as R sub N and explores the trade-off between packing a lot of information into such sequences and being able to model them accurately."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69854558"
                        ],
                        "name": "S. S. Marcus",
                        "slug": "S.-S.-Marcus",
                        "structuredName": {
                            "firstName": "Sm",
                            "lastName": "Marcus",
                            "middleNames": [
                                "Stephen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. S. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 142765410,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "65b66e5c72b217338cd7fee7d9b714fb38c0e7bd",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ERIS-context-sensitive-coding-in-speech-perception-Marcus",
            "title": {
                "fragments": [],
                "text": "ERIS-context sensitive coding in speech perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801257"
                        ],
                        "name": "A. Noll",
                        "slug": "A.-Noll",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Noll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Noll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62160232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62ea07f01c5a8efb820540d261de6675ede2fb4b",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the training of phoneme models used in a speaker-dependent continuous-speech understanding system. Three different methods for estimating model parameters are described which are based on the standard Markov modelling approach. The first method deals with phoneme models with continuous-emission probability density functions. For the second and third method phoneme models with discrete probability-density functions and two different parameter-estimation methods are described. The test and training speech-database consists of two independent sets of spoken sentences of several speakers. The complete recognition vocabulary contains 917 words with an overlap of 51 words (e.g. articles) with the training vocabulary. Recognition results are given for the different training methods and some other experiments."
            },
            "slug": "Training-of-phoneme-models-in-a-sentence-system-Noll-Ney",
            "title": {
                "fragments": [],
                "text": "Training of phoneme models in a sentence recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper describes the training of phoneme models used in a speaker-dependent continuous-speech understanding system and three different methods for estimating model parameters are described which are based on the standard Markov modelling approach."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781290"
                        ],
                        "name": "H. Murveit",
                        "slug": "H.-Murveit",
                        "structuredName": {
                            "firstName": "Hy",
                            "lastName": "Murveit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murveit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744182"
                        ],
                        "name": "M. Weintraub",
                        "slug": "M.-Weintraub",
                        "structuredName": {
                            "firstName": "Mitch",
                            "lastName": "Weintraub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Weintraub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58735336,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f1f5d28339e93e196ee0337574acfd999e05214a",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm based on hidden Markov models is applied to the task of speaker-independent continuous-speech recognition for a vocabulary of 1000 words with no syntactic constraints. The signal is limited to 4000 Hz. Word models were built from three-state representations of phonetic units, concatenated according to entries in a lexicon. Performance as measured on DARPAs resource management database was 40% correct word recognition. It was found that the use of several different acoustic features and the use of word-specific phonetic modeling, where possible, improved system performance.<<ETX>>"
            },
            "slug": "1000-word-speaker-independent-continuous-speech-Murveit-Weintraub",
            "title": {
                "fragments": [],
                "text": "1000-word speaker-independent continuous-speech recognition using hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An algorithm based on hidden Markov models is applied to the task of speaker-independent continuous-speech recognition for a vocabulary of 1000 words with no syntactic constraints, and it was found that the use of several different acoustic features and theUse of word-specific phonetic modeling, where possible, improved system performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "More specifically, the factors of (9) are assumed dependent on the previous state only and on a signal window of length 2p + 1 centered around the current acoustic vector."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 148
                            }
                        ],
                        "text": "The multilayer perceptron (MLP) is now a familiar and promising tool in connectionist approach for classification problems [Rumelhart et al., 1986; Lippmann, 1987} and has already been widely tested on speech recognition problems [Waibel et aI., 1988; Watrous & Shastri, 1987; Bourlard & Wellekens,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Now, each factor of (9) may be simplified by relaxing the conditional constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 184
                            }
                        ],
                        "text": "It is also important to point out that the same kind of recurrent A1LP could also be used to estimate local probabilities of higher order Markov models where the local contribution in (9) are no longer assumed dependent on the previous state only but also on several preceding ones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": true,
            "numCitedBy": 3801,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "A language model provides the value of P(Wi ) independently of the acoustic decoding [Jelinek, 1976]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 28
                            }
                        ],
                        "text": "Hidden Markov models (HMM) [Jelinek, 1976; Bourlard et al., 1985] are widely used for automatic isolated and connected speech recognition."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61949946,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e66aade2a97005b9f0638357421b1f72f770f75f",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "HIS PAPER DESCRIBES statistical methods of automatic recognition (transcription) of continuous speech that have been used successfully by the Speech Processing Group at the IBM Thomas J. Watson Research Center. The sources of these procedures will be referenced where practicable, but the working style of the Group has been deliberately cooperative (as the Acknowledgment Section indicates), so a certain amount of inadequate or udust crediting is inevitable. The author tried his best to keep it at a minimum. The exposition, appearing as it does in an IEEE publication, is aimed mostly at engineers who are less familiar with speech and language than with information transmission, statistics, or signal processing. At the same time, the author would like to enable speech specialists to read the more mathematical parts of the paper. Inevitably, a compromise between these two audiences has been attempted that resulted in a somewhat lengthened presentation. The author would like to invite his readers to skip rather boldly over material familiar to them. The fust six sections contain the essence of the formulation that is centered on the design of an actual speech recognition system. Readers of Section VI that contains experimental results may feel somewhat dissatisfied with the fact that no comparisons are attempted with performance achieved by alternate design philosophies. Unfortunately, such judgments are made difficult by the great variety in utterance corpora to be recognized, in experimental conditions, and in recognizer function goals.\u2019 However, the accompanying survey paper by Reddy [231 does assess the merits of the various speech recognition projects, and can serve as an excellent introduction to the field for the nonspecialist. In the speech recognition community, our project is somewhat controversial, since it attempts to model utterance production statistically, rather than through a grammar that would describe syntactically and semantically the allowable (mini-) universe of discourse. It is much too early to tell which emphasis is sounder. There is little doubt that before automatic recognition of speech is accomplished, the statistical"
            },
            "slug": "Speech-Recognition-by-Statistical-Methods-Jelinek",
            "title": {
                "fragments": [],
                "text": "Speech Recognition by Statistical Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The author's project is somewhat controversial, since it attempts to model utterance production statistically, rather than through a grammar that would describe syntactically and semantically the allowable (mini-) universe of discourse."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251336"
                        ],
                        "name": "D. Burr",
                        "slug": "D.-Burr",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Burr",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8b35ac5beb1f62c995e0fa78e3efe51da01aa11",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural networks (ANNs) are capable of accurate recognition of simple speech vocabularies such as isolated digits [1]. This paper looks at two more difficult vocabularies, the alphabetic E-set and a set of polysyllabic words. The E-set is difficult because it contains weak discriminants and polysyllables are difficult because of timing variation. Polysyllabic word recognition is aided by a time pre-alignment technique based on dynamic programming and E-set recognition is improved by focusing attention. Recognition accuracies are better than 98% for both vocabularies when implemented with a single layer perceptron."
            },
            "slug": "Speech-Recognition-Experiments-with-Perceptrons-Burr",
            "title": {
                "fragments": [],
                "text": "Speech Recognition Experiments with Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper looks at two more difficult vocabularies, the alphabetic E-set and a set of polysyllabic words, and finds recognition accuracies are better than 98% for both vocABularies when implemented with a single layer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14789841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58",
            "isKey": false,
            "numCitedBy": 1403,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Continuous Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 89
                            }
                        ],
                        "text": "The architecture of the resulting MLP is then similar to NETtaik initially described in [Sejnowski & Rosenberg, 1987] for mapping written texts to phoneme strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "The difference between criterion (12) and that of a memoryless machine is the additional index kn which takes account of the previous decision."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "Collecting all terms depending on the same indexes, (12) can thus be rewritten as:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "N K E = 4 L L [g(in, kn, f) - d(in, f)]2 , n=1 l=1 (12)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": true,
            "numCitedBy": 1885,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699205"
                        ],
                        "name": "S. Furui",
                        "slug": "S.-Furui",
                        "structuredName": {
                            "firstName": "Sadaoki",
                            "lastName": "Furui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Furui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 40519557,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f64038de5e388bce2f0575cfb4a291a41e3bab57",
            "isKey": false,
            "numCitedBy": 899,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new isolated word recognition technique based on a combination of instantaneous and dynamic features of the speech spectrum. This technique is shown to be highly effective in speaker-independent speech recognition. Spoken utterances are represented by time sequences of cepstrum coefficients and energy. Regression coefficients for these time functions are extracted for every frame over an approximately 50 ms period. Time functions of regression coefficients extracted for cepstrum and energy are combined with time functions of the original cepstrum coefficients, and used with a staggered array DP matching algorithm to compare multiple templates and input speech. Speaker-independent isolated word recognition experiments using a vocabulary of 100 Japanese city names indicate that a recognition error rate of 2.4 percent can be obtained with this method. Using only the original cepstrum coefficients the error rate is 6.2 percent."
            },
            "slug": "Speaker-independent-isolated-word-recognition-using-Furui",
            "title": {
                "fragments": [],
                "text": "Speaker-independent isolated word recognition using dynamic features of speech spectrum"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper proposes a new isolated word recognition technique based on a combination of instantaneous and dynamic features of the speech spectrum that is shown to be highly effective in speaker-independent speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145048087"
                        ],
                        "name": "E. A. Martin",
                        "slug": "E.-A.-Martin",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Martin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. A. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5730960"
                        ],
                        "name": "D. Paul",
                        "slug": "D.-Paul",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Paul",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Paul"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61685722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1923e2149604d4365d29e81d888b763a85fa18c1",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a two-stage isolated word speech recognition system that uses a Hidden Markov Model (HMM) recognizer in the first stage and a discriminant analysis system in the second stage. During recognition, when the first-stage recognizer is unable to clearly differentiate between acoustically similar words such as \"go\" and \"no\" the second-stage discriminator is used. The second-stage system focuses on those parts of the unknown token which are most effective at discriminating the confused words. The system was tested on a 35 word, 10,710 token stress speech isolated word data base created at Lincoln Laboratory. Adding the second-stage discriminating system produced the best results to date on this data base, reducing the overall error rate by more than a factor of two."
            },
            "slug": "Two-stage-discriminant-analysis-for-improved-Martin-Lippmann",
            "title": {
                "fragments": [],
                "text": "Two-stage discriminant analysis for improved isolated-word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A two-stage isolated word speech recognition system that uses a Hidden Markov Model (HMM) recognizer in the first stage and a discriminant analysis system in the second stage, reducing the overall error rate by more than a factor of two."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18821787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f3175b3930d0c71495a52a7bccb3889e5f33520",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We have done an empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance. Two experiments are reported. In one, we use simulated data sets with well-controlled parameters, such as the signal-to-noise ratio of continuous-valued data. In the second, we train the network on vector-quantized mel cepstra from real speech samples. In each case, we use back-propagation to train the feedforward net to discriminate in a multiple class pattern classification problem. We report the results of these studies, and show the application of cross-validation techniques to prevent overfitting."
            },
            "slug": "Generalization-and-Parameter-Estimation-in-Netws:-Morgan-Bourlard",
            "title": {
                "fragments": [],
                "text": "Generalization and Parameter Estimation in Feedforward Netws: Some Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance and the application of cross-validation techniques to prevent overfitting is done."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213649"
                        ],
                        "name": "C. Wellekens",
                        "slug": "C.-Wellekens",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wellekens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wellekens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119573500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cca0e1afb4bc9a13bbadd84d4b79803db666c3cf",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A connected speech recognition method based on the Baum forward backward algorithm is presented. The segmentation of the test sentence uses the probability that an acoustic vector lays at the separation of two speech subunit models (Hidden Markov models). The labelling rests on the highest probability that a vector has been emitted on the last state of a subunit model. Results are presented for word- and phoneme-recognition."
            },
            "slug": "Global-connected-digit-recognition-using-Baum-Welch-Wellekens",
            "title": {
                "fragments": [],
                "text": "Global connected digit recognition using Baum-Welch algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A connected speech recognition method based on the Baum forward backward algorithm is presented that segmentation of the test sentence uses the probability that an acoustic vector lays at the separation of two speech subunit models."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "It can easily be proved that the estimate of the Bayes probabilities in (6) are: (6)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 140
                            }
                        ],
                        "text": "In that way, it is interesting to note that the same optimal values (14) may also result from other criteria as, for instance, the entropy [Hinton, 1987] or relative entropy [Solla et al., 1988] of the targets with respect to outputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7840452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a57c6d627ffc667ae3547073876c35d6420accff",
            "isKey": false,
            "numCitedBy": 1574,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-Procedures-Hinton",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning Procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 27867182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5146d7902132bcb0b2e6fe5f607358768fc47323",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-and-architecture-for-neural-computation-Pineda",
            "title": {
                "fragments": [],
                "text": "Dynamics and architecture for neural computation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "Indeed, as the correct criterion should be based on (1), comparing with (4), the \"Viterbi formulation\" of this probability is (8)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "Taking account of the fact that the models are mutually exclusive and if A represents the parameter set (for all possible models), (1) may then be rewritten as:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 175
                            }
                        ],
                        "text": "This maximization with respect to the whole parameter space has been shown equivalent to the maximization of Mutual Information (l\\fMI) between a model and a vector sequence [Bahl et al., 1986; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 140
                            }
                        ],
                        "text": "This fact is clearly illustrated in [Bourlard & Wellekens, 1989; Waibel et al., 1988] and several solutions have recently been proposed in [Bahl et al., 1986; Bourlard & VVellekens, 1989; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56128297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f09ce0dd760857e0d0e4879f6e2543f04c5d33",
            "isKey": true,
            "numCitedBy": 926,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation."
            },
            "slug": "Maximum-mutual-information-estimation-of-hidden-for-Bahl-Brown",
            "title": {
                "fragments": [],
                "text": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method for estimating the parameters of hidden Markov models of speech is described and recognition results are presented comparing this method with maximum likelihood estimation."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9860,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213649"
                        ],
                        "name": "C. Wellekens",
                        "slug": "C.-Wellekens",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wellekens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wellekens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120363714,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ff5a3d6464f842c8bcd4f2c60a9796de0aac25b9",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The Hidden Markov models are generalized by defining a new emission probability which takes the correlation between successive feature vectors into account. Estimation formulas for the iterative learning both along Viterbi and Maximum likelihood criteria are presented."
            },
            "slug": "Explicit-time-correlation-in-hidden-Markov-models-Wellekens",
            "title": {
                "fragments": [],
                "text": "Explicit time correlation in hidden Markov models for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The Hidden Markov models are generalized by defining a new emission probability which takes the correlation between successive feature vectors into account andimation formulas for the iterative learning both along Viterbi and Maximum likelihood criteria are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Of course, from the local contributions (10), P(WIX) can still be obtained by the classical one-stage dynamic programming [Ney, 1984; Bourlard et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 123
                            }
                        ],
                        "text": "Of course, from the local contributions (10), P(WIX) can still be obtained by the classical one-stage dynamic programming [Ney, 1984; Bourlard et al., 1985] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "It is indeed proved that, for the training vectors, the optimal outputs of a recurrent and context-sensitive MLP are the estimates of the local probabilities (10)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": ", X n \u2022 If input contextual information is neglected (p = 0), equation (10) represents nothing else but the discriminant local probability (7) and is at the root of a discriminant discrete HMM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "the discriminant local probabilities defined by (7) if no context is used and by (10) in the contextual case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16642315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e4193d03eb3c5a695a3d8b3506f80704f9dfc19",
            "isKey": true,
            "numCitedBy": 380,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is of tutorial nature and describes a one-stage dynamic programming algorithm for file problem of connected word recognition. The algorithm to be developed is essentially identical to one presented by Vintsyuk [1] and later by Bridle and Brown [2] ; but the notation and the presentation have been clarified. The derivation used for optimally time synchronizing a test pattern, consisting of a sequence of connected words, is straightforward and simple in comparison with other approaches decomposing the pattern matching problem into several levels. The approach presented relies basically on parameterizing the time warping path by a single index and on exploiting certain path constraints both in the word interior and at the word boundaries. The resulting algorithm turns out to be significantly more efficient than those proposed by Sakoe [3] as well as Myers and Rabiner [4], while providing the same accuracy in estimating the best possible matching string. Its most important feature is that the computational expenditure per word is independent of the number of words in the input string. Thus, it is well suited for recognizing comparatively long word sequences and for real-time operation. Furthermore, there is no need to specify the maximum number of words in the input string. The practical implementation of the algorithm is discussed; it requires no heuristic rules and no overhead. The algorithm can be modified to deal with syntactic constraints in terms of a finite state syntax."
            },
            "slug": "The-use-of-a-one-stage-dynamic-programming-for-word-Ney",
            "title": {
                "fragments": [],
                "text": "The use of a one-stage dynamic programming algorithm for connected word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The algorithm to be developed is essentially identical to one presented by Vintsyuk and later by Bridle and Brown, but the notation and the presentation have been clarified and the computational expenditure per word is independent of the number of words in the input string."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145881625"
                        ],
                        "name": "M. Fleisher",
                        "slug": "M.-Fleisher",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fleisher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fleisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 175
                            }
                        ],
                        "text": "In that way, it is interesting to note that the same optimal values (14) may also result from other criteria as, for instance, the entropy [Hinton, 1987] or relative entropy [Solla et al., 1988] of the targets with respect to outputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Replacing in (13) dei, m) by the optimal values (14) provides a new criterion where the target outputs depend now on the current vector, the considered output and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2024543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "247698d0a716f0d99c0645050d049525e0b08ec2",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abst ract . Learning in layered neu ral networks is posed as the mini\u00ad miz at ion of an error function defined over t he training set. A proba\u00ad bilistic interpretation of the target act ivities sugges ts th e use of rela\u00ad t ive entro py as an error measure. We investigate t he merits of using this error function over t he traditional quad ratic function for gradient descent learni ng. Com parative numerical sim ulations for the conrf\u00ad guity problem show marked redu ct ion s in learn ing t imes. This im \u00ad provement is explained in terms of the characteristic steepness of the landscape defined by the error function in configuration space."
            },
            "slug": "Accelerated-Learning-in-Layered-Neural-Networks-Solla-Levin",
            "title": {
                "fragments": [],
                "text": "Accelerated Learning in Layered Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work investigates the merits of using this error function over t he traditional quad ratic function for gradient descent for conrf\u00ad guity problem and explains the characteristic steepness of the landscape defined by the error function in configuration space."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781290"
                        ],
                        "name": "H. Murveit",
                        "slug": "H.-Murveit",
                        "structuredName": {
                            "firstName": "Hy",
                            "lastName": "Murveit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murveit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786606"
                        ],
                        "name": "R. Brodersen",
                        "slug": "R.-Brodersen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Brodersen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brodersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10142942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb0978da7a46fd60c42278408b21fcde97992cbd",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A high-performance, flexible, and potentially inexpensive speech recognition system is described. The system is based on two special-purpose integrated circuits that perform the speech recognition algorithms very efficiently. One of these integrated circuits is the front-end processor, which computes spectral coefficients from incoming speech. The second integrated circuit computes a dynamic-time-warp algorithm. The system can compare an input word with 1000-word templates and respond to a user within \\frac{1}{4} s. The system demonstrates that computational complexity need not be a major limiting factor in the design of speech recognition systems."
            },
            "slug": "An-integrated-circuit-based-speech-recognition-Murveit-Brodersen",
            "title": {
                "fragments": [],
                "text": "An integrated-circuit-based speech recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A high-performance, flexible, and potentially inexpensive speech recognition system that can compare an input word with 1000-word templates and respond to a user within 1-4 s demonstrates that computational complexity need not be a major limiting factor in the design of speech recognition systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48429353"
                        ],
                        "name": "Pineda",
                        "slug": "Pineda",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Pineda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pineda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40994937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6602985bd326d9996c68627b56ed389e2c90fd08",
            "isKey": false,
            "numCitedBy": 905,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\ensuremath{\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "slug": "Generalization-of-back-propagation-to-recurrent-Pineda",
            "title": {
                "fragments": [],
                "text": "Generalization of back-propagation to recurrent neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An adaptive neural network with asymmetric connections is introduced that bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62359231,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f1277592f221ea26fa1d2321a38b64c58b33d75b",
            "isKey": false,
            "numCitedBy": 8010,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises."
            },
            "slug": "Introduction-to-Statistical-Pattern-Recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition, which is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "A language model provides the value of P(Wi ) independently of the acoustic decoding [Jelinek, 1976]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 28
                            }
                        ],
                        "text": "Hidden Markov models (HMM) [Jelinek, 1976; Bourlard et al., 1985] are widely used for automatic isolated and connected speech recognition."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31408841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9",
            "isKey": false,
            "numCitedBy": 991,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods."
            },
            "slug": "Continuous-speech-recognition-by-statistical-Jelinek",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition by statistical methods"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Experimental results are presented that indicate the power of the methods and concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111289375"
                        ],
                        "name": "Michael D. Brown",
                        "slug": "Michael-D.-Brown",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brown",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael D. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39515755"
                        ],
                        "name": "R. Chamberlain",
                        "slug": "R.-Chamberlain",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Chamberlain",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chamberlain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5823473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b49fce77d75f671e5f41326e85794501835b0fc",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of an efficient one-pass dynamic programming whole-word pattern matching algorithm for the recognition of spoken sequences of connected words are described. Particular attention is given to the technique for keeping track of word-sequence decisions, which may be constrained by a finite-state syntax. Some extensions of the technique are discussed."
            },
            "slug": "An-algorithm-for-connected-word-recognition-Bridle-Brown",
            "title": {
                "fragments": [],
                "text": "An algorithm for connected word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "The principles of an efficient one-pass dynamic programming whole-word pattern matching algorithm for the recognition of spoken sequences of connected words are described and some extensions of the technique are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "Supervision comes from the a-priori knowledge of the classification of each Vi n \u2022 The training of the MLP parameters is usually based on the minimization of a mean square criterion (LMSE) [Rumelhart et al., 1986] which, with our requirements, takes the form: N K E = 4 L L [g(in, kn, f ) -d(in, f)]2 , n=1 l=1 (12) where d(in, f) represents the target value of the f-th output associated with the input vector Vi n \u2022 Since the purpose is to associate each input vector with a single class, the target outputs, for a vector Vi E q(f), are: d(i, f) d(i, m) 1, 0, Vm ;f:. f , which can also be expressed, for each particular Vi E q(f) as: d(i, m) = bml ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 124
                            }
                        ],
                        "text": "The multilayer perceptron (MLP) is now a familiar and promising tool in connectionist approach for classification problems [Rumelhart et al., 1986; Lippmann, 1987} and has already been widely tested on speech recognition problems [Waibel et aI., 1988; Watrous & Shastri, 1987; Bourlard & Wellekens,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 143
                            }
                        ],
                        "text": "\u2026of the classification of each Vi n \u2022 The training of the MLP parameters is usually based on the minimization of a mean square criterion (LMSE) [Rumelhart et al., 1986] which, with our requirements, takes the form: N K E = 4 L L [g(in, kn, f ) -d(in, f)]2 , n=1 l=1 (12) where d(in, f)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Replacing in (13) dei, m) by the optimal values (14) provides a new criterion where the target outputs depend now on the current vector, the considered output and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "In 1 - C k f) , n=l l=l 9 I n , n, 9 In, n, (16) and canceling its partial derivative versus g(i, k, m) yields the optimal values (14)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "In that way, it is interesting to note that the same optimal values (14) may also result from other criteria as, for instance, the entropy [Hinton, 1987] or relative entropy [Solla et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "and it is clear (by canceling the partial derivative of E'\" versus g(i, k, m)) that the lower bound for E'\" is reached for the same optimal outputs as (14) but is now equal to zero, what provides a very useful control parameter during the training phase."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Of course, since these results are independent of the topology of the models, they remain also valid for linear discriminant functions but, in that case, it is not guaranteed that the optimal values (14) can be reached."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 65
                            }
                        ],
                        "text": "This fact is clearly illustrated in [Bourlard & Wellekens, 1989; Waibel et al., 1988] and several solutions have recently been proposed in [Bahl et al., 1986; Bourlard & VVellekens, 1989; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phoneme Recognition Using Time-Delay"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks, Proc. ICASSP-88,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Of course, as for (7), these local probabilities could also be simply estimated by counting on the training set, but the exponential increase of the number of parameters with the width 2p + 1 of the contextual window would require an exceedingly large storage capacity as an excessive size of training data to obtain statistically significant parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 28
                            }
                        ],
                        "text": "Hidden Markov models (HMM) [Jelinek, 1976; Bourlard et al., 1985] are widely used for automatic isolated and connected speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "A language model provides the value of P(Wi ) independently of the acoustic decoding [Jelinek, 1976]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": ", X n \u2022 If input contextual information is neglected (p = 0), equation (10) represents nothing else but the discriminant local probability (7) and is at the root of a discriminant discrete HMM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "the discriminant local probabilities defined by (7) if no context is used and by (10) in the contextual case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Continuous Recognition by Statistical Methods"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE,"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Maximization of P(WdX, A) as given by (2) is usually simplified by restricting it to the subspace of the Uti parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "The summation term in the denominator is constant over the parameter space of Uti and thus, maximization of P(XIWi' A) implies that of its bilinear map (2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 43
                            }
                        ],
                        "text": "Hidden Markov models (HMM) [Jelinek, 1976; Bourlard et al., 1985] are widely used for automatic isolated and connected speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 134
                            }
                        ],
                        "text": "Of course, from the local contributions (10), P(WIX) can still be obtained by the classical one-stage dynamic programming [Ney, 1984; Bourlard et al., 1985] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speaker-Dependent Connected Speech Recognition via Dynamic Programming and Statistical Methods"
            },
            "venue": {
                "fragments": [],
                "text": "Speech and Speaker Recognition,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "Indeed, as the correct criterion should be based on (1), comparing with (4), the \"Viterbi formulation\" of this probability is (8)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "1 N P(XIW) = max P(qll,\u00b7\u00b7\u00b7,qlN,XIW) , (4) ll, ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 194
                            }
                        ],
                        "text": "This maximization with respect to the whole parameter space has been shown equivalent to the maximization of Mutual Information (l\\fMI) between a model and a vector sequence [Bahl et al., 1986; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 188
                            }
                        ],
                        "text": "This fact is clearly illustrated in [Bourlard & Wellekens, 1989; Waibel et al., 1988] and several solutions have recently been proposed in [Bahl et al., 1986; Bourlard & VVellekens, 1989; Brown, 1987]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Acoustic-Modeling Problem in A utomatic Speech Recognition,  Ph.D. thesis, Comp.Sc.Dep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "Supervision comes from the a-priori knowledge of the classification of each Vi n \u2022 The training of the MLP parameters is usually based on the minimization of a mean square criterion (LMSE) [Rumelhart et al., 1986] which, with our requirements, takes the form: N K E = 4 L L [g(in, kn, f ) -d(in, f)]2 , n=1 l=1 (12) where d(in, f) represents the target value of the f-th output associated with the input vector Vi n \u2022 Since the purpose is to associate each input vector with a single class, the target outputs, for a vector Vi E q(f), are: d(i, f) d(i, m) 1, 0, Vm ;f:. f , which can also be expressed, for each particular Vi E q(f) as: d(i, m) = bml ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 124
                            }
                        ],
                        "text": "The multilayer perceptron (MLP) is now a familiar and promising tool in connectionist approach for classification problems [Rumelhart et al., 1986; Lippmann, 1987} and has already been widely tested on speech recognition problems [Waibel et aI., 1988; Watrous & Shastri, 1987; Bourlard & Wellekens,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 143
                            }
                        ],
                        "text": "\u2026of the classification of each Vi n \u2022 The training of the MLP parameters is usually based on the minimization of a mean square criterion (LMSE) [Rumelhart et al., 1986] which, with our requirements, takes the form: N K E = 4 L L [g(in, kn, f ) -d(in, f)]2 , n=1 l=1 (12) where d(in, f)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 238440002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eccfb47fa72551b2951ab927eadc8358b2609027",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Internal-Representations-by-Error-Parallel-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations by Error Propagation, Parallel Distributed Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60735762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71c0f082a41c7f0b102c3ca9e4cf6b31f361d06a",
            "isKey": false,
            "numCitedBy": 4228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-statistical-pattern-recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to statistical pattern recognition (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203175244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e570de6e8f260804a673dcb53f13b33c13a3014",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "CONCENTRATION-INFORMATION-IN-TIME:-ANALOG-NEURAL-TO-Tank-Hopfield",
            "title": {
                "fragments": [],
                "text": "CONCENTRATION INFORMATION IN TIME: ANALOG NEURAL NETWORKS WITH APPLICATIONS TO SPEECH RECOGNITION PROBLEMS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 43
                            }
                        ],
                        "text": "Hidden Markov models (HMM) [Jelinek, 1976; Bourlard et al., 1985] are widely used for automatic isolated and connected speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 134
                            }
                        ],
                        "text": "Of course, from the local contributions (10), P(WIX) can still be obtained by the classical one-stage dynamic programming [Ney, 1984; Bourlard et al., 1985] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speaker - Dependent Connected Speech Recognition via Dynamic Programming and Statistical Methods , Speech and Speaker Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Links-Between-Markov-Models-and-Multilayer-Bourlard-Wellekens/ee50abb5aff3e5c43a38f24396b9552d593a9ae0?sort=total-citations"
}