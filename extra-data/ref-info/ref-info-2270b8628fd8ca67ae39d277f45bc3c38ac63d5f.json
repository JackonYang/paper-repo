{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10419477"
                        ],
                        "name": "A. Gholami",
                        "slug": "A.-Gholami",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Gholami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gholami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032988"
                        ],
                        "name": "A. Azad",
                        "slug": "A.-Azad",
                        "structuredName": {
                            "firstName": "Ariful",
                            "lastName": "Azad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Azad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32294544"
                        ],
                        "name": "Peter H. Jin",
                        "slug": "Peter-H.-Jin",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Jin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter H. Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238795"
                        ],
                        "name": "A. Bulu\u00e7",
                        "slug": "A.-Bulu\u00e7",
                        "structuredName": {
                            "firstName": "Ayd\u0131n",
                            "lastName": "Bulu\u00e7",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bulu\u00e7"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[18] analytically showed that using both data and model parallelism at the same time can be more beneficial than using just one of them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3307678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9654d0807b807ec7c0a78f717b0dc3d4dcb1ad7c",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new integrated method of exploiting model, batch and domain parallelism for the training of deep neural networks (DNNs) on large distributed-memory computers using minibatch stochastic gradient descent (SGD). Our goal is to find an efficient parallelization strategy for a fixed batch size using P processes. Our method is inspired by the communication-avoiding algorithms in numerical linear algebra. We see P processes as logically divided into a P_r x P_c grid where the P_r dimension is implicitly responsible for model/domain parallelism and the P_c dimension is implicitly responsible for batch parallelism. In practice, the integrated matrix-based parallel algorithm encapsulates these types of parallelism automatically. We analyze the communication complexity and analytically demonstrate that the lowest communication costs are often achieved neither with pure model nor with pure data parallelism. We also show how the domain parallel approach can help in extending the theoretical scaling limit of the typical batch parallel method."
            },
            "slug": "Integrated-Model,-Batch,-and-Domain-Parallelism-in-Gholami-Azad",
            "title": {
                "fragments": [],
                "text": "Integrated Model, Batch, and Domain Parallelism in Training Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A new integrated method of exploiting model, batch and domain parallelism for the training of deep neural networks (DNNs) on large distributed-memory computers using minibatch stochastic gradient descent (SGD)."
            },
            "venue": {
                "fragments": [],
                "text": "SPAA"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072782550"
                        ],
                        "name": "Zhihao Jia",
                        "slug": "Zhihao-Jia",
                        "structuredName": {
                            "firstName": "Zhihao",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhihao Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143834867"
                        ],
                        "name": "M. Zaharia",
                        "slug": "M.-Zaharia",
                        "structuredName": {
                            "firstName": "Matei",
                            "lastName": "Zaharia",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zaharia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[20, 19] implemented a framework that uses cost modeling to pick the best parallelization strategy, including how to partition work for each operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 49868726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f971658ab845d7573c4bbb760d5e7e5332025254",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The computational requirements for training deep neural networks (DNNs) have grown to the point that it is now standard practice to parallelize training. Existing deep learning systems commonly use data or model parallelism, but unfortunately, these strategies often result in suboptimal parallelization performance. \nIn this paper, we define a more comprehensive search space of parallelization strategies for DNNs called SOAP, which includes strategies to parallelize a DNN in the Sample, Operation, Attribute, and Parameter dimensions. We also propose FlexFlow, a deep learning framework that uses guided randomized search of the SOAP space to find a fast parallelization strategy for a specific parallel machine. To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy's performance and is three orders of magnitude faster than prior approaches that have to execute each strategy. We evaluate FlexFlow with six real-world DNN benchmarks on two GPU clusters and show that FlexFlow can increase training throughput by up to 3.8x over state-of-the-art approaches, even when including its search time, and also improves scalability."
            },
            "slug": "Beyond-Data-and-Model-Parallelism-for-Deep-Neural-Jia-Zaharia",
            "title": {
                "fragments": [],
                "text": "Beyond Data and Model Parallelism for Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A more comprehensive search space of parallelization strategies for DNNs called SOAP, which includes strategies to parallelize a DNN in the Sample, Operation, Attribute, and Parameter dimensions is defined and FlexFlow, a deep learning framework that uses guided randomized search of the SOAP space to find a fast parallelization strategy for a specific parallel machine is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "MLSys"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10419477"
                        ],
                        "name": "A. Gholami",
                        "slug": "A.-Gholami",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Gholami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gholami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032988"
                        ],
                        "name": "A. Azad",
                        "slug": "A.-Azad",
                        "structuredName": {
                            "firstName": "Ariful",
                            "lastName": "Azad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Azad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238795"
                        ],
                        "name": "A. Bulu\u00e7",
                        "slug": "A.-Bulu\u00e7",
                        "structuredName": {
                            "firstName": "Ayd\u0131n",
                            "lastName": "Bulu\u00e7",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bulu\u00e7"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] analytically showed that using both data and model parallelism at the same time can be more beneficial than using just one of them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195346470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f541c71646b330cb52f9b60957c950dcb75fcaf",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new integrated method of exploiting both model and data parallelism for the training of deep neural networks (DNNs) on large distributed-memory computers using mini-batch stochastic gradient descent (SGD). Our goal is to find an efficient parallelization strategy for a fixed batch size using $P$ processes. Our method is inspired by the communication-avoiding algorithms in numerical linear algebra. We see $P$ processes as logically divided into a $P_r \\times P_c$ grid where the $P_r$ dimension is implicitly responsible for model parallelism and the $P_c$ dimension is implicitly responsible for data parallelism. In practice, the integrated matrix-based parallel algorithm encapsulates both types of parallelism automatically. We analyze the communication complexity and analytically demonstrate that the lowest communication costs are often achieved neither with pure model parallelism nor with pure data parallelism. We also show the positive effect of our approach in the computational performance of SGD based DNN training where the reduced number of processes responsible for data parallelism result in \"fatter\" matrices that enable higher-throughput matrix multiplication."
            },
            "slug": "Integrated-Model-and-Data-Parallelism-in-Training-Gholami-Azad",
            "title": {
                "fragments": [],
                "text": "Integrated Model and Data Parallelism in Training Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The integrated matrix-based parallel algorithm encapsulates both types of parallelism automatically and analytically demonstrates that the lowest communication costs are often achieved neither with pure model parallelism nor with pure data parallelism."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072782550"
                        ],
                        "name": "Zhihao Jia",
                        "slug": "Zhihao-Jia",
                        "structuredName": {
                            "firstName": "Zhihao",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhihao Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349220"
                        ],
                        "name": "Sina Lin",
                        "slug": "Sina-Lin",
                        "structuredName": {
                            "firstName": "Sina",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sina Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329939"
                        ],
                        "name": "C. Qi",
                        "slug": "C.-Qi",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[20, 19] implemented a framework that uses cost modeling to pick the best parallelization strategy, including how to partition work for each operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3619071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ea088eae8637530d1108065acab244f3b6c280d",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The past few years have witnessed growth in the size and computational requirements for training deep convolutional neural networks. Current approaches parallelize the training process onto multiple devices by applying a single parallelization strategy (e.g., data or model parallelism) to all layers in a network. Although easy to reason about, this design results in suboptimal runtime performance in large-scale distributed training, since different layers in a network may prefer different parallelization strategies. In this paper, we propose layer-wise parallelism that allows each layer in a network to use an individual parallelization strategy. We jointly optimize how each layer is parallelized by solving a graph search problem. Our experiments show that layer-wise parallelism outperforms current parallelization approaches by increasing training speed, reducing communication costs, achieving better scalability to multiple GPUs, while maintaining the same network accuracy."
            },
            "slug": "Exploring-Hidden-Dimensions-in-Parallelizing-Neural-Jia-Lin",
            "title": {
                "fragments": [],
                "text": "Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experiments show that layer-wise parallelism outperforms current parallelization approaches by increasing training speed, reducing communication costs, achieving better scalability to multiple GPUs, while maintaining the same network accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880213"
                        ],
                        "name": "Edgar Solomonik",
                        "slug": "Edgar-Solomonik",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Solomonik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Solomonik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849855"
                        ],
                        "name": "D. Matthews",
                        "slug": "D.-Matthews",
                        "structuredName": {
                            "firstName": "Devin",
                            "lastName": "Matthews",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Matthews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50534795"
                        ],
                        "name": "J. Hammond",
                        "slug": "J.-Hammond",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Hammond",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hammond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144000027"
                        ],
                        "name": "J. Stanton",
                        "slug": "J.-Stanton",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stanton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stanton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15039006,
            "fieldsOfStudy": [
                "Computer Science",
                "Chemistry"
            ],
            "id": "f20340ebc8f466fc10b46015ccfc6dfaa61ea40f",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-massively-parallel-tensor-contraction-framework-Solomonik-Matthews",
            "title": {
                "fragments": [],
                "text": "A massively parallel tensor contraction framework for coupled-cluster computations"
            },
            "venue": {
                "fragments": [],
                "text": "J. Parallel Distributed Comput."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143781496"
                        ],
                        "name": "Ke Yang",
                        "slug": "Ke-Yang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Different distribution strategies (model-parallelism [7]) can solve these issues, but specifying these strategies can be complicated, and the current MIMD implementations generate very large programs which can be difficult to compile and to optimize."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 372467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "isKey": false,
            "numCitedBy": 3026,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."
            },
            "slug": "Large-Scale-Distributed-Deep-Networks-Dean-Corrado",
            "title": {
                "fragments": [],
                "text": "Large Scale Distributed Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper considers the problem of training a deep network with billions of parameters using tens of thousands of CPU cores and develops two algorithms for large-scale distributed training, Downpour SGD and Sandblaster L-BFGS, which increase the scale and speed of deep network training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861312"
                        ],
                        "name": "Azalia Mirhoseini",
                        "slug": "Azalia-Mirhoseini",
                        "structuredName": {
                            "firstName": "Azalia",
                            "lastName": "Mirhoseini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Azalia Mirhoseini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50351613"
                        ],
                        "name": "Krzysztof Maziarz",
                        "slug": "Krzysztof-Maziarz",
                        "structuredName": {
                            "firstName": "Krzysztof",
                            "lastName": "Maziarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krzysztof Maziarz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36347083"
                        ],
                        "name": "Andy Davis",
                        "slug": "Andy-Davis",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48448318"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 259
                            }
                        ],
                        "text": "MPI-alltoall is used in the case where different dimensions in the input and the output are split across the same mesh dimension, as might be the case when switching between data-parallelism and model-parallelism for different layers of the same model, as in [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12462234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "510e26733aaff585d65701b9f1be7ca9d5afc586",
            "isKey": false,
            "numCitedBy": 862,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
            },
            "slug": "Outrageously-Large-Neural-Networks:-The-Layer-Shazeer-Mirhoseini",
            "title": {
                "fragments": [],
                "text": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks, and applies the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3070204"
                        ],
                        "name": "Justus A. Calvin",
                        "slug": "Justus-A.-Calvin",
                        "structuredName": {
                            "firstName": "Justus",
                            "lastName": "Calvin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justus A. Calvin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48516914"
                        ],
                        "name": "C. Lewis",
                        "slug": "C.-Lewis",
                        "structuredName": {
                            "firstName": "Cannada",
                            "lastName": "Lewis",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925289"
                        ],
                        "name": "Edward F. Valeev",
                        "slug": "Edward-F.-Valeev",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Valeev",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward F. Valeev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "This technique is sometimes called iteration space tiling [2], replication [6], or task parallelism [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 100319,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "132540b32d857d1841b028184036ff24c496a31b",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A task-based formulation of Scalable Universal Matrix Multiplication Algorithm (SUMMA), a popular algorithm for matrix multiplication (MM), is applied to the multiplication of hierarchy-free, rank-structured matrices that appear in the domain of quantum chemistry (QC). The novel features of our formulation are: (1) concurrent scheduling of multiple SUMMA iterations, and (2) fine-grained task-based composition. These features make it tolerant of the load imbalance due to the irregular matrix structure and eliminate all artifactual sources of global synchronization. Scalability of iterative computation of square-root inverse of block-rank-sparse QC matrices is demonstrated; for full-rank (dense) matrices the performance of our SUMMA formulation usually exceeds that of the state-of-the-art dense MM implementations (ScaLAPACK and Cyclops Tensor Framework)."
            },
            "slug": "Scalable-task-based-algorithm-for-multiplication-of-Calvin-Lewis",
            "title": {
                "fragments": [],
                "text": "Scalable task-based algorithm for multiplication of block-rank-sparse matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A task-based formulation of Scalable Universal Matrix Multiplication Algorithm (SUMMA), a popular algorithm for matrix multiplication, is applied to the multiplication of hierarchy-free, rank-structured matrices that appear in the domain of quantum chemistry (QC)."
            },
            "venue": {
                "fragments": [],
                "text": "IA3@SC"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344182"
                        ],
                        "name": "Penporn Koanantakool",
                        "slug": "Penporn-Koanantakool",
                        "structuredName": {
                            "firstName": "Penporn",
                            "lastName": "Koanantakool",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Penporn Koanantakool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319704"
                        ],
                        "name": "Alnur Ali",
                        "slug": "Alnur-Ali",
                        "structuredName": {
                            "firstName": "Alnur",
                            "lastName": "Ali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alnur Ali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032988"
                        ],
                        "name": "A. Azad",
                        "slug": "A.-Azad",
                        "structuredName": {
                            "firstName": "Ariful",
                            "lastName": "Azad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Azad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238795"
                        ],
                        "name": "A. Bulu\u00e7",
                        "slug": "A.-Bulu\u00e7",
                        "structuredName": {
                            "firstName": "Ayd\u0131n",
                            "lastName": "Bulu\u00e7",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bulu\u00e7"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140594906"
                        ],
                        "name": "D. Morozov",
                        "slug": "D.-Morozov",
                        "structuredName": {
                            "firstName": "Dmitriy",
                            "lastName": "Morozov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Morozov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757847"
                        ],
                        "name": "L. Oliker",
                        "slug": "L.-Oliker",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Oliker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Oliker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731111"
                        ],
                        "name": "K. Yelick",
                        "slug": "K.-Yelick",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Yelick",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yelick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103048024"
                        ],
                        "name": "Sang-Yun Oh",
                        "slug": "Sang-Yun-Oh",
                        "structuredName": {
                            "firstName": "Sang-Yun",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Yun Oh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Furthermore, in most existing work, when multiple multiplications are composed together, the user has to specify the data layout for each matrix separately [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3644127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16b0e58f16e3336311fdb4a40ffb4755edca31ca",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Author(s): Koanantakool, P; Ali, A; Azad, A; Buluc, A; Morozov, D; Oliker, L; Yelick, K; Oh, SY | Abstract: Copyright 2018 by the author(s). Across a variety of scientific disciplines, sparse inverse covariance estimation is a popular tool for capturing the underlying dependency relationships in multivariate data. Unfortunately, most estimators are not scalable enough to handle the sizes of modern high-dimensional data sets (often on the order of terabytes), and assume Gaussian samples. To address these deficiencies, we introduce HP-CONCORD, a highly scalable optimization method for estimating a sparse inverse covariance matrix based on a regularized pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal gradient method uses a novel communication-avoiding linear algebra algorithm and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving parallel scalability on problems with up to \u2248819 billion parameters (1.28 million dimensions); even on a single node, HP-CONCORD demonstrates scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to estimate the underlying dependency structure of the brain from fMRI data, and use the result to identify functional regions automatically. The results show good agreement with a clustering from the neuroscience literature."
            },
            "slug": "Communication-Avoiding-Optimization-Methods-for-Koanantakool-Ali",
            "title": {
                "fragments": [],
                "text": "Communication-Avoiding Optimization Methods for Distributed Massive-Scale Sparse Inverse Covariance Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "HP-CONCORD is introduced, a highly scalable optimization method for estimating a sparse inverse covariance matrix based on a regularized pseudolikelihood framework, without assuming Gaussianity, and is used to estimate the underlying dependency structure of the brain from fMRI data."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "However, they only explored the parallelization of AlexNet [10] and they have not implemented the algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "However, they only explored the parallelization of AlexNet [8] and they have not implemented the algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80950,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737715"
                        ],
                        "name": "Grey Ballard",
                        "slug": "Grey-Ballard",
                        "structuredName": {
                            "firstName": "Grey",
                            "lastName": "Ballard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grey Ballard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682226"
                        ],
                        "name": "Olga Holtz",
                        "slug": "Olga-Holtz",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Holtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Holtz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14411446"
                        ],
                        "name": "O. Schwartz",
                        "slug": "O.-Schwartz",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11553197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2905b93d056f3c71a4ac92b737a4c77680ce28e",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "In 1981 Hong and Kung proved a lower bound on the amount of communication (amount of data moved between a small, fast memory and large, slow memory) needed to perform dense, n-by-n matrix multiplication using the conventional O(n3) algorithm, where the input matrices were too large to fit in the small, fast memory. In 2004 Irony, Toledo, and Tiskin gave a new proof of this result and extended it to the parallel case (where communication means the amount of data moved between processors). In both cases the lower bound may be expressed as \u03a9(#arithmetic_operations/M), where M is the size of the fast memory (or local memory in the parallel case). Here we generalize these results to a much wider variety of algorithms, including LU factorization, Cholesky factorization, LDLT factorization, QR factorization, the Gram\u2013Schmidt algorithm, and algorithms for eigenvalues and singular values, i.e., essentially all direct methods of linear algebra. The proof works for dense or sparse matrices and for sequential or para..."
            },
            "slug": "Minimizing-Communication-in-Numerical-Linear-Ballard-Demmel",
            "title": {
                "fragments": [],
                "text": "Minimizing Communication in Numerical Linear Algebra"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work generalizes a lower bound on the amount of communication needed to perform dense, n-by-n matrix multiplication using the conventional O(n3) algorithm to a much wider variety of algorithms, including LU factorization, Cholesky factors, LDLT factors, QR factors, the Gram\u2013Schmidt algorithm, and algorithms for eigenvalues and singular values."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Matrix Anal. Appl."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 55
                            }
                        ],
                        "text": "The following samples were randomly generated from the Transformer language models described in the paper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 32
                            }
                        ],
                        "text": "Applying Mesh-TensorFlow to the Transformer model, we are able to train models with 5 billion parameters on up to 512-core clusters, establishing new state-of-the-art results for WMT14 En-Fr translation task and the One Billion Word language modeling benchmark."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 46
                            }
                        ],
                        "text": "We implemented a model-parallel layout of the Transformer attention-based sequence-to-sequence model described in [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 46
                            }
                        ],
                        "text": "Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state-of-the-art results on WMT\u201914 English-to-French translation task and the one-billion-word Language modeling benchmark."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 30
                            }
                        ],
                        "text": "Using this layout, we trained Transformer models with feed-forward hidden dimensions up to 262144 and up to 256 attention heads on 2- dimensional TPUv2 meshes of up to 16x32=512 cores, maintaining computational efficiency of over 50% (6 PFLOP/s out of a maximum 11.5 PFLOP/s) on the largest models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 94
                            }
                        ],
                        "text": "We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer [21] sequence-to-sequence model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "We implemented a model-parallel layout of the Transformer attention-based sequence-to-sequence model described in [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 137
                            }
                        ],
                        "text": "For example, a 512-core TPU cluster with a 16x16x2 toroidal network interconnect could be represented by a 3-dimensional mesh with shape [16, 16, 2], a two-dimensional mesh with shape [32, 16], a one-dimensional mesh with shape [512], etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 38
                            }
                        ],
                        "text": "To examine the benefit of scaling the Transformer model in the manner suggested by the previous section, we trained such models on machine translation and language modeling tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer [16] sequence-to-sequence model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": false,
            "numCitedBy": 35176,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054069005"
                        ],
                        "name": "Nikhil Jain",
                        "slug": "Nikhil-Jain",
                        "structuredName": {
                            "firstName": "Nikhil",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikhil Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787471"
                        ],
                        "name": "Yogish Sabharwal",
                        "slug": "Yogish-Sabharwal",
                        "structuredName": {
                            "firstName": "Yogish",
                            "lastName": "Sabharwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yogish Sabharwal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18633558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3492a16bccda6e64cd9eee62aabcab304c16ce7e",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Collectives are an important and frequently used component of MPI. Bucket algorithms, also known as \"large vector\" algorithms, were introduced in the early 90's and have since evolved as a well known paradigm for large MPI collectives. Many modern day supercomputers such as the IBM Blue Gene and Cray XT are based on torus interconnects that offer a highly scalable interconnection architecture for distributed memory systems. While near optimal algorithms have been developed for torus interconnects in other paradigms, such as spanning trees, bucket algorithms have not been optimally extended to these networks. In this paper, we study the basic \"divide, distribute and gather\" MPI collectives for bucket algorithms -- Allgather, Reduce-scatter and Allreduce -- for large messages on torus interconnects.\n We present bucket-based algorithms for these collectives on bidirectional links. We show that these algorithms are optimal in terms of bandwidth and computation for symmetric torus networks (i.e. when all the dimensions are equal), matching the theoretical lower bounds For an asymmetric torus, our algorithms are asymptotically optimal and converge to the lower bound for large dimension sizes. We also argue that our bucket algorithms are more scalable on multi-cores in comparison to spanning tree algorithms. Previous studies of bucket algorithms on torus interconnects have focused on unidirectional links and have been unable to obtain tight lower bounds and optimal algorithms. We close this gap by providing stronger lower bounds and showing that our bidirectional algorithms can easily be adapted to the unidirectional case, matching our lower bounds in terms of bandwidth and computational complexity.\n We implement our algorithms on the IBM Blue Gene/P Supercomputer, which has quad-core nodes connected in a 3-dimensional torus, using the low level communication interface. We demonstrate that our algorithms perform within 7--30% of the lower bounds for different MPI collectives. We demonstrate good scaling using multicores. We also demonstrate a factor of 3 to 17 speedup for various collectives in comparison to the latest optimized MPI implementation."
            },
            "slug": "Optimal-bucket-algorithms-for-large-MPI-collectives-Jain-Sabharwal",
            "title": {
                "fragments": [],
                "text": "Optimal bucket algorithms for large MPI collectives on torus interconnects"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper studies the basic \"divide, distribute and gather\" MPI collectives for bucket algorithms -- Allgather, Reduce-scatter and Allreduce -- for large messages on torus interconnects and shows that these algorithms are optimal in terms of bandwidth and computation for symmetric torus networks, matching the theoretical lower bounds."
            },
            "venue": {
                "fragments": [],
                "text": "ICS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32294544"
                        ],
                        "name": "Peter H. Jin",
                        "slug": "Peter-H.-Jin",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Jin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter H. Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31963005"
                        ],
                        "name": "Boris Ginsburg",
                        "slug": "Boris-Ginsburg",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Ginsburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Ginsburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "For example, convolutions on spatiallypartitioned tensors will require the communication of \"halo\" regions, as described in [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 93002444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c39c1e0eec76c3bad44d088aad0d9cfb8e95f2e2",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The training of convolutional neural networks with large inputs on GPUs is limited by the available GPU memory capacity. In this work, we describe spatially parallel convolutions, which sidestep the memory capacity limit of a single GPU by partitioning tensors along their spatial axes across multiple GPUs. On modern multi-GPU systems, we demonstrate that spatially parallel convolutions attain excellent scaling when applied to input tensors with large spatial dimensions."
            },
            "slug": "Spatially-Parallel-Convolutions-Jin-Ginsburg",
            "title": {
                "fragments": [],
                "text": "Spatially Parallel Convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Spatially parallel convolutions are described, which sidestep the memory capacity limit of a single GPU by partitioning tensors along their spatial axes across multiple GPUs."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36031131"
                        ],
                        "name": "Grace Dinh",
                        "slug": "Grace-Dinh",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Dinh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grace Dinh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "5D [14] algorithm for matrices with different sparsities, best tile sizes for direct convolutions [17], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3437163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd8cf4e6a1f3a4c61ac46a58bfa86c904d8d546",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficiently executing convolutional neural nets (CNNs) is important in many machine-learning tasks. Since the cost of moving a word of data, either between levels of a memory hierarchy or between processors over a network, is much higher than the cost of an arithmetic operation, minimizing data movement is critical to performance optimization. In this paper, we present both new lower bounds on data movement needed for CNNs, and optimal sequential algorithms that attain these lower bounds. In most common cases, our optimal algorithms can attain significantly more data reuse than matrix multiplication."
            },
            "slug": "Communication-Optimal-Convolutional-Neural-Nets-Demmel-Dinh",
            "title": {
                "fragments": [],
                "text": "Communication-Optimal Convolutional Neural Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "New lower bounds on data movement needed for CNNs, and optimal sequential algorithms that attain these lower bounds that can attain significantly more data reuse than matrix multiplication are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944541"
                        ],
                        "name": "R. J\u00f3zefowicz",
                        "slug": "R.-J\u00f3zefowicz",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "J\u00f3zefowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J\u00f3zefowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48607963"
                        ],
                        "name": "Yonghui Wu",
                        "slug": "Yonghui-Wu",
                        "structuredName": {
                            "firstName": "Yonghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonghui Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "1 Best Ensemble (different methods)[13] > 100 23."
                    },
                    "intents": []
                }
            ],
            "corpusId": 260422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "isKey": false,
            "numCitedBy": 951,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon."
            },
            "slug": "Exploring-the-Limits-of-Language-Modeling-J\u00f3zefowicz-Vinyals",
            "title": {
                "fragments": [],
                "text": "Exploring the Limits of Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This work explores recent advances in Recurrent Neural Networks for large scale Language Modeling, and extends current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158546"
                        ],
                        "name": "D. Eliahu",
                        "slug": "D.-Eliahu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eliahu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eliahu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143608596"
                        ],
                        "name": "A. Fox",
                        "slug": "A.-Fox",
                        "structuredName": {
                            "firstName": "Armando",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683292"
                        ],
                        "name": "S. Kamil",
                        "slug": "S.-Kamil",
                        "structuredName": {
                            "firstName": "Shoaib",
                            "lastName": "Kamil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kamil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738053"
                        ],
                        "name": "B. Lipshitz",
                        "slug": "B.-Lipshitz",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Lipshitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lipshitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14411446"
                        ],
                        "name": "O. Schwartz",
                        "slug": "O.-Schwartz",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416400"
                        ],
                        "name": "Omer Spillinger",
                        "slug": "Omer-Spillinger",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Spillinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Spillinger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 200
                            }
                        ],
                        "text": "Mesh-TensorFlow can express a wide range of uniform partitionings of the iteration space and therefore can adopt many best known mappings, e.g., 3D [3, 1] and 2.5D [8] algorithms for square matrices, CARMA [13] for rectangular matrices, 1.5D [18] algorithm for matrices with different sparsities, best tile sizes for direct convolutions [22], etc., although sometimes with higher memory requirements."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "5D [6] algorithms for square matrices, CARMA [9] for rectangular matrices, 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7685882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8106b64eebf3b7dab5ddbc2226fc3c5f000176f",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Communication-optimal algorithms are known for square matrix multiplication. Here, we obtain the first communication-optimal algorithm for all dimensions of rectangular matrices. Combining the dimension-splitting technique of Frigo, Leiserson, Prokop and Ramachandran (1999) with the recursive BFS/DFS approach of Ballard, Demmel, Holtz, Lipshitz and Schwartz (2012) allows for a communication-optimal as well as cache and network-oblivious algorithm. Moreover, the implementation is simple: approximately 50 lines of code for the shared-memory version. Since the new algorithm minimizes communication across the network, between NUMA domains, and between levels of cache, it performs well in practice on both shared and distributed-memory machines. We show significant speedups over existing parallel linear algebra libraries both on a 32-core shared-memory machine and on a distributed-memory supercomputer."
            },
            "slug": "Communication-Optimal-Parallel-Recursive-Matrix-Demmel-Eliahu",
            "title": {
                "fragments": [],
                "text": "Communication-Optimal Parallel Recursive Rectangular Matrix Multiplication"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work obtains the first communication-optimal algorithm for all dimensions of rectangular matrices by combining the dimension-splitting technique with the recursive BFS/DFS approach, and shows significant speedups over existing parallel linear algebra libraries both on a 32-core shared-memory machine and on a distributed-memory supercomputer."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE 27th International Symposium on Parallel and Distributed Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344182"
                        ],
                        "name": "Penporn Koanantakool",
                        "slug": "Penporn-Koanantakool",
                        "structuredName": {
                            "firstName": "Penporn",
                            "lastName": "Koanantakool",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Penporn Koanantakool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032988"
                        ],
                        "name": "A. Azad",
                        "slug": "A.-Azad",
                        "structuredName": {
                            "firstName": "Ariful",
                            "lastName": "Azad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Azad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238795"
                        ],
                        "name": "A. Bulu\u00e7",
                        "slug": "A.-Bulu\u00e7",
                        "structuredName": {
                            "firstName": "Ayd\u0131n",
                            "lastName": "Bulu\u00e7",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bulu\u00e7"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140594906"
                        ],
                        "name": "D. Morozov",
                        "slug": "D.-Morozov",
                        "structuredName": {
                            "firstName": "Dmitriy",
                            "lastName": "Morozov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Morozov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103048024"
                        ],
                        "name": "Sang-Yun Oh",
                        "slug": "Sang-Yun-Oh",
                        "structuredName": {
                            "firstName": "Sang-Yun",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Yun Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757847"
                        ],
                        "name": "L. Oliker",
                        "slug": "L.-Oliker",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Oliker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Oliker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731111"
                        ],
                        "name": "K. Yelick",
                        "slug": "K.-Yelick",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Yelick",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yelick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4637551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a845b1493914f7ea7fcb45cd140d38515a95b556",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiplication of a sparse matrix with a dense matrix is a building block of an increasing number of applications in many areas such as machine learning and graph algorithms. However, most previous work on parallel matrix multiplication considered only both dense or both sparse matrix operands. This paper analyzes the communication lower bounds and compares the communication costs of various classic parallel algorithms in the context of sparse-dense matrix-matrix multiplication. We also present new communication-avoiding algorithms based on a 1D decomposition, called 1.5D, which - while suboptimal in dense-dense and sparse-sparse cases - outperform the 2D and 3D variants both theoretically and in practice for sparse-dense multiplication. Our analysis separates one-time costs from per iteration costs in an iterative machine learning context. Experiments demonstrate speedups up to 100x over a baseline 3D SUMMA implementation and show parallel scaling over 10 thousand cores."
            },
            "slug": "Communication-Avoiding-Parallel-Sparse-Dense-Koanantakool-Azad",
            "title": {
                "fragments": [],
                "text": "Communication-Avoiding Parallel Sparse-Dense Matrix-Matrix Multiplication"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper analyzes the communication lower bounds and compares the communication costs of various classic parallel algorithms in the context of sparse-dense matrix-matrix multiplication and presents new communication-avoiding algorithms based on a 1D decomposition, called 1.5D."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880213"
                        ],
                        "name": "Edgar Solomonik",
                        "slug": "Edgar-Solomonik",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Solomonik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Solomonik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "This technique is sometimes called iteration space tiling [2], replication [6], or task parallelism [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "5D [6] algorithms for square matrices, CARMA [9] for rectangular matrices, 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13931921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ffae44b736b6dbd6d715977bb6381297dd94304",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Extra memory allows parallel matrix multiplication to be done with asymptotically less communication than Cannon's algorithm and be faster in practice. \"3D\" algorithms arrange the p processors in a 3D array, and store redundant copies of the matrices on each of p1/3 layers. \"2D\" algorithms such as Cannon's algorithm store a single copy of the matrices on a 2D array of processors. We generalize these 2D and 3D algorithms by introducing a new class of \"2.5D algorithms\". For matrix multiplication, we can take advantage of any amount of extra memory to store c copies of the data, for any c \u2208 {1, 2,..., \u230ap1/3\u230b}, to reduce the bandwidth cost of Cannon's algorithm by a factor of c1/2 and the latency cost by a factor c3/2. We also show that these costs reach the lower bounds, modulo polylog(p) factors. We introduce a novel algorithm for 2.5D LU decomposition. To the best of our knowledge, this LU algorithm is the first to minimize communication along the critical path of execution in the 3D case. Our 2.5D LU algorithm uses communicationavoiding pivoting, a stable alternative to partial-pivoting. We prove a novel lower bound on the latency cost of 2.5D and 3D LU factorization, showing that while c copies of the data can also reduce the bandwidth by a factor of c1/2, the latency must increase by a factor of c1/2, so that the 2D LU algorithm (c = 1) in fact minimizes latency. We provide implementations and performance results for 2D and 2.5D versions of all the new algorithms. Our results demonstrate that 2.5D matrix multiplication and LU algorithms strongly scale more efficiently than 2D algorithms. Each of our 2.5D algorithms performs over 2X faster than the corresponding 2D algorithm for certain problem sizes on 65,536 cores of a BG/P supercomputer."
            },
            "slug": "Communication-Optimal-Parallel-2.5D-Matrix-and-LU-Solomonik-Demmel",
            "title": {
                "fragments": [],
                "text": "Communication-Optimal Parallel 2.5D Matrix Multiplication and LU Factorization Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel lower bound on the latency cost of 2.5D and 3D LU factorization is proved, showing that while c copies of the data can be reduced, the latency must increase by a factor of c1/2, so that the 2D LU algorithm (c = 1) in fact minimizes latency."
            },
            "venue": {
                "fragments": [],
                "text": "Euro-Par"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47231631"
                        ],
                        "name": "M. Christ",
                        "slug": "M.-Christ",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Christ",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Christ"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143776057"
                        ],
                        "name": "Nicholas Knight",
                        "slug": "Nicholas-Knight",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13017127"
                        ],
                        "name": "T. Scanlon",
                        "slug": "T.-Scanlon",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Scanlon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Scanlon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731111"
                        ],
                        "name": "K. Yelick",
                        "slug": "K.-Yelick",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Yelick",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yelick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6823565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdf57b4bca2ccf01ae5a00a8b3e761270bdf32ea",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Communication, i.e., moving data, between levels of a memory hierarchy or between parallel processors on a network, can greatly dominate the cost of computation, so algorithms that minimize communication can run much faster (and use less energy) than algorithms that do not. Motivated by this, attainable communication lower bounds were established in [12, 13, 4] for a variety of algorithms including matrix computations. The lower bound approach used initially in [13] for Theta(N3) matrix multiplication, and later in [4] for many other linear algebra algorithms, depended on a geometric result by Loomis and Whitney [16]: this result bounded the volume of a 3D set (representing multiply-adds done in the inner loop of the algorithm) using the product of the areas of certain 2D projections of this set (representing the matrix entries available locally, i.e., without communication). Using a recent generalization of Loomis' and Whitney's result, we generalize this lower bound approach to a much larger class of algorithms, that may have arbitrary numbers of loops and arrays with arbitrary dimensions as long as the index expressions are a ne combinations of loop variables. In other words, the algorithm can do arbitrary operations on any number of variables like A(i(sub 1), i(sub 2), i(sub 2) - 2i(sub 1), 3 - 4i(sub 3) + 7i(sub 4), ...). Moreover, the result applies to recursive programs, irregular iteration spaces, sparse matrices, and other data structures as long as the computation can be logically mapped to loops and indexed data structure accesses. We also discuss when optimal algorithms exist that attain the lower bounds; this leads to new asymptotically faster algorithms for several problems."
            },
            "slug": "Communication-lower-bounds-and-optimal-algorithms-1-Christ-Demmel",
            "title": {
                "fragments": [],
                "text": "Communication lower bounds and optimal algorithms for programs that reference arrays - Part 1"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work generalizes the lower bound approach used initially for Theta(N3) matrix multiplication to a much larger class of algorithms, that may have arbitrary numbers of loops and arrays with arbitrary dimensions as long as the index expressions are a ne combinations of loop variables."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2992791"
                        ],
                        "name": "Dror Irony",
                        "slug": "Dror-Irony",
                        "structuredName": {
                            "firstName": "Dror",
                            "lastName": "Irony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dror Irony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287613"
                        ],
                        "name": "Sivan Toledo",
                        "slug": "Sivan-Toledo",
                        "structuredName": {
                            "firstName": "Sivan",
                            "lastName": "Toledo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sivan Toledo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835401"
                        ],
                        "name": "A. Tiskin",
                        "slug": "A.-Tiskin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Tiskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tiskin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 762539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd6d2cf4ec3d765ecd084a747ab4c1d71d8609e0",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Communication-lower-bounds-for-distributed-memory-Irony-Toledo",
            "title": {
                "fragments": [],
                "text": "Communication lower bounds for distributed-memory matrix multiplication"
            },
            "venue": {
                "fragments": [],
                "text": "J. Parallel Distributed Comput."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50073003"
                        ],
                        "name": "M. Wolfe",
                        "slug": "M.-Wolfe",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wolfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wolfe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 137
                            }
                        ],
                        "text": "For example, a 512-core TPU cluster with a 16x16x2 toroidal network interconnect could be represented by a 3-dimensional mesh with shape [16, 16, 2], a two-dimensional mesh with shape [32, 16], a one-dimensional mesh with shape [512], etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "This technique is sometimes called iteration space tiling [2], replication [6], or task parallelism [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8906913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "831cc6d9b7a333b38d34d923b52aed438e90ee1e",
            "isKey": false,
            "numCitedBy": 455,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Subdividing the iteration space of a loop into blocks or tiles with a fixed maximum size has several advantages. Tiles become a natural candidate as the unit of work for parallel task scheduling. Synchronization between processors can be done between tiles, reducing synchronization frequency (at some loss of potential parallelism). The shape and size of a tile can be optimized to take advantage of memory locality for memory hierarchy utilization. Vectorization and register locality naturally fits into the optimization within a tile, while parallelization and cache locality fits into optimization between tiles."
            },
            "slug": "More-iteration-space-tiling-Wolfe",
            "title": {
                "fragments": [],
                "text": "More iteration space tiling"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Subdividing the iteration space of a loop into blocks or tiles with a fixed maximum size has several advantages, and tiles become a natural candidate as the unit of work for parallel task scheduling."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1989 ACM/IEEE Conference on Supercomputing (Supercomputing '89)"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737715"
                        ],
                        "name": "Grey Ballard",
                        "slug": "Grey-Ballard",
                        "structuredName": {
                            "firstName": "Grey",
                            "lastName": "Ballard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grey Ballard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238795"
                        ],
                        "name": "A. Bulu\u00e7",
                        "slug": "A.-Bulu\u00e7",
                        "structuredName": {
                            "firstName": "Ayd\u0131n",
                            "lastName": "Bulu\u00e7",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bulu\u00e7"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724962"
                        ],
                        "name": "L. Grigori",
                        "slug": "L.-Grigori",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Grigori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Grigori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738053"
                        ],
                        "name": "B. Lipshitz",
                        "slug": "B.-Lipshitz",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Lipshitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lipshitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14411446"
                        ],
                        "name": "O. Schwartz",
                        "slug": "O.-Schwartz",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287613"
                        ],
                        "name": "Sivan Toledo",
                        "slug": "Sivan-Toledo",
                        "structuredName": {
                            "firstName": "Sivan",
                            "lastName": "Toledo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sivan Toledo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2836595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce1109de9fd72ad10da896211ca50a0ff0f292ed",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Parallel algorithms for sparse matrix-matrix multiplication typically spend most of their time on inter-processor communication rather than on computation, and hardware trends predict the relative cost of communication will only increase. Thus, sparse matrix multiplication algorithms must minimize communication costs in order to scale to large processor counts. In this paper, we consider multiplying sparse matrices corresponding to Erd\u0151s-R\u00e9nyi random graphs on distributed-memory parallel machines. We prove a new lower bound on the expected communication cost for a wide class of algorithms. Our analysis of existing algorithms shows that, while some are optimal for a limited range of matrix density and number of processors, none is optimal in general. We obtain two new parallel algorithms and prove that they match the expected communication cost lower bound, and hence they are optimal."
            },
            "slug": "Communication-optimal-parallel-multiplication-of-Ballard-Bulu\u00e7",
            "title": {
                "fragments": [],
                "text": "Communication optimal parallel multiplication of sparse random matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Two new parallel algorithms are obtained and it is proved that they match the expected communication cost lower bound, and hence they are optimal."
            },
            "venue": {
                "fragments": [],
                "text": "SPAA"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238719"
                        ],
                        "name": "Pitch Patarasuk",
                        "slug": "Pitch-Patarasuk",
                        "structuredName": {
                            "firstName": "Pitch",
                            "lastName": "Patarasuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pitch Patarasuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143642078"
                        ],
                        "name": "Xin Yuan",
                        "slug": "Xin-Yuan",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Yuan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 204
                            }
                        ],
                        "text": "The physical network topology does affect performance; particularly important is the performance of MPI Allreduce, grouped by splitting the mesh by a subset of the dimensions, which can be very efficient [4] [5] if each such group is physically connected."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7433454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4e48c2a5de9337d147ebbb7d0ff0e555adceca",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bandwidth-optimal-all-reduce-algorithms-for-of-Patarasuk-Yuan",
            "title": {
                "fragments": [],
                "text": "Bandwidth optimal all-reduce algorithms for clusters of workstations"
            },
            "venue": {
                "fragments": [],
                "text": "J. Parallel Distributed Comput."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055613741"
                        ],
                        "name": "Himanshu Gupta",
                        "slug": "Himanshu-Gupta",
                        "structuredName": {
                            "firstName": "Himanshu",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Himanshu Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293384"
                        ],
                        "name": "P. Sadayappan",
                        "slug": "P.-Sadayappan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sadayappan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sadayappan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 36739255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b582d4a005c3288858eb3910e9233edb35323f49",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an efficient dense matrix multiplication algorithm for distributed memory computers with a hypercube topology. The proposed algorithm performs better than all previously proposed algorithms for a wide range of matrix sizes and number of processors, especially for large matrices. We analyze the performance of the algorithms for two types of hypercube architectures, one in which each node can use (to send and receive) at most one communication link at a time and the other in which each node can use all communication links simultaneously."
            },
            "slug": "Communication-efficient-matrix-multiplication-on-Gupta-Sadayappan",
            "title": {
                "fragments": [],
                "text": "Communication efficient matrix multiplication on hypercubes"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "An efficient dense matrix multiplication algorithm for distributed memory computers with a hypercube topology that performs better than all previously proposed algorithms for a wide range of matrix sizes and number of processors, especially for large matrices."
            },
            "venue": {
                "fragments": [],
                "text": "SPAA '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737322"
                        ],
                        "name": "A. Aggarwal",
                        "slug": "A.-Aggarwal",
                        "structuredName": {
                            "firstName": "Alok",
                            "lastName": "Aggarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aggarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757332"
                        ],
                        "name": "A. K. Chandra",
                        "slug": "A.-K.-Chandra",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Chandra",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. K. Chandra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699887"
                        ],
                        "name": "M. Snir",
                        "slug": "M.-Snir",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Snir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Snir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ing [2], replication [8], or task parallelism [15]. Mesh-TensorFlow can express a wide range of uniform partitionings of the iteration space and therefore can adopt many best known mappings, e.g., 3D [3, 1] and 2.5D [8] algorithms for square matrices, CARMA [13] for rectangular matrices, 1.5D [18] algorithm for matrices with different sparsities, best tile sizes for direct convolutions [22], etc., altho"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46070306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34d51059c4b97a888970354be43603fa4fa86c84",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Communication-Complexity-of-PRAMs-Aggarwal-Chandra",
            "title": {
                "fragments": [],
                "text": "Communication Complexity of PRAMs"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "senior vice president of sales and marketing at Aspen , there is a percentage of the online response for the auto company escalating to 18 customers and increasing to 30 days a year"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "who will make an appearance at this year 's Consumer Electronics Show in Las Vegas next week , these mobile gadgets will be able to \" talk"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model with 0.37B Parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "director of the Economic Policy Institute , India ranks 21st out of 221 nations in the current budget deficit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "creator of the Review of Medical UNK , the organisation could be the \" holy grail"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the chief executive of ITC Emerging Markets , the metals sector should contribute about $ 210 billion by year-end 2008 -a figure that has now risen to $ 3 billion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "a transplant surgeon and Harvard astrophysicist , the results are illuminated in Honolulu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "of UNK , a software company in California , computer games are alive and well , with millions of gamers , but most games do not make money"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the physicist who has kept many of these principles alive , the world has vanished into \" more and more of the static world \" -and in many ways we 're losing the earth 's ability to appreciate water"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "we are saving the most jobs in the world"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "According to Ray Kurzweil"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Smoking : The Letters of Hysteria and Reclining the State of South Carolina"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "creator of the first modern computer , the sexy cyborg was the brainchild of an MIT professor , Thomas Harris , and a former banking entrepreneur , Henry Lee , who was looking for an UNK"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and a panel of experts testifying in Los Angeles , six years in Congress attempts to improve \" brain \" of Americans by hitting them with a $ 50 annual fee"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the futurist son of the futurist who wrote The Singularity is Near , the \" early days"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "chief technology officer for the US Department of Energy , aviation has \" potential to be the largest and fastest growing source of consumer and commercial emissions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the expert and co-author of \" The Singularity is Near : Comprehending the Technological Future of Engineering"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "creator of the Star Wars creator and creator of the popular game"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "professor of musical economics at the University of California , Berkeley , the military 's \" biggest challenge \" might have been growing the region 's growing urban sprawl"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the inventor of the modern personal computer , the shrinking human brain could eventually replace the Internet as a tool of human intelligence and imagination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the head of PMI , he has now written off all his employees fairly"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the owner and web guru at Stanford University , even a single person might fall in love with the internet simultaneously"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": ", and Marc Snir . \u201c Communication Complexity of PRAMs \u201d"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "chief executive of the company , very few people work through chronology , and most people can 't use tables on their machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the famously headstrong and egocentric stack-on-stack , a keyboard is more than a part of the laptop 's muscle when it is standing upright rather than crouching"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the futurist turned futurist , the onset of Alzheimer 's coincided precisely with the rate of unemployment in America"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "The Mesh-TensorFlow language is nearly identical to TensorFlow [12], with the familiar notions of graphs, tensors, operations, variables, devices (called meshes), and automatic gradient computation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 53,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Mesh-TensorFlow:-Deep-Learning-for-Supercomputers-Shazeer-Cheng/2270b8628fd8ca67ae39d277f45bc3c38ac63d5f?sort=total-citations"
}