{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3230391"
                        ],
                        "name": "A. Georghiades",
                        "slug": "A.-Georghiades",
                        "structuredName": {
                            "firstName": "Athinodoros",
                            "lastName": "Georghiades",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Georghiades"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9234219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6642e9c6cf7432e2d11b7edf7cd47f1285acd54e",
            "isKey": true,
            "numCitedBy": 4697,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test results show that the method performs almost without error, except on the most extreme lighting directions."
            },
            "slug": "From-Few-to-Many:-Illumination-Cone-Models-for-Face-Georghiades-Belhumeur",
            "title": {
                "fragments": [],
                "text": "From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A generative appearance-based method for recognizing human faces under variation in lighting and viewpoint that exploits the fact that the set of images of an object in fixed pose but under all possible illumination conditions, is a convex cone in the space of images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107387180"
                        ],
                        "name": "P. Sharma",
                        "slug": "P.-Sharma",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145708716"
                        ],
                        "name": "R. Reilly",
                        "slug": "R.-Reilly",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Reilly",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Reilly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "color [32]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15095500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b733bfe6554e0c8900312a057a5dde50ce9bf517",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "With increasing research in the area of face segmentation, new methods for detecting human faces automatically are being developed. However, less attention is being paid to the development of a standard face image database to evaluate these new algorithms. This paper recognizes the need for a colour face image database and creates such a database for direct benchmarking of automatic face detection algorithms. The database has two parts. Part one contains colour pictures of faces having a high degree of variability in scale, location, orientation, pose, facial expression and lighting conditions, while part two has manually segmented results for each of the images in part one of the database. This allows direct comparison of algorithms. These images are acquired from a wide variety of sources such as digital cameras, pictures scanned in from a photo-scanner and the World Wide Web. The database is intended for distribution to researchers. Details of the face database such as the development process and file information along with a common criterion for performance evaluation measures is also discussed in this paper."
            },
            "slug": "A-colour-face-image-database-for-benchmarking-of-Sharma-Reilly",
            "title": {
                "fragments": [],
                "text": "A colour face image database for benchmarking of automatic face detection algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The need for a colour face image database is recognized and such a database is created for direct benchmarking of automatic face detection algorithms for direct comparison of algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings EC-VIP-MC 2003. 4th EURASIP Conference focused on Video/Image Processing and Multimedia Communications (IEEE Cat. No.03EX667)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34497462"
                        ],
                        "name": "Jaety Edwards",
                        "slug": "Jaety-Edwards",
                        "structuredName": {
                            "firstName": "Jaety",
                            "lastName": "Edwards",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaety Edwards"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109562930"
                        ],
                        "name": "Ryan White",
                        "slug": "Ryan-White",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "White",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan White"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17113597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804db97075c4e371177e5bfffe8012de237ae44d",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We show quite good face clustering is possible for a dataset of inaccurately and ambiguously labelled face images. Our dataset is 44,773 face images, obtained by applying a face finder to approximately half a million captioned news images. This dataset is more realistic than usual face recognition datasets, because it contains faces captured \"in the wild\" in a variety of configurations with respect to the camera, taking a variety of expressions, and under illumination of widely varying color. Each face image is associated with a set of names, automatically extracted from the associated caption. Many, but not all such sets contain the correct name. We cluster face images in appropriate discriminant coordinates. We use a clustering procedure to break ambiguities in labelling and identify incorrectly labelled faces. A merging procedure then identifies variants of names that refer to the same individual. The resulting representation can be used to label faces in news images or to organize news pictures by individuals present. An alternative view of our procedure is as a process that cleans up noisy supervised data. We demonstrate how to use entropy measures to evaluate such procedures."
            },
            "slug": "Names-and-faces-in-the-news-Berg-Berg",
            "title": {
                "fragments": [],
                "text": "Names and faces in the news"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "It is shown quite good face clustering is possible for a dataset of inaccurately and ambiguously labelled face images, obtained by applying a face finder to approximately half a million captioned news images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37110739"
                        ],
                        "name": "Wen Gao",
                        "slug": "Wen-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48437987"
                        ],
                        "name": "B. Cao",
                        "slug": "B.-Cao",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145455919"
                        ],
                        "name": "S. Shan",
                        "slug": "S.-Shan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Shan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2104086"
                        ],
                        "name": "Delong Zhou",
                        "slug": "Delong-Zhou",
                        "structuredName": {
                            "firstName": "Delong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Delong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108584873"
                        ],
                        "name": "Xiaohua Zhang",
                        "slug": "Xiaohua-Zhang",
                        "structuredName": {
                            "firstName": "Xiaohua",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohua Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725937"
                        ],
                        "name": "Debin Zhao",
                        "slug": "Debin-Zhao",
                        "structuredName": {
                            "firstName": "Debin",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debin Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "CAS-PEAL Face Database 1040 99,594 very large, expression, accessories, lighting, simultaneous capture of multiple poses, Chinese [10]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7326413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2485c98aa44131d1a2f7d1355b1e372f2bb148ad",
            "isKey": false,
            "numCitedBy": 918,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms."
            },
            "slug": "The-CAS-PEAL-Large-Scale-Chinese-Face-Database-and-Gao-Cao",
            "title": {
                "fragments": [],
                "text": "The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The evaluation protocol based on the CAS-PEAL-R1 database is discussed and the performance of four algorithms are presented as a baseline to do the following: elementarily assess the difficulty of the database for face recognition algorithms; preference evaluation results for researchers using the database; and identify the strengths and weaknesses of the commonly used algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To establish baseline results as well as validate the difficulty of LFW, we used the standard face recognition method of Eigenfaces [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[15], [16]), there is a pre-specified gallery consisting of face images of a set of people, where the identity of each face image is known."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16193920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d2bfef811f514391aa2a7b8f4020d1c9e033016",
            "isKey": false,
            "numCitedBy": 3604,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner.<<ETX>>"
            },
            "slug": "Face-recognition-using-eigenfaces-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Face recognition using eigenfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685300"
                        ],
                        "name": "J. Hespanha",
                        "slug": "J.-Hespanha",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Hespanha",
                            "middleNames": [
                                "Pedro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hespanha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Yale Face Database 15 165 expressions, eye glasses, lighting [2]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be86da00efdd8c2a7fdeb2334605796c24b370f0",
            "isKey": false,
            "numCitedBy": 11721,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases."
            },
            "slug": "Eigenfaces-vs.-Fisherfaces:-Recognition-Using-Class-Belhumeur-Hespanha",
            "title": {
                "fragments": [],
                "text": "Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A face recognition algorithm which is insensitive to large variation in lighting direction and facial expression is developed, based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variations in lighting and facial expressions."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110065403"
                        ],
                        "name": "Jennifer Huang",
                        "slug": "Jennifer-Huang",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684626"
                        ],
                        "name": "B. Heisele",
                        "slug": "B.-Heisele",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Heisele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Heisele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880906"
                        ],
                        "name": "V. Blanz",
                        "slug": "V.-Blanz",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Blanz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Blanz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "MIT-CBCL Face Recognition Database 10 > 2000 synthetic images from 3D models, illumination, pose, background [37]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1553425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d45ab87e0f93006a53769e52fcd508c91d69083",
            "isKey": false,
            "numCitedBy": 423,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for pose and illumination invariant face recognition that combines two recent advances in the computer vision field: 3D morphable models and component-based recognition. A 3D morphable model is used to compute 3D face models from three input images of each subject in the training database. The 3D models are rendered under varying pose and illumination conditions to build a large set of synthetic images. These images are then used for training a component-based face recognition system. The face recognition module is preceded by a fast hierarchical face detector resulting in a system that can detect and identify faces in video images at about 4 Hz. The system achieved a recognition rate of 88% on a database of 2000 real images of ten people, which is significantly better than a comparable global face recognition system. The results clearly show the potential of the combination of morphable models and component-based recognition towards pose and illumination invariant face recognition."
            },
            "slug": "Component-Based-Face-Recognition-with-3D-Morphable-Huang-Heisele",
            "title": {
                "fragments": [],
                "text": "Component-Based Face Recognition with 3D Morphable Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A 3D morphable model is used to compute 3D face models from three input images of each subject in the training database and the system achieved a recognition rate significantly better than a comparable global face recognition system."
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14814282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66505cb708b098a93331471f079965f6ded4ea7f",
            "isKey": false,
            "numCitedBy": 449,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "To create a pose-invariant face recognizer, one strategy is the view-based approach, which uses a set of real example views at different poses. But what if we only have one real view available, such as a scanned passport photo-can we still recognize faces under different poses? Given one real view at a known pose, it is still possible to use the view-based approach by exploiting prior knowledge of faces to generate virtual views, or views of the face as seen from different poses. To represent prior knowledge, we use 2D example views of prototype faces under different rotations. We develop example-based techniques for applying the rotation seen in the prototypes to essentially \"rotate\" the single real view which is available. Next, the combined set of one real and multiple virtual views is used as example views for a view-based, pose-invariant face recognizer. Oar experiments suggest that among the techniques for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques.<<ETX>>"
            },
            "slug": "Face-recognition-from-one-example-view-Beymer-Poggio",
            "title": {
                "fragments": [],
                "text": "Face recognition from one example view"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Oar experiments suggest that among the techniques for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219900"
                        ],
                        "name": "Gary B. Huang",
                        "slug": "Gary-B.-Huang",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Huang",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary B. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246870"
                        ],
                        "name": "Vidit Jain",
                        "slug": "Vidit-Jain",
                        "structuredName": {
                            "firstName": "Vidit",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vidit Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Shortly after the publication of the original paper describing Faces in the Wild, a variety of authors started to inquire about using such a distribution of face images in their work [14], [15], [25], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3028943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "592e329fd243f38c4cd1ff393af1f24f6a10e3b8",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Many recognition algorithms depend on careful positioning of an object into a canonical pose, so the position of features relative to a fixed coordinate system can be examined. Currently, this positioning is done either manually or by training a class-specialized learning algorithm with samples of the class that have been hand-labeled with parts or poses. In this paper, we describe a novel method to achieve this positioning using poorly aligned examples of a class with no additional labeling. Given a set of unaligned examplars of a class, such as faces, we automatically build an alignment mechanism, without any additional labeling of parts or poses in the data set. Using this alignment mechanism, new members of the class, such as faces resulting from a face detector, can be precisely aligned for the recognition process. Our alignment method improves performance on a face recognition task, both over unaligned images and over images aligned with a face alignment algorithm specifically developed for and trained on hand-labeled face images. We also demonstrate its use on an entirely different class of objects (cars), again without providing any information about parts or pose to the learning algorithm."
            },
            "slug": "Unsupervised-Joint-Alignment-of-Complex-Images-Huang-Jain",
            "title": {
                "fragments": [],
                "text": "Unsupervised Joint Alignment of Complex Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The alignment method improves performance on a face recognition task, both over unaligned images and over images aligned with a face alignment algorithm specifically developed for and trained on hand-labeled face images."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145010470"
                        ],
                        "name": "D. Ozkan",
                        "slug": "D.-Ozkan",
                        "structuredName": {
                            "firstName": "Derya",
                            "lastName": "Ozkan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ozkan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Shortly after the publication of the original paper describing Faces in the Wild, a variety of authors started to inquire about using such a distribution of face images in their work [14], [15], [25], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2403033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9665247bb7e00310121a9d557db7889246e3d71c",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to associate names and faces for querying people in large news photo collections. On the assumption that a person\u2019s face is likely to appear when his/her name is mentioned in the caption, first all the faces associated with the query name are selected. Among these faces, there could be many faces corresponding to the queried person in different conditions, poses and times, but there could also be other faces corresponding to other people in the caption or some non-face images due to the errors in the face detection method used. However, in most cases, the number of corresponding faces of the queried person will be large, and these faces will be more similar to each other than to others. In this study, we propose a graph based method to find the most similar subset among the set of possible faces associated with the query name, where the most similar subset is likely to correspond to the faces of the queried person. When the similarity of faces are represented in a graph structure, the set of most similar faces will be the densest component in the graph. We represent the similarity of faces using SIFT descriptors. The matching interest points on two faces are decided after the application of two constraints, namely the geometrical constraint and the unique match constraint. The average distance of the matching points are used to construct the similarity graph. The most similar set of faces is then found based on a greedy densest component algorithm. The experiments are performed on thousands of news photographs taken in real life conditions and, therefore, having a large variety of poses, illuminations and expressions."
            },
            "slug": "A-Graph-Based-Approach-for-Naming-Faces-in-News-Ozkan-Sahin",
            "title": {
                "fragments": [],
                "text": "A Graph Based Approach for Naming Faces in News Photos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This study proposes a graph based method to find the most similar subset among the set of possible faces associated with the query name, where the mostsimilar subset is likely to correspond to the faces of the queried person."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052858104"
                        ],
                        "name": "D. B. Graham",
                        "slug": "D.-B.-Graham",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Graham",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Graham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50729465"
                        ],
                        "name": "N. Allinson",
                        "slug": "N.-Allinson",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Allinson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Allinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 141914838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d40689398f8b055dc0111da1405fcedf16a403e",
            "isKey": true,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an eigenspace manifold for the representation and recognition of pose-varying faces. The distribution of faces in this manifold allows us to determine theoretical recognition characteristics which are then verified experimentally. Using this manifold a framework is proposed which can be used for both familiar and unfamiliar face recognition. A simple implementation demonstrates the pose dependent nature of the system over the transition from unfamiliar to familiar face recognition. Furthermore we show that multiple test images, whether real or virtual, can be used to augment the recognition process. The results compare favourably with reported human face recognition experiments. Finally, we describe how this framework can be used as a mechanism for characterising faces from video for general purpose recognition."
            },
            "slug": "Characterising-Virtual-Eigensignatures-for-General-Graham-Allinson",
            "title": {
                "fragments": [],
                "text": "Characterising Virtual Eigensignatures for General Purpose Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An eigenspace manifold for the representation and recognition of pose-varying faces is described and a framework is proposed which can be used for both familiar and unfamiliar face recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704876"
                        ],
                        "name": "P. Flynn",
                        "slug": "P.-Flynn",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Flynn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Flynn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067993"
                        ],
                        "name": "W. T. Scruggs",
                        "slug": "W.-T.-Scruggs",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Scruggs",
                            "middleNames": [
                                "Todd"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. T. Scruggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153056438"
                        ],
                        "name": "Jin Chang",
                        "slug": "Jin-Chang",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054926934"
                        ],
                        "name": "Kevin Hoffman",
                        "slug": "Kevin-Hoffman",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Hoffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39544740"
                        ],
                        "name": "Joe Marques",
                        "slug": "Joe-Marques",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Marques",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joe Marques"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221917"
                        ],
                        "name": "Jaesik Min",
                        "slug": "Jaesik-Min",
                        "structuredName": {
                            "firstName": "Jaesik",
                            "lastName": "Min",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaesik Min"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061326"
                        ],
                        "name": "W. Worek",
                        "slug": "W.-Worek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Worek",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Worek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The Face Recognition Grand Challenge Databases [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "Face Recognition Grand Challenge Databases >466 >50,000 images and 3D scans very large, lighting, expression, background, 3D, sequences [29]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 18
                            }
                        ],
                        "text": "In describing the Face Recognition Grand Challenge [29], the authors note that using sequestered test sets, i.e. test sets not publicly available to researchers, is the best way to ensure that algorithm developers do not unfairly fit the parametersof their algorithms to the test data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 283
                            }
                        ],
                        "text": "The Face Recognition Grand Challenge (FRGC) was not just a set of databases, but a carefully planned scientific program designed to promote rigorous scientific analysis of face recognition, fair comparison of face recognition tech nologies, and advances in face recognition research [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 4
                            }
                        ],
                        "text": "The Face Recognition Grand Challenge Databases[29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "In describing the Face Recognition Grand Challenge [29], th e authors note that using sequestered test sets, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 16
                            }
                        ],
                        "text": "Overview of the Face Recognition Grand Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 4
                            }
                        ],
                        "text": "The Face Recognition Grand Challenge (FRGC) was not just a set of databases, but a carefully planned scientific program designed to promote rigorous scientific analysis of face recognition, fair comparison of face recognition technologies, and advances in face recognition research [29]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 844981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "279538cc476114a415a96d5889e01f26a4a9a00a",
            "isKey": true,
            "numCitedBy": 2530,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery."
            },
            "slug": "Overview-of-the-face-recognition-grand-challenge-Phillips-Flynn",
            "title": {
                "fragments": [],
                "text": "Overview of the face recognition grand challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Viola-Jones face detector [35], but have been rescaled and cropped to a fixed size (see Section VI for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, every face image in our database is the output of the Viola-Jones face detection algorithm [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A version of the Viola-Jones face detector [35] was run on each image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": false,
            "numCitedBy": 11222,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2567871"
                        ],
                        "name": "E. Marszalec",
                        "slug": "E.-Marszalec",
                        "structuredName": {
                            "firstName": "Elzbieta",
                            "lastName": "Marszalec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Marszalec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2960140"
                        ],
                        "name": "B. Martinkauppi",
                        "slug": "B.-Martinkauppi",
                        "structuredName": {
                            "firstName": "Birgitta",
                            "lastName": "Martinkauppi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Martinkauppi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47250261"
                        ],
                        "name": "M. Soriano",
                        "slug": "M.-Soriano",
                        "structuredName": {
                            "firstName": "Maricor",
                            "lastName": "Soriano",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Soriano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "University of Oulu PhysicsBased Face Database 125 > 2000 highly varied illumination, eye glasses [20]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14966241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da8eb0d7666d481ba0d50a03067dbc1913131495",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Most face databases are meant for pattern recognition use. We report the creation of a unique face database which may be used for color-related studies of faces. We describe it as physics- based because it not only contains color images but also data on color image formation such as spectral reflectance measurements from facial skin, illuminant spectral power distribution, and camera spectral response. We demonstrate the usefulness of the database by an approach we developed to color correct face images taken under different illumination conditions. \u00a9 2000 SPIE and IS&T."
            },
            "slug": "Physics-based-face-database-for-color-research-Marszalec-Martinkauppi",
            "title": {
                "fragments": [],
                "text": "Physics-based face database for color research"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The creation of a unique face database which may be used for color-related studies of faces, described as physics- based because it not only contains color images but also data on color image formation such as spectral reflectance measurements from facial skin, illuminant spectral power distribution, and camera spectral response."
            },
            "venue": {
                "fragments": [],
                "text": "J. Electronic Imaging"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40638847"
                        ],
                        "name": "Hyeonjoon Moon",
                        "slug": "Hyeonjoon-Moon",
                        "structuredName": {
                            "firstName": "Hyeonjoon",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeonjoon Moon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958806"
                        ],
                        "name": "S. A. Rizvi",
                        "slug": "S.-A.-Rizvi",
                        "structuredName": {
                            "firstName": "Syed",
                            "lastName": "Rizvi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. A. Rizvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[15], [16]), there is a pre-specified gallery consisting of face images of a set of people, where the identity of each face image is known."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 497801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "791e530f6a4098bb39696d1476032821a7a1c569",
            "isKey": false,
            "numCitedBy": 2330,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1199 individuals are included in the FERET database, which is divided into development and sequestered portions. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to (1) assess the state of the art, (2) identify future areas of research, and (3) measure algorithm performance on large databases."
            },
            "slug": "The-FERET-evaluation-methodology-for-algorithms-Phillips-Moon",
            "title": {
                "fragments": [],
                "text": "The FERET evaluation methodology for face-recognition algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067071261"
                        ],
                        "name": "Terence Sim",
                        "slug": "Terence-Sim",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terence Sim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3240967"
                        ],
                        "name": "Maan Bsat",
                        "slug": "Maan-Bsat",
                        "structuredName": {
                            "firstName": "Maan",
                            "lastName": "Bsat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maan Bsat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sion [33]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16950643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ffa7a36e5414a0f2b16b1d8f93442ab15e2235d",
            "isKey": false,
            "numCitedBy": 1796,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database."
            },
            "slug": "The-CMU-Pose,-Illumination,-and-Expression-Database-Sim-Baker",
            "title": {
                "fragments": [],
                "text": "The CMU Pose, Illumination, and Expression Database"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "In the Fall of 2000, a database of more than 40,000 facial images of 68 people was collected using the Carnegie Mellon University 3D Room to imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113288329"
                        ],
                        "name": "A. Ferencz",
                        "slug": "A.-Ferencz",
                        "structuredName": {
                            "firstName": "Andras",
                            "lastName": "Ferencz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ferencz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "to see examples of this usage [7], [8], [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5681488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7831a61c857991ad73c8eda7695794af8a31eca",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of identifying specific instances of a class (cars) from a set of images all belonging to that class. Although we cannot build a model for any particular instance (as we may be provided with only one \"training\" example of it), we can use information extracted from observing other members of the class. We pose this task as a learning problem, in which the learner is given image pairs, labeled as matching or not, and must discover which image features are most consistent for matching instances and discriminative for mismatches. We explore a patch based representation, where we model the distributions of similarity measurements defined on the patches. Finally, we describe an algorithm that selects the most salient patches based on a mutual information criterion. This algorithm performs identification well for our challenging dataset of car images, after matching only a few, well chosen patches."
            },
            "slug": "Learning-Hyper-Features-for-Visual-Identification-Ferencz-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Learning Hyper-Features for Visual Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work addresses the problem of identifying specific instances of a class (cars) from a set of images all belonging to that class, and describes an algorithm that selects the most salient patches based on a mutual information criterion."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144228552"
                        ],
                        "name": "C. Sanderson",
                        "slug": "C.-Sanderson",
                        "structuredName": {
                            "firstName": "Conrad",
                            "lastName": "Sanderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sanderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 107420831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db4c924e8386a8fb544d81dedf0def3da47b33cb",
            "isKey": true,
            "numCitedBy": 108,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the last decade, interest in biometric based identification and verification systems has increased considerably. One application is the use of speech signals, face images or fingerprints in order to supplement security systems based on passwords. Biometric recognition can also be applied to other areas, such as passport control (immigration checkpoints), forensic work (to determine whether a biometric sample belongs to a suspect) and law enforcement applications (e.g. surveillance). While biometric systems based on face images and/or speech signals can be effective, their performance can degrade in the presence of challenging conditions. In face based systems this can be in the form of a change in the illumination direction and/or face pose variations. Multi-modal systems use more than one biometric at the same time. This is done for two main reasons -- to achieve better robustness and to increase discrimination power. This book can serve as a useful primer for face and speech processing, as well as information fusion. It reviews relevant backgrounds and reports research aimed at increasing the robustness of single- and multi-modal biometric identity verification systems."
            },
            "slug": "Biometric-Person-Recognition:-Face,-Speech-and-Sanderson",
            "title": {
                "fragments": [],
                "text": "Biometric Person Recognition: Face, Speech and Fusion"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This book reviews relevant backgrounds and reports research aimed at increasing the robustness of single- and multi-modal biometric identity verification systems and can serve as a useful primer for face and speech processing, as well as information fusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152353"
                        ],
                        "name": "O. Jesorsky",
                        "slug": "O.-Jesorsky",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Jesorsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Jesorsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2661823"
                        ],
                        "name": "K. Kirchberg",
                        "slug": "K.-Kirchberg",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Kirchberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kirchberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857160"
                        ],
                        "name": "Robert Frischholz",
                        "slug": "Robert-Frischholz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Frischholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Frischholz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2274476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4053e3423fb70ad9140ca89351df49675197196a",
            "isKey": false,
            "numCitedBy": 999,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The localization of human faces in digital images is a fundamental step in the process of face recognition. This paper presents a shape comparison approach to achieve fast, accurate face detection that is robust to changes in illumination and background. The proposed method is edge-based and works on grayscale still images. The Hausdorff distance is used as a similarity measure between a general face model and possible instances of the object within the image. The paper describes an efficient implementation, making this approach suitable for real-time applications. A two-step process that allows both coarse detection and exact localization of faces is presented. Experiments were performed on a large test set base and rated with a new validation measurement."
            },
            "slug": "Robust-Face-Detection-Using-the-Hausdorff-Distance-Jesorsky-Kirchberg",
            "title": {
                "fragments": [],
                "text": "Robust Face Detection Using the Hausdorff Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A two-step process that allows both coarse detection and exact localization of faces is presented and an efficient implementation is described, making this approach suitable for real-time applications."
            },
            "venue": {
                "fragments": [],
                "text": "AVBPA"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113288329"
                        ],
                        "name": "A. Ferencz",
                        "slug": "A.-Ferencz",
                        "structuredName": {
                            "firstName": "Andras",
                            "lastName": "Ferencz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ferencz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[5], and the unseen pair match problem (see for example [7])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There are numerous examples of this kind of face recognition research [7], [15], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "to see examples of this usage [7], [8], [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1381730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "191f87dfa04796ea7662a111f7c385bfe5d871a8",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Object identification (OID) is specialized recognition where the category is known (e.g. cars) and the algorithm recognizes an object's exact identity (e.g. Bob's BMW). Two special challenges characterize OID. (1) Interclass variation is often small (many cars look alike) and may be dwarfed by illumination or pose changes. (2) There may be many classes but few or just one positive \"training\" examples per class. Due to (1), a solution must locate possibly subtle object-specific salient features (a door handle) while avoiding distracting ones (a specular highlight). However, (2) rules out direct techniques of feature selection. We describe an online algorithm that takes one model image from a known category and builds an efficient \"same\" vs. \"different\" classification cascade by predicting the most discriminative feature set for that object. Our method not only estimates the saliency and scoring function for each candidate feature, but also models the dependency between features, building an ordered feature sequence unique to a specific model image, maximizing cumulative information content. Learned stopping thresholds make the classifier very efficient. To make this possible, category-specific characteristics are learned automatically in an off-line training procedure from labeled image pairs of the category, without prior knowledge about the category. Our method, using the same algorithm for both cars and faces, outperforms a wide variety of other methods."
            },
            "slug": "Building-a-classification-cascade-for-visual-from-Ferencz-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Building a classification cascade for visual identification from one example"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An online algorithm that takes one model image from a known category and builds an efficient \"same\" vs. \"different\" classification cascade by predicting the most discriminative feature set for that object, which outperforms a wide variety of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246870"
                        ],
                        "name": "Vidit Jain",
                        "slug": "Vidit-Jain",
                        "structuredName": {
                            "firstName": "Vidit",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vidit Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113288329"
                        ],
                        "name": "A. Ferencz",
                        "slug": "A.-Ferencz",
                        "structuredName": {
                            "firstName": "Andras",
                            "lastName": "Ferencz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ferencz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There are numerous examples of this kind of face recognition research [7], [15], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Shortly after the publication of the original paper describing Faces in the Wild, a variety of authors started to inquire about using such a distribution of face images in their work [14], [15], [25], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "to see examples of this usage [7], [8], [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 292710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d31e258f6af40f457c27ce118986ea157673c9c4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Object identification is the task of identifying specific obj ects belonging to the same class such as cars. We often need to recognize an object that we have only seen a few times. In fact, we often observe only one example of a particular object before we need to recognize it again. Thus we are interested in building a system which can learn to extract distinctive markers from a single example and which can then be used to identify the object in another image as \u201csame\u201d or \u201cdifferent\u201d. Previous work by Ferencz et al. introduced the notion of hyper-features, which are properties of an image patch that can be used to estimate the utility of the patch in subsequent matching tasks. In this work, we show that hyper-feature based models can be more efficiently estimate d using discriminative training techniques. In particular, we describe a n ew hyper-feature model based upon logistic regression that shows improved performance over previously published techniques. Our approach significant ly outperforms Bayesian face recognition that is considered as a standard benchmark for face recognition."
            },
            "slug": "Discriminative-Training-of-Hyper-feature-Models-for-Jain-Ferencz",
            "title": {
                "fragments": [],
                "text": "Discriminative Training of Hyper-feature Models for Object Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work describes a hyper-feature model based upon logistic regression that shows improved performance over previously published techniques and significant ly outperforms Bayesian face recognition that is considered as a standard benchmark for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737918"
                        ],
                        "name": "J. Cohn",
                        "slug": "J.-Cohn",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Cohn",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2689877"
                        ],
                        "name": "A. Zlochower",
                        "slug": "A.-Zlochower",
                        "structuredName": {
                            "firstName": "Adena",
                            "lastName": "Zlochower",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zlochower"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3461535"
                        ],
                        "name": "J. Lien",
                        "slug": "J.-Lien",
                        "structuredName": {
                            "firstName": "Jenn-Jier",
                            "lastName": "Lien",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "Cohn-Kanade AU-Coded Facial Expression Database 100 500 sequences dynamic sequences of facial expressions [6]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12915390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e22cb2c15359915d9ffeae9cf64dd372dcc79aa",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "The face is a rich source of information about human behavior. Available methods for coding facial displays, however, are human-observer dependent, labor intensive, and difficult to standardize. To enable rigorous and efficient quantitative measurement of facial displays, we have developed an automated method of facial display analysis. In this report, we compare the results with this automated system with those of manual FACS (Facial Action Coding System, Ekman & Friesen, 1978a) coding. One hundred university students were videotaped while performing a series of facial displays. The image sequences were coded from videotape by certified FACS coders. Fifteen action units and action unit combinations that occurred a minimum of 25 times were selected for automated analysis. Facial features were automatically tracked in digitized image sequences using a hierarchical algorithm for estimating optical flow. The measurements were normalized for variation in position, orientation, and scale. The image sequences were randomly divided into a training set and a cross-validation set, and discriminant function analyses were conducted on the feature point measurements. In the training set, average agreement with manual FACS coding was 92% or higher for action units in the brow, eye, and mouth regions. In the cross-validation set, average agreement was 91%, 88%, and 81% for action units in the brow, eye, and mouth regions, respectively. Automated face analysis by feature point tracking demonstrated high concurrent validity with manual FACS coding."
            },
            "slug": "Automated-face-analysis-by-feature-point-tracking-Cohn-Zlochower",
            "title": {
                "fragments": [],
                "text": "Automated face analysis by feature point tracking has high concurrent validity with manual FACS coding."
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "An automated method of facial display analysis by feature point tracking demonstrated high concurrent validity with manual FACS coding."
            },
            "venue": {
                "fragments": [],
                "text": "Psychophysiology"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Superpixels have recently started replacing pixels as the basic building block for an image in several object recognition and segmentation models [19], [20], [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5276705d71e3dac961ab5d06b86a7b806cc9af64",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans have an amazing ability to instantly grasp the overall 3D structure of a scene\u2014ground orientation, relative positions of major landmarks, etc.\u2014even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \u201csurface layout\u201d of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout."
            },
            "slug": "Recovering-Surface-Layout-from-an-Image-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Recovering Surface Layout from an Image"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper takes the first step towards constructing the surface layout, a labeling of the image intogeometric classes, to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143920486"
                        ],
                        "name": "F. Samaria",
                        "slug": "F.-Samaria",
                        "structuredName": {
                            "firstName": "Ferdinando",
                            "lastName": "Samaria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Samaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144024405"
                        ],
                        "name": "A. Harter",
                        "slug": "A.-Harter",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Harter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Harter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2153469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "294adb9b8dca839832836bee0f06e15be550aaaa",
            "isKey": true,
            "numCitedBy": 2543,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work on face identification using continuous density Hidden Markov Models (HMMs) has shown that stochastic modelling can be used successfully to encode feature information. When frontal images of faces are sampled using top-bottom scanning, there is a natural order in which the features appear and this can be conveniently modelled using a top-bottom HMM. However, a top-bottom HMM is characterised by different parameters, the choice of which has so far been based on subjective intuition. This paper presents a set of experimental results in which various HMM parameterisations are analysed.<<ETX>>"
            },
            "slug": "Parameterisation-of-a-stochastic-model-for-human-Samaria-Harter",
            "title": {
                "fragments": [],
                "text": "Parameterisation of a stochastic model for human face identification"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper presents a set of experimental results in which various HMM parameterisations are analysed and shows that stochastic modelling can be used successfully to encode feature information."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE Workshop on Applications of Computer Vision"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064850418"
                        ],
                        "name": "E. Nowak",
                        "slug": "E.-Nowak",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nowak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nowak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "There are numerous examples of this kind of face recognition research [7], [15], [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "Shortly after the publication of the original paper describ ing Faces in the Wild, a variety of authors started to inquire about using such a distribution of face images in their work [14], [15], [25], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 216034155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05b2b57e388b218949628fd52bda33d36762446d",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose and evaluate an algorithm that learns a similarity measure for comparing never seen objects. The measure is learned from pairs of training images labeled \"same\" or \"different\". This is far less informative than the commonly used individual image labels (e.g., \"car model X\"), but it is cheaper to obtain. The proposed algorithm learns the characteristic differences between local descriptors sampled from pairs of \"same\" and \"different\" images. These differences are vector quantized by an ensemble of extremely randomized binary trees, and the similarity measure is computed from the quantized differences. The extremely randomized trees are fast to learn, robust due to the redundant information they carry and they have been proved to be very good clusterers. Furthermore, the trees efficiently combine different feature types (SIFT and geometry). We evaluate our innovative similarity measure on four very different datasets and consistently outperform the state-of-the-art competitive approaches."
            },
            "slug": "Learning-Visual-Similarity-Measures-for-Comparing-Nowak-Jurie",
            "title": {
                "fragments": [],
                "text": "Learning Visual Similarity Measures for Comparing Never Seen Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An algorithm that learns a similarity measure for comparing never seen objects that is fast to learn, robust due to the redundant information they carry and they have been proved to be very good clusterers is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145426908"
                        ],
                        "name": "A. Angelova",
                        "slug": "A.-Angelova",
                        "structuredName": {
                            "firstName": "Anelia",
                            "lastName": "Angelova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Angelova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 136
                            }
                        ],
                        "text": "Caltech Faces 27 450 lighting, expression, background Caltech 10000 Web Faces \u2248 10000 10000 wide variability, facial features annotated [1]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 56
                            }
                        ],
                        "text": "3\nWhile some other databases (such as the Caltech 10000 Web Faces [1]) also present highly diverse image sets, these databases are not designed for face recognition, but ratherfor face detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "While some other databases (such as the Caltech 10000 Web Faces [1]) also present highly diverse image sets, these databases are not designed for face recognition, but rather for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 14
                            }
                        ],
                        "text": "Caltech 10000 Web Faces[1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 18
                            }
                        ],
                        "text": "The Caltech 10000 Web Faces database is interesting in that it also provides a very broad distribution of faces."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14019086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d97d5ca9e104805c98178f878581221553c0d065",
            "isKey": true,
            "numCitedBy": 138,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Training datasets for learning of object categories are often contaminated or imperfect. We explore an approach to automatically identify examples that are noisy or troublesome for learning and exclude them from the training set. The problem is relevant to learning in semi-supervised or unsupervised setting, as well as to learning when the training data is contaminated with wrongly labeled examples or when correctly labeled, but hard to learn examples, are present. We propose a fully automatic mechanism for noise cleaning, called 'data pruning' and demonstrate its success on learning of human faces. It is not assumed that the data or the noise can be modeled or that additional training examples are available. Our experiments show that data pruning can improve on generalization performance for algorithms with various robustness to noise. It outperforms methods with regularization properties and is superior to commonly applied aggregation methods, such as bagging."
            },
            "slug": "Pruning-training-sets-for-learning-of-object-Angelova-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Pruning training sets for learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a fully automatic mechanism for noise cleaning, called 'data pruning', and demonstrates its success on learning of human faces and shows that data pruning can improve on generalization performance for algorithms with various robustness to noise."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753932"
                        ],
                        "name": "K. Kryszczuk",
                        "slug": "K.-Kryszczuk",
                        "structuredName": {
                            "firstName": "Krzysztof",
                            "lastName": "Kryszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kryszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078574451"
                        ],
                        "name": "Andrzej Drygaj",
                        "slug": "Andrzej-Drygaj",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Drygaj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrzej Drygaj"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6275877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aee9b778ff6432723018e5067a96e640c76d37d",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a method of automatic color correction of face images and its application in a face detection algorithm. The color correction method is based on the phenomenon of color constancy observed in human visual perception. This technique is further applied in a face detection system, which draws upon the analogy to the parallel organization of visual neural pathways, the magnoand parvocellular channels. Presented method proved to be efficient in diverse background and illumination conditions, including face images with background chromatically close to human skin and where prominent facial features are obscured by adverse illumination conditions."
            },
            "slug": "Color-Correction-for-Face-Detection-Based-on-Human-Kryszczuk-Drygaj",
            "title": {
                "fragments": [],
                "text": "Color Correction for Face Detection Based on Human Visual Perception Metaphor"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This method of automatic color correction of face images and its application in a face detection algorithm proved to be efficient in diverse background and illumination conditions, including face images with background chromatically close to human skin and where prominent facial features are obscured by adverse illumination conditions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50160727"
                        ],
                        "name": "Michael J. Lyons",
                        "slug": "Michael-J.-Lyons",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyons",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Lyons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50473499"
                        ],
                        "name": "M. Kamachi",
                        "slug": "M.-Kamachi",
                        "structuredName": {
                            "firstName": "Miyuki",
                            "lastName": "Kamachi",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kamachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8365437"
                        ],
                        "name": "J. Gyoba",
                        "slug": "J.-Gyoba",
                        "structuredName": {
                            "firstName": "Jiro",
                            "lastName": "Gyoba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gyoba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Japanese Female Facial Expression (JAFFE) Database 10 213 rated for emotional content, female, Japanese [20]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1586662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee48190c8c5429c1633100777d226c4855cc6224",
            "isKey": false,
            "numCitedBy": 1964,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for extracting information about facial expressions from images is presented. Facial expression images are coded using a multi-orientation multi-resolution set of Gabor filters which are topographically ordered and aligned approximately with the face. The similarity space derived from this representation is compared with one derived from semantic ratings of the images by human observers. The results show that it is possible to construct a facial expression classifier with Gabor coding of the facial images as the input stage. The Gabor representation shows a significant degree of psychological plausibility, a design feature which may be important for human-computer interfaces."
            },
            "slug": "Coding-facial-expressions-with-Gabor-wavelets-Lyons-Akamatsu",
            "title": {
                "fragments": [],
                "text": "Coding facial expressions with Gabor wavelets"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The results show that it is possible to construct a facial expression classifier with Gabor coding of the facial images as the input stage and the Gabor representation shows a significant degree of psychological plausibility, a design feature which may be important for human-computer interfaces."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore, we provide superpixel representations for all the images in the database based on Mori\u2019s online implementation [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Superpixels have recently started replacing pixels as the basic building block for an image in several object recognition and segmentation models [19], [20], [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2983119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32510e7f88bc0767fbbc811397ba068dbc4cf549",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we show how segmentation as preprocessing paradigm can be used to improve the efficiency and accuracy of model search in an image. We operationalize this idea using an over-segmentation of an image into superpixels. The problem domain we explore is human body pose estimation from still images. The superpixels prove useful in two ways. First, we restrict the joint positions in our human body model to lie at centers of superpixels, which reduces the size of the model search space. In addition, accurate support masks for computing features on half-limbs of the body model are obtained by using agglomerations of superpixels as half limb segments. We present results on a challenging dataset of people in sports news images"
            },
            "slug": "Guiding-model-search-using-segmentation-Mori",
            "title": {
                "fragments": [],
                "text": "Guiding model search using segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown how segmentation as preprocessing paradigm can be used to improve the efficiency and accuracy of model search in an image by using an over-segmentation of an image into superpixels."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Superpixels have recently started replacing pixels as the basic building block for an image in several object recognition and segmentation models [19], [20], [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13571735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9049a50dfe94fa4473880a9b60c99333ade685",
            "isKey": false,
            "numCitedBy": 1644,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images."
            },
            "slug": "Learning-a-classification-model-for-segmentation-Ren-Malik",
            "title": {
                "fragments": [],
                "text": "Learning a classification model for segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A two-class classification model for grouping is proposed that defines a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation, and trains a linear classifier to combine these features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49583346"
                        ],
                        "name": "H. Arora",
                        "slug": "H.-Arora",
                        "structuredName": {
                            "firstName": "Himanshu",
                            "lastName": "Arora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Arora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983856"
                        ],
                        "name": "Nicolas Loeff",
                        "slug": "Nicolas-Loeff",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Loeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Loeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Superpixels have recently started replacing pixels as the basic building block for an image in several object recognition and segmentation models [19], [20], [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Superpixel representations have already been successfully applied to face segmentation [22] and we believe they can also be useful for detection and recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1926500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "632042b27eb63cc3758ec6c91f5ed76fabccefef",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an unsupervised method to segment objects detected in images using a novel variant of an interest point template, which is very efficient to train and evaluate. Once an object has been detected, our method segments an image using a conditional random field (CRF) model. This model integrates image gradients, the location and scale of the object, the presence of object parts, and the tendency of these parts to have characteristic patterns of edges nearby. We enhance our method using multiple unsegmented images of objects to learn the parameters of the CRF, in an iterative conditional maximization framework. We show quantitative results on images of real scenes that demonstrate the accuracy of segmentation."
            },
            "slug": "Unsupervised-Segmentation-of-Objects-using-Learning-Arora-Loeff",
            "title": {
                "fragments": [],
                "text": "Unsupervised Segmentation of Objects using Efficient Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "An unsupervised method to segment objects detected in images using a novel variant of an interest point template, which is very efficient to train and evaluate and shows quantitative results on images of real scenes that demonstrate the accuracy of segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143905691"
                        ],
                        "name": "J. Beveridge",
                        "slug": "J.-Beveridge",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Beveridge",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Beveridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055443465"
                        ],
                        "name": "J. Griffith",
                        "slug": "J.-Griffith",
                        "structuredName": {
                            "firstName": "Joey",
                            "lastName": "Griffith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Griffith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38612825"
                        ],
                        "name": "R. Kohler",
                        "slug": "R.-Kohler",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Kohler",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kohler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733922"
                        ],
                        "name": "A. Hanson",
                        "slug": "A.-Hanson",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "segmentations has existed in the vision community dating back to at least 1989 [23] 9 http://www."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3072226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8575ee56e2cfe5968e0b9c78b2f892ef099d5534",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "A working system for segmenting images of complex scenes is presented. The system integrates techniques that have evolved out of many years of research in low-level image segmentation at the University of Massachusetts and elsewhere. This paper documents the result of this historical evolution. Segmentations produced by the system are used extensively in related image interpretation research.The system first produces segmentations based upon an analysis of spatially localized feature histograms. These initial segmentations are then simplified using a region merging algorithm. Parameter selection for the local histogram segmentation algorithm is facilitated by mapping the multidimensional parameter space to a one-dimensional parameter which regulates region fragmentation. An extension of this algorithm to multiple features is also presented. Experience with roughly 100 images from different domains has shown the system to be robust and effective. Samples of these results are included."
            },
            "slug": "Segmenting-images-using-localized-histograms-and-Beveridge-Griffith",
            "title": {
                "fragments": [],
                "text": "Segmenting images using localized histograms and region merging"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A working system for segmenting images of complex scenes is presented that integrates techniques that have evolved out of many years of research in low-level image segmentation at the University of Massachusetts and elsewhere."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We also experimented with the Felzenszwalb and Huttenlocher [24] algorithm but found that Mori\u2019s method, while more computationally expensive, did a much better job at preserving the face-background boundary, a crucial property for superpixel-based segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207663697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeeffe327e6c93e9010c7b1e401caa9113723851",
            "isKey": false,
            "numCitedBy": 3750,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions."
            },
            "slug": "Efficient-Graph-Based-Image-Segmentation-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Efficient Graph-Based Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An efficient segmentation algorithm is developed based on a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image and it is shown that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107833293"
                        ],
                        "name": "M. U. R. S\u00e1nchez",
                        "slug": "M.-U.-R.-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "S\u00e1nchez",
                            "middleNames": [
                                "Ulises",
                                "Ramos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. U. R. S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15510253,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "0b1fbac88dc540d7bd4cc89c080adf5b17915654",
            "isKey": true,
            "numCitedBy": 43,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for lip tracking intended to support personal verification is presented in this paper. Lip contours are represented by means of quadratic B-splines. The lips are automatically localised in the original image and an elliptic B-spline is generated to start up tracking. Lip localisation exploits grey-level gradient projections as well as chromaticity models to find the lips in an automatically segmented region corresponding to the face area. Tracking proceeds by estimating new lip contour positions according to a statistical chromaticity model for the lips. The current tracker implementation follows a deterministic second order model for the spline motion based on a Lagrangian formulation of contour dynamics. The method has been tested on the M2VTS database[1]. Lips were accurately tracked on sequences consisting of more than hundred frames. localisation"
            },
            "slug": "Statistical-Chromaticity-Models-for-Lip-Tracking-S\u00e1nchez-Matas",
            "title": {
                "fragments": [],
                "text": "Statistical Chromaticity Models for Lip Tracking with B-splines"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A method for lip tracking intended to support personal verification that exploits grey-level gradient projections as well as chromaticity models to find the lips in an automatically segmented region corresponding to the face area."
            },
            "venue": {
                "fragments": [],
                "text": "AVBPA"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060785901"
                        ],
                        "name": "F. Quimby",
                        "slug": "F.-Quimby",
                        "structuredName": {
                            "firstName": "Freddy",
                            "lastName": "Quimby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Quimby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "The resulting data set, which achieved a labelling accuracy of 7 7% [3], was informally referred to as the \u201cFaces in the Wild\u201d dat a set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "The impetus for the Labeled Faces in the Wild database grew out of work at Berkeley by Tamara Berg, David Forsyth, and the computer vision group at UC Berkeley [3], [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 108
                            }
                        ],
                        "text": "As a starting point, we used the raw images from the Faces in the Wild database collected by Tamara Berg at Berkeley."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6082135,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "2788d0f1f7fbd44ebb5185e59c4aaf09aad97013",
            "isKey": true,
            "numCitedBy": 104,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\u2022 A picture in Radiance is a map of RGB radiance (or irradiance) values \u2022 The exposure of a Radiance picture may be adjusted without loss since it contains a dynamic range on the order of 10^77 \u2022 Individual radiance (or luminance) values may be displayed on demand by the X11 viewer, ximage \u2022 The falsecolor program may be used to convert an image to a numerically readable value map with legend \u2022 The glare program may be used to identify and analyze glare sources in a picture or scene \u2022 Other programs (principally rtrace) may be used to compute values that are not easily represented as a map"
            },
            "slug": "What's-in-a-picture-Quimby",
            "title": {
                "fragments": [],
                "text": "What's in a picture?"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The exposure of a Radiance picture may be adjusted without loss since it contains a dynamic range on the order of 10^77 and individual radiance values may be displayed on demand by the X11 viewer, ximage."
            },
            "venue": {
                "fragments": [],
                "text": "Laboratory animal science"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49168578"
                        ],
                        "name": "J. Phillips",
                        "slug": "J.-Phillips",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Phillips",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144975752"
                        ],
                        "name": "V. Bruce",
                        "slug": "V.-Bruce",
                        "structuredName": {
                            "firstName": "Vicki",
                            "lastName": "Bruce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66890955"
                        ],
                        "name": "F. F. Souli\u00e9",
                        "slug": "F.-F.-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Souli\u00e9",
                            "middleNames": [
                                "Fogelman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. F. Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60461382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfec6ad5ab86f9a7bf96184c2c58950a4d5876a8",
            "isKey": false,
            "numCitedBy": 553,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-Recognition:-From-Theory-to-Applications-Phillips-Bruce",
            "title": {
                "fragments": [],
                "text": "Face Recognition: From Theory to Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2173900"
                        ],
                        "name": "K. Messer",
                        "slug": "K.-Messer",
                        "structuredName": {
                            "firstName": "Kieron",
                            "lastName": "Messer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Messer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678373"
                        ],
                        "name": "J. Luettin",
                        "slug": "J.-Luettin",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Luettin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Luettin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2488626"
                        ],
                        "name": "G. Ma\u00eetre",
                        "slug": "G.-Ma\u00eetre",
                        "structuredName": {
                            "firstName": "Gilbert",
                            "lastName": "Ma\u00eetre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ma\u00eetre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "of Surrey, UK 295 1180 videos rotating head, speaking subjects, 3D models, high quality images [23]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15312675,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "b62628ac06bbac998a3ab825324a41a11bc3a988",
            "isKey": false,
            "numCitedBy": 1456,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: vision Reference EPFL-CONF-82502 URL: ftp://ftp.idiap.ch/pub/papers/vision/avbpa99.pdf Record created on 2006-03-10, modified on 2017-05-10"
            },
            "slug": "XM2VTSDB:-The-Extended-M2VTS-Database-Messer-Matas",
            "title": {
                "fragments": [],
                "text": "XM2VTSDB: The Extended M2VTS Database"
            },
            "tldr": {
                "abstractSimilarityScore": 31,
                "text": "This poster presents a poster presenting a probabilistic procedure for estimating the response of the immune system to laser-spot assisted surgery to treat central giant cell granuloma."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2792277"
                        ],
                        "name": "M. Kleiner",
                        "slug": "M.-Kleiner",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Kleiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kleiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793750"
                        ],
                        "name": "C. Wallraven",
                        "slug": "C.-Wallraven",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Wallraven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wallraven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "Face Video Database of the Max Planck Institute for Biological Cybernetics ? 246 video sequences 6 simulataneous viewpoints, carefully synchronized, video data [18]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15270985,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "295bfeecfe960730d82500bc8dca33ce6c076226",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The MPI VideoLab is a custom built, flexible digital videoand audio recording studio that enables high quality, time synchronized recordings of human actions from multiple viewpoints. This technical report describes the requirements to the system in the context of our applications, its hardwareand software equipment and the special features of the recording setup. Important aspects of the hardware and software implementation are discussed in detail."
            },
            "slug": "The-MPI-VideoLab:-A-system-for-high-quality-of-and-Kleiner-Wallraven",
            "title": {
                "fragments": [],
                "text": "The MPI VideoLab: A system for high quality synchronous recording of video and audio from multiple viewpoints"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The MPI VideoLab is a custom built, flexible digital videoand audio recording studio that enables high quality, time synchronized recordings of human actions from multiple viewpoints and its hardware and software implementation are discussed in detail."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "(See caveats below.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "3 See [9], Section IIIA for more detail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "These methods and details on how the training sets can be generated can be found in Section IV of [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "See Section III for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "5 See [9] Section VI for specific details of each step."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "2 See [9] for more detailed comparisons and a more complete list of existing face databases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 07-49, University of Massachusetts, Amherst"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nist mugshot identification database"
            },
            "venue": {
                "fragments": [],
                "text": "Nist mugshot identification database"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Psychological image collection at stirling"
            },
            "venue": {
                "fragments": [],
                "text": "Psychological image collection at stirling"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Georgia Tech Face Database"
            },
            "venue": {
                "fragments": [],
                "text": "The Georgia Tech Face Database"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Institute of Automation. Nlpr face database"
            },
            "venue": {
                "fragments": [],
                "text": "Institute of Automation. Nlpr face database"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Who's in the picture. NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Who's in the picture. NIPS"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The images were then saved in the JPEG 2.0 format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Color FERET Database"
            },
            "venue": {
                "fragments": [],
                "text": "The Color FERET Database"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117070857"
                        ],
                        "name": "A. Mart\u00ednez",
                        "slug": "A.-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Mart\u00ednez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mart\u00ednez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "eye glasses, scarves [22]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 221632808,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "cd520bc4b5301bc51b8b6bf1226c3f2f88e8e444",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-AR-face-databasae-Mart\u00ednez",
            "title": {
                "fragments": [],
                "text": "The AR face databasae"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "78933667"
                        ],
                        "name": "\u5c1a\u5f18 \u5cf6\u5f71",
                        "slug": "\u5c1a\u5f18-\u5cf6\u5f71",
                        "structuredName": {
                            "firstName": "\u5c1a\u5f18",
                            "lastName": "\u5cf6\u5f71",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5c1a\u5f18 \u5cf6\u5f71"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 92579383,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "6f7248a3650e02373401f3464a3e246dc041c5f4",
            "isKey": false,
            "numCitedBy": 3209,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "National-Institute-of-Standards-and-\u5c1a\u5f18",
            "title": {
                "fragments": [],
                "text": "National Institute of Standards and Technology\u306b\u304a\u3051\u308b\u8d85\u4f1d\u5c0e\u7814\u7a76\u53ca\u3073\u751f\u6d3b"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384255355"
                        ],
                        "name": "Aleix M. Martinez",
                        "slug": "Aleix-M.-Martinez",
                        "structuredName": {
                            "firstName": "Aleix M.",
                            "lastName": "Martinez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleix M. Martinez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "AR Face Database, Purdue University, USA 126 4000 frontal pose, expression, illumination, occlusions, eye glasses, scarves [21]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57227467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d96f946aaabc734af7fe3fc4454cf8547fcd5ed",
            "isKey": false,
            "numCitedBy": 3767,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-AR-face-database-Martinez",
            "title": {
                "fragments": [],
                "text": "The AR face database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153514754"
                        ],
                        "name": "B. Weyrauch",
                        "slug": "B.-Weyrauch",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Weyrauch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Weyrauch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684626"
                        ],
                        "name": "B. Heisele",
                        "slug": "B.-Heisele",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Heisele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Heisele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110065403"
                        ],
                        "name": "Jennifer Huang",
                        "slug": "Jennifer-Huang",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880906"
                        ],
                        "name": "V. Blanz",
                        "slug": "V.-Blanz",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Blanz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Blanz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "MIT-CBCL Face Recognition Database 10 > 2000 synthetic images from 3D models, illumination, pose, background [37]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29782137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67d2d5a4266d406abbba08c7bf9ab310172031a2",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for pose and illumination invariant face recognition that combines two recent advances in the computer vision field: 3D morphable models and component-based recognition. A 3D morphable model is used to compute 3D face models from three input images of each subject in the training database. The 3D models are rendered under varying pose and illumination conditions to build a large set of synthetic images. These images are then used for training a component-based face recognition system. The face recognition module is preceded by a fast hierarchical face detector resulting in a system that can detect and identify faces in video images at about 4 Hz. The system achieved a recognition rate of 88% on a database of 2000 real images of ten people, which is significantly better than a comparable global face recognition system. The results clearly show the potential of the combination of morphable models and component-based recognition towards pose and illumination invariant face recognition."
            },
            "slug": "Component-Based-Face-Recognition-with-3D-Morphable-Weyrauch-Heisele",
            "title": {
                "fragments": [],
                "text": "Component-Based Face Recognition with 3D Morphable Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A 3D morphable model is used to compute 3D face models from three input images of each subject in the training database and the system achieved a recognition rate significantly better than a comparable global face recognition system."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR Workshops"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[15], [16]), there is a pre-specified gallery consisting of face images of a set of people, where the identity of each face image is known."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38520234,
            "fieldsOfStudy": [],
            "id": "d675629b0f3beac3c4ef92ea05ae6bfdf83af34f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FERET Evaluation Methodology for Face-Recognition Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Forsyth . Who \u2019 s in the picture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "MIT-CBCL Face Recognition Database 10 > 2000 synthetic images from 3D models, illumination, pose, background [37]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Componen t-based face recognition with 3D morphable models. In First IEEE Workshop on face processing in video"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Indian Face Database 40 > 440 frontal, Indian subjects [16]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Indian Face Database"
            },
            "venue": {
                "fragments": [],
                "text": "http://vis-www.cs.umass.edu/ vidit/IndianFaceDatabase/index.html,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Jaesik Min, and William Worek. Overview of the Face Recognition Grand Challenge. In CVPR"
            },
            "venue": {
                "fragments": [],
                "text": "Jaesik Min, and William Worek. Overview of the Face Recognition Grand Challenge. In CVPR"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "VALID Database 106 530 highly variable office conditions [9]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The realistic multimodal VALID database and visual speaker identification comparison experiments"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 5th International Conference on Audio- and Video-Based Biometric Person Authentication,"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Viola-Jones face detector [35], but have been rescaled and cropped to a fixed size (see Section VI for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "In particular, every face image in our database is the output of the Viola-Jones face detection algorithm [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "A version of the Viola-Jones face detector [35] was run on each image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust real-time face dete  ction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The CMU pose, illumination, a  d expression database.  IEEE Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "Labeled databases for recognition, such as the Face Recognition Grand Challenge [4], BioID [5], FERET [6], and CMU PIE [7], are typically taken under very controlled conditions, with less people and diversity than LFW."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "The Color FERET Database. http://www.itl.nist.gov/iad/humanid/colorferet/home.html, 2003."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Color FERET Database"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.itl.nist.gov/iad/humanid/colorferet/home.html"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Yale Face Database 15 165 expressions, eye glasses, lighting [2]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eigenfa c s vs. fisherfaces: Recognition using class specific linear projec  ti ns"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Pattern Analysis and Machine Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Yale Face Database B 10 5760 pose, illumination [11]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fro m few to many: Illumination cone models for face recognition under va  riable lighting and pose.IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions on Pattern Analysis and Machine Intelligence,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Indian Face Database 40 > 440 frontal, Indian subjects [16]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Indian Face Datab se. http://vis-www.cs.umass.edu/ vidit/IndianFaceDatabase  /ind x.html"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "VALID Database 106 530 highly variable office conditions [9]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The realistic multimodal VALID database and visual speaker identification compar ison experiments"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the 5th International Conference on Audio- and Video-Based Biometric Person Authentication"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "University of Essex, UK 395 7900 racial diversity, eye glasses, beards, college age [34]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "University of Essex collection of facial  images"
            },
            "venue": {
                "fragments": [],
                "text": "http://cswww.essex.ac.uk/mv/allfaces/index.html,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kryszczuk and Drygajlo . Color correction for faced tection based on human visual perception metaphor"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Workshop on Multi - Modal User Authentication"
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 36,
            "methodology": 19
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 64,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Labeled-Faces-in-the-Wild:-A-Database-forStudying-Huang-Mattar/c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3?sort=total-citations"
}