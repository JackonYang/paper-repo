{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 196
                            }
                        ],
                        "text": "Some papers treat both labeled and unlabeled observat ions in the construct ion of a classif ication scheme (Tolat and Peterson, 1990; Pao and Sobajic, 1992; Kester, 1985; Greenspan et al., 1991; McLachlan, 1977; McLachlan and Ganesal ingam, 1982; O'Neil l , 1978; Shahshahani and Landgrebe, 1992 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122353862,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science"
            ],
            "id": "c2c4aa2580e53ae163fa69d43c5ed9c21956cc08",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Estimation of the linear discriminant function L is considered in the case where there are n 1 and n 2 observations from the populations II1 and II2 and M unclassified observations. Estimates of L using all n 1 + n 2 + M observations are proposed and evaluated in terms of the expected error rate under the assumption that M is small relative to n 1 and n 2. By appropriately weighting the sample means of the unclassified observations, an estimate of L is given which dominates the usual estimate based on just the n 1 + n 2 classified observations."
            },
            "slug": "Estimating-the-Linear-Discriminant-Function-from-a-McLachlan",
            "title": {
                "fragments": [],
                "text": "Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209630"
                        ],
                        "name": "Behzad M. Shahshahani",
                        "slug": "Behzad-M.-Shahshahani",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahshahani",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Behzad M. Shahshahani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 265
                            }
                        ],
                        "text": "Some papers treat both labeled and unlabeled observat ions in the construct ion of a classif ication scheme (Tolat and Peterson, 1990; Pao and Sobajic, 1992; Kester, 1985; Greenspan et al., 1991; McLachlan, 1977; McLachlan and Ganesal ingam, 1982; O'Neil l , 1978; Shahshahani and Landgrebe, 1992 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121122974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1349b5a746d02023bd8704165e54ce2255b889df",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The effect of additional unlabeled samples in improving the supervised learning process is studied in this paper. Three learning processes, supervised, unsupervised, and combined supervised-unsupervised, are compared by studying the asymptotic behavior of the estimates obtained under each process. Upper and lower bounds on the asymptotic covariance matrices are derived. It is shown that under a normal mixture density assumption for the probability density function of the feature space, the combined supervised-unsupervised learning is always superior to the supervised learning in achieving better estimates. Experimental results are provided to verify the theoretical concepts."
            },
            "slug": "Asymptotic-improvement-of-supervised-learning-by-Shahshahani-Landgrebe",
            "title": {
                "fragments": [],
                "text": "Asymptotic improvement of supervised learning by utilizing additional unlabeled samples: normal mixture density case"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that under a normal mixture density assumption for the probability density function of the feature space, the combined supervised-unsupervised learning is always superior to the supervised learning in achieving better estimates."
            },
            "venue": {
                "fragments": [],
                "text": "Optics & Photonics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145414142"
                        ],
                        "name": "B. Chandrasekaran",
                        "slug": "B.-Chandrasekaran",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Chandrasekaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chandrasekaran"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116860574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55ab64d91344cdde7d4959a181c7652245c19597",
            "isKey": false,
            "numCitedBy": 575,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "39-Dimensionality-and-sample-size-considerations-in-Jain-Chandrasekaran",
            "title": {
                "fragments": [],
                "text": "39 Dimensionality and sample size considerations in pattern recognition practice"
            },
            "venue": {
                "fragments": [],
                "text": "Classification, Pattern Recognition and Reduction of Dimensionality"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30883040"
                        ],
                        "name": "S. Ganesalingam",
                        "slug": "S.-Ganesalingam",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Ganesalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ganesalingam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 101
                            }
                        ],
                        "text": "A comprehensive treatment of mixture distributions, as well as a large bibliography, can be found in McLachlan and Basford (1988). The mixture fx,(\" ) --\" qf~ ( ' ) + rlfz (\") can be estimated from the unlabeled samples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "A comprehensive treatment of mixture distributions, as well as a large bibliography, can be found in McLachlan and Basford (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115980920,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "63f4c77d8900b1d5595091a6821c3f005e90cfb5",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of updating a discriminant function on the basis of data of unknown origin is studied. There are observations of known origin from each of the underlying populations, and subsequently there is available a limited number of unclassified observations assumed to have been drawn from a mixture of the underlying populations. A sample discriminant function can be formed initially from the classified data. The question of whether the subsequent updating of this discriminant function on the basis of the unclassified data produces a reduction in the error rate of sufficient magnitude to warrant the computational effort is considered by carrying out a series of Monte Carlo experiments. The simulation results are contrasted with available asymptotic results."
            },
            "slug": "Updating-a-discriminant-function-in-basis-of-data-McLachlan-Ganesalingam",
            "title": {
                "fragments": [],
                "text": "Updating a discriminant function in basis of unclassified data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6686370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e85a68602abf92fcc1efb8b7aa90d27d141a80c2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "A test sequence is used to select the best rule from a class of discrimination rules defined in terms of the training sequence. The Vapnik-Chervonenkis and related inequalities are used to obtain distribution-free bounds on the difference between the probability of error of the selected rule and the probability of error of the best rule in the given class. The bounds are used to prove the consistency and asymptotic optimality for several popular classes, including linear discriminators, nearest-neighbor rules, kernel-based rules, histogram rules, binary tree classifiers, and Fourier series classifiers. In particular, the method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules. >"
            },
            "slug": "Automatic-Pattern-Recognition:-A-Study-of-the-of-Devroye",
            "title": {
                "fragments": [],
                "text": "Automatic Pattern Recognition: A Study of the Probability of Error"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Vapnik-Chervonenkis method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405428555"
                        ],
                        "name": "Terence J. O'Neill",
                        "slug": "Terence-J.-O'Neill",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "O'Neill",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terence J. O'Neill"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123184012,
            "fieldsOfStudy": [
                "Environmental Science",
                "Mathematics"
            ],
            "id": "4b82bce6a021aea889a1e4f734277ffa73b1595b",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Fisher's linear discriminant rule may be estimated by maximum likelihood estimation using unclassified observations. It is shown that the ratio of the relevant information contained in unclassified observations to that in classified observations varies from approximately one-fifth to two-thirds for the statistically interesting range of separation of the populations. Thus, more information may be obtained from large numbers of inexpensive unclassified observations than from a small classified sample. Also, all available unclassified and classified data should be used for estimating Fisher's linear discriminant rule."
            },
            "slug": "Normal-Discrimination-with-Unclassified-O'Neill",
            "title": {
                "fragments": [],
                "text": "Normal Discrimination with Unclassified Observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 41
                            }
                        ],
                        "text": "By a standard large deviations argument (Cover and Thomas, 1991 ) it is easily seen that\n- Jim / log Pr{A (I)} =A*(O) (8)\n(9) = - infA(O \u00a3~ o\n(OA(x) . . . . ] = - i n f l o ~ f [ ~A ( x ) ~ \u00a2 - \u00a2 \u00a240 L a\n--- lo~2fxblf l (x)Of2(x)dx ] (11)\nwhere A (() is the log-moment generating function of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42795,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144763324"
                        ],
                        "name": "Y. Pao",
                        "slug": "Y.-Pao",
                        "structuredName": {
                            "firstName": "Yoh-Han",
                            "lastName": "Pao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Pao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414393"
                        ],
                        "name": "D. Sobajic",
                        "slug": "D.-Sobajic",
                        "structuredName": {
                            "firstName": "Dejan",
                            "lastName": "Sobajic",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sobajic"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110353032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4caff7fdf821594f4ffe98fda02c62df92d07291",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "It is highly desirable that the security and stability of electric power systems after exposure to large disturbances be assessable. In this connection, the critical clearing time (CCT) is an attribute which provides significant information about the quality of the post-fault system behavior. It may be regarded as a complex mapping of the prefault, fault-on, and post-fault system conditions in the time domain. Y.-H. Pao and D.J. Solajic (1989) showed that a feedforward neural network can be used to learn this mapping and successfully perform under variable system operating conditions and topologies. In that work the system was described in terms of some conventionally used parameters. In contrast to using those pragmatic features selected on the basis of the engineering understanding of the problem, the possibility of using unsupervised and supervised learning paradigms to discover what combination of raw measurements are significant in determining CCT is considered. Correlation analysis and Euclidean metric are used to specify interfeature dependencies. An example of a 4-machine power system is used to illustrate the suggested approach.<<ETX>>"
            },
            "slug": "Combined-use-of-unsupervised-and-supervised-for-Pao-Sobajic",
            "title": {
                "fragments": [],
                "text": "Combined use of unsupervised and supervised learning for dynamic security assessment"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The possibility of using unsupervised and supervised learning paradigms to discover what combination of raw measurements are significant in determining CCT is considered and an example of a 4-machine power system is used to illustrate the suggested approach."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3129654"
                        ],
                        "name": "S. Raudys",
                        "slug": "S.-Raudys",
                        "structuredName": {
                            "firstName": "Sarunas",
                            "lastName": "Raudys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raudys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 243
                            }
                        ],
                        "text": "\u2026unsupervised learning is usually associated with clustering, the main stream of work in pat tern recognit ion focuses on supervised learning, the construct ion of classification rules based on labeled examples (Jain and Chandrasekaran, 1992; Raudys and Jain, 1991; Vapnik, 1982; Devroye, 1988 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37044463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cf994cc7cec3c76f49f98aa6ded0824187e786d",
            "isKey": false,
            "numCitedBy": 1263,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The effects of sample size on feature selection and error estimation for several types of classifiers are discussed. The focus is on the two-class problem. Classifier design in the context of small design sample size is explored. The estimation of error rates under small test sample size is given. Sample size effects in feature selection are discussed. Recommendations for the choice of learning and test sample sizes are given. In addition to surveying prior work in this area, an emphasis is placed on giving practical advice to designers and users of statistical pattern recognition systems. >"
            },
            "slug": "Small-Sample-Size-Effects-in-Statistical-Pattern-Raudys-Jain",
            "title": {
                "fragments": [],
                "text": "Small Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The effects of sample size on feature selection and error estimation for several types of classifiers are discussed and an emphasis is placed on giving practical advice to designers and users of statistical pattern recognition systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652670"
                        ],
                        "name": "V. V. Tolat",
                        "slug": "V.-V.-Tolat",
                        "structuredName": {
                            "firstName": "Viral",
                            "lastName": "Tolat",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. V. Tolat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847347"
                        ],
                        "name": "A. Peterson",
                        "slug": "A.-Peterson",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Peterson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Peterson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 109
                            }
                        ],
                        "text": "Some papers treat both labeled and unlabeled observat ions in the construct ion of a classif ication scheme (Tolat and Peterson, 1990; Pao and Sobajic, 1992; Kester, 1985; Greenspan et al., 1991; McLachlan, 1977; McLachlan and Ganesal ingam, 1982; O'Neil l , 1978; Shahshahani and Landgrebe, 1992 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61712907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "49ba93ab5a46d3b4e5defc69ad92b0ab736e6a4b",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of interpolating unknown mappings from known mappings is addressed. This problem arises when a large number of mappings must be learned and it is impractical to train the network on all possible mappings. Described is a network model that can learn nonlinear mappings with a minimal amount of supervised training. A combination of supervised and supervised learning is used to train the network. It is shown that the network is able to interpolate mappings on which it has not been previously trained.<<ETX>>"
            },
            "slug": "Nonlinear-mapping-with-minimal-supervised-learning-Tolat-Peterson",
            "title": {
                "fragments": [],
                "text": "Nonlinear mapping with minimal supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that the network is able to interpolate mappings on which it has not been previously trained, and a combination of supervised and supervised learning is used to train the network."
            },
            "venue": {
                "fragments": [],
                "text": "Twenty-Third Annual Hawaii International Conference on System Sciences"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143942875"
                        ],
                        "name": "H. Greenspan",
                        "slug": "H.-Greenspan",
                        "structuredName": {
                            "firstName": "Hayit",
                            "lastName": "Greenspan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Greenspan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145135018"
                        ],
                        "name": "R. Goodman",
                        "slug": "R.-Goodman",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 172
                            }
                        ],
                        "text": "Some papers treat both labeled and unlabeled observat ions in the construct ion of a classif ication scheme (Tolat and Peterson, 1990; Pao and Sobajic, 1992; Kester, 1985; Greenspan et al., 1991; McLachlan, 1977; McLachlan and Ganesal ingam, 1982; O'Neil l , 1978; Shahshahani and Landgrebe, 1992 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62417386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "244776352718776c8259b7653411da23f1e766c1",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A framework for texture analysis based on combined unsupervised and supervised learning is proposed. The textured input is represented in the frequency-orientation space via a Gabor-wavelet pyramidal decomposition. In the unsupervised learning phase a neural network vector quantization scheme is used for the quantization of the feature-vector attributes and a projection onto a reduced dimension clustered map for initial segmentation. A supervised stage follows, in which labeling of the textured map is achieved using a rule-based system. A set of informative features are extracted in the supervised stage as congruency rules between attributes using an information-theoretic measure. This learned set can now act as a classification set for test images. This approach is suggested as a general framework for pattern classification. Simulation results for the texture classification are given.<<ETX>>"
            },
            "slug": "Texture-analysis-via-unsupervised-and-supervised-Greenspan-Goodman",
            "title": {
                "fragments": [],
                "text": "Texture analysis via unsupervised and supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A framework for texture analysis based on combined unsupervised and supervised learning and suggested as a general framework for pattern classification for texture classification."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49232442"
                        ],
                        "name": "H. Teicher",
                        "slug": "H.-Teicher",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Teicher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Teicher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "The theory of mixtures of distributions has been extensively developed by Teicher in a series of papers (Teicher, 1960, 1963)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123616083,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "937797973e4a66534ef38eee8ca09902d9edaddf",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Mixture-of-Distributions-Teicher",
            "title": {
                "fragments": [],
                "text": "On the Mixture of Distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49232442"
                        ],
                        "name": "H. Teicher",
                        "slug": "H.-Teicher",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Teicher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Teicher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "The theory of mixtures of distributions has been extensively developed by Teicher in a series of papers (Teicher, 1960, 1963)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123131579,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ea30725f3a91dd89612e54cf2d0317adbc450ee9",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Identifiability-of-Finite-Mixtures-Teicher",
            "title": {
                "fragments": [],
                "text": "Identifiability of Finite Mixtures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121643472"
                        ],
                        "name": "A. Kester",
                        "slug": "A.-Kester",
                        "structuredName": {
                            "firstName": "Adri",
                            "lastName": "Kester",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 158
                            }
                        ],
                        "text": "Some papers treat both labeled and unlabeled observat ions in the construct ion of a classif ication scheme (Tolat and Peterson, 1990; Pao and Sobajic, 1992; Kester, 1985; Greenspan et al., 1991; McLachlan, 1977; McLachlan and Ganesal ingam, 1982; O'Neil l , 1978; Shahshahani and Landgrebe, 1992 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121887022,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2fd532c4cf057beca4be68d32f141d8d87616c6e",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-large-deviation-results-in-statistics-Kester",
            "title": {
                "fragments": [],
                "text": "Some large deviation results in statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126132505,
            "fieldsOfStudy": [],
            "id": "fa3611acf748674c9e74727cf2fbf45b55648d1f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Testing Statistical Hypotheses"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/On-the-exponential-value-of-labeled-samples-Castelli-Cover/f67e9a6bd7c688f1c9c653584a4fa1f9c7fda2a6?sort=total-citations"
}