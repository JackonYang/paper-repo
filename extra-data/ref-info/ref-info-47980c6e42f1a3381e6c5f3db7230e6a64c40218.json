{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898850"
                        ],
                        "name": "R\u00e9mi Ronfard",
                        "slug": "R\u00e9mi-Ronfard",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Ronfard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Ronfard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 168
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 167
                            }
                        ],
                        "text": "\u2026of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture, or combinations of several or all of these [Martin et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9478443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac7f973658b55563f4d56e5b763c9049dd1034e0",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting people in images is a key problem for video indexing, browsing and retrieval. The main difficulties are the large appearance variations caused by action, clothing, illumination, viewpoint and scale. Our goal is to find people in static video frames using learned models of both the appearance of body parts (head, limbs, hands), and of the geometry of their assemblies. We build on Forsyth & Fleck's general 'body plan' methodology and Felzenszwalb & Huttenlocher's dynamic programming approach for efficiently assembling candidate parts into 'pictorial structures'. However we replace the rather simple part detectors used in these works with dedicated detectors learned for each body part using SupportVector Machines (SVMs) or RelevanceVector Machines (RVMs). We are not aware of any previous work using SVMs to learn articulated body plans, however they have been used to detect both whole pedestrians and combinations of rigidly positioned subimages (typically, upper body, arms, and legs) in street scenes, under a wide range of illumination, pose and clothing variations. RVMs are SVM-like classifiers that offer a well-founded probabilistic interpretation and improved sparsity for reduced computation. We demonstrate their benefits experimentally in a series of results showing great promise for learning detectors in more general situations."
            },
            "slug": "Learning-to-Parse-Pictures-of-People-Ronfard-Schmid",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Pictures of People"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work builds on Forsyth & Fleck's general 'body plan' methodology and Felzenszwalb & Huttenlocher's dynamic programming approach for efficiently assembling candidate parts into 'pictorial structures' but replaces the rather simple part detectors used in these works with dedicated detectors learned for each body part using SupportVector Machines (SVMs) or Relevance Vector Machines (RVMs)."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797063"
                        ],
                        "name": "S. Agarwal",
                        "slug": "S.-Agarwal",
                        "structuredName": {
                            "firstName": "Shivani",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 157
                            }
                        ],
                        "text": "\u2026salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al. 2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 262977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d081b80b1850df9b1e382f97a7a244890d6485e",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest. Images are then represented using parts from this vocabulary, along with spatial relations observed among them. Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class. The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration. We report experiments on images of side views of cars. Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation.In addition, we discuss and offer solutions to several methodological issues that are significant for the research community to be able to evaluate object detection approaches."
            },
            "slug": "Learning-a-Sparse-Representation-for-Object-Agarwal-Roth",
            "title": {
                "fragments": [],
                "text": "Learning a Sparse Representation for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects, that achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188179"
                        ],
                        "name": "A. Opelt",
                        "slug": "A.-Opelt",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Opelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Opelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718587"
                        ],
                        "name": "A. Pinz",
                        "slug": "A.-Pinz",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Pinz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 62
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9118611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca9da67e2c427e59a5a54c9b31157e6b8b4843e",
            "isKey": false,
            "numCitedBy": 401,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this work is the detection of object classes, such as airplanes or horses. Instead of using a model based on salient image fragments, we show that object class detection is also possible using only the object's boundary. To this end, we develop a novel learning technique to extract class-discriminative boundary fragments. In addition to their shape, these \u201ccodebook\u201d entries also determine the object's centroid (in the manner of Leibe et al. [19]). Boosting is used to select discriminative combinations of boundary fragments (weak detectors) to form a strong \u201cBoundary-Fragment-Model\u201d (BFM) detector. The generative aspect of the model is used to determine an approximate segmentation. \n \nWe demonstrate the following results: (i) the BFM detector is able to represent and detect object classes principally defined by their shape, rather than their appearance; and (ii) in comparison with other published results on several object classes (airplanes, cars-rear, cows) the BFM detector is able to exceed previous performances, and to achieve this with less supervision (such as the number of training images)."
            },
            "slug": "A-Boundary-Fragment-Model-for-Object-Detection-Opelt-Pinz",
            "title": {
                "fragments": [],
                "text": "A Boundary-Fragment-Model for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The BFM detector is able to represent and detect object classes principally defined by their shape, rather than their appearance, and to achieve this with less supervision (such as the number of training images)."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145328018"
                        ],
                        "name": "M. Oren",
                        "slug": "M.-Oren",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Oren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6476085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76f560991d56ad689ec32f9e9d13291e0193f4cf",
            "isKey": false,
            "numCitedBy": 1604,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general."
            },
            "slug": "A-general-framework-for-object-detection-Papageorgiou-Oren",
            "title": {
                "fragments": [],
                "text": "A general framework for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A general trainable framework for object detection in static images of cluttered scenes based on a wavelet representation of an object class derived from a statistical analysis of the class instances and a motion-based extension to enhance the performance of the detection algorithm over video sequences is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8729004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44f3ac3277c2eb6e5599739eb875888c46e21d4c",
            "isKey": false,
            "numCitedBy": 1776,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8% miss rate on our Test Set 1."
            },
            "slug": "Human-Detection-Using-Oriented-Histograms-of-Flow-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Human Detection Using Oriented Histograms of Flow and Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A detector for standing and moving people in videos with possibly moving cameras and backgrounds is developed, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124866301"
                        ],
                        "name": "A. Mohan",
                        "slug": "A.-Mohan",
                        "structuredName": {
                            "firstName": "Anuj",
                            "lastName": "Mohan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mohan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets. Papageorgiou and Poggio [2000] use absolute values of Haar wavelet coefficients at different orientations and scales as their local descriptors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2595,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets. Papageorgiou and Poggio [2000] use absolute values of Haar wavelet coefficients at different orientations and scales as their local descriptors. Images are mapped from pixel space to an over-complete dictionary of Haar wavelets that is rich enough to describe patterns. Horizontal, vertical, and diagonal wavelets are used. To obtain an over-complete basis, the wavelets are computed with overlapping supports. Haar wavelets can be calculated efficiently, while still remaining rich enough to encode visually significant patterns, and the use of over-completeness provides a reasonable degree of translation invariance. The descriptor vectors are used in a kernelised Support Vector Machine (SVM) framework (see Sect. 2.2.1), so the final decision criterion is a sum of weighted kernel distances from selected training examples. However we find that linear SVMs (weighted sums of rectified wavelet outputs) give similar results and are much faster to calculate. Papageorgiou and Poggio [2000] show results for pedestrian, face, and car detection. Mohan et al. [2001] add simple part based descriptors to this approach. Four part detectors are built for pedestrians: head, left arm, right arm and leg detectors. Each detector is a part classifier built on Papageorgiou and Poggio\u2019s Haar wavelets. The responses of the part detectors are checked for a proper geometric configuration, and the final classification is performed by using a SVM on their outputs. A highly optimised version of this method is presented in de Poortere et al. [2002]. Rather than using a single complex classifier, Viola and Jones [2001] and Viola et al. [2003] build a more efficient progressive rejection based classification chain that uses a generalisation of Haar wavelets \u2014 differences of rectangular regions arranged in several Haar and bar-like arrangements \u2014 as features. The classifiers become progressively more complex with depth in the chain. Each stage is designed to reject as many of the remaining negative cases as possible, while still retaining all but a negligible fraction of the positives. To train each stage, features with all possible rectangle dimensions are tested and the sample reweighting procedure AdaBoost [Schapire 2002] is used as a greedy feature selection method. This approach was used to build a real time face detector in Viola and Jones [2001], and extended to pedestrian detection from video in Viola et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1873,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets. Papageorgiou and Poggio [2000] use absolute values of Haar wavelet coefficients at different orientations and scales as their local descriptors. Images are mapped from pixel space to an over-complete dictionary of Haar wavelets that is rich enough to describe patterns. Horizontal, vertical, and diagonal wavelets are used. To obtain an over-complete basis, the wavelets are computed with overlapping supports. Haar wavelets can be calculated efficiently, while still remaining rich enough to encode visually significant patterns, and the use of over-completeness provides a reasonable degree of translation invariance. The descriptor vectors are used in a kernelised Support Vector Machine (SVM) framework (see Sect. 2.2.1), so the final decision criterion is a sum of weighted kernel distances from selected training examples. However we find that linear SVMs (weighted sums of rectified wavelet outputs) give similar results and are much faster to calculate. Papageorgiou and Poggio [2000] show results for pedestrian, face, and car detection. Mohan et al. [2001] add simple part based descriptors to this approach. Four part detectors are built for pedestrians: head, left arm, right arm and leg detectors. Each detector is a part classifier built on Papageorgiou and Poggio\u2019s Haar wavelets. The responses of the part detectors are checked for a proper geometric configuration, and the final classification is performed by using a SVM on their outputs. A highly optimised version of this method is presented in de Poortere et al. [2002]. Rather than using a single complex classifier, Viola and Jones [2001] and Viola et al. [2003] build a more efficient progressive rejection based classification chain that uses a generalisation of Haar wavelets \u2014 differences of rectangular regions arranged in several Haar and bar-like arrangements \u2014 as features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1304,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets. Papageorgiou and Poggio [2000] use absolute values of Haar wavelet coefficients at different orientations and scales as their local descriptors. Images are mapped from pixel space to an over-complete dictionary of Haar wavelets that is rich enough to describe patterns. Horizontal, vertical, and diagonal wavelets are used. To obtain an over-complete basis, the wavelets are computed with overlapping supports. Haar wavelets can be calculated efficiently, while still remaining rich enough to encode visually significant patterns, and the use of over-completeness provides a reasonable degree of translation invariance. The descriptor vectors are used in a kernelised Support Vector Machine (SVM) framework (see Sect. 2.2.1), so the final decision criterion is a sum of weighted kernel distances from selected training examples. However we find that linear SVMs (weighted sums of rectified wavelet outputs) give similar results and are much faster to calculate. Papageorgiou and Poggio [2000] show results for pedestrian, face, and car detection. Mohan et al. [2001] add simple part based descriptors to this approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1230,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets. Papageorgiou and Poggio [2000] use absolute values of Haar wavelet coefficients at different orientations and scales as their local descriptors. Images are mapped from pixel space to an over-complete dictionary of Haar wavelets that is rich enough to describe patterns. Horizontal, vertical, and diagonal wavelets are used. To obtain an over-complete basis, the wavelets are computed with overlapping supports. Haar wavelets can be calculated efficiently, while still remaining rich enough to encode visually significant patterns, and the use of over-completeness provides a reasonable degree of translation invariance. The descriptor vectors are used in a kernelised Support Vector Machine (SVM) framework (see Sect. 2.2.1), so the final decision criterion is a sum of weighted kernel distances from selected training examples. However we find that linear SVMs (weighted sums of rectified wavelet outputs) give similar results and are much faster to calculate. Papageorgiou and Poggio [2000] show results for pedestrian, face, and car detection."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2667,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets. Papageorgiou and Poggio [2000] use absolute values of Haar wavelet coefficients at different orientations and scales as their local descriptors. Images are mapped from pixel space to an over-complete dictionary of Haar wavelets that is rich enough to describe patterns. Horizontal, vertical, and diagonal wavelets are used. To obtain an over-complete basis, the wavelets are computed with overlapping supports. Haar wavelets can be calculated efficiently, while still remaining rich enough to encode visually significant patterns, and the use of over-completeness provides a reasonable degree of translation invariance. The descriptor vectors are used in a kernelised Support Vector Machine (SVM) framework (see Sect. 2.2.1), so the final decision criterion is a sum of weighted kernel distances from selected training examples. However we find that linear SVMs (weighted sums of rectified wavelet outputs) give similar results and are much faster to calculate. Papageorgiou and Poggio [2000] show results for pedestrian, face, and car detection. Mohan et al. [2001] add simple part based descriptors to this approach. Four part detectors are built for pedestrians: head, left arm, right arm and leg detectors. Each detector is a part classifier built on Papageorgiou and Poggio\u2019s Haar wavelets. The responses of the part detectors are checked for a proper geometric configuration, and the final classification is performed by using a SVM on their outputs. A highly optimised version of this method is presented in de Poortere et al. [2002]. Rather than using a single complex classifier, Viola and Jones [2001] and Viola et al. [2003] build a more efficient progressive rejection based classification chain that uses a generalisation of Haar wavelets \u2014 differences of rectangular regions arranged in several Haar and bar-like arrangements \u2014 as features. The classifiers become progressively more complex with depth in the chain. Each stage is designed to reject as many of the remaining negative cases as possible, while still retaining all but a negligible fraction of the positives. To train each stage, features with all possible rectangle dimensions are tested and the sample reweighting procedure AdaBoost [Schapire 2002] is used as a greedy feature selection method. This approach was used to build a real time face detector in Viola and Jones [2001], and extended to pedestrian detection from video in Viola et al. [2003]. For improved accuracy, the pedestrian method includes temporal information (motion descriptors based on difference of rectangular region sums between the current frame and the next) in the descriptor pool."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1778,
                                "start": 94
                            }
                        ],
                        "text": "Some well known approach to object detection are described in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001]. These approaches use dense encoding of image regions based on operators similar to Haar wavelets. Papageorgiou and Poggio [2000] use absolute values of Haar wavelet coefficients at different orientations and scales as their local descriptors. Images are mapped from pixel space to an over-complete dictionary of Haar wavelets that is rich enough to describe patterns. Horizontal, vertical, and diagonal wavelets are used. To obtain an over-complete basis, the wavelets are computed with overlapping supports. Haar wavelets can be calculated efficiently, while still remaining rich enough to encode visually significant patterns, and the use of over-completeness provides a reasonable degree of translation invariance. The descriptor vectors are used in a kernelised Support Vector Machine (SVM) framework (see Sect. 2.2.1), so the final decision criterion is a sum of weighted kernel distances from selected training examples. However we find that linear SVMs (weighted sums of rectified wavelet outputs) give similar results and are much faster to calculate. Papageorgiou and Poggio [2000] show results for pedestrian, face, and car detection. Mohan et al. [2001] add simple part based descriptors to this approach. Four part detectors are built for pedestrians: head, left arm, right arm and leg detectors. Each detector is a part classifier built on Papageorgiou and Poggio\u2019s Haar wavelets. The responses of the part detectors are checked for a proper geometric configuration, and the final classification is performed by using a SVM on their outputs. A highly optimised version of this method is presented in de Poortere et al. [2002]. Rather than using a single complex classifier, Viola and Jones [2001] and Viola et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2559322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "448bd4e124175ad358078a7b930ecad994c97812",
            "isKey": true,
            "numCitedBy": 1137,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head, legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a \"person\" or a \"nonperson.\" We call this type of hierarchical architecture, in which learning occurs at multiple stages, an adaptive combination of classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded views of people and people whose body parts have little contrast with the background."
            },
            "slug": "Example-Based-Object-Detection-in-Images-by-Mohan-Papageorgiou",
            "title": {
                "fragments": [],
                "text": "Example-Based Object Detection in Images by Components"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results suggest that the improvement in performance is due to the component-based approach and the ACC data classification architecture, which is capable of locating partially occluded views of people and people whose body parts have little contrast with the background."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2798041"
                        ],
                        "name": "I. Haritaoglu",
                        "slug": "I.-Haritaoglu",
                        "structuredName": {
                            "firstName": "Ismail",
                            "lastName": "Haritaoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Haritaoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298122"
                        ],
                        "name": "D. Harwood",
                        "slug": "D.-Harwood",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harwood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6837802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "518597d91ed49c28f5cf3f0a0b05609568b7e084",
            "isKey": false,
            "numCitedBy": 2784,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "W/sup 4/ is a real time visual surveillance system for detecting and tracking multiple people and monitoring their activities in an outdoor environment. It operates on monocular gray-scale video imagery, or on video imagery from an infrared camera. W/sup 4/ employs a combination of shape analysis and tracking to locate people and their parts (head, hands, feet, torso) and to create models of people's appearance so that they can be tracked through interactions such as occlusions. It can determine whether a foreground region contains multiple people and can segment the region into its constituent people and track them. W/sup 4/ can also determine whether people are carrying objects, and can segment objects from their silhouettes, and construct appearance models for them so they can be identified in subsequent frames. W/sup 4/ can recognize events between people and objects, such as depositing an object, exchanging bags, or removing an object. It runs at 25 Hz for 320/spl times/240 resolution images on a 400 MHz dual-Pentium II PC."
            },
            "slug": "W4:-Real-Time-Surveillance-of-People-and-Their-Haritaoglu-Harwood",
            "title": {
                "fragments": [],
                "text": "W4: Real-Time Surveillance of People and Their Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "W/sup 4/ employs a combination of shape analysis and tracking to locate people and their parts and to create models of people's appearance so that they can be tracked through interactions such as occlusions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 157
                            }
                        ],
                        "text": "The proposed algorithm is general and fast, and our experiments show that it gives good performance for other detectors such as the face detection system of Viola and Jones [2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 97
                            }
                        ],
                        "text": "2006], the run time is still significantly higher than a similar framework using integral arrays Viola and Jones [2001] to compute wavelet like feature vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2715202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63",
            "isKey": false,
            "numCitedBy": 17887,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection."
            },
            "slug": "Rapid-object-detection-using-a-boosted-cascade-of-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Rapid object detection using a boosted cascade of simple features"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates and the introduction of a new image representation called the \"integral image\" which allows the features used by the detector to be computed very quickly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144626092"
                        ],
                        "name": "D. Snow",
                        "slug": "D.-Snow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Snow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Snow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "One way to approach motion encoding is to stabilise the backgrounds of the video sequences followed by a conventional motion-based detector such as Viola et al. [2003]. However, it is common to find sequences with non-rigid backgrounds or substantial camera translation and thus parallax in deep 3-D scenes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "To compare our motion descriptors with other existing ones, we evaluated a scheme inspired by Viola et al. [2003] based on simple spatiotemporal differencing rather than flow."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 47726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b1226910fd8deb67c0b64b37c120e0f0aa967e",
            "isKey": false,
            "numCitedBy": 1526,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on motion information or detectors based on appearance information, but ours is the first to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20 \u00d7 15 pixels), and has a very low false positive rate.Our approach builds on the detection work of Viola and Jones. Novel contributions of this paper include: (i) development of a representation of image motion which is extremely efficient, and (ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow)."
            },
            "slug": "Detecting-Pedestrians-Using-Patterns-of-Motion-and-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Detecting Pedestrians Using Patterns of Motion and Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This pedestrian detection system is the first to combine both sources of information in a single detector, and operates on low resolution images under difficult conditions (such as rain and snow)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 159
                            }
                        ],
                        "text": "limb classifiers built over 1st and 2nd order Gaussian filters in a dynamic programming framework similar to those of Felzenszwalb and Huttenlocher [2000] and Ioffe and Forsyth [2001b]1. Mikolajczyk et al. [2004] design features specifically tuned for human detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 159
                            }
                        ],
                        "text": "limb classifiers built over 1st and 2nd order Gaussian filters in a dynamic programming framework similar to those of Felzenszwalb and Huttenlocher [2000] and Ioffe and Forsyth [2001b]1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 809,
                                "start": 116
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]. For example, Forsyth and Fleck [1997], Ioffe and Forsyth [1999, 2001a,b] and Ramanan and Forsyth [2003] use explicit human body segments (forearm, upper arm, upper leg, lower leg, torso, etc.) which are assumed to be well represented by cylinders. Parallel edge detectors are then used to detect the corresponding image segments, and body-geometry based detectors are built by using articulation constraints or graphical models to constrain the relative geometry of the limbs. 3D limb detectors have also been used, c.f . Sigal et al. [2003]. One problem with these approaches is that the assumption that limbs can be represented by parallel lines is rather simplistic and its scalability to real world examples is questionable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 116
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]. For example, Forsyth and Fleck [1997], Ioffe and Forsyth [1999, 2001a,b] and Ramanan and Forsyth [2003] use explicit human body segments (forearm, upper arm, upper leg, lower leg, torso, etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 230
                            }
                        ],
                        "text": "[2002], 15 SVM and Relevance Vector Machine (RVM) based limb detectors are created based on first and second order image gradients, but the final classifier is based on dynamic programming over assemblies of limb detections as in Ioffe and Forsyth [2001b,a]. Dork\u00f3 and Schmid [2003] use SVM based classifiers over interest points as intermediate part detectors for general object recognition, and test two types of final classifiers: (a) likelihood ratios for detecting parts P(part=1|ob ject=1) P(part=1|ob ject=0) , and (b) mutual information between detected parts and object classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18650202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15d53db3001fb150ab2e17e02730379b429b95ed",
            "isKey": true,
            "numCitedBy": 40,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient detection of objects in images is complicated by variations of object appearance due to intra-class object differences, articulation, lighting, occlusions, and aspect variations. To reduce the search required for detection, we employ the bottom-up approach where we find candidate image features and associate some of them with parts of the object model. We represent objects as collections of local features, and would like to allow any of them to be absent, with only a small subset sufficient for detection;furthermore, our model should allow efficient correspondence search. We propose a model, Mixture of Trees, that achieves these goals. With a mixture of trees, we can model the individual appearances of the features, relationships among them, and the aspect, and handle occlusions. Independences captured in the model make efficient inference possible. In our earlier work, we have shown that mixtures of trees can be used to model objects with a natural tree structure, in the context of human tracking. Now we show that a natural tree structure is not required, and use a mixture of trees for both frontal and view-invariant face detection. We also show that by modeling faces as collections of features we can establish an intrinsic coordinate frame for a face, and estimate the out-of-plane rotation of a face."
            },
            "slug": "Mixtures-of-trees-for-object-recognition-Ioffe-Forsyth",
            "title": {
                "fragments": [],
                "text": "Mixtures of trees for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that a natural tree structure is not required, and a mixture of trees is used for both frontal and view-invariant face detection, and by modeling faces as collections of features the authors can establish an intrinsic coordinate frame for a face, and estimate the out-of-plane rotation of a face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 11
                            }
                        ],
                        "text": "[2002] and Mikolajczyk et al. [2004]. Ronfard et al. [2002] build an articulated body detector by incorporating SVM based te l-0 03 90 30 3, v er si on 1 1 Ju n 20 09"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Mikolajczyk et al. [2004] also use likelihood ratios P(descriptor|ob ject=1) P(descriptor|ob ject=0) as weak classifiers, but relax the independence assumption by using likelihoods over pairs of descriptors as te l-0 03 90 30 3, v er si on 1 1 Ju n 20 09"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 188
                            }
                        ],
                        "text": "\u2026of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture, or combinations of several or all of these [Martin et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3870070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9cc6155cd69023a736a7b8f8680bcd6232c840e",
            "isKey": false,
            "numCitedBy": 764,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion. Humans are modeled as flexible assemblies of parts, and robust part detection is the key to the approach. The parts are represented by co-occurrences of local features which captures the spatial layout of the partrsquos appearance. Feature selection and the part detectors are learnt from training images using AdaBoost. The detection algorithm is very efficient as (i) all part detectors use the same initial features, (ii) a coarse-to-fine cascade approach is used for part detection, (iii) a part assembly strategy reduces the number of spurious detections and the search space. The results outperform existing human detectors."
            },
            "slug": "Human-Detection-Based-on-a-Probabilistic-Assembly-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "Human Detection Based on a Probabilistic Assembly of Robust Part Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 234
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14133530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36cd88ed2c17a596001e9c7d89533ac46c28dec0",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively.Each classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated class-conditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation\u2014labeling the input as non-object\u2014or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation."
            },
            "slug": "Object-Detection-Using-the-Statistics-of-Parts-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "Object Detection Using the Statistics of Parts"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": false,
            "numCitedBy": 11229,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 273
                            }
                        ],
                        "text": "The static (appearance based) detectors shown are: R-HOG \u2013 our default R-HOG trained on combined data set; R-HOG (static) \u2013 R-HOG trained on the Static Test Set alone, essentially same as in Chapter 4; and Wavelet \u2013 our version of the static Haar wavelet based detector of Papageorgiou and Poggio [2000] described in Sect."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13308232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system."
            },
            "slug": "A-Trainable-System-for-Object-Detection-Papageorgiou-Poggio",
            "title": {
                "fragments": [],
                "text": "A Trainable System for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A general, trainable system for object detection in unconstrained, cluttered scenes that derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2721983"
                        ],
                        "name": "F. Fleuret",
                        "slug": "F.-Fleuret",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Fleuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fleuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6754141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b535f4edc4cbf8d4fb6182ec6b5c54db3c1cccb",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We study visual selection: Detect and roughly localize all instances of a generic object class, such as a face, in a greyscale scene, measuring performance in terms of computation and false alarms. Our approach is sequential testing which is coarse-to-fine in both in the exploration of poses and the representation of objects. All the tests are binary and indicate the presence or absence of loose spatial arrangements of oriented edge fragments. Starting from training examples, we recursively find larger and larger arrangements which are \u201cdecomposable,\u201d which implies the probability of an arrangement appearing on an object decays slowly with its size. Detection means finding a sufficient number of arrangements of each size along a decreasing sequence of pose cells. At the beginning, the tests are simple and universal, accommodating many poses simultaneously, but the false alarm rate is relatively high. Eventually, the tests are more discriminating, but also more complex and dedicated to specific poses. As a result, the spatial distribution of processing is highly skewed and detection is rapid, but at the expense of (isolated) false alarms which, presumably, could be eliminated with localized, more intensive, processing."
            },
            "slug": "Coarse-to-Fine-Face-Detection-Fleuret-Geman",
            "title": {
                "fragments": [],
                "text": "Coarse-to-Fine Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach is sequential testing which is coarse-to-fine in both in the exploration of poses and the representation of objects, and the spatial distribution of processing is highly skewed and detection is rapid, but at the expense of (isolated) false alarms which could be eliminated with localized, more intensive, processing."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29266,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14097182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31635ba161c6cea677f87a88d9874e5506819207",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding people in pictures presents a particularly difficult object recognition problem. We show how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to inspect every group, due to the huge combinatorial complexity.We propose two approaches to this problem. In one, the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classifier, and demonstrate that our approach can be used to determine whether images of real scenes contain people.The second approach employs a probabilistic framework, so that we can draw samples of assemblies, with probabilities proportional to their likelihood, which allows to draw human-like assemblies more often than the non-person ones. The main performance problem is in segmentation of images, but the overall results of both approaches on real images of people are encouraging."
            },
            "slug": "Probabilistic-Methods-for-Finding-People-Ioffe-Forsyth",
            "title": {
                "fragments": [],
                "text": "Probabilistic Methods for Finding People"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work shows how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties, using an efficient projection algorithm for one popular classifier."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 40
                            }
                        ],
                        "text": "[1998], Papageorgiou and Poggio [2000], Mohan et al. [2001] proposed a Haar-wavelet [Mallat 1989] based pedestrian detection system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 272
                            }
                        ],
                        "text": "\u2026salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al. 2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7686054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f071b068f3ba4a88f755f4b9bca51eb32d360b57",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare the performance of local detectors and descriptors in the context of object class recognition. Recently, many detectors/descriptors have been evaluated in the context of matching as well as invariance to viewpoint changes (Mikolajczyk and Schmid, 2004). However, it is unclear if these results can be generalized to categorization problems, which require different properties of features. We evaluate 5 state-of-the-art scale invariant region detectors and 5 descriptors. Local features are computed for 20 object classes and clustered using hierarchical agglomerative clustering. We measure the quality of appearance clusters and location distributions using entropy as well as precision. We also measure how the clusters generalize from training set to novel test data. Our results indicate that attended SIFT descriptors (Mikolajczyk and Schmid, 2005) computed on Hessian-Laplace regions perform best. Second score is obtained by salient regions (Kadir and Brady, 2001). The results also show that these two detectors provide complementary features. The new detectors/descriptors significantly improve the performance of a state-of-the art recognition approach (Leibe, et al., 2005) in pedestrian detection task"
            },
            "slug": "Local-features-for-object-class-recognition-Mikolajczyk-Leibe",
            "title": {
                "fragments": [],
                "text": "Local features for object class recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The new detectors/descriptors significantly improve the performance of a state-of-the art recognition approach (Leibe, et al., 2005) in pedestrian detection task."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188179"
                        ],
                        "name": "A. Opelt",
                        "slug": "A.-Opelt",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Opelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Opelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40446025"
                        ],
                        "name": "M. Fussenegger",
                        "slug": "M.-Fussenegger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fussenegger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fussenegger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718587"
                        ],
                        "name": "A. Pinz",
                        "slug": "A.-Pinz",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Pinz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543541"
                        ],
                        "name": "P. Auer",
                        "slug": "P.-Auer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Auer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 234
                            }
                        ],
                        "text": "\u2026salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al. 2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16562909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c91808994a250d7be332400a534a9291ca3b60e",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the first stage of a new learning system for object detection and recognition. For our system we propose Boosting (5) as the underlying learning technique. This allows the use of very diverse sets of visual features in the learning process within a com- mon framework: Boosting \u2014 together with a weak hypotheses finder \u2014 may choose very inhomogeneous features as most relevant for combina- tion into a final hypothesis. As another advantage the weak hypotheses finder may search the weak hypotheses space without explicit calculation of all available hypotheses, reducing computation time. This contrasts the related work of Agarwal and Roth (1) where Winnow was used as learning algorithm and all weak hypotheses were calculated explicitly. In our first empirical evaluation we use four types of local descriptors: two basic ones consisting of a set of grayvalues and intensity moments and two high level descriptors: moment invariants (8) and SIFTs (12). The descriptors are calculated from local patches detected by an inter- est point operator. The weak hypotheses finder selects one of the local patches and one type of local descriptor and efficiently searches for the most discriminative similarity threshold. This differs from other work on Boosting for object recognition where simple rectangular hypotheses (22) or complex classifiers (20) have been used. In relatively simple images, where the objects are prominent, our approach yields results comparable to the state-of-the-art (3). But we also obtain very good results on more complex images, where the objects are located in arbitrary positions, poses, and scales in the images. These results indicate that our flexible approach, which also allows the inclusion of features from segmented re- gions and even spatial relationships, leads us a significant step towards generic object recognition."
            },
            "slug": "Weak-Hypotheses-and-Boosting-for-Generic-Object-and-Opelt-Fussenegger",
            "title": {
                "fragments": [],
                "text": "Weak Hypotheses and Boosting for Generic Object Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The first stage of a new learning system for object detection and recognition using Boosting as the underlying learning technique and the inclusion of features from segmented re- gions and even spatial relationships leads us a significant step towards generic object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35195550"
                        ],
                        "name": "E. Sali",
                        "slug": "E.-Sali",
                        "structuredName": {
                            "firstName": "Erez",
                            "lastName": "Sali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398995671"
                        ],
                        "name": "Michel Vidal-Naquet",
                        "slug": "Michel-Vidal-Naquet",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Vidal-Naquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michel Vidal-Naquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "A person detector that incorporates motion descriptors has been proposed by Viola et al. [2003]. They build a human detector for static-camera surveillance applications, using generalised Haar wavelets and block averages of spatiotemporal differences as image and motion features and a computationally efficient rejection chain classifier [Baker and Nayar 1996, Viola and Jones 2001, Sun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 140
                            }
                        ],
                        "text": "\u2026and Schmid 2002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture, or combinations of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12923066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2809b20aea8d2a15ac655e17b47fd5dfb304aa4c",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of visual classification is thce recognition of an object in the image as belonging to a general class of similar objects, such as a face, a car, a dog, and the like. This is a fundamental and natural task for biological visual systems, but it has proven difficult to perform visual classification by artificial computer vision systems. The main reason for this difficulty is the variability of shape within a class: different objects vary widely in appearance, and it is difficult to capture the essential shape features that characterize the members of one category and distinguish them from another, such as dogs from cats. \n \nIn this paper we describe an approach to classification using a fragment-based representation. In this approach, objects within a class are represented in terms of common image fragments that are used as building blocks for representing a large variety of different objects that belong to a common class. The fragments are selected from a training set of images based on a criterion of maximizing the mutual information of the fragments and the class they represent. For the purpose of classification the fragments are also organized into types, where each type is a collection of alternative fragments, such as different hairline or eye regions for face classification. During classification, the algorithm detects fragments of the different types, and then combines the evidence for the detected fragments to reach a final decision. Experiments indicate that it is possible to trade off the complexity of fragments with the complexity of the combination and decision stage, and this tradeoff is discussed. \n \nThe method is different from previous part-based methods in using class-specific object fragments of varying complexity, the method of selecting fragments, and the organization into fragment types. Experimental results of detecting face and car views show that the fragment-based approach can generalize well to a variety of novel image views within a class while maintaining low mis-classification error rates. We briefly discuss relationships between the proposed method and properties of parts of the primate visual system involved in object perception"
            },
            "slug": "A-Fragment-Based-Approach-to-Object-Representation-Ullman-Sali",
            "title": {
                "fragments": [],
                "text": "A Fragment-Based Approach to Object Representation and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Experimental results of detecting face and car views show that the fragment-based approach can generalize well to a variety of novel image views within a class while maintaining low mis-classification error rates."
            },
            "venue": {
                "fragments": [],
                "text": "IWVF"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891864"
                        ],
                        "name": "Gyuri Dork\u00f3",
                        "slug": "Gyuri-Dork\u00f3",
                        "structuredName": {
                            "firstName": "Gyuri",
                            "lastName": "Dork\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyuri Dork\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7887211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96d9ab468299fe51a4e14d86d8ea953ccf62b900",
            "isKey": false,
            "numCitedBy": 355,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel method for constructing and selecting scale-invariant object parts. Scale-invariant local descriptors are first grouped into basic parts. A classifier is then learned for each of these parts, and feature selection is used to determine the most discriminative ones. This approach allows robust pan detection, and it is invariant under scale changes-that is, neither the training images nor the test images have to be normalized. The proposed method is evaluated in car detection tasks with significant variations in viewing conditions, and promising results are demonstrated. Different local regions, classifiers and feature selection methods are quantitatively compared. Our evaluation shows that local invariant descriptors are an appropriate representation for object classes such as cars, and it underlines the importance of feature selection."
            },
            "slug": "Selection-of-scale-invariant-parts-for-object-class-Dork\u00f3-Schmid",
            "title": {
                "fragments": [],
                "text": "Selection of scale-invariant parts for object class recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The evaluation shows that local invariant descriptors are an appropriate representation for object classes such as cars, and it underlines the importance of feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6533591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38101fac622a70b78f13625fc6502000b8756d3a",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for object categorization in real-world scenes. Following a common consensus in the field, we do not assume that a figure- ground segmentation is available prior to recognition. However, in contrast to most standard approaches for object class recognition, our approach automati- cally segments the object as a result of the categorization. This combination of recognition and segmentation into one process is made pos- sible by our use of an Implicit Shape Model, which integrates both into a common probabilistic framework. In addition to the recognition and segmentation result, it also generates a per-pixel confidence measure specifying the area that supports a hypothesis and how much it can be trusted. We use this confidence to derive a nat- ural extension of the approach to handle multiple objects in a scene and resolve ambiguities between overlapping hypotheses with a novel MDL-based criterion. In addition, we present an extensive evaluation of our method on a standard dataset for car detection and compare its performance to existing methods from the literature. Our results show that the proposed method significantly outper- forms previously published methods while needing one order of magnitude less training examples. Finally, we present results for articulated objects, which show that the proposed method can categorize and segment unfamiliar objects in differ- ent articulations and with widely varying texture patterns, even under significant partial occlusion."
            },
            "slug": "Combined-Object-Categorization-and-Segmentation-an-Leibe-Leonardis",
            "title": {
                "fragments": [],
                "text": "Combined Object Categorization and Segmentation With an Implicit Shape Model"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results for articulated objects, which show that the proposed method can categorize and segment unfamiliar objects in differ- ent articulations and with widely varying texture patterns, even under significant partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083209039"
                        ],
                        "name": "Edgar Seemann",
                        "slug": "Edgar-Seemann",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Seemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Seemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 253
                            }
                        ],
                        "text": "\u2026salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al. 2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14395688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1854005a7178b2df6afaacdcf91bc35d90616075",
            "isKey": false,
            "numCitedBy": 932,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the problem of detecting pedestrians in crowded real-world scenes with severe overlaps. Our basic premise is that this problem is too difficult for any type of model or feature alone. Instead, we present an algorithm that integrates evidence in multiple iterations and from different sources. The core part of our method is the combination of local and global cues via probabilistic top-down segmentation. Altogether, this approach allows examining and comparing object hypotheses with high precision down to the pixel level. Qualitative and quantitative results on a large data set confirm that our method is able to reliably detect pedestrians in crowded scenes, even when they overlap and partially occlude each other. In addition, the flexible nature of our approach allows it to operate on very small training sets."
            },
            "slug": "Pedestrian-detection-in-crowded-scenes-Leibe-Seemann",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection in crowded scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Qualitative and quantitative results on a large data set confirm that the core part of the method is the combination of local and global cues via probabilistic top-down segmentation that allows examining and comparing object hypotheses with high precision down to the pixel level."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144238410"
                        ],
                        "name": "Qiang Zhu",
                        "slug": "Qiang-Zhu",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39369497"
                        ],
                        "name": "Mei-Chen Yeh",
                        "slug": "Mei-Chen-Yeh",
                        "structuredName": {
                            "firstName": "Mei-Chen",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mei-Chen Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766349"
                        ],
                        "name": "K. Cheng",
                        "slug": "K.-Cheng",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815078"
                        ],
                        "name": "S. Avidan",
                        "slug": "S.-Avidan",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Avidan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Avidan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 94
                            }
                        ],
                        "text": "A comparative study of multiple scale HOG blocks in conjunction with AdaBoost is presented in Zhu et al. [2006]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 95
                            }
                        ],
                        "text": "with an efficient detection algorithm such as AdaBoost it allows near real-time implementation [Zhu et al. 2006], the run time is still significantly higher than a similar framework using integral arrays Viola and Jones [2001] to compute wavelet like feature vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7800101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05fe01b57b3ba58dc5029c068a48567b55018ea5",
            "isKey": false,
            "numCitedBy": 1568,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We integrate the cascade-of-rejectors approach with the Histograms of Oriented Gradients (HoG) features to achieve a fast and accurate human detection system. The features used in our system are HoGs of variable-size blocks that capture salient features of humans automatically. Using AdaBoost for feature selection, we identify the appropriate set of blocks, from a large set of possible blocks. In our system, we use the integral image representation and a rejection cascade which significantly speed up the computation. For a 320 \u00d7 280 image, the system can process 5 to 30 frames per second depending on the density in which we scan the image, while maintaining an accuracy level similar to existing methods."
            },
            "slug": "Fast-Human-Detection-Using-a-Cascade-of-Histograms-Zhu-Yeh",
            "title": {
                "fragments": [],
                "text": "Fast Human Detection Using a Cascade of Histograms of Oriented Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work integrates the cascade-of-rejectors approach with the Histograms of Oriented Gradients features to achieve a fast and accurate human detection system that can process 5 to 30 frames per second depending on the density in which the image is scanned, while maintaining an accuracy level similar to existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 180
                            }
                        ],
                        "text": "\u2026salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al. 2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5745749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
            "isKey": false,
            "numCitedBy": 2487,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "slug": "Object-class-recognition-by-unsupervised-learning-Fergus-Perona",
            "title": {
                "fragments": [],
                "text": "Object class recognition by unsupervised scale-invariant learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1350374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804d86dd7ab3498266922244e73a88c1add5a6ab",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as two forms of data-based action synthesis \"do as I do\" and \"do as I say\". Results are demonstrated on ballet, tennis as well as football datasets."
            },
            "slug": "Recognizing-action-at-a-distance-Efros-Berg",
            "title": {
                "fragments": [],
                "text": "Recognizing action at a distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure is introduced, and an associated similarity measure to be used in a nearest-neighbor framework is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 134
                            }
                        ],
                        "text": "Currently the most popular approaches are image gradient based descriptors such as the Scale Invariant Feature Transformation (SIFT) [Lowe 1999, 2004] and shape contexts [Belongie et al. 2001, 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5258236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9f836d28f52ad260213d32224a6d227f8e8849a",
            "isKey": false,
            "numCitedBy": 16258,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "slug": "Object-recognition-from-local-scale-invariant-Lowe",
            "title": {
                "fragments": [],
                "text": "Object recognition from local scale-invariant features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18901556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49a7ff6c753a79ed063fe2c4bf3eca3fa03c2f7e",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of our work is object categorization in real-world scenes. That is, given a novel image we want to recognize and localize unseen-before objects based on their similarity to a learned object category. For use in a real-world system, it is important that this includes the ability to recognize objects at multiple scales. In this paper, we present an approach to multi-scale object categorization using scale-invariant interest points and a scale-adaptive Mean-Shift search. The approach builds on the method from [12], which has been demonstrated to achieve excellent results for the single-scale case, and extends it to multiple scales. We present an experimental comparison of the influence of different interest point operators and quantitatively show the method's robustness to large scale changes."
            },
            "slug": "Scale-Invariant-Object-Categorization-Using-a-Leibe-Schiele",
            "title": {
                "fragments": [],
                "text": "Scale-Invariant Object Categorization Using a Scale-Adaptive Mean-Shift Search"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents an approach to multi-scale object categorization using scale-invariant interest points and a scale-adaptive Mean-Shift search, and presents an experimental comparison of the influence of different interest point operators and quantitatively shows the method's robustness to large scale changes."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 275
                            }
                        ],
                        "text": "\u2026of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture, or combinations of several or all of these [Martin et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "2004], colour, texture, or combinations of several or all of these [Martin et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8165754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33a7a59f785ef46091c30c4c85ef88c6bdabab51",
            "isKey": false,
            "numCitedBy": 2381,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images."
            },
            "slug": "Learning-to-detect-natural-image-boundaries-using-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "Learning to detect natural image boundaries using local brightness, color, and texture cues"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The two main results are that cue combination can be performed adequately with a simple linear model and that a proper, explicit treatment of texture is required to detect boundaries in natural images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 150
                            }
                        ],
                        "text": "\u2026be based on points [Harris and Stephens 1988, Mikolajczyk and Schmid 2002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al.\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 678,
                                "start": 40
                            }
                        ],
                        "text": "R-HOGs are similar to SIFT descriptors [Lowe 2004] but are used quite differently. SIFTs are computed at a sparse set of scale-invariant key points, rotated to align their dominant orientations and used individually, whereas R-HOGs are computed in dense grids at a single scale without dominant orientation alignment. The grid position of the block implicitly encodes spatial position relative to the detection window in the final code vector. SIFTs are optimised for sparse wide baseline matching, R-HOGs for dense robust coding of spatial form. Other precursors to R-HOG include the edge orientation histograms of Freeman and Roth [1995], c.f . Sect. 2.5.1. As in Lowe [2004], we find that it is useful to down-weight pixels near the edges of the R-HOG block by applying a Gaussian spatial window to each pixel before accumulating orientation votes into cells."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 6
                            }
                        ],
                        "text": "As in Lowe [2004], down-weighting pixels near the edges of the block by applying a Gaussian spatial window to each pixel before accumulating orientation votes into cells improves performance \u2013 here by 1% at 10\u22124 FPPW for a Gaussian with \u03c3 = 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 6
                            }
                        ],
                        "text": "1996, Lowe 1999, 2004, Belongie et al. 2002]. McConnell [1986] proposed edge-slope histograms over spatial neighbourhoods for pattern recognition tasks, which was later extended to orientation histograms of image gradients in Bichsel [1991], Freeman and Roth [1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 149
                            }
                        ],
                        "text": "\u2026of salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al.\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 640,
                                "start": 40
                            }
                        ],
                        "text": "R-HOGs are similar to SIFT descriptors [Lowe 2004] but are used quite differently. SIFTs are computed at a sparse set of scale-invariant key points, rotated to align their dominant orientations and used individually, whereas R-HOGs are computed in dense grids at a single scale without dominant orientation alignment. The grid position of the block implicitly encodes spatial position relative to the detection window in the final code vector. SIFTs are optimised for sparse wide baseline matching, R-HOGs for dense robust coding of spatial form. Other precursors to R-HOG include the edge orientation histograms of Freeman and Roth [1995], c."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 6
                            }
                        ],
                        "text": "1996, Lowe 1999, 2004, Belongie et al. 2002]. McConnell [1986] proposed edge-slope histograms over spatial neighbourhoods for pattern recognition tasks, which was later extended to orientation histograms of image gradients in Bichsel [1991], Freeman and Roth [1995]. Freeman and Roth [1995] used it for hand recognition system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 556474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9188b661f2ae65a080676617cc83b7d09773c59f",
            "isKey": true,
            "numCitedBy": 588,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "There have been important recent advances in object recognition through the matching of invariant local image features. However, the existing approaches are based on matching to individual training images. This paper presents a method for combining multiple images of a 3D object into a single model representation. This provides for recognition of 3D objects from any viewpoint, the generalization of models to non-rigid changes, and improved robustness through the combination of features acquired under a range of imaging conditions. The decision of whether to cluster a training image into an existing view representation or to treat it as a new view is based on the geometric accuracy of the match to previous model views. A new probabilistic model is developed to reduce the false positive matches that would otherwise arise due to loosened geometric constraints on matching 3D and non-rigid models. A system has been developed based on these approaches that is able to robustly recognize 3D objects in cluttered natural images in sub-second times."
            },
            "slug": "Local-feature-view-clustering-for-3D-object-Lowe",
            "title": {
                "fragments": [],
                "text": "Local feature view clustering for 3D object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a method for combining multiple images of a 3D object into a single model representation that provides for recognition of 3D objects from any viewpoint, the generalization of models to non-rigid changes, and improved robustness through the combination of features acquired under a range of imaging conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2467894"
                        ],
                        "name": "Wei-Lwun Lu",
                        "slug": "Wei-Lwun-Lu",
                        "structuredName": {
                            "firstName": "Wei-Lwun",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Lwun Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710980"
                        ],
                        "name": "J. Little",
                        "slug": "J.-Little",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Little",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Little"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3018519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "949a4e3fc017c370d7d5986fde33bbc4b8ecec8d",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a template-based algorithm to track and recognize athlete\u2019s actions in an integrated system using only visual information. Conventional template-based action recognition systems usually consider action recognition and tracking as two independent problems, and solve them separately. In contrast, our algorithm emphasizes that tracking and action recognition can be tightly coupled into a single framework, where tracking assists action recognition and vise versa. Moreover, this paper proposes to represent the athletes by the PCA-HOG descriptor, which can be computed by first transforming the athletes to the grids of Histograms of Oriented Gradient (HOG) descriptor and then project it to a linear subspace by Principal Component Analysis (PCA). The exploitation of the PCA-HOG descriptor not only helps the tracker to be robust under illumination, pose, and view-point changes, but also implicitly centers the figure in the tracking region, which makes action recognition possible. Empirical results in hockey and soccer sequences show the effectiveness of this algorithm."
            },
            "slug": "Simultaneous-Tracking-and-Action-Recognition-using-Lu-Little",
            "title": {
                "fragments": [],
                "text": "Simultaneous Tracking and Action Recognition using the PCA-HOG Descriptor"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes to represent the athletes by the PCA-HOG descriptor, which can be computed by first transforming the athletes to the grids of Histograms of Oriented Gradient (HOG) descriptor and then project it to a linear subspace by Principal Component Analysis (PCA)."
            },
            "venue": {
                "fragments": [],
                "text": "The 3rd Canadian Conference on Computer and Robot Vision (CRV'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7149126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "339c13bfe3371a71ab486381721dbb689ff415ab",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for object detection in cluttered real images, given a single hand-drawn example as model. The image edges are partitioned into contour segments and organized in an image representation which encodes their interconnections: the Contour Segment Network. The object detection problem is formulated as finding paths through the network resembling the model outlines, and a computationally efficient detection technique is presented. An extensive experimental evaluation on detecting five diverse object classes over hundreds of images demonstrates that our method works in very cluttered images, allows for scale changes and considerable intra-class shape variation, is robust to interrupted contours, and is computationally efficient."
            },
            "slug": "Object-Detection-by-Contour-Segment-Networks-Ferrari-Tuytelaars",
            "title": {
                "fragments": [],
                "text": "Object Detection by Contour Segment Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An extensive experimental evaluation on detecting five diverse object classes over hundreds of images demonstrates that the proposed method works in very cluttered images, allows for scale changes and considerable intra-class shape variation, is robust to interrupted contours, and is computationally efficient."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 54
                            }
                        ],
                        "text": "Other approaches using image gradient descriptors are Ronfard et al. [2002] and Mikolajczyk et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 189
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15039233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "768b9d18ebfc5ad2de18ab613d7baa0500239de8",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a tracker that can track moving people in long sequences without manual initialization. Moving people are modeled with the assumption that, while configuration can vary quite substantially from frame to frame, appearance does not. This leads to an algorithm that firstly builds a model of the appearance of the body of each individual by clustering candidate body segments, and then uses this model to find all individuals in each frame. Unusually, the tracker does not rely on a model of human dynamics to identify possible instances of people; such models are unreliable, because human motion is fast and large accelerations are common. We show our tracking algorithm can be interpreted as a loopy inference procedure on an underlying Bayes net. Experiments on video of real scenes demonstrate that this tracker can (a) count distinct individuals; (b) identify and track them; (c) recover when it loses track, for example, if individuals are occluded or briefly leave the view; (d) identify the configuration of the body largely correctly; and (e) is not dependent on particular models of human motion."
            },
            "slug": "Finding-and-tracking-people-from-the-bottom-up-Ramanan-Forsyth",
            "title": {
                "fragments": [],
                "text": "Finding and tracking people from the bottom up"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A tracker that can track moving people in long sequences without manual initialization is described and it is shown the tracking algorithm can be interpreted as a loopy inference procedure on an underlying Bayes net."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6153430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5331349557fababfac48d47e49b44583e3bd5f6",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes. The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator. Each object category then has its own distribution over these parts, which are shared between objects. We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters. Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available. We also extend this hierarchical framework to scenes containing multiple objects"
            },
            "slug": "Learning-hierarchical-models-of-scenes,-objects,-Sudderth-Torralba",
            "title": {
                "fragments": [],
                "text": "Learning hierarchical models of scenes, objects, and parts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available and this hierarchical probabilistic model is extended to scenes containing multiple objects."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 27
                            }
                        ],
                        "text": "(c) Results obtained using Hoiem et al. [2006] approach which takes into consideration the interplay between rough 3-D scene geometry, approximate camera position/orientation and low-level person and car detectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 29
                            }
                        ],
                        "text": "2005, Kumar and Hebert 2005, Hoiem et al. 2006]. In particular Hoiem et al. [2006] show that by using the interplay of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 161
                            }
                        ],
                        "text": "They have been used for several other computer vision tasks, ranging from tracking and activity recognition [Lu and Little 2006], context based object detection [Hoiem et al. 2006] to secure multi-party communication protocols [Avidan and Butman 2006]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6152006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4081e007d7eced95cc618164e976a80d44ff5f4e",
            "isKey": true,
            "numCitedBy": 656,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
            },
            "slug": "Putting-Objects-in-Perspective-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Putting Objects in Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper provides a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint by allowing probabilistic object hypotheses to refine geometry and vice-versa."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 433,
                                "start": 8
                            }
                        ],
                        "text": "[1998], Mohan et al. [2001], SVM are used as classifiers over parts-based descriptors. Mohan et al. [2001] created a two stage cascade of SVM classifiers. The first stage creates part (head, left arm, etc) detectors from Haar wavelets. The second combines the part detections to obtain the final object detector. SVMs are also used as an intermediate stage in Ronfard et al. [2002], Dork\u00f3 and Schmid [2003]. In Ronfard et al. [2002], 15 SVM and Relevance Vector Machine (RVM) based limb detectors are created based on first and second order image gradients, but the final classifier is based on dynamic programming over assemblies of limb detections as in Ioffe and Forsyth [2001b,a]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 8
                            }
                        ],
                        "text": "[1998], Mohan et al. [2001], SVM are used as classifiers over parts-based descriptors. Mohan et al. [2001] created a two stage cascade of SVM classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 8
                            }
                        ],
                        "text": "[1998], Mohan et al. [2001], SVM are used as classifiers over parts-based descriptors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2572455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69401bfdafab7cde00bb8e5b2f6c28e9d72d8cfb",
            "isKey": true,
            "numCitedBy": 3666,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors."
            },
            "slug": "A-performance-evaluation-of-local-descriptors-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "A performance evaluation of local descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is observed that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best and Moments and steerable filters show the best performance among the low dimensional descriptors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078230370"
                        ],
                        "name": "Andrew Zisserman April",
                        "slug": "Andrew-Zisserman-April",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "April",
                            "middleNames": [
                                "Zisserman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman April"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16601529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5307e2ffdfe34264385c26ea998a05656170179",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this challenge is to recognize objects from a number of visual object classes in images of realistic scenes. It is fundamentally a supervised learning learning problem in that a training set of labelled images is provided. The object classes are: motorbikes, bicycles, people and cars. Twelve participants entered the challenge. A full description of the challenge including software and image sets is available on the web page http://www.pascal-network.org/challenges/VOC/voc/index.html."
            },
            "slug": "Pascal-Visual-Object-Classes-Challenge-Results-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "Pascal Visual Object Classes Challenge Results"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The goal of this challenge is to recognize objects from a number of visual object classes in images of realistic scenes using a training set of labelled images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719780"
                        ],
                        "name": "Yan Ke",
                        "slug": "Yan-Ke",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Ke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Ke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 37
                            }
                        ],
                        "text": "One reason is that, in comparison to Ke and Sukthankar [2004], many more (80 of 512) eigenvectors have to be retained to capture the same proportion of the variance."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 494,
                                "start": 114
                            }
                        ],
                        "text": "PCA-SIFT descriptors are based on projecting gradient images onto a basis learned from training images using PCA [Ke and Sukthankar 2004]. Ke and Sukthankar computed PCAs on xand y- image gradients over an N\u00d7 N image window and found that they outperformed SIFT for key point based matching, but this has been disputed [Mikolajczyk and Schmid 2005]. Our motivation for using PCA-SIFT is its similarity to reduce dimension appearance models to the kind popularised by Sirovitch and Kirby [1987], Turk and Pentland [1991], Belhumeur et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 545,
                                "start": 114
                            }
                        ],
                        "text": "PCA-SIFT descriptors are based on projecting gradient images onto a basis learned from training images using PCA [Ke and Sukthankar 2004]. Ke and Sukthankar computed PCAs on xand y- image gradients over an N\u00d7 N image window and found that they outperformed SIFT for key point based matching, but this has been disputed [Mikolajczyk and Schmid 2005]. Our motivation for using PCA-SIFT is its similarity to reduce dimension appearance models to the kind popularised by Sirovitch and Kirby [1987], Turk and Pentland [1991], Belhumeur et al. [1997]. However PCA-SIFT computes bases over image gradients instead image intensity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 288876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a46f093f587eaa29c4d87c755b4b1fa3eabecdb",
            "isKey": true,
            "numCitedBy": 1998,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching."
            },
            "slug": "PCA-SIFT:-a-more-distinctive-representation-for-Ke-Sukthankar",
            "title": {
                "fragments": [],
                "text": "PCA-SIFT: a more distinctive representation for local image descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper examines (and improves upon) the local image descriptor used by SIFT, and demonstrates that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110604335"
                        ],
                        "name": "Markus Weber",
                        "slug": "Markus-Weber",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "Recently, Zhu et al. [2006] showed that centre-surround HOG, in conjunction with integral histograms [Porikli 2005] and AdaBoost [Freund and Schapire 1996a,b, Schapire 2002], can be used to build a near real-time filter cascade style detector."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 9
                            }
                        ],
                        "text": "Recently Zhu et al. [2006] used histograms of oriented gradient features proposed in this thesis with a cascade of rejecters based approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 137
                            }
                        ],
                        "text": "The use of salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8970876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cf1527807ebb16020b04d4166e7ba8d27652302",
            "isKey": false,
            "numCitedBy": 757,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars."
            },
            "slug": "Unsupervised-Learning-of-Models-for-Recognition-Weber-Welling",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Models for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method to learn object class models from unlabeled and unsegmented cluttered cluttered scenes for the purpose of visual object recognition that achieves very good classification results on human faces and rear views of cars."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40309692"
                        ],
                        "name": "C. G. Harris",
                        "slug": "C.-G.-Harris",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Harris",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40365651"
                        ],
                        "name": "M. Stephens",
                        "slug": "M.-Stephens",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Stephens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stephens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 37
                            }
                        ],
                        "text": "Some point detectors such as DoGs or Harris-Laplace also provide additional local scale and/or dominant orientation information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Agarwal and Roth [2002] use the Fo\u0308rstner and Gu\u0308lch [1987] interest point operator, while Dorko\u0301 and Schmid [2003] and Opelt et al. [2004] use Harris [Harris and Stephens 1988], Harris-Laplace [Mikolajczyk and Schmid 2002] and Laplacian of Gaussian (LoG) [Mikolajczyk and Schmid 2002] based\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 38
                            }
                        ],
                        "text": "Such features can be based on points [Harris and Stephens 1988, Mikolajczyk and Schmid 2002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture, or combinations of several or all of these [Martin et al. 2004]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 108
                            }
                        ],
                        "text": "Commonly used key point detectors include Fo\u0308rstnerHarris [Fo\u0308rstner and Pertl 1986, Fo\u0308rstner and Gu\u0308lch 1987, Harris and Stephens 1988], Laplacian [Lindeberg 1998] or Difference of Gaussians (DoGs) [Lowe 2004], and scale invariant HarrisLaplace [Mikolajczyk and Schmid 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 38
                            }
                        ],
                        "text": "Such features can be based on points [Harris and Stephens 1988, Mikolajczyk and Schmid 2002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 290
                            }
                        ],
                        "text": "For example Fergus et al. [2003] used the entropy based region detector of Kadir and Brady [2001] for object representation, Weber et al. [2000] and Agarwal and Roth [2002] use the Fo\u0308rstner and Gu\u0308lch [1987] interest point operator, while Dorko\u0301 and Schmid [2003] and Opelt et al. [2004] use Harris [Harris and Stephens 1988], Harris-Laplace [Mikolajczyk and Schmid 2002] and Laplacian of Gaussian (LoG) [Mikolajczyk and Schmid 2002] based interest point detectors."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1694378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "isKey": true,
            "numCitedBy": 14112,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed."
            },
            "slug": "A-Combined-Corner-and-Edge-Detector-Harris-Stephens",
            "title": {
                "fragments": [],
                "text": "A Combined Corner and Edge Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem the authors are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work."
            },
            "venue": {
                "fragments": [],
                "text": "Alvey Vision Conference"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404428"
                        ],
                        "name": "Yang Song",
                        "slug": "Yang-Song",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149680415"
                        ],
                        "name": "L. Goncalves",
                        "slug": "L.-Goncalves",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Goncalves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goncalves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772763"
                        ],
                        "name": "E. Bernardo",
                        "slug": "E.-Bernardo",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Bernardo",
                            "middleNames": [
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bernardo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 70
                            }
                        ],
                        "text": "The most relevant fragments are selected using an approach similar to Ullman et al. [2001], Vidal-Naquet and Ullman [2003]. Next, for each relevant fragment, the approach computes the distribution of the fragment location w."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 70
                            }
                        ],
                        "text": "The most relevant fragments are selected using an approach similar to Ullman et al. [2001], Vidal-Naquet and Ullman [2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2185149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dfba037b2bd101f3ec172cac1589625edb539c3",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer perception of biological motion is key to developing convenient and powerful human-computer interfaces. Successful body tracking algorithms have been developed; however initialization is done by hand. We propose a method for detecting a moving human body and for labeling its parts automatically. It is based on maximizing the joint probability density function (PDF) of the position and velocity of the body parts. The PDF is estimated from training data. Dynamic programming is used for calculating efficiently the best global labeling on an approximation of the PDF. The computational cost is on the order of N/sup 4/ where N is the number of features detected. We explore the performance of our method with experiments carried on a variety of periodic and non-periodic body motions viewed monocularly for a total of approximately 30,000 frames. Point-markers were strapped to the joints of the subject for facilitating image analysis. We find an average of 2.3% labeling error; the experiments also suggest a high degree of viewpoint-invariance."
            },
            "slug": "Monocular-perception-of-biological-motion-detection-Song-Goncalves",
            "title": {
                "fragments": [],
                "text": "Monocular perception of biological motion-detection and labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A method for detecting a moving human body and for labeling its parts automatically based on maximizing the joint probability density function (PDF) of the position and velocity of the body parts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748980"
                        ],
                        "name": "A. Bartoli",
                        "slug": "A.-Bartoli",
                        "structuredName": {
                            "firstName": "Adrien",
                            "lastName": "Bartoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bartoli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52036297,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d202035f09930d99a887e5aeb4c12c88136fc596",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a method for analysing video sequences and for representing them as mosaics or panoramas. Previous work on video mosaicking essentially concentrated on static scenes. We generalize these approaches to the case of a rotating camera observing both static and moving objects where the static portions of the scene are not necessarily dominant, as it has been often hypothesized in the past. We start by describing a robust technique for accurately aligning a large number of video frames under unknown camera rotations and camera settings. The alignment technique combines a feature\u2010based method (initialization and refinement) with rough motion segmentation followed by a colour\u2010based direct method (final adjustment). This precise frame\u2010to\u2010frame alignment allows the dynamic building of a background representation as well as an efficient segmentation of each image such that moving regions of arbitrary shape and size are aligned with the static background. Thus a motion panorama visualizes both dynamic and static scene elements in a geometrically consistent way. Extensive experiments applied to archived videos of track\u2010and\u2010field events validate the approach. Copyright \u00a9 2004 John Wiley & Sons, Ltd."
            },
            "slug": "Motion-Panoramas-Bartoli-Dalal",
            "title": {
                "fragments": [],
                "text": "Motion Panoramas"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A robust technique for accurately aligning a large number of video frames under unknown camera rotations and camera settings is described, which allows the dynamic building of a background representation as well as an efficient segmentation of each image such that moving regions of arbitrary shape and size are aligned with the static background."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Animat. Virtual Worlds"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762260"
                        ],
                        "name": "V. Philomin",
                        "slug": "V.-Philomin",
                        "structuredName": {
                            "firstName": "Vasanth",
                            "lastName": "Philomin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Philomin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 766556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb490d879512b3d43b267e3ac8931c099a5a2fd3",
            "isKey": false,
            "numCitedBy": 760,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an efficient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; efficient hierarchies can be generated offline for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-to-fine approach over the shape hierarchy and over the transformation parameters. Very large speed-up factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of traffic signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardware-specific implementations of the proposed method as far as SIMD parallelism is concerned."
            },
            "slug": "Real-time-object-detection-for-\"smart\"-vehicles-Gavrila-Philomin",
            "title": {
                "fragments": [],
                "text": "Real-time object detection for \"smart\" vehicles"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An efficient shape-based object detection method based on Distance Transforms is presented and its use for real-time vision on-board vehicles and some hardware-specific implementations of the proposed method as far as SIMD parallelism is concerned are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1053619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "310dda18ecb3817aef260239f43a7821faf4cf55",
            "isKey": false,
            "numCitedBy": 674,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we explore object recognition in clutter. We test our object recognition techniques on Gimpy and EZ-Gimpy, examples of visual CAPTCHAs. A CAPTCHA (\"Completely Automated Public Turing test to Tell Computers and Humans Apart\") is a program that can generate and grade tests that most humans can pass, yet current computer programs can't pass. EZ-Gimpy, currently used by Yahoo, and Gimpy are CAPTCHAs based on word recognition in the presence of clutter. These CAPTCHAs provide excellent test sets since the clutter they contain is adversarial; it is designed to confuse computer programs. We have developed efficient methods based on shape context matching that can identify the word in an EZ-Gimpy image with a success rate of 92%, and the requisite 3 words in a Gimpy image 33% of the time. The problem of identifying words in such severe clutter provides valuable insight into the more general problem of object recognition in scenes. The methods that we present are instances of a framework designed to tackle this general problem."
            },
            "slug": "Recognizing-objects-in-adversarial-clutter:-a-Mori-Malik",
            "title": {
                "fragments": [],
                "text": "Recognizing objects in adversarial clutter: breaking a visual CAPTCHA"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Efficient methods based on shape context matching are developed that can identify the word in an EZ-Gimpy image with a success rate of 92%, and the requisite 3 words in a Gimpy image 33% of the time."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748980"
                        ],
                        "name": "A. Bartoli",
                        "slug": "A.-Bartoli",
                        "structuredName": {
                            "firstName": "Adrien",
                            "lastName": "Bartoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bartoli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34675140"
                        ],
                        "name": "B. Bose",
                        "slug": "B.-Bose",
                        "structuredName": {
                            "firstName": "Biswajit",
                            "lastName": "Bose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 218536915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19ac95fb1365dfb78b4f66cf35dcca56f33135d1",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of constructing mosaics from video sequences taken by rotating cameras. In particular we investigate the widespread case where the scene is not only static but may also contain large dynamic areas, induced by moving or deforming objects. Most of the existing techniques fail to produce reliable results on such video sequences. For such alignment purposes, two classes of techniques may be used: feature-based and direct methods. We derive both of them in a unified statistical manner and propose an integrated framework to construct what we call motion panoramas, based on a mixed feature-based and direct approach. Experimental results are provided on large image sequences. In particular we consider sport videos where the moving and deforming athlete is visible in every frame of the sequence, thereby making the alignment task tricky."
            },
            "slug": "From-video-sequences-to-motion-panoramas-Bartoli-Dalal",
            "title": {
                "fragments": [],
                "text": "From video sequences to motion panoramas"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work considers sport videos where the moving and deforming athlete is visible in every frame of the sequence, thereby making the alignment task tricky, and proposes an integrated framework to construct what is called motion panoramas, based on a mixed feature-based and direct approach."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Motion and Video Computing, 2002. Proceedings."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3205375"
                        ],
                        "name": "T. Lindeberg",
                        "slug": "T.-Lindeberg",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Lindeberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lindeberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "Such features can be based on points [Harris and Stephens 1988, Mikolajczyk and Schmid 2002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture, or combinations of several or all of these [Martin et al. 2004]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 146
                            }
                        ],
                        "text": "Commonly used key point detectors include Fo\u0308rstnerHarris [Fo\u0308rstner and Pertl 1986, Fo\u0308rstner and Gu\u0308lch 1987, Harris and Stephens 1988], Laplacian [Lindeberg 1998] or Difference of Gaussians (DoGs) [Lowe 2004], and scale invariant HarrisLaplace [Mikolajczyk and Schmid 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "Although Haar wavelet like features in Papageorgiou and Poggio [2000], Mohan et al. [2001], Viola and Jones [2001] form dense and overcomplete representations, they do not exploit the lessons learnt from SIFT [Lowe 2004], shape context [Belongie et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 124
                            }
                        ],
                        "text": "Such features can be based on points [Harris and Stephens 1988, Mikolajczyk and Schmid 2002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 381,
                                "start": 374
                            }
                        ],
                        "text": "For example Fergus et al. [2003] used the entropy based region detector of Kadir and Brady [2001] for object representation, Weber et al. [2000] and Agarwal and Roth [2002] use the Fo\u0308rstner and Gu\u0308lch [1987] interest point operator, while Dorko\u0301 and Schmid [2003] and Opelt et al. [2004] use Harris [Harris and Stephens 1988], Harris-Laplace [Mikolajczyk and Schmid 2002] and Laplacian of Gaussian (LoG) [Mikolajczyk and Schmid 2002] based interest point detectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 149
                            }
                        ],
                        "text": "For example, interest point based approaches to salient region detection are usually tuned to fire on blob like structures (one such detector is the Laplacian of Gaussians) while their feature vector computations are tuned to encode gradient or contour information (e.g. using SIFT [Lowe 2004] or shape context [Belongie et al. 2001])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 723210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b02f474196fb9bd61fa3d418a7ba8ac500e8d422",
            "isKey": true,
            "numCitedBy": 2940,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "The fact that objects in the world appear in different ways depending on the scale of observation has important implications if one aims at describing them. It shows that the notion of scale is of utmost importance when processing unknown measurement data by automatic methods. In their seminal works, Witkin (1983) and Koenderink (1984) proposed to approach this problem by representing image structures at different scales in a so-called scale-space representation. Traditional scale-space theory building on this work, however, does not address the problem of how to select local appropriate scales for further analysis. This article proposes a systematic methodology for dealing with this problem. A framework is presented for generating hypotheses about interesting scale levels in image data, based on a general principle stating that local extrema over scales of different combinations of \u03b3-normalized derivatives are likely candidates to correspond to interesting structures. Specifically, it is shown how this idea can be used as a major mechanism in algorithms for automatic scale selection, which adapt the local scales of processing to the local image structure.Support for the proposed approach is given in terms of a general theoretical investigation of the behaviour of the scale selection method under rescalings of the input pattern and by integration with different types of early visual modules, including experiments on real-world and synthetic data. Support is also given by a detailed analysis of how different types of feature detectors perform when integrated with a scale selection mechanism and then applied to characteristic model patterns. Specifically, it is described in detail how the proposed methodology applies to the problems of blob detection, junction detection, edge detection, ridge detection and local frequency estimation.In many computer vision applications, the poor performance of the low-level vision modules constitutes a major bottleneck. It is argued that the inclusion of mechanisms for automatic scale selection is essential if we are to construct vision systems to automatically analyse complex unknown environments."
            },
            "slug": "Feature-Detection-with-Automatic-Scale-Selection-Lindeberg",
            "title": {
                "fragments": [],
                "text": "Feature Detection with Automatic Scale Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown how the proposed methodology applies to the problems of blob detection, junction detection, edge detection, ridge detection and local frequency estimation and how it can be used as a major mechanism in algorithms for automatic scale selection, which adapt the local scales of processing to the local image structure."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 84
                            }
                        ],
                        "text": "The use of salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 729473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b634b2001e3fb6159ab15d5375eb4f78213d1eee",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a probabilistic object recognition technique which does not require correspondence matching of images. This technique is an extension of our earlier work (1996) on object recognition using matching of multi-dimensional receptive field histograms. In the earlier paper we have shown that multi-dimensional receptive field histograms can be matched to provide object recognition which is robust in the face of changes in viewing position and independent of image plane rotation and scale. In this paper we extend this method to compute the probability of the presence of an object in an image. The paper begins with a review of the method and previously presented experimental results. We then extend the method for histogram matching to obtain a genuine probability of the presence of an object. We present experimental results on a database of 100 objects showing that the approach is capable recognizing all objects correctly by using only a small portion of the image. Our results show that receptive field histograms provide a technique for object recognition which is robust, has low computational cost and a computational complexity which is linear with the number of pixels."
            },
            "slug": "Probabilistic-object-recognition-using-receptive-Schiele-Crowley",
            "title": {
                "fragments": [],
                "text": "Probabilistic object recognition using multidimensional receptive field histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The method for histogram matching is extended to compute the probability of the presence of an object in an image and shows that receptive field histograms provide a technique for object recognition which is robust, has low computational cost and a computational complexity which is linear with the number of pixels."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 607,
                                "start": 0
                            }
                        ],
                        "text": "Opelt et al. [2004] use a similar AdaBoost framework for their interest point based weak classifiers. Schneiderman and Kanade [2000, 2004] propose a more elaborate classification model. They define parts as functions of specific groups of wavelet coefficients, represented with respect to a common coordinate frame. This implicitly captures the geometric relationships between parts. Independent (\u201cnaive Bayes\u201d) combinations of likelihood ratios P(part|ob ject=1) P(part|ob ject=0) are combined to form the final classifier. The original detector did not use AdaBoost, but in Schneiderman and Kanade [2004], conditional probability scores are estimated using a modification of AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 14
                            }
                        ],
                        "text": "A super-pixel [Mori et al. 2004] based approach was also evaluated but resulted in similar conclusions."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Opelt et al. [2004] use a similar AdaBoost framework for their interest point based weak classifiers."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9177303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5d6d6f5d9caaba221d785f0b92d07ce2bfa3a48",
            "isKey": true,
            "numCitedBy": 570,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to detect a human figure image and localize his joints and limbs along with their associated pixel masks. In this work we attempt to tackle this problem in a general setting. The dataset we use is a collection of sports news photographs of baseball players, varying dramatically in pose and clothing. The approach that we take is to use segmentation to guide our recognition algorithm to salient bits of the image. We use this segmentation approach to build limb and torso detectors, the outputs of which are assembled into human figures. We present quantitative results on torso localization, in addition to shortlisted full body configurations."
            },
            "slug": "Recovering-human-body-configurations:-combining-and-Mori-Ren",
            "title": {
                "fragments": [],
                "text": "Recovering human body configurations: combining segmentation and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work uses segmentation to build limb and torso detectors, the outputs of which are assembled into human figures, and presents quantitative results on torso localization, in addition to shortlisted full body configurations."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398995671"
                        ],
                        "name": "Michel Vidal-Naquet",
                        "slug": "Michel-Vidal-Naquet",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Vidal-Naquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michel Vidal-Naquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 149
                            }
                        ],
                        "text": "\u20262002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture, or combinations of several or all of these\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 140
                            }
                        ],
                        "text": "The component classifiers could also be combined in a more sophisticated way, for example using a rejection cascade Baker and Nayar [1996], Viola and Jones [2001], Sun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15620181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d4b0618eb4e5ebcd3f86e9948921ba9f49b77c",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that efficient object recognition can be obtained by combining informative features with linear classification. The results demonstrate the superiority of informative class-specific features, as compared with generic type features such as wavelets, for the task of object recognition. We show that information rich features can reach optimal performance with simple linear separation rules, while generic feature based classifiers require more complex classification schemes. This is significant because efficient and optimal methods have been developed for spaces that allow linear separation. To compare different strategies for feature extraction, we trained and compared classifiers working in feature spaces of the same low dimensionality, using two feature types (image fragments vs. wavelets) and two classification rules (linear hyperplane and a Bayesian network). The results show that by maximizing the individual information of the features, it is possible to obtain efficient classification by a simple linear separating rule, as well as more efficient learning."
            },
            "slug": "Object-recognition-with-informative-features-and-Vidal-Naquet-Ullman",
            "title": {
                "fragments": [],
                "text": "Object recognition with informative features and linear classification"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The results show that by maximizing the individual information of the features, it is possible to obtain efficient classification by a simple linear separating rule, as well as more efficient learning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 359,
                                "start": 116
                            }
                        ],
                        "text": "2002], image classification and matching [Mikolajczyk and Schmid 2005], and texture representation and recognition [Lazebnik et al. 2003, 2005]. This thesis builds on these recent advances and proposes a monolithic encoding of image regions based on SIFT like features for object detection within a framework similar to that of Papageorgiou and Poggio [2000], Mohan et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1381,
                                "start": 116
                            }
                        ],
                        "text": "2002], image classification and matching [Mikolajczyk and Schmid 2005], and texture representation and recognition [Lazebnik et al. 2003, 2005]. This thesis builds on these recent advances and proposes a monolithic encoding of image regions based on SIFT like features for object detection within a framework similar to that of Papageorgiou and Poggio [2000], Mohan et al. [2001]. The approach is simple and compliments bag-of-keypoints based approaches [Agarwal and Roth 2002, Fergus et al. 2003, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al. 2005] by explicitly encoding spatial information. Although it is not robust to occlusions and does not support parts based detectors as such, recent results of Everingham et al. [2006a,b] show that the new approach outperforms the feature point based methods for scenarios involving fully visible and unoccluded object classes. Also some object classes have lot of within-class variation and key point based descriptors have limited performance in this case. In particular, for humans it is unclear which key point detector to use. All the standard ones return either points in textured regions or blob like structures. On humans, texture is seen mainly on clothing so detections are not repeatable across changes of clothing. Variations in pose tend to confuse the blob based detectors. In a recent work, Leibe et al. [2005] build a profile-view pedestrian detection system using Difference of Gaussian as the key point detector and SIFT [Lowe 2004] as the image descriptor, but their test set contains only a limited range of poses and some repeatability is expected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 380,
                                "start": 116
                            }
                        ],
                        "text": "2002], image classification and matching [Mikolajczyk and Schmid 2005], and texture representation and recognition [Lazebnik et al. 2003, 2005]. This thesis builds on these recent advances and proposes a monolithic encoding of image regions based on SIFT like features for object detection within a framework similar to that of Papageorgiou and Poggio [2000], Mohan et al. [2001]. The approach is simple and compliments bag-of-keypoints based approaches [Agarwal and Roth 2002, Fergus et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16176576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26e68296c159b72185c769f37d25ff92f65ace28",
            "isKey": true,
            "numCitedBy": 90,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel texture representation suitabl e for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and non-rigid deformations. Unlike many existing feature extraction methods, which treat the neighborhood of every pixel as a candidate texture element, the proposed algorithm works by selecting a sparse set of affine-invariant local patches. This spatial selection process, besides provi ding greater computational efficiency and reducing redundancy in texton dictionaries, permits the computation of characteristic scale and neighborhood shape for every texture element. Shape information is used to determine the right support region for computing intensity-based descri ptors. When affine invariance is not required, shape can itsel f become a discriminative feature. The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a collection of photographs of textured surfaces featuring viewpoint changes and non-rigid deformations, as well as variations in illumination and appearance ."
            },
            "slug": "Sparse-Texture-Representation-Using-Neighborhoods",
            "title": {
                "fragments": [],
                "text": "Sparse Texture Representation Using Affine-Invariant Neighborhoods CVPR Paper"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a collection of photographs of textured surfaces featuring viewpoint changes and non-rigid deformations, as well as variations in illumination and appearance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4837721"
                        ],
                        "name": "J. Giebel",
                        "slug": "J.-Giebel",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Giebel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Giebel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2919518"
                        ],
                        "name": "S. Munder",
                        "slug": "S.-Munder",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Munder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Munder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1045253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77aaf61f2bd6e21bbd5abed7427e34116f52ed3d",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results of the first large-scale field tests on vision-based pedestrian protection from a moving vehicle. Our PROTECTOR system combines pedestrian detection, trajectory estimation, risk assessment and driver warning. The paper pursues a \"system approach\" related to the detection component. An optimization scheme models the system as a succession of individual modules and finds a good overall parameter setting by combining individual ROCs using a convex-hull technique. On the experimental side, we present a methodology for the validation of the pedestrian detection performance in an actual vehicle setting. We hope this test methodology to contribute towards the establishment of benchmark testing, enabling this application to mature. We validate the PROTECTOR system using the proposed methodology and present interesting quantitative results based on tens of thousands of images from hours of driving. Although results are promising, more research is needed before such systems can be placed at the hands of ordinary vehicle drivers."
            },
            "slug": "Vision-based-pedestrian-detection:-the-PROTECTOR-Gavrila-Giebel",
            "title": {
                "fragments": [],
                "text": "Vision-based pedestrian detection: the PROTECTOR system"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The PROTECTOR system combines pedestrian detection, trajectory estimation, risk assessment and driver warning, and an optimization scheme models the system as a succession of individual modules and finds a good overall parameter setting by combining individual ROCs using a convex-hull technique."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intelligent Vehicles Symposium, 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 138
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12209481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3565c5a65842f26091578b9d71d496cc1561239d",
            "isKey": false,
            "numCitedBy": 1292,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints."
            },
            "slug": "A-statistical-method-for-3D-object-detection-to-and-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "A statistical method for 3D object detection applied to faces and cars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using this method, this work has developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithms thatCan reliably detect passenger cars over a wide range of viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3841331"
                        ],
                        "name": "Moray Allan",
                        "slug": "Moray-Allan",
                        "structuredName": {
                            "firstName": "Moray",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moray Allan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891864"
                        ],
                        "name": "Gyuri Dork\u00f3",
                        "slug": "Gyuri-Dork\u00f3",
                        "structuredName": {
                            "firstName": "Gyuri",
                            "lastName": "Dork\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyuri Dork\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762557"
                        ],
                        "name": "S. Duffner",
                        "slug": "S.-Duffner",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Duffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Duffner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059148801"
                        ],
                        "name": "J. Eichhorn",
                        "slug": "J.-Eichhorn",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Eichhorn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Eichhorn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48173155"
                        ],
                        "name": "Jason D. R. Farquhar",
                        "slug": "Jason-D.-R.-Farquhar",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Farquhar",
                            "middleNames": [
                                "D.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. R. Farquhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739548"
                        ],
                        "name": "Mario Fritz",
                        "slug": "Mario-Fritz",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Fritz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mario Fritz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723337"
                        ],
                        "name": "Christophe Garcia",
                        "slug": "Christophe-Garcia",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758971"
                        ],
                        "name": "M. Koskela",
                        "slug": "M.-Koskela",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Koskela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Koskela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708642"
                        ],
                        "name": "Jorma T. Laaksonen",
                        "slug": "Jorma-T.-Laaksonen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Laaksonen",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorma T. Laaksonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295553"
                        ],
                        "name": "Diane Larlus",
                        "slug": "Diane-Larlus",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Larlus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diane Larlus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37192632"
                        ],
                        "name": "H. Meng",
                        "slug": "H.-Meng",
                        "structuredName": {
                            "firstName": "Hongying",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083209039"
                        ],
                        "name": "Edgar Seemann",
                        "slug": "Edgar-Seemann",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Seemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Seemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728216"
                        ],
                        "name": "A. Storkey",
                        "slug": "A.-Storkey",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Storkey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Storkey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540580"
                        ],
                        "name": "S. Szedm\u00e1k",
                        "slug": "S.-Szedm\u00e1k",
                        "structuredName": {
                            "firstName": "S\u00e1ndor",
                            "lastName": "Szedm\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szedm\u00e1k"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145150531"
                        ],
                        "name": "ilkay Ulusoy",
                        "slug": "ilkay-Ulusoy",
                        "structuredName": {
                            "firstName": "ilkay",
                            "lastName": "Ulusoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ilkay Ulusoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279074"
                        ],
                        "name": "Ville Viitaniemi",
                        "slug": "Ville-Viitaniemi",
                        "structuredName": {
                            "firstName": "Ville",
                            "lastName": "Viitaniemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ville Viitaniemi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2875887"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2078231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb0ab2ee44ebf9c08bd2bf478c7444adfdcb2bd7",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes Challenge ran from February to March 2005. The goal of the challenge was to recognize objects from a number of visual object classes in realistic scenes (i.e. not pre-segmented objects). Four object classes were selected: motorbikes, bicycles, cars and people. Twelve teams entered the challenge. In this chapter we provide details of the datasets, algorithms used by the teams, evaluation criteria, and results achieved."
            },
            "slug": "The-2005-PASCAL-Visual-Object-Classes-Challenge-Everingham-Zisserman",
            "title": {
                "fragments": [],
                "text": "The 2005 PASCAL Visual Object Classes Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This chapter provides details of the datasets, algorithms used by the teams, evaluation criteria, and results achieved in the PASCAL Visual Object Classes Challenge."
            },
            "venue": {
                "fragments": [],
                "text": "MLCW"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145349582"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6958332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83bf1e6d239dd5bc1b8f7499f7241a8802a43e22",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a two-layer hierarchical formulation to exploit different levels of contextual information in images for robust classification. Each layer is modeled as a conditional field that allows one to capture arbitrary observation-dependent label interactions. The proposed framework has two main advantages. First, it encodes both the short-range interactions (e.g., pixelwise label smoothing) as well as the long-range interactions (e.g., relative configurations of objects or regions) in a tractable manner. Second, the formulation is general enough to be applied to different domains ranging from pixelwise image labeling to contextual object detection. The parameters of the model are learned using a sequential maximum-likelihood approximation. The benefits of the proposed framework are demonstrated on four different datasets and comparison results are presented"
            },
            "slug": "A-hierarchical-field-framework-for-unified-Kumar-Hebert",
            "title": {
                "fragments": [],
                "text": "A hierarchical field framework for unified context-based classification"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A two-layer hierarchical formulation to exploit different levels of contextual information in images for robust classification and is general enough to be applied to different domains ranging from pixelwise image labeling to contextual object detection."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11350229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c778b994e49421aa21361ac548c49fcc46a9a8f3",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Given two or more video sequences containing similar human activities (running, jumping, etc.), we want to devise a method which extracts spatio-temporal signatures associated with these activities, compares these signatures, and aligns key positions from different videos. We introduce a method which, in conjunction with a number of hypotheses, allows the analysis of the motion of specific body parts and extracts their 2D (image plane) time-varying trajectories as well as their 3D trajectories. Two such trajectories recovered from two different videos have different characteristics. We develop a curve registration technique which consists of estimating a transformation which maps one time-basis (of the first curve) onto another time-basis (the second curve). We also analyse in depth the conditions under which such curve registration techniques are valid. Finally, we show results with two similar athletic events performed by two different athletes."
            },
            "slug": "Indexing-key-positions-between-multiple-videos-Dalal-Horaud",
            "title": {
                "fragments": [],
                "text": "Indexing key positions between multiple videos"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method is introduced which allows the analysis of the motion of specific body parts and extracts their 2D time-varying trajectories as well as their 3D trajectories and develops a curve registration technique which consists of estimating a transformation which maps one time-basis (of the first curve) onto another time- Basis (the second curve)."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Motion and Video Computing, 2002. Proceedings."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059594591"
                        ],
                        "name": "Michal Roth",
                        "slug": "Michal-Roth",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 53
                            }
                        ],
                        "text": "They are reminiscent of edge orientation histograms [Freeman and Roth 1995, Freeman et al. 1996], SIFT descriptors [Lowe 2004] and shape contexts [Belongie et al. 2001], but they are computed on a dense grid of uniformly spaced cells and they use overlapping descriptors for improved performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13146480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a63c0ae8cb411040a29ad85f2d009a17bf5a9a2",
            "isKey": false,
            "numCitedBy": 614,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture class cation and interpolation. This method is simple and fast to compute, and o ers some robustness to scene illumination changes. We have implemented a real-time version, which can distinguish a small vocabulary of about 10 di erent hand gestures. All the computation occurs on a workstation; special hardware is used only to digitize the image. A user can operate a computer graphic crane under hand gesture control, or play a game. We discuss limitations of this method. For moving or \\dynamic gestures\", the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition. Reprinted from: IEEE Intl. Wkshp. on Automatic Face and Gesture Recognition, Zurich, June,"
            },
            "slug": "Orientation-Histograms-for-Hand-Gesture-Recognition-Freeman-Roth",
            "title": {
                "fragments": [],
                "text": "Orientation Histograms for Hand Gesture Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A method to recognize hand gestures, based on a pattern recognition technique developed by McConnell employing histograms of local orientation, which is simple and fast to compute, and which can distinguish a small vocabulary of about 10 hand gestures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685300"
                        ],
                        "name": "J. Hespanha",
                        "slug": "J.-Hespanha",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Hespanha",
                            "middleNames": [
                                "Pedro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hespanha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be86da00efdd8c2a7fdeb2334605796c24b370f0",
            "isKey": false,
            "numCitedBy": 11723,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases."
            },
            "slug": "Eigenfaces-vs.-Fisherfaces:-Recognition-Using-Class-Belhumeur-Hespanha",
            "title": {
                "fragments": [],
                "text": "Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A face recognition algorithm which is insensitive to large variation in lighting direction and facial expression is developed, based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variations in lighting and facial expressions."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084178"
                        ],
                        "name": "B. Sigelman",
                        "slug": "B.-Sigelman",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Sigelman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sigelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 215
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 88
                            }
                        ],
                        "text": "One of the primary works using simple image intensities is the \u201ceigenfaces\u201d approach of Sirovitch and Kirby [1987] and Turk and Pentland [1991], where the pixels of fixed-resolution face images are rearranged to forms large feature vector and Principal Component Analysis (PCA) is used to characterise the main variations of the ensemble of face vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2071938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c8c0b0c5ba3e99889170184300da6f3dae1b79f",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuous-valued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter."
            },
            "slug": "Attractive-People:-Assembling-Loose-Limbed-Models-Sigal-Isard",
            "title": {
                "fragments": [],
                "text": "Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work represents the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions, and exploits a recently introduced generalization of the particle filter to approximate belief propagation in such a graph."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145349582"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10689850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e54f01884e1fba4a0bbd2f0989ad21a16ebb13e3",
            "isKey": false,
            "numCitedBy": 530,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present discriminative random fields (DRFs), a discriminative framework for the classification of image regions by incorporating neighborhood interactions in the labels as well as the observed data. The discriminative random fields offer several advantages over the conventional Markov random field (MRF) framework. First, the DRFs allow to relax the strong assumption of conditional independence of the observed data generally used in the MRF framework for tractability. This assumption is too restrictive for a large number of applications in vision. Second, the DRFs derive their classification power by exploiting the probabilistic discriminative models instead of the generative models used in the MRF framework. Finally, all the parameters in the DRF model are estimated simultaneously from the training data unlike the MRF framework where likelihood parameters are usually learned separately from the field parameters. We illustrate the advantages of the DRFs over the MRF framework in an application of man-made structure detection in natural images taken from the Corel database."
            },
            "slug": "Discriminative-random-fields:-a-discriminative-for-Kumar-Hebert",
            "title": {
                "fragments": [],
                "text": "Discriminative random fields: a discriminative framework for contextual interaction in classification"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This work presents discriminative random fields (DRFs), a discrim inative framework for the classification of image regions by incorporating neighborhood interactions in the labels as well as the observed data that offers several advantages over the conventional Markov random field framework."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1661,
                                "start": 6
                            }
                        ],
                        "text": "2004, Leibe et al. 2005, Mikolajczyk et al. 2005]. These approaches extract local image features at a sparse set of salient image points \u2013 usually called points of interest or key points. The final detectors are then based on feature vectors computed from these key point descriptors. The hypothesis is that key point detectors select stable and more reliable image regions, which are especially informative about local image content. The overall detector performance thus depends on the reliability, accuracy and repeatability with which these key points can be found for the given object class and the informativeness of the points chosen. Commonly used key point detectors include F\u00f6rstnerHarris [F\u00f6rstner and Pertl 1986, F\u00f6rstner and G\u00fclch 1987, Harris and Stephens 1988], Laplacian [Lindeberg 1998] or Difference of Gaussians (DoGs) [Lowe 2004], and scale invariant HarrisLaplace [Mikolajczyk and Schmid 2004]. Some point detectors such as DoGs or Harris-Laplace also provide additional local scale and/or dominant orientation information. One advantage of sparse key point based approaches is the compactness of the representation: there are many fewer key point descriptors than image pixels, so the latter stages of the classification process are speeded up. However note that most key point detectors are designed to fire repeatedly on particular objects and may have limitations when generalising to object classes or categories, i.e. they may not be repeatable for general object classes. Depending on the object class, most approaches can use any one of these key point detectors or combinations of several of them. For example Fergus et al. [2003] used the entropy based region detector of Kadir and Brady [2001] for object representation, Weber et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206763997,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "72bf4b2ce534b95bc24118491dbc4f8d550734a2",
            "isKey": false,
            "numCitedBy": 1158,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a texture representation suitable for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and nonrigid deformations. At the feature extraction stage, a sparse set of affine Harris and Laplacian regions is found in the image. Each of these regions can be thought of as a texture element having a characteristic elliptic shape and a distinctive appearance pattern. This pattern is captured in an affine-invariant fashion via a process of shape normalization followed by the computation of two novel descriptors, the spin image and the RIFT descriptor. When affine invariance is not required, the original elliptical shape serves as an additional discriminative feature for texture recognition. The proposed approach is evaluated in retrieval and classification tasks using the entire Brodatz database and a publicly available collection of 1,000 photographs of textured surfaces taken from different viewpoints."
            },
            "slug": "A-sparse-texture-representation-using-local-affine-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "A sparse texture representation using local affine regions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a publicly available collection of 1,000 photographs of textured surfaces taken from different viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 114
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18092381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90cf693b3713777e6fd1c24bd76c96c6d72123be",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use a sampling method to find sparsely clad people in static images. People are modeled as an assembly of nine cylindrical segments. Segments are found using an EM algorithm and then assembled into hypotheses incrementally, using a learned likelihood model. Each assembly step passes on a set of samples of its likelihood to the next; this yields effective pruning of the space of hypotheses. The collection of available nine-segment hypotheses is then represented by a set of equivalence classes, which yield an efficient pruning process. The posterior for the number of people is obtained from the class representatives. People are counted quite accurately in images of real scenes using a MAP estimate. We show the method allows top-down as well as bottom up reasoning. While the method can be overwhelmed by very large numbers of segments, we show that this problem can be avoided by quite simple pruning steps."
            },
            "slug": "Finding-people-by-sampling-Ioffe-Forsyth",
            "title": {
                "fragments": [],
                "text": "Finding people by sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown how to use a sampling method to find sparsely clad people in static images using an EM algorithm and a learned likelihood model, which allows top-down as well as bottom up reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38011631"
                        ],
                        "name": "B. Galvin",
                        "slug": "B.-Galvin",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Galvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Galvin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716758"
                        ],
                        "name": "B. McCane",
                        "slug": "B.-McCane",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "McCane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. McCane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169548"
                        ],
                        "name": "K. Novins",
                        "slug": "K.-Novins",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Novins",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Novins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054088172"
                        ],
                        "name": "David Mason",
                        "slug": "David-Mason",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Mason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144569016"
                        ],
                        "name": "S. Mills",
                        "slug": "S.-Mills",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Mills",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mills"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1162319,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "6add4ca7b133ca763023b98886865fa1a8efd833",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluating the performance of optical flow algorithms has been difficult because of the lack of ground-truth data sets for complex scenes. We describe a simple modification to a ray tracer that allows us to generate ground-truth motion fields for scenes of arbitrary complexity. The resulting flow maps are used to assist in the comparison of eight optical flow algorithms using three complex, synthetic scenes. Our study found that a modified version of Lucas and Kanade\u2019s algorithm has superior performance but produces sparse flow maps. Proesmans et al.\u2019s algorithm performs slightly worse, on average, but produces a very dense depth map."
            },
            "slug": "Recovering-Motion-Fields:-An-Evaluation-of-Eight-Galvin-McCane",
            "title": {
                "fragments": [],
                "text": "Recovering Motion Fields: An Evaluation of Eight Optical Flow Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This study found that a modified version of Lucas and Kanade's algorithm has superior performance but produces sparse flow maps, while Proesmans et al."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6585372,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2c8c97918a24c26da7c949a7d6aaf3201d9d9cf9",
            "isKey": false,
            "numCitedBy": 4599,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalizing-the-Hough-transform-to-detect-shapes-Ballard",
            "title": {
                "fragments": [],
                "text": "Generalizing the Hough transform to detect arbitrary shapes"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685020"
                        ],
                        "name": "D. Comaniciu",
                        "slug": "D.-Comaniciu",
                        "structuredName": {
                            "firstName": "Dorin",
                            "lastName": "Comaniciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Comaniciu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776090"
                        ],
                        "name": "P. Meer",
                        "slug": "P.-Meer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Meer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Meer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 691081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74f4ecc3e4e5b91fbb54330b285ed5214afe2001",
            "isKey": false,
            "numCitedBy": 11481,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance."
            },
            "slug": "Mean-Shift:-A-Robust-Approach-Toward-Feature-Space-Comaniciu-Meer",
            "title": {
                "fragments": [],
                "text": "Mean Shift: A Robust Approach Toward Feature Space Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 244
                            }
                        ],
                        "text": "Commonly used key point detectors include Fo\u0308rstnerHarris [Fo\u0308rstner and Pertl 1986, Fo\u0308rstner and Gu\u0308lch 1987, Harris and Stephens 1988], Laplacian [Lindeberg 1998] or Difference of Gaussians (DoGs) [Lowe 2004], and scale invariant HarrisLaplace [Mikolajczyk and Schmid 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1704741,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8b440596b28dc6683caa2b5f6fbca70963e5909e",
            "isKey": false,
            "numCitedBy": 4161,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix.Our scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. The characteristic scale determines a scale invariant region for each point. We extend the scale invariant detector to affine invariance by estimating the affine shape of a point neighborhood. An iterative algorithm modifies location, scale and neighborhood of each point and converges to affine invariant points. This method can deal with significant affine transformations including large scale changes. The characteristic scale and the affine shape of neighborhood determine an affine invariant region for each point.We present a comparative evaluation of different detectors and show that our approach provides better results than existing methods. The performance of our detector is also confirmed by excellent matching results; the image is described by a set of scale/affine invariant descriptors computed on the regions associated with our points."
            },
            "slug": "Scale-&-Affine-Invariant-Interest-Point-Detectors-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "Scale & Affine Invariant Interest Point Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A comparative evaluation of different detectors is presented and it is shown that the proposed approach for detecting interest points invariant to scale and affine transformations provides better results than existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16193920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d2bfef811f514391aa2a7b8f4020d1c9e033016",
            "isKey": false,
            "numCitedBy": 3604,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner.<<ETX>>"
            },
            "slug": "Face-recognition-using-eigenfaces-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Face recognition using eigenfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8571961,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9c7a96155f10f152cae0866102c061cdf6da02e8",
            "isKey": false,
            "numCitedBy": 1683,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas : 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images."
            },
            "slug": "An-Affine-Invariant-Interest-Point-Detector-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "An Affine Invariant Interest Point Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A novel approach for detecting affine invariant interest points that can deal with significant affine transformations including large scale changes and shows an excellent performance in the presence of large perspective transformations including significant scale changes."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Rowley et al. [1998] proposed a heuristic method for fusing overlapping detections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 473,
                                "start": 29
                            }
                        ],
                        "text": "Heuristic fusion approaches [Rowley et al. 1998, Schneiderman and Kanade 2004, Viola and Jones 2004, Intel OpenCV 2006] will not work for these cases. For example, Viola and Jones [2004] partitions the set of detections are into disjoint subsets, each subset providing a single final detection. As the training windows are larger than the actual person in our person data set, some detections may be overlapping and classified as single detection by Viola and Jones [2004] method even though the images contain two people occurring at very different scales (one detection occurring in another detection)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 29
                            }
                        ],
                        "text": "Heuristic fusion approaches [Rowley et al. 1998, Schneiderman and Kanade 2004, Viola and Jones 2004, Intel OpenCV 2006] will not work for these cases. For example, Viola and Jones [2004] partitions the set of detections are into disjoint subsets, each subset providing a single final detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 68
                            }
                        ],
                        "text": "Another work using intensity images is the face detection system of Rowley et al. [1998] who locally correct the lighting of the images by performing histogram equalisation before passing them to a neural network classifier [Bishop 1995] for face/non-face detections. Ullman et al. [2001], Vidal-Naquet and Ullman [2003] use simple object fragments based on image intensity patterns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 68
                            }
                        ],
                        "text": "Another work using intensity images is the face detection system of Rowley et al. [1998] who locally correct the lighting of the images by performing histogram equalisation before passing them to a neural network classifier [Bishop 1995] for face/non-face detections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40120983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3885c13438132b516e5ffc8b640d20b4e41a7a4",
            "isKey": true,
            "numCitedBy": 4156,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Neural-network-based-face-detection-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Neural network-based face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that arbitrates between multiple networks to improve performance over a single network using a bootstrap algorithm, which eliminates the difficult task of manually selecting non-face training examples."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 84
                            }
                        ],
                        "text": "The use of salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18689886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a30589d274c2867425ead17780a0d22c69fc672",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a technique to determine the identity of objects in a scene using histograms of the responses of a vector of local linear neighborhood operators (receptive fields). This technique can be used to determine the most probable objects in a scene, independent of the object's position, image-plane orientation and scale. In this paper we describe the mathematical foundations of the technique and present the results of experiments which compare robustness and recognition rates for different local neighborhood operators and histogram similarity measurements."
            },
            "slug": "Object-Recognition-Using-Multidimensional-Receptive-Schiele-Crowley",
            "title": {
                "fragments": [],
                "text": "Object Recognition Using Multidimensional Receptive Field Histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The mathematical foundations of the technique are described and the results of experiments which compare robustness and recognition rates for different local neighborhood operators and histogram similarity measurements are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891864"
                        ],
                        "name": "Gyuri Dork\u00f3",
                        "slug": "Gyuri-Dork\u00f3",
                        "structuredName": {
                            "firstName": "Gyuri",
                            "lastName": "Dork\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyuri Dork\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 65
                            }
                        ],
                        "text": "These descriptors perform remarkably well in pattern recognition [Schmid et al. 2005], object detection [Opelt et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2128680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2981465359a1acb7d1a2881a63c520b5b99aa6a2",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Local invariant features have shown to be very successful for recognition. They are robust to occlusion and clutter, distinctive as well as invariant to image transformations. In this chapter recent progress on local invariant features is summarized. It is explained how to extract scale and affine-invariant regions and how to obtain discriminant descriptors for these regions. It is then demonstrated that combining local features with pattern classification techniques allows for texture and category-level object recognition in the presence of varying viewpoints and background clutter."
            },
            "slug": "Pattern-recognition-with-local-invariant-features-Schmid-Dork\u00f3",
            "title": {
                "fragments": [],
                "text": "Pattern recognition with local invariant features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that combining local features with pattern classification techniques allows for texture and category-level object recognition in the presence of varying viewpoints and background clutter."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6105822"
                        ],
                        "name": "Pamela Reinagel",
                        "slug": "Pamela-Reinagel",
                        "structuredName": {
                            "firstName": "Pamela",
                            "lastName": "Reinagel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pamela Reinagel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3199565"
                        ],
                        "name": "A. Zador",
                        "slug": "A.-Zador",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Zador",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zador"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Rowley et al. [1998], Papageorgiou and Poggio [2000], Mohan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 273
                            }
                        ],
                        "text": "\u2026regions with corners and multiple superimposed orientations [Zetzsche et al. 1998, Barth et al. 1998], and that local spatial contrast is significantly higher at these points than at random locations, while image uniformity and pixel correlations are significantly lower [Reinagel and Zador 1999]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1489243,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "559c26761e288404911b333326cb19a8a5f768ff",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Early stages of visual processing may exploit the characteristic structure of natural visual stimuli. This structure may differ from the intrinsic structure of natural scenes, because sampling of the environment is an active process. For example, humans move their eyes several times a second when looking at a scene. The portions of a scene that fall on the fovea are sampled at high spatial resolution, and receive a disproportionate fraction of cortical processing. We recorded the eye positions of human subjects while they viewed images of natural scenes. We report that active selection affected the statistics of the stimuli encountered by the fovea, and also by the parafovea up to eccentricities of 4 degrees. We found two related effects. First, subjects looked at image regions that had high spatial contrast. Second, in these regions, the intensities of nearby image points (pixels) were less correlated with each other than in images selected at random. These effects could serve to increase the information available to the visual system for further processing. We show that both of these effects can be simply obtained by constructing an artificial ensemble comprised of the highest-contrast regions of images."
            },
            "slug": "Natural-scene-statistics-at-the-centre-of-gaze.-Reinagel-Zador",
            "title": {
                "fragments": [],
                "text": "Natural scene statistics at the centre of gaze."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that active selection affected the statistics of the stimuli encountered by the fovea, and also by the parafovea up to eccentricities of 4 degrees, and that both effects can be simply obtained by constructing an artificial ensemble comprised of the highest-contrast regions of images."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29905643"
                        ],
                        "name": "F. Porikli",
                        "slug": "F.-Porikli",
                        "structuredName": {
                            "firstName": "Fatih",
                            "lastName": "Porikli",
                            "middleNames": [
                                "Murat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Porikli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 692,
                                "start": 160
                            }
                        ],
                        "text": "The centre-surround HOG architecture avoids these block specific computations, which allows to be optimised for very rapid computation via integral histograms [Porikli 2005]. Centre-Surround HOGs implement an alternative centre-surround style cell normalisation scheme. They create \u03b2 \u201corientation images\u201d, where \u03b2 is the number of orientation bins. The image gradient orientation at each pixel is soft quantised into \u03b2 bins using linear interpolation and weighted using gradient magnitude and stored in the corresponding orientation image. The set of \u03b2 orientation images is analogous to the oriented histogram in R-HOG or C-HOG. A similar architecture has been proposed by Zhu et al. [2006], who use integral histograms [Porikli 2005] to rapidly tile the orientation images without linear interpolation voting with a grid of cells of any desired dimension."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1122429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbf4d36e787e2e5c8444e1a2229b821e9cd68adf",
            "isKey": false,
            "numCitedBy": 810,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel method, which we refer as an integral histogram, to compute the histograms of all possible target regions in a Cartesian data space. Our method has three distinct advantages: 1) It is computationally superior to the conventional approach. The integral histogram method makes it possible to employ even an exhaustive search process in real-time, which was impractical before. 2) It can be extended to higher data dimensions, uniform and nonuniform bin formations, and multiple target scales without sacrificing its computational advantages. 3) It enables the description of higher level histogram features. We exploit the spatial arrangement of data points, and recursively propagate an aggregated histogram by starting from the origin and traversing through the remaining points along either a scan-line or a wave-front. At each step, we update a single bin using the values of integral histogram at the previously visited neighboring data points. After the integral histogram is propagated, histogram of any target region can be computed easily by using simple arithmetic operations."
            },
            "slug": "Integral-histogram:-a-fast-way-to-extract-in-spaces-Porikli",
            "title": {
                "fragments": [],
                "text": "Integral histogram: a fast way to extract histograms in Cartesian spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The integral histogram method makes it possible to employ even an exhaustive search process in real-time, which was impractical before, and enables the description of higher level histogram features."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7788290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4e4b41b6010ac1e6c90791168f57bcd75b696ab",
            "isKey": false,
            "numCitedBy": 2210,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. Because of many potentially important applications, \u201clooking at people\u201d is currently one of the most active application domains in computer vision. This survey identifies a number of promising applications and provides an overview of recent developments in this domain. The scope of this survey is limited to work on whole-body or hand motion; it does not include work on human faces. The emphasis is on discussing the various methodologies; they are grouped in 2-D approaches with or without explicit shape models and 3-D approaches. Where appropriate, systems are reviewed. We conclude with some thoughts about future directions."
            },
            "slug": "The-Visual-Analysis-of-Human-Movement:-A-Survey-Gavrila",
            "title": {
                "fragments": [],
                "text": "The Visual Analysis of Human Movement: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A number of promising applications are identified and an overview of recent developments in this domain is provided, including work on whole-body or hand motion and the various methodologies."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685020"
                        ],
                        "name": "D. Comaniciu",
                        "slug": "D.-Comaniciu",
                        "structuredName": {
                            "firstName": "Dorin",
                            "lastName": "Comaniciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Comaniciu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "Basically, negative scores from the linear SVM are zeroed and a 3D position-scale mean shift process [Comaniciu 2003b] is run to identify significant local peaks in the resulting score."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36598725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39402ae6ab66b97f1195db05ea1b576073196916",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of information fusion appears in many forms in vision. Tasks such as motion estimation, multimodal registration, tracking, and robot localization, often require the synergy of estimates coming from multiple sources. Most of the fusion algorithms, however, assume a single source model and are not robust to outliers. If the data to be fused follow different underlying models, the traditional algorithms would produce poor estimates. We present in this paper a nonparametric approach to information fusion called variable-bandwidth density-based fusion (VBDF). The fusion estimator is computed as the location of the most significant mode of a density function, which takes into account the uncertainty of the estimates to be fused. A mode detection scheme is presented, which relies on variable-bandwidth mean shift computed at multiple scales. We show that the proposed estimator is consistent and conservative, while handling naturally outliers in the data and multiple source models. The new theory is tested for the task of multiple motion estimation. Numerous experiments validate the theory and provide very competitive results."
            },
            "slug": "Nonparametric-information-fusion-for-motion-Comaniciu",
            "title": {
                "fragments": [],
                "text": "Nonparametric information fusion for motion estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a nonparametric approach to information fusion called variable-bandwidth density-based fusion (VBDF), where the fusion estimator is computed as the location of the most significant mode of a density function, which takes into account the uncertainty of the estimates to be fused."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814637"
                        ],
                        "name": "Berthold K. P. Horn",
                        "slug": "Berthold-K.-P.-Horn",
                        "structuredName": {
                            "firstName": "Berthold",
                            "lastName": "Horn",
                            "middleNames": [
                                "K.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berthold K. P. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717435"
                        ],
                        "name": "B. G. Schunck",
                        "slug": "B.-G.-Schunck",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Schunck",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. G. Schunck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1371968,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a3229dc33ecb80c59a75b906c46b586dd059b781",
            "isKey": false,
            "numCitedBy": 11343,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image."
            },
            "slug": "Determining-Optical-Flow-Horn-Schunck",
            "title": {
                "fragments": [],
                "text": "Determining Optical Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences and is robust in that it can handle image sequences that are quantified rather coarsely in space and time."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144603445"
                        ],
                        "name": "Tom\u00e1s Rodr\u00edguez",
                        "slug": "Tom\u00e1s-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1s Rodr\u00edguez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32440785"
                        ],
                        "name": "M. G\u00f6tz",
                        "slug": "M.-G\u00f6tz",
                        "structuredName": {
                            "firstName": "Marcelo",
                            "lastName": "G\u00f6tz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. G\u00f6tz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8445839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f75c7d1ada6ccbc5a7097ef6a3dc0ccd5d62103",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "View interpolation has been explored in the scientific community as a means to avoid the complexity of full 3D in the construction of photo-realistic interactive scenarios. EVENTS project attempts to apply state of the art view interpolation to the field of professional sports. The aim is to populate a wide scenario such as a stadium with a number of cameras and, via computer vision, to produce photo-realistic moving or static images from virtual viewpoints, i.e where there is no physical camera.EVENTS proposes an innovative view interpolation scheme based on the Joint View Triangulation algorithm developed by the project participants. Joint View Triangulation is combined within the EVENTS framework with new initiatives in the field of multiple view layered representation, automatic seed matching, image-based rendering, tracking occluding layers and constrained scene analysis. The computer vision software has been implemented on top of a novel high performance computing platform with the aim to achieve real-time interpolation."
            },
            "slug": "Image-interpolation-for-virtual-sports-scenarios-Rodr\u00edguez-Reid",
            "title": {
                "fragments": [],
                "text": "Image interpolation for virtual sports scenarios"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The EVENTS project attempts to apply state of the art view interpolation to the field of professional sports to populate a wide scenario such as a stadium with a number of cameras and, via computer vision, to produce photo-realistic moving or static images from virtual viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1915667"
                        ],
                        "name": "Y. Gdalyahu",
                        "slug": "Y.-Gdalyahu",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Gdalyahu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gdalyahu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079194"
                        ],
                        "name": "G. Hayun",
                        "slug": "G.-Hayun",
                        "structuredName": {
                            "firstName": "Gaby",
                            "lastName": "Hayun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hayun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14981509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37a15ce03c26ec83d95bf4aaf756a41370d50353",
            "isKey": false,
            "numCitedBy": 404,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the functional and architectural breakdown of a monocular pedestrian detection system. We describe in detail our approach for single-frame classification based on a novel scheme of breaking down the class variability by repeatedly training a set of relatively simple classifiers on clusters of the training set. Single-frame classification performance results and system level performance figures for daytime conditions are presented with a discussion about the remaining gap to meet a daytime normal weather condition production system."
            },
            "slug": "Pedestrian-detection-for-driving-assistance-and-Shashua-Gdalyahu",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection for driving assistance systems: single-frame classification and system level performance"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The functional and architectural breakdown of a monocular pedestrian detection system is described and the approach for single-frame classification based on a novel scheme of breaking down the class variability by repeatedly training a set of relatively simple classifiers on clusters of the training set is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intelligent Vehicles Symposium, 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685020"
                        ],
                        "name": "D. Comaniciu",
                        "slug": "D.-Comaniciu",
                        "structuredName": {
                            "firstName": "Dorin",
                            "lastName": "Comaniciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Comaniciu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320582"
                        ],
                        "name": "Visvanathan Ramesh",
                        "slug": "Visvanathan-Ramesh",
                        "structuredName": {
                            "firstName": "Visvanathan",
                            "lastName": "Ramesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Visvanathan Ramesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776090"
                        ],
                        "name": "P. Meer",
                        "slug": "P.-Meer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Meer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Meer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14661676,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a67e0c5b11caceafc0002b0aaff8d7db21eb3312",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two solutions for the scale selection problem in computer vision. The first one is completely nonparametric and is based on the the adaptive estimation of the normalized density gradient. Employing the sample point estimator, we define the Variable Bandwidth Mean Shift, prove its convergence, and show its superiority over the fixed bandwidth procedure. The second technique has a semiparametric nature and imposes a local structure on the data to extract reliable scale information. The local scale of the underlying density is taken as the bandwidth which maximizes the magnitude of the normalized mean shift vector. Both estimators provide practical tools for autonomous image and quasi real-time video analysis and several examples are shown to illustrate their effectiveness."
            },
            "slug": "The-variable-bandwidth-mean-shift-and-data-driven-Comaniciu-Ramesh",
            "title": {
                "fragments": [],
                "text": "The variable bandwidth mean shift and data-driven scale selection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The sample point estimator is defined, prove its convergence, and show its superiority over the fixed bandwidth procedure, and an alternative approach for data-driven scale selection which imposes a local structure on the data is studied."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815078"
                        ],
                        "name": "S. Avidan",
                        "slug": "S.-Avidan",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Avidan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Avidan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39525950"
                        ],
                        "name": "Moshe Butman",
                        "slug": "Moshe-Butman",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Butman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moshe Butman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11400571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "406661f2fbf6c6cb4f2b98ab1d8b58aef0ffb3e0",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Alice would like to detect faces in a collection of sensitive surveillance images she own. Bob has a face detection algorithm that he is willing to let Alice use, for a fee, as long as she learns nothing about his detector. Alice is willing to use Bob's detector provided that he will learn nothing about her images, not even the result of the face detection operation. Blind vision is about applying secure multi-party techniques to vision algorithms so that Bob will learn nothing about the images he operates on, not even the result of his own operation and Alice will learn nothing about the detector. The proliferation of surveillance cameras raises privacy concerns that can be addressed by secure multi-party techniques and their adaptation to vision algorithms."
            },
            "slug": "Blind-Vision-Avidan-Butman",
            "title": {
                "fragments": [],
                "text": "Blind Vision"
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121009"
                        ],
                        "name": "J. Puzicha",
                        "slug": "J.-Puzicha",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Puzicha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Puzicha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 147
                            }
                        ],
                        "text": "They are reminiscent of edge orientation histograms [Freeman and Roth 1995, Freeman et al. 1996], SIFT descriptors [Lowe 2004] and shape contexts [Belongie et al. 2001], but they are computed on a dense grid of uniformly spaced cells and they use overlapping descriptors for improved performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 171
                            }
                        ],
                        "text": "Currently the most popular approaches are image gradient based descriptors such as the Scale Invariant Feature Transformation (SIFT) [Lowe 1999, 2004] and shape contexts [Belongie et al. 2001, 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 268
                            }
                        ],
                        "text": "\u2026to salient region detection are usually tuned to fire on blob like structures (one such detector is the Laplacian of Gaussians) while their feature vector computations are tuned to encode gradient or contour information (e.g. using SIFT [Lowe 2004] or shape context [Belongie et al. 2001])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8446909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "500db68171e4a961d7fa87b8020b3a3e62133caf",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by (1) solving for correspondences between points on the two shapes, (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. Dis-similarity between two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework. Results are presented for silhouettes, trademarks, handwritten digits and the COIL dataset."
            },
            "slug": "Matching-shapes-Belongie-Malik",
            "title": {
                "fragments": [],
                "text": "Matching shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A novel approach to measuring similarity between shapes and exploiting it for object recognition in a nearest-neighbor classification framework that applies regularized thin-plate splines to the transformation maps for this purpose."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10716734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc1b4ca121fef59517f24863b113bce3e5acd1a",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem we consider in this paper is to take a single two-dimensional image containing a human body, locate the joint positions, and use these to estimate the body configuration and pose in three-dimensional space. The basic approach is to store a number of exemplar 2D views of the human body in a variety of different configurations and viewpoints with respect to the camera. On each of these stored views, the locations of the body joints (left elbow, right knee, etc.) are manually marked and labelled for future use. The test shape is then matched to each stored view, using the technique of shape context matching in conjunction with a kinematic chain-based deformation model. Assuming that there is a stored view sufficiently similar in configuration and pose, the correspondence process will succeed. The locations of the body joints are then transferred from the exemplar view to the test shape. Given the joint locations, the 3D body configuration and pose are then estimated. We can apply this technique to video by treating each frame independently - tracking just becomes repeated recognition! We present results on a variety of datasets."
            },
            "slug": "Estimating-Human-Body-Configurations-Using-Shape-Mori-Malik",
            "title": {
                "fragments": [],
                "text": "Estimating Human Body Configurations Using Shape Context Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The problem is to take a single two-dimensional image containing a human body, locate the joint positions, and use these to estimate the body configuration and pose in three-dimensional space."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46968797"
                        ],
                        "name": "Jie Sun",
                        "slug": "Jie-Sun",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 164
                            }
                        ],
                        "text": "The component classifiers could also be combined in a more sophisticated way, for example using a rejection cascade Baker and Nayar [1996], Viola and Jones [2001], Sun et al. [2004] to improve the run time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 741060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e40c95e3f18447a3534bef9b8c23942d6d02448",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection methods based on cascade architecture have demonstrated fast and robust performance. Cascade learning is aided by the modularity of the architecture in which nodes are chained together to form a cascade. In this paper we present two new cascade learning results which address the decoupled nature of the cascade learning task. First, we introduce a cascade indifference curve framework, which connects the learning objectives for a node to the overall cascade performance. We derive a new cost function for node learning, which yields fully-automatic stopping conditions and improved detection performance. Second, we introduce the concept of perturbation bias, which leverages the statistical differences between target and non-target classes in a detection problem to obtain improved performance and robustness. We derive necessary and sufficient conditions for the success of the method and present experimental results."
            },
            "slug": "Automatic-cascade-training-with-perturbation-bias-Sun-Rehg",
            "title": {
                "fragments": [],
                "text": "Automatic cascade training with perturbation bias"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A cascade indifference curve framework is introduced, which connects the learning objectives for a node to the overall cascade performance, and a new cost function for node learning is derived, which yields fully-automatic stopping conditions and improved detection performance."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735767"
                        ],
                        "name": "Margaret M. Fleck",
                        "slug": "Margaret-M.-Fleck",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Fleck",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret M. Fleck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 90
                            }
                        ],
                        "text": "Local \u201cparts\u201d-based detectors are also widely used in object and human recognition systems [Forsyth and Fleck 1997, Ioffe and Forsyth 1999, Schneiderman and Kanade 2000, Ronfard et al. 2002, Ramanan and Forsyth 2003, Sigal et al. 2003, Schneiderman and Kanade 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7423091,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "26a46be6e55a495f24e00f00efae8c1829f2c479",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a representation for people and animals, called a body plan, which is adapted to segmentation and to recognition in complex environments. The representation is an organized collection of grouping hints obtained from a combination of constraints on color and texture and constraints on geometric properties such as the structure of individual parts and the relationships between parts. Body plans can be learned from image data, using established statistical learning techniques. The approach is illustrated with two examples of programs that successfully use body plans for recognition: one example involves determining whether a picture contains a scantily clad human, using a body plan built by hand; the other involves determining whether a picture contains a horse, using a body plan learned from image data. In both cases, the system demonstrates excellent performance on large, uncontrolled test sets and very large and diverse control sets."
            },
            "slug": "Body-plans-Forsyth-Fleck",
            "title": {
                "fragments": [],
                "text": "Body plans"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A representation for people and animals, called a body plan, which is adapted to segmentation and to recognition in complex environments is described, an organized collection of grouping hints obtained from a combination of constraints on color and texture and constraints on geometric properties."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145299933"
                        ],
                        "name": "R. Mohr",
                        "slug": "R.-Mohr",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Mohr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mohr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 115
                            }
                        ],
                        "text": "The use of salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 325871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49fcd806450d947e56c82ef2b438ad9c484069dc",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations."
            },
            "slug": "Local-Grayvalue-Invariants-for-Image-Retrieval-Schmid-Mohr",
            "title": {
                "fragments": [],
                "text": "Local Grayvalue Invariants for Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This paper addresses the problem of retrieving images from large image databases with a method based on local grayvalue invariants which are computed at automatically detected interest points and allows for efficient retrieval from a database of more than 1,000 images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12057260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74f7565782a6e619ef6ac2022880225f9a80a056",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The efficiency of pattern recognition is particularly crucial in two scenarios; whenever there are a large number of classes to discriminate, and, whenever recognition must be performed a large number of times. We propose a single technique, namely, pattern rejection, that greatly enhances efficiency in both cases. A rejector is a generalization of a classifier, that quickly eliminates a large fraction of the candidate classes or inputs. This allows a recognition algorithm to dedicate its efforts to a much smaller number of possibilities. Importantly, a collection of rejectors may be combined to form a composite rejector, which is shown to be far more effective than any of its individual components. A simple algorithm is proposed for the construction of each of the component rejectors. Its generality is established through close relationships with the Karhunen-Loeve expansion and Fisher's discriminant analysis. Composite rejectors were constructed for two representative applications, namely, appearance matching based object recognition and local feature detection. The results demonstrate substantial efficiency improvements over existing approaches, most notably Fisher's discriminant analysis."
            },
            "slug": "Pattern-rejection-Baker-Nayar",
            "title": {
                "fragments": [],
                "text": "Pattern rejection"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 84
                            }
                        ],
                        "text": "The use of salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17085053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1496689ab9743304831463ef69f1faf8bc6cd80",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors (1996) introduced the use of multidimensional receptive field histograms for probabilistic object recognition. In this paper we reverse the object recognition problem by asking the question \"where should we look?\", when we want to verify the presence of an object, to track an object or to actively explore a scene. This paper describes the statistical framework from which we obtain a network of salient points for an object. This network of salient points may be used for fixation control in the context of active object recognition."
            },
            "slug": "Where-to-look-next-and-what-to-look-for-Schiele-Crowley",
            "title": {
                "fragments": [],
                "text": "Where to look next and what to look for"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes the statistical framework from which a network of salient points for an object is obtained and may be used for fixation control in the context of active object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1842570,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "2d0b50d3df26b64ec5a2f949bc241b4fce515fa9",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for recovering 3D human body motion from monocular video sequences using robust image matching, joint limits and non-self-intersection constraints, and a new sample-and-refine search strategy guided by rescaled cost-function covariances. Monocular 3D body tracking is challenging: for reliable tracking at least 30 joint parameters need to be estimated, subject to highly nonlinear physical constraints; the problem is chronically ill conditioned as about 1/3 of the d.o.f. (the depth-related ones) are almost unobservable in any given monocular image; and matching an imperfect, highly flexible self-occluding model to cluttered image features is intrinsically hard. To reduce correspondence ambiguities we use a carefully designed robust matching-cost metric that combines robust optical flow, edge energy, and motion boundaries. Even so, the ambiguity, nonlinearity and non-observability make the parameter-space cost surface multi-modal, unpredictable and ill conditioned, so minimizing it is difficult. We discuss the limitations of CONDENSATION-like samplers, and introduce a novel hybrid search algorithm that combines inflated-covariance-scaled sampling and continuous optimization subject to physical constraints. Experiments on some challenging monocular sequences show that robust cost modelling, joint and self-intersection constraints, and informed sampling are all essential for reliable monocular 3D body tracking."
            },
            "slug": "Covariance-scaled-sampling-for-monocular-3D-body-Sminchisescu-Triggs",
            "title": {
                "fragments": [],
                "text": "Covariance scaled sampling for monocular 3D body tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Experiments on some challenging monocular sequences show that robust cost modelling, joint and self-intersection constraints, and informed sampling are all essential for reliable monocular 3D body tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053178525"
                        ],
                        "name": "G. Johansson",
                        "slug": "G.-Johansson",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "Johansson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Johansson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 438,
                                "start": 68
                            }
                        ],
                        "text": "Human beings need only a fifth of a second to recognise the action [Johansson 1973]. If the actor is known to the viewer, the viewer is often able to recognise her or him [Cutting and Kozlowski 1977] or in the worst case determine his or her gender [Kozlowski and Cutting 1977, 1978, Cutting and Kozlowski 1978]. 2 Even in computer vision, Johansson\u2019s movies of dots in biological motion have been an inspiration, e.g. Song et al. [1999] proposed a probability based method for detecting and labelling human motions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 37
                            }
                        ],
                        "text": "Such experiments were popularised by Johansson [1973], who placed light emitters on their joints and filmed actors performing various activities like walking, jogging, dancing in the dark."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54046837,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "58ea2fa0580b2117618be6e1cc9658a5c9531dba",
            "isKey": false,
            "numCitedBy": 4094,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports the first phase of a research program on visual perception of motion patterns characteristic of living organisms in locomotion. Such motion patterns in animals and men are termed here as biological motion. They are characterized by a far higher degree of complexity than the patterns of simple mechanical motions usually studied in our laboratories. In everyday perceptions, the visual information from biological motion and from the corresponding figurative contour patterns (the shape of the body) are intermingled. A method for studying information from the motion pattern per se without interference with the form aspect was devised. In short, the motion of the living body was represented by a few bright spots describing the motions of the main joints. It is found that 10\u201312 such elements in adequate motion combinations in proximal stimulus evoke a compelling impression of human walking, running, dancing, etc. The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to these biological motion patterns. The validity of this model in the present context was experimentally tested and the results turned out to be highly positive."
            },
            "slug": "Visual-perception-of-biological-motion-and-a-model-Johansson",
            "title": {
                "fragments": [],
                "text": "Visual perception of biological motion and a model for its analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to biological motion patterns and the results turned out to be highly positive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1839848"
                        ],
                        "name": "E. Barth",
                        "slug": "E.-Barth",
                        "structuredName": {
                            "firstName": "Erhardt",
                            "lastName": "Barth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Barth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69995399"
                        ],
                        "name": "C. Zetzsche",
                        "slug": "C.-Zetzsche",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zetzsche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Zetzsche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580914"
                        ],
                        "name": "I. Rentschler",
                        "slug": "I.-Rentschler",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Rentschler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rentschler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 144
                            }
                        ],
                        "text": "\u2026where it was inferred that gaze preferably fixates on image regions with corners and multiple superimposed orientations [Zetzsche et al. 1998, Barth et al. 1998], and that local spatial contrast is significantly higher at these points than at random locations, while image uniformity and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30130363,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "54c748f2c471fc619b028eb6ea8af9512b3a0973",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We suggest that intrinsic two-dimensional (i2D) features, computationally defined as the outputs of nonlinear operators that model the activity of end-stopped neurons, play a role in preattentive texture discrimination. We first show that for discriminable textures with identical power spectra the predictions of traditional models depend on the type of nonlinearity and fail for energy measures. We then argue that the concept of intrinsic dimensionality, and the existence of end-stopped neurons, can help us to understand the role of the nonlinearities. Furthermore, we show examples in which models without strong i2D selectivity fail to predict the correct ranking order of perceptual segregation. Our arguments regarding the importance of i2D features resemble the arguments of Julesz and co-workers regarding textons such as terminators and crossings. However, we provide a computational framework that identifies textons with the outputs of nonlinear operators that are selective to i2D features."
            },
            "slug": "Intrinsic-two-dimensional-features-as-textons.-Barth-Zetzsche",
            "title": {
                "fragments": [],
                "text": "Intrinsic two-dimensional features as textons."
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is suggested that intrinsic two-dimensional (i2D) features, computationally defined as the outputs of nonlinear operators that model the activity of end-stopped neurons, play a role in preattentive texture discrimination."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics, image science, and vision"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151490155"
                        ],
                        "name": "Ken-ichi Tanaka",
                        "slug": "Ken-ichi-Tanaka",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145434530"
                        ],
                        "name": "J. Ohta",
                        "slug": "J.-Ohta",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109665"
                        ],
                        "name": "K. Kyuma",
                        "slug": "K.-Kyuma",
                        "structuredName": {
                            "firstName": "Kazuo",
                            "lastName": "Kyuma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kyuma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "They are reminiscent of edge orientation histograms [Freeman and Roth 1995, Freeman et al. 1996], SIFT descriptors [Lowe 2004] and shape contexts [Belongie et al. 2001], but they are computed on a dense grid of uniformly spaced cells and they use overlapping descriptors for improved performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1762073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59c9d35a342ad4e9540d4fa37f7bbaf35913994b",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The appeal of computer games may be enhanced by vision-based user inputs. The high speed and low cost requirements for near-term, mass-market game applications make system design challenging. The response time of the vision interface should be less than a video frame time and the interface should cost less than $50 U.S. We meet these constraints with algorithms tailored to particular hardware. We have developed a special detector, called the artificial retina chip, which allows for fast, on-chip image processing. We describe two algorithms, based on image moments and orientation histograms, which exploit the capabilities of the chip to provide interactive response to the player's hand or body positions at 10 msec frame time and at low-cost. We show several possible game interactions."
            },
            "slug": "Computer-vision-for-computer-games-Freeman-Tanaka",
            "title": {
                "fragments": [],
                "text": "Computer vision for computer games"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Two algorithms are described, based on image moments and orientation histograms, which exploit the capabilities of the chip to provide interactive response to the player's hand or body positions at 10 msec frame time and at low-cost."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5544,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 223
                            }
                        ],
                        "text": "\u2026salient local points or regions for object detection has a long history [Schiele and Crowley 1996a,b,c, Schmid and Mohr 1997, Weber et al. 2000, Lowe 2001, Agarwal and Roth 2002, Fergus et al. 2003, Dorko\u0301 and Schmid 2003, Lowe 2004, Opelt et al. 2004, Leibe et al. 2005, Mikolajczyk et al. 2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 134
                            }
                        ],
                        "text": "Currently the most popular approaches are image gradient based descriptors such as the Scale Invariant Feature Transformation (SIFT) [Lowe 1999, 2004] and shape contexts [Belongie et al. 2001, 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 197
                            }
                        ],
                        "text": "Commonly used key point detectors include Fo\u0308rstnerHarris [Fo\u0308rstner and Pertl 1986, Fo\u0308rstner and Gu\u0308lch 1987, Harris and Stephens 1988], Laplacian [Lindeberg 1998] or Difference of Gaussians (DoGs) [Lowe 2004], and scale invariant HarrisLaplace [Mikolajczyk and Schmid 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 239
                            }
                        ],
                        "text": "\u2026to salient region detection are usually tuned to fire on blob like structures (one such detector is the Laplacian of Gaussians) while their feature vector computations are tuned to encode gradient or contour information (e.g. using SIFT [Lowe 2004] or shape context [Belongie et al. 2001])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 39
                            }
                        ],
                        "text": "Although Haar wavelet like features in Papageorgiou and Poggio [2000], Mohan et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 116
                            }
                        ],
                        "text": "They are reminiscent of edge orientation histograms [Freeman and Roth 1995, Freeman et al. 1996], SIFT descriptors [Lowe 2004] and shape contexts [Belongie et al. 2001], but they are computed on a dense grid of uniformly spaced cells and they use overlapping descriptors for improved performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": true,
            "numCitedBy": 25505,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 0
                            }
                        ],
                        "text": "Viola and Jones [2001], Viola et al. [2003] use AdaBoost to train cascades of weak classifiers for face and pedestrian detection, using spatial and temporal difference-of-rectangle based descriptors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7596571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bff76c25f7c416834655ba664553b14eb67a11c",
            "isKey": false,
            "numCitedBy": 1629,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning."
            },
            "slug": "Sparse-Bayesian-Learning-and-the-Relevance-Vector-Tipping",
            "title": {
                "fragments": [],
                "text": "Sparse Bayesian Learning and the Relevance Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that by exploiting a probabilistic Bayesian learning framework, the 'relevance vector machine' (RVM) can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4688361"
                        ],
                        "name": "E. Land",
                        "slug": "E.-Land",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Land",
                            "middleNames": [
                                "Herbert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Land"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 1550216,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "0a0985f3db38f116fa24b7090490fd62353d50d0",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "In the Color Vision Symposium at the Academy in April 1958, we showed a series of experiments which demonstrated that \"4whereas in color-mixing theory the wavelengths of the stimuli and the energy content at each wavelength are significant in determining the sense of color . . . in images neither the wavelength of the stimulus nor the energy at each wavelength determines the color. This departure from what we expect on the basis of colorimetry is not a small effect, but is complete . .. .' (1, 2). The initial and most engaging experiment comprised taking two black-and-white photographs of the same scene, one through a red filter and one through a green filter, and projecting these two black-and-white pictures in superposition on the screen to yield a single black-and-white panchromatic image of the scene. When a red filter was placed in the path of the light from the projector that contained the picture taken through a red filter, the whole scene became dramatically colored as if in many respects it were a standard full-color photograph. The first paradox was that the radiation coming to the eye of the observer consisted only of various ratios of red light to white light which should have yielded only a variety of pinks. The second paradox was that the overall ratio of light from the one projector to light from the other projector could be changed markedly without changing the color names of the objects in the colored picture: the colors of the individual objects must be determined by the ratio of red light to white light, but a change in the overall ratio of red light to white light did not change the colors. In light of the understanding which we now have, this simple experiment, which was a shock to the intuitive understanding of all of us, turns out to be the most sophisticated experiment we could have undertaken. For the flavor of the many experiments described at the Symposium, I refer you to the two papers (1, 2) at that time. Here, I want to turn to the quantitative procedures which we now use. We prepared a laboratory display which we dubbed a \"Mondrian\" (although it actually is closer to a van Doesburg), utilizing about 100 colored papers. A paper of a given color would appear many times in different parts of the display, each time having a different size and shape and each time being surrounded by a different set of other colored papers. One reason for the design was to prohibit the superposition of afterimages of areas onto other areas (3), and another reason for the design was to obviate explanations of results in terms of the size or shape or surrounding of any given paper. The Mondrian is illuminated by using three 35-mm slide projectors with no slides in the slide holder. The output of each projector/illuminator is controlled independently. An interference filter passing long waves is placed in the path of one projector, a middle wave filter, in the path of the second, and a short-wave filter, in the path of the third (Fig. 1). One may think of these as relating roughly to the three visual pigments. A telescopic photometer (Spectra Pritchard photometer, model 1980A), placed roughly where the observers will be, receives and measures radiation from about 1/16th of a square inch on each chosen area of the Mondrian when it is pointed at that area. The instrument is calibrated so that at any wavelength it reports directly in watts per steradian per square meter. Let me call your attention to these four papers: yellow, white, green, and blue. The telescope is pointed at a yellow paper. The short-wave and middle-wave illuminators are turned off, and the whole Mondrian is illuminated with the long-wave illuminator. The output of this projector is then changed until the meter reads exactly \"one\" (0.1 W per Sr2 per m2). The longwave illuminator is turned off and the middle-wave illuminator is turned on. Its output is adjusted until the meter reads one. This ensures that the amount of middle-wave energy now reaching the meter from that small patch is equal to the amount of long-wave energy. Finally, after the middle-wave illuminator is turned off, the short-wave illuminator is turned on, and its output is set so that the meter (which we must remember is reading the radiation to our eyes) reads one. All three illuminators are now turned on. While looking at the Mondrian as a whole, we note that the yellow paper looks yellow. We now turn our attention to the white paper, pointing the telephotometer at it. We go through the same procedure of illuminating with one illuminator at a time and of setting each illuminator so that the light coming this time from the white paper to the meter, and hence to our eyes, measures one for the long wave and one for the middle wave and one for the short wave. Thus, we have arranged to have coming to our eye from the piece of white paper exactly the same flux-the same wavelength composition, the same energy composition-which a moment earlier we had arranged to have coming to our eye from the piece of yellow paper. The somewhat indigestible question is \"what color will the piece of paper be which was white in the Mondrian previously?\" Keep in mind that the information now coming to our eye from that piece of paper dictates classically that, if one, one, and one coming to our eye gave yellow, then one, one, and one must again be yellow. This conviction dates back to Newton's proposition V (4):"
            },
            "slug": "Recent-advances-in-retinex-theory-and-some-for-and-Land",
            "title": {
                "fragments": [],
                "text": "Recent advances in retinex theory and some implications for cortical computations: color vision and the natural image."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A series of experiments demonstrated that in images neither the wavelength of the stimulus nor the energy at each wavelength determines the color, and this departure from what the authors expect on the basis of colorimetry is complete."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2247062,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "bcf3282a1f8774be1ddb8bb6cd8aa27c772a4f42",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper concerns the robustness of phase information for measuring image velocity and binocular disparity, its stability with respect to geometric deformations, and its linearity as a function of spatial position. These properties are shown to depend on the form of the filters used and their frequency bandwidths. The authors also discuss situations in which phase is unstable, many of which can be detected using the model of phase singularities (see Image Vis. Comput. (UK) vol.9, no.5, p.333-7 (Oct. 1991)).<<ETX>>"
            },
            "slug": "Stability-of-phase-information-Fleet-Jepson",
            "title": {
                "fragments": [],
                "text": "Stability of phase information"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The robustness of local phase information for measuring image velocity and binocular disparity is addressed, particularly in the stability of phase with respect to geometric deformations, and its linearity as a function of spatial position."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE Workshop on Visual Motion"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2356353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b78626ce1a562c05b1c06f9c805e839f9760b9ab",
            "isKey": false,
            "numCitedBy": 20815,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiresolution representations are effective for analyzing the information content of images. The properties of the operator which approximates a signal at a given resolution were studied. It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/sup j/ (where j is an integer) can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions. In L/sup 2/(R), a wavelet orthonormal basis is a family of functions which is built by dilating and translating a unique function psi (x). This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror filters. Wavelet representation lies between the spatial and Fourier domains. For images, the wavelet representation differentiates several spatial orientations. The application of this representation to data compression in image coding, texture discrimination and fractal analysis is discussed. >"
            },
            "slug": "A-Theory-for-Multiresolution-Signal-Decomposition:-Mallat",
            "title": {
                "fragments": [],
                "text": "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/Sup j/ can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762240"
                        ],
                        "name": "W. Bialek",
                        "slug": "W.-Bialek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bialek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "Given this matching cost, a set of highly relevant contour fragments is selected using the Information Bottleneck (IB) principle of Tishby et al. [1999]. This tries to minimise the Mutual Information (MI) between the training images and the set of fragments (compressing the information required to express the images) while at same time maximising the MI between the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8936496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c76c62c5ab6c076a80f925d277ef04dd36f6bf9c",
            "isKey": false,
            "numCitedBy": 2410,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We define the relevant information in a signal $x\\in X$ as being the information that this signal provides about another signal $y\\in \\Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\\X$ that preserves the maximum information about $\\Y$. That is, we squeeze the information that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a limited set of codewords $\\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \\to \\tX$ and $\\tX \\to \\Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere."
            },
            "slug": "The-information-bottleneck-method-Tishby-Pereira",
            "title": {
                "fragments": [],
                "text": "The information bottleneck method"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9018871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6deeed19c56ec6e737a909de9ee172b93f0d0a89",
            "isKey": false,
            "numCitedBy": 449,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A pictorial structure is a collection of parts arranged in a deformable configuration. Each part is represented using a simple appearance model and the deformable configuration is represented by spring-like connections between pairs of parts. While pictorial structures were introduced a number of years ago, they have not been broadly applied to matching and recognition problems. This has been due in part to the computational difficulty of matching pictorial structures to images. In this paper we present an efficient algorithm for finding the best global match of a pictorial stucture to an image. With this improved algorithm, pictorial structures provide a practical and powerful framework for quantitative descriptions of objects and scenes, and are suitable for many generic image recognition problems. We illustrate the approach using simple models of a person and a car."
            },
            "slug": "Efficient-matching-of-pictorial-structures-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Efficient matching of pictorial structures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An efficient algorithm for finding the best global match of a pictorial stucture to an image is presented and it is shown that this approach is suitable for many generic image recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782193"
                        ],
                        "name": "Ingo Steinwart",
                        "slug": "Ingo-Steinwart",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Steinwart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingo Steinwart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2534678"
                        ],
                        "name": "A. Christmann",
                        "slug": "A.-Christmann",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Christmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Christmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 52
                            }
                        ],
                        "text": "Linear Support Vector Machines (SVMs) [Vapnik 1995, Cristianini and Shawe-Taylor 2000, Scho\u0308lkopf and Smola 2002] are used as the classifiers as they offer the state-of-the-art performance and are fast to run."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 661123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
            "isKey": false,
            "numCitedBy": 4654,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology."
            },
            "slug": "Support-Vector-Machines-Steinwart-Christmann",
            "title": {
                "fragments": [],
                "text": "Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications and provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "Information science and statistics"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49053294"
                        ],
                        "name": "K. Okada",
                        "slug": "K.-Okada",
                        "structuredName": {
                            "firstName": "Kazunori",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685020"
                        ],
                        "name": "D. Comaniciu",
                        "slug": "D.-Comaniciu",
                        "structuredName": {
                            "firstName": "Dorin",
                            "lastName": "Comaniciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Comaniciu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48467036"
                        ],
                        "name": "A. Krishnan",
                        "slug": "A.-Krishnan",
                        "structuredName": {
                            "firstName": "Arun",
                            "lastName": "Krishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krishnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14888406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47de38a26a8a56b42fb52536c1f4261dbbccc292",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a robust estimation and validation framework for characterizing local structures in a positive multi-variate continuous function approximated by a Gaussian-based model. The new solution is robust against data with large deviations from the model and margin-truncations induced by neighboring structures. To this goal, it unifies robust statistical estimation for parametric model fitting and multi-scale analysis based on continuous scale-space theory. The unification is realized by formally extending the mean shift-based density analysis towards continuous signals whose local structure is characterized by an anisotropic fully-parameterized covariance matrix. A statistical validation method based on analyzing residual error of the chi-square fitting is also proposed to complement this estimation framework. The strength of our solution is the aforementioned robustness. Experiments with synthetic 1D and 2D data clearly demonstrate this advantage in comparison with the \u03b3-normalized Laplacian approach [12] and the standard sample estimation approach [13, p.179]. The new framework is applied to 3D volumetric analysis of lung tumors. A 3D implementation is evaluated with high-resolution CT images of 14 patients with 77 tumors, including 6 part-solid or ground-glass opacity nodules that are highly non-Gaussian and clinically significant. Our system accurately estimated 3D anisotropic spread and orientation for 82% of the total tumors and also correctly rejected all the failures without any false rejection and false acceptance. This system processes each 32-voxel volume-of-interest by an average of two seconds with a 2.4GHz Intel CPU. Our framework is generic and can be applied for the analysis of blob-like structures in various other applications."
            },
            "slug": "A-Robust-Algorithm-for-Characterizing-Anisotropic-Okada-Comaniciu",
            "title": {
                "fragments": [],
                "text": "A Robust Algorithm for Characterizing Anisotropic Local Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A robust estimation and validation framework for characterizing local structures in a positive multi-variate continuous function approximated by a Gaussian-based model that unifies robust statistical estimation for parametric model fitting and multi-scale analysis based on continuous scale-space theory."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109165"
                        ],
                        "name": "M. Proesmans",
                        "slug": "M.-Proesmans",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Proesmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Proesmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3266794"
                        ],
                        "name": "E. Pauwels",
                        "slug": "E.-Pauwels",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Pauwels",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Pauwels"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769671"
                        ],
                        "name": "A. Oosterlinck",
                        "slug": "A.-Oosterlinck",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Oosterlinck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oosterlinck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 58
                            }
                        ],
                        "text": "(c) Flow fields estimated using Proesmans\u2019 flow algorithm [Proesmans et al. 1994]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 13
                            }
                        ],
                        "text": "1998] of the Proesmans et al. [1994] multi-scale nonlinear diffusion based algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26501806,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "25232f8c1e61b84ab2d3e11567c3815d099daca5",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for optical flow computation by means of a coupled set of non-linear diffusion equations is presented. This approach integrates the classical differential approach with the correlation type of motion detectors. A measure of inconsistency within the optical flow field which indicates optical flow boundaries. This information is fed back to the optical flow equations in a non-linear way and allows the flow field to be reconstructed while preserving the discontinuities. The whole scheme is also applicable to stereo matching. The model is applied to a set of synthetic and real image sequences to illustrate the behaviour of the coupled diffusion equations."
            },
            "slug": "Determination-of-Optical-Flow-and-its-using-Proesmans-Gool",
            "title": {
                "fragments": [],
                "text": "Determination of Optical Flow and its Discontinuities using Non-Linear Diffusion"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for optical flow computation by means of a coupled set of non-linear diffusion equations that integrates the classical differential approach with the correlation type of motion detectors and is applicable to stereo matching."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685020"
                        ],
                        "name": "D. Comaniciu",
                        "slug": "D.-Comaniciu",
                        "structuredName": {
                            "firstName": "Dorin",
                            "lastName": "Comaniciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Comaniciu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "Basically, negative scores from the linear SVM are zeroed and a 3D position-scale mean shift process [Comaniciu 2003b] is run to identify significant local peaks in the resulting score."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7872611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "365129731fdfa1a9948d56f0caecb8642cc98c32",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The analysis of a feature space that exhibits multiscale patterns often requires kernel estimation techniques with locally adaptive bandwidths, such as the variable-bandwidth mean shift. Proper selection of the kernel bandwidth is, however, a critical step for superior space analysis and partitioning. This paper presents a mean shift-based approach for local bandwidth selection in the multimodal, multivariate case. The method is based on a fundamental property of normal distributions regarding the bias of the normalized density gradient. This paper demonstrates that, within the large sample approximation, the local covariance is estimated by the matrix that maximizes the magnitude of the normalized mean shift vector. Using this property, the paper develops a reliable algorithm which takes into account the stability of local bandwidth estimates across scales. The validity of the theoretical results is proven in various space partitioning experiments involving the variable-bandwidth mean shift."
            },
            "slug": "An-Algorithm-for-Data-Driven-Bandwidth-Selection-Comaniciu",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Data-Driven Bandwidth Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper develops a reliable algorithm which takes into account the stability of local bandwidth estimates across scales, and demonstrates that, within the large sample approximation, the local covariance is estimated by the matrix that maximizes the magnitude of the normalized mean shift vector."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29187618,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "993b1083455b5c4d631eaf44f230b061994e75c3",
            "isKey": false,
            "numCitedBy": 3379,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation. Steerable filters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. The authors show how to design and steer the filters and present examples of their use in the analysis of orientation and phase, angularly adaptive filtering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3-D steerable filters. >"
            },
            "slug": "The-Design-and-Use-of-Steerable-Filters-Freeman-Adelson",
            "title": {
                "fragments": [],
                "text": "The Design and Use of Steerable Filters"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707686"
                        ],
                        "name": "W. F\u00f6rstner",
                        "slug": "W.-F\u00f6rstner",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "F\u00f6rstner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. F\u00f6rstner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52541548"
                        ],
                        "name": "A. Pertl",
                        "slug": "A.-Pertl",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Pertl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pertl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56917046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d90a0c8624e0567a3db101002b56e50333d1f819",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "PHOTOGRAMMETRIC-STANDARD-METHODS-AND-DIGITAL-IMAGE-F\u00f6rstner-Pertl",
            "title": {
                "fragments": [],
                "text": "PHOTOGRAMMETRIC STANDARD METHODS AND DIGITAL IMAGE MATCHING TECHNIQUES FOR HIGH PRECISION SURFACE MEASUREMENTS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1",
            "isKey": false,
            "numCitedBy": 2592,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is presented for the representation of (pictures of) faces. Within a specified framework the representation is ideal. This results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector. The method is illustrated in detail by the use of an ensemble of pictures taken for this purpose."
            },
            "slug": "Low-dimensional-procedure-for-the-characterization-Sirovich-Kirby",
            "title": {
                "fragments": [],
                "text": "Low-dimensional procedure for the characterization of human faces."
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A method is presented for the representation of faces that results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50114694"
                        ],
                        "name": "C. Barclay",
                        "slug": "C.-Barclay",
                        "structuredName": {
                            "firstName": "Constance",
                            "lastName": "Barclay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barclay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2345416"
                        ],
                        "name": "J. Cutting",
                        "slug": "J.-Cutting",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cutting",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cutting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7265948"
                        ],
                        "name": "L. Kozlowski",
                        "slug": "L.-Kozlowski",
                        "structuredName": {
                            "firstName": "Lynn",
                            "lastName": "Kozlowski",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kozlowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43618504,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9fe46acfdedf3e22bd6d48fe3c2d5404ad3d4627",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Several temporal and spatial factors affect gender recognition of a walker when portrayed, without familiarity cues, as a dynamic point-light display. We demonstrate that, among temporal parameters, the duration of the dynamic stimulus must be longer than 1.6 sec, but that 2.7 sec is fully adequate. Given the speed of our walkers, the recognition threshold appears to be roughly two step cycles. In addition, presentation rate of the stimulus must be near to normal, perhaps because nonnormal rates alter apparent gravity and obscure the normal relationship between output and conservation of energy. We demonstrate that, among spatial factors, the discreteness of the joint information must be maintained for accurate recognition. We go on to argue that it is the information about the shoulder and the hip of a walker that is of primary importance. Finally, inversion of the stimulus display produces the unexpected effect of reversing the apparent sex of most walkers. That is, when presented upside down, male walkers appear female and female walkers appear male."
            },
            "slug": "Temporal-and-spatial-factors-in-gait-perception-Barclay-Cutting",
            "title": {
                "fragments": [],
                "text": "Temporal and spatial factors in gait perception that influence gender recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that it is the information about the shoulder and the hip of a walker that is of primary importance, and inversion of the stimulus display produces the unexpected effect of reversing the apparent sex of most walkers."
            },
            "venue": {
                "fragments": [],
                "text": "Perception & psychophysics"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 225
                            }
                        ],
                        "text": "Another work using intensity images is the face detection system of Rowley et al. [1998] who locally correct the lighting of the images by performing histogram equalisation before passing them to a neural network classifier [Bishop 1995] for face/non-face detections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15339,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61116019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "isKey": false,
            "numCitedBy": 5454,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-SVM-learning-practical-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large scale SVM learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2345416"
                        ],
                        "name": "J. Cutting",
                        "slug": "J.-Cutting",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cutting",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cutting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7265948"
                        ],
                        "name": "L. Kozlowski",
                        "slug": "L.-Kozlowski",
                        "structuredName": {
                            "firstName": "Lynn",
                            "lastName": "Kozlowski",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kozlowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143687558,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a6f0aa2485265e2f714708570a495cf0469ca717",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Viewers can recognize themselves and others in an abstract display of their movements. Light sources mounted on joints prominent during the act of walking are sufficient cues for identification. No other information, no feedback, and little practice with such a display are needed. This procedure, developed by Johansson, holds promise for inquiry into the dimensions and features of event perception: It is both naturalistic and experimentally manageable."
            },
            "slug": "Recognizing-friends-by-their-walk:-Gait-perception-Cutting-Kozlowski",
            "title": {
                "fragments": [],
                "text": "Recognizing friends by their walk: Gait perception without familiarity cues"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7265948"
                        ],
                        "name": "L. Kozlowski",
                        "slug": "L.-Kozlowski",
                        "structuredName": {
                            "firstName": "Lynn",
                            "lastName": "Kozlowski",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kozlowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2345416"
                        ],
                        "name": "J. Cutting",
                        "slug": "J.-Cutting",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cutting",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cutting"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143871783,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "437da98046e99632e989b26b21acfb8d9d73f1e9",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We and our associates have explored the recognition of the gender of walkers from dynamic displays of point-light sources (Barclay, Cutting, & Kozlowski, 1978; Kozlowski & Cutting, 1977). One of our most remarkable findings has been that the sex of a walker can be judged from only two lights-one on each ankle (Kozlowski & Cutting, 1977, Experiment 5). We would like to discuss the limitations of our own result and present the findings of another test of whether ankles-alone information is sufficient for the recognition of the sex of a walker. Kozlowski and Cutting (1977) had only two male and one female walker in the ankles-alone condition. One male was seen as female (61% of guesses, p < .05). The other was seen as male (60070, p < .05). Of course, this pattern provides a totally inconclusive test of whether males can be recognized from ankles alone. Our one female was seen as female (61070, p < .05). Overall, the recognition of sex was accurate (54070, p < .05, n = 259), but these two successes out of three walkers would persuade only the faithful. Discontent with the power of our first demonstration, we tried a second. We retaped the stimuli from Barclay et al. (1978, Experiment I), blocking out all but the lights on the ankles. A random sequence of two tokens each of seven male and six female walkers was shown to 11 male and 12 female viewers. The study was conducted in small groups. The subjects were paid $2. Overall, guesses were 46.3% correct (SD = 9.99), t(22) = 1.78, p < .10. If anything, this indicates that people tend to be wrong in their guesses. Further analyses showed that longer strides were seen as masculine [r(ll) = + .87, P < .01]."
            },
            "slug": "Recognizing-the-gender-of-walkers-from-point-lights-Kozlowski-Cutting",
            "title": {
                "fragments": [],
                "text": "Recognizing the gender of walkers from point-lights mounted on ankles: Some second thoughts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The limitations of the Kozlowski and Cutting result are discussed, the findings of another test of whether ankles-alone information is sufficient for the recognition of the sex of a walker are presented and longer strides were seen as masculine."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7265948"
                        ],
                        "name": "L. Kozlowski",
                        "slug": "L.-Kozlowski",
                        "structuredName": {
                            "firstName": "Lynn",
                            "lastName": "Kozlowski",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kozlowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2345416"
                        ],
                        "name": "J. Cutting",
                        "slug": "J.-Cutting",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cutting",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cutting"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143941887,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1c25a91fdb4e8da680eb19cb20d0f2ffbb1ea34f",
            "isKey": false,
            "numCitedBy": 853,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The sex of human walkers can be recognized without familiarity cues from displays of pointlight sources mounted on major joints. Static versions of these abstract displays do not permit accurate recognition of sex. Variation in the degree of armswing or in walking speed generally interferes with recognition, except that faster speeds are associated somewhat with improved recognition of females. Lights on upper-body joints permit more accurate guesses than do Lights on lower-body joints, but identification is possible even from minimal displays, with lights placed only on the ankles. No feedback was given to observers. Confidence judgments of sex relate to the accuracy of responses in a manner that suggests that viewers know what they are doing."
            },
            "slug": "Recognizing-the-sex-of-a-walker-from-a-dynamic-Kozlowski-Cutting",
            "title": {
                "fragments": [],
                "text": "Recognizing the sex of a walker from a dynamic point-light display"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772264"
                        ],
                        "name": "E. Schwartz",
                        "slug": "E.-Schwartz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Schwartz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32711604,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "852a0d535dc5ead3417f8305f7da1eadd44b2204",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The retinotopic mapping of the visual field to the surface of the striate cortex is characterized as a longarithmic conformal mapping. This summarizes in a concise way the observed curve of cortical magnification, the linear scaling of receptive field size with eccentricity, and the mapping of global visual field landmarks. It is shown that if this global structure is reiterated at the local level, then the sequence regularity of the simple cells of area 17 may be accounted for as well. Recently published data on the secondary visual area, the medial visual area, and the inferior pulvinar of the owl monkey suggests that same global logarithmic structure holds for these areas as well. The available data on the structure of the somatotopic mapping (areaS-1) supports a similar analysis. The possible relevance of the analytical form of the cortical receptotopic maps to perception is examined and a brief discussion of the developmental implications of these findings is presented."
            },
            "slug": "Spatial-mapping-in-the-primate-sensory-projection:-Schwartz",
            "title": {
                "fragments": [],
                "text": "Spatial mapping in the primate sensory projection: Analytic structure and relevance to perception"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "It is shown that if this global structure is reiterated at the local level, then the sequence regularity of the simple cells of area 17 may be accounted for as well and the observed curve of cortical magnification, the linear scaling of receptive field size with eccentricity, and the mapping of global visual field landmarks."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080435504"
                        ],
                        "name": "B. Fischer",
                        "slug": "B.-Fischer",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fischer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 207
                            }
                        ],
                        "text": "Support comes from studies of the mammalian visual system: the first level of visual coding in mammals involves the computation of dense and overlapping centre-surround receptive-fields of different scales [Fischer 1973, Hubel and Wiesel 1974, Hubel 1995, Chapter 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43835460,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4b6c9a1ebbadca8366152999f2497b3d52a54a26",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Overlap-of-receptive-field-centers-and-of-the-field-Fischer",
            "title": {
                "fragments": [],
                "text": "Overlap of receptive field centers and representation of the visual field in the cat's optic tract."
            },
            "venue": {
                "fragments": [],
                "text": "Vision research"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 29
                            }
                        ],
                        "text": "2004] trained with AdaBoost [Schapire 2002] feature selection. The inclusion of motion features increases the performance by an order of magnitude relative to a similar static detector, but the adoption of a static camera greatly simplifies the problem because the mere presence of motion already provides a strong cue for human presence. Other approaches to motion descriptors are the phase based features of Fleet and Jepson [1993]. Other surveillance based detectors include the flow-based activity recognition system of Haritaoglu et al. [2000]. Efros et al. [2003] used appearance and flow features in an exemplar based detector for long shots of sports players, but quantitative performance results were not given. For pedestrian detection in video sequences, Gavrila et al. [2004] use a static appearance based Chamfer matching system [Gavrila 1999] in conjunction with texture classification, stereo verification and tracking of detections over time to consolidate the results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 434,
                                "start": 29
                            }
                        ],
                        "text": "2004] trained with AdaBoost [Schapire 2002] feature selection. The inclusion of motion features increases the performance by an order of magnitude relative to a similar static detector, but the adoption of a static camera greatly simplifies the problem because the mere presence of motion already provides a strong cue for human presence. Other approaches to motion descriptors are the phase based features of Fleet and Jepson [1993]. Other surveillance based detectors include the flow-based activity recognition system of Haritaoglu et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 549,
                                "start": 29
                            }
                        ],
                        "text": "2004] trained with AdaBoost [Schapire 2002] feature selection. The inclusion of motion features increases the performance by an order of magnitude relative to a similar static detector, but the adoption of a static camera greatly simplifies the problem because the mere presence of motion already provides a strong cue for human presence. Other approaches to motion descriptors are the phase based features of Fleet and Jepson [1993]. Other surveillance based detectors include the flow-based activity recognition system of Haritaoglu et al. [2000]. Efros et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1008,
                                "start": 29
                            }
                        ],
                        "text": "2004] trained with AdaBoost [Schapire 2002] feature selection. The inclusion of motion features increases the performance by an order of magnitude relative to a similar static detector, but the adoption of a static camera greatly simplifies the problem because the mere presence of motion already provides a strong cue for human presence. Other approaches to motion descriptors are the phase based features of Fleet and Jepson [1993]. Other surveillance based detectors include the flow-based activity recognition system of Haritaoglu et al. [2000]. Efros et al. [2003] used appearance and flow features in an exemplar based detector for long shots of sports players, but quantitative performance results were not given. For pedestrian detection in video sequences, Gavrila et al. [2004] use a static appearance based Chamfer matching system [Gavrila 1999] in conjunction with texture classification, stereo verification and tracking of detections over time to consolidate the results. Shashua et al. [2004] follow a similar approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 570,
                                "start": 29
                            }
                        ],
                        "text": "2004] trained with AdaBoost [Schapire 2002] feature selection. The inclusion of motion features increases the performance by an order of magnitude relative to a similar static detector, but the adoption of a static camera greatly simplifies the problem because the mere presence of motion already provides a strong cue for human presence. Other approaches to motion descriptors are the phase based features of Fleet and Jepson [1993]. Other surveillance based detectors include the flow-based activity recognition system of Haritaoglu et al. [2000]. Efros et al. [2003] used appearance and flow features in an exemplar based detector for long shots of sports players, but quantitative performance results were not given."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221284382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84bb60b83f82ad847e19d96403ad0011abfc888f",
            "isKey": true,
            "numCitedBy": 1888,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is a general method for improving the accuracy of any given learning algorithm. Focusing primarily on the AdaBoost algorithm, this chapter overviews some of the recent work on boosting including analyses of AdaBoost\u2019s training error and generalization error; boosting\u2019s connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of AdaBoost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting."
            },
            "slug": "The-Boosting-Approach-to-Machine-Learning-An-Schapire",
            "title": {
                "fragments": [],
                "text": "The Boosting Approach to Machine Learning An Overview"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This chapter overviews some of the recent work on boosting including analyses of AdaBoost's training error and generalization error; boosting\u2019s connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of Ada boost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40244041"
                        ],
                        "name": "D. Ingle",
                        "slug": "D.-Ingle",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ingle",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ingle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36200136,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "37f01db3a812ba05b1f529140ce7275319c07eb7",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In an experiment designed to test color constancy in a situation comparable to that used in E. H. Land's experiments with human observers, goldfish were trained to approach a particular color within a richly colored but variable \"Mondrian\" background. They retained the ability to identify colors accurately even when the spectral composition of the illuminant was radically altered in generalization tests. Since the behavior of fish resembles that of human beings in these tests, Land's retinex theory seems to apply to a relatively primitive vertebrate as well as to humans."
            },
            "slug": "The-goldfish-as-a-retinex-animal.-Ingle",
            "title": {
                "fragments": [],
                "text": "The goldfish as a retinex animal."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In an experiment designed to test color constancy in a situation comparable to that used in E. H. Land's experiments with human observers, goldfish were trained to approach a particular color within a richly colored but variable \"Mondrian\" background."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 221
                            }
                        ],
                        "text": "Support comes from studies of the mammalian visual system: the first level of visual coding in mammals involves the computation of dense and overlapping centre-surround receptive-fields of different scales [Fischer 1973, Hubel and Wiesel 1974, Hubel 1995, Chapter 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13140630,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d1d186fe8eae26824a380f70454be04d6a756bef",
            "isKey": false,
            "numCitedBy": 896,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with the relationship between orientation columns, ocular\u2010dominance columns, the topographic mapping of visual fields onto cortex, and receptive\u2010field size and scatter. Although the orientation columns are an order of magnitude smaller than the ocular\u2010dominance columns, the horizontal distance corresponding to a complete cycle of orientation columns, representing a rotation through 180\u00b0, seems to be roughly the same size as a left\u2010plus\u2010right ocular dominance set, with a thickness of about 0.5\u20131 mm, independent of eccentricity at least out to 15\u00b0. We use the term hypercolumn to refer to a complete set of either type (180\u00b0, or left\u2010plus\u2010right eyes)."
            },
            "slug": "Uniformity-of-monkey-striate-cortex:-A-parallel-and-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Uniformity of monkey striate cortex: A parallel relationship between field size, scatter, and magnification factor"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The term hypercolumn is used to refer to a complete set of either type (180\u00b0, or left\u2010plus\u2010right eyes), with implications for the topographic mapping of visual fields onto cortex, and receptive\u2010field size and scatter."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of comparative neurology"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145562602"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Daw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53190350,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "44fd887b3b8723c6089756519d8205207fd384d4",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-psychology-and-physiology-of-colour-vision-Daw",
            "title": {
                "fragments": [],
                "text": "The psychology and physiology of colour vision"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1638095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "888c09de60ce427669fe5a264fa3e787803eb9d2",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this gameplaying algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the \u201cdual\u201d of this game."
            },
            "slug": "Game-theory,-on-line-prediction-and-boosting-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Game theory, on-line prediction and boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth is described, which yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11079833"
                        ],
                        "name": "S. Sutherland",
                        "slug": "S.-Sutherland",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Sutherland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sutherland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 244
                            }
                        ],
                        "text": "Support comes from studies of the mammalian visual system: the first level of visual coding in mammals involves the computation of dense and overlapping centre-surround receptive-fields of different scales [Fischer 1973, Hubel and Wiesel 1974, Hubel 1995, Chapter 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 729,
                                "start": 134
                            }
                        ],
                        "text": "Physiological studies also highlight the fact that mammalian visual system has overlapping cells in its primary cortex [Fischer 1973, Hubel and Wiesel 1974]. Traditional centre-surround style schemes are probably not the best choice, but they offer significant speed advantages. If used in conjunction with AdaBoost, they offer comparable performance with significant speed up. In summary, we have shown that using locally normalised histogram of gradient orientation features similar to SIFT descriptors [Lowe 2004] in dense overlapping grids gives very good results for person detection, reducing false positive rates by more than an order of magnitude relative to the best Haar wavelet based detector from Mohan et al. [2001]. The HOG encoding is generally useful and gives equally good performance for many other object classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35236366,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "ef46c51a9f9db65311accbfa5405b1d0dc280f93",
            "isKey": false,
            "numCitedBy": 444,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Vision and Visual Dysfunction.General editor: John Cronly-Dillon. Macmillan: 1991. 17 volumes. Approximately 5,000 pages. \u00a31,250, $2,295."
            },
            "slug": "Eye,-brain-and-vision-Sutherland",
            "title": {
                "fragments": [],
                "text": "Eye, brain and vision"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64295966,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b7caf811d6980627caad1a8b3053f40348693508",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Fitting a Sigmoid After the SVM, Empirical Tests, Conclusions, Appendix: Pseudo-code for the Sigmoid Training"
            },
            "slug": "Probabilities-for-SV-Machines-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Probabilities for SV Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction, Fitting a Sigmoid After the SVM, Empirical Tests, Conclusions, Appendix: Pseudo-code for the Sigmoids Training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 39
                            }
                        ],
                        "text": "Linear Support Vector Machines (SVMs) [Vapnik 1995, Cristianini and Shawe-Taylor 2000, Scho\u0308lkopf and Smola 2002] are used as the classifiers as they offer the state-of-the-art performance and are fast to run."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1462,
                                "start": 49
                            }
                        ],
                        "text": "[2003] used the entropy based region detector of Kadir and Brady [2001] for object representation, Weber et al. [2000] and Agarwal and Roth [2002] use the F\u00f6rstner and G\u00fclch [1987] interest point operator, while Dork\u00f3 and Schmid [2003] and Opelt et al. [2004] use Harris [Harris and Stephens 1988], Harris-Laplace [Mikolajczyk and Schmid 2002] and Laplacian of Gaussian (LoG) [Mikolajczyk and Schmid 2002] based interest point detectors. Regarding the computation of feature vectors or descriptors over the local image regions surrounding the key points, many approaches have been tried. Currently the most popular approaches are image gradient based descriptors such as the Scale Invariant Feature Transformation (SIFT) [Lowe 1999, 2004] and shape contexts [Belongie et al. 2001, 2002]. Both compute local histograms of image gradients or edges. SIFT uses the local scale and dominant orientation given by the key point detector to vote into orientation histograms with weighting based on gradient magnitudes. It thus computes scale and rotation invariant feature vectors. The scale information is also used to define an appropriate smoothing scale when computing image gradients. SIFT computes histograms over rectangular grids, whereas shape contexts use log-polar grids. The initial shape context method [Belongie et al. 2002] used edges to vote into 2-D spatial histograms, but this was later extended to generalised shape contexts by Mori and Malik [2003] who use gradient orientations to vote into 3-D spatial and orientation histograms with gradient magnitude weighting similar to SIFT."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026and Stephens 1988, Mikolajczyk and Schmid 2002], blobs (Laplacian of Gaussian [Lindeberg 1998] or Difference of Gaussian [Lowe 2001]), intensities [Kadir and Brady 2001, Ullman et al. 2001, Vidal-Naquet and Ullman 2003], gradients [Ronfard et al. 2002, Mikolajczyk et al. 2004], colour, texture,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 49
                            }
                        ],
                        "text": "[2003] used the entropy based region detector of Kadir and Brady [2001] for object representation, Weber et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 49
                            }
                        ],
                        "text": "[2003] used the entropy based region detector of Kadir and Brady [2001] for object representation, Weber et al. [2000] and Agarwal and Roth [2002] use the F\u00f6rstner and G\u00fclch [1987] interest point operator, while Dork\u00f3 and Schmid [2003] and Opelt et al. [2004] use Harris [Harris and Stephens 1988], Harris-Laplace [Mikolajczyk and Schmid 2002] and Laplacian of Gaussian (LoG) [Mikolajczyk and Schmid 2002] based interest point detectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 49
                            }
                        ],
                        "text": "[2003] used the entropy based region detector of Kadir and Brady [2001] for object representation, Weber et al. [2000] and Agarwal and Roth [2002] use the F\u00f6rstner and G\u00fclch [1987] interest point operator, while Dork\u00f3 and Schmid [2003] and Opelt et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scale, saliency and image description"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision,"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 238926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8185a04652b9ff239d56958f2127e60bae850c5",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Graphical-Model-For-Recognizing-Scenes-and-Objects.-Murphy-Torralba",
            "title": {
                "fragments": [],
                "text": "Graphical Model For Recognizing Scenes and Objects."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683937"
                        ],
                        "name": "C. Zetzsche",
                        "slug": "C.-Zetzsche",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Zetzsche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Zetzsche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755034"
                        ],
                        "name": "K. Schill",
                        "slug": "K.-Schill",
                        "structuredName": {
                            "firstName": "Kerstin",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938628"
                        ],
                        "name": "H. Deubel",
                        "slug": "H.-Deubel",
                        "structuredName": {
                            "firstName": "Heiner",
                            "lastName": "Deubel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Deubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145090451"
                        ],
                        "name": "G. Krieger",
                        "slug": "G.-Krieger",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Krieger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Krieger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063425"
                        ],
                        "name": "E. Umkehrer",
                        "slug": "E.-Umkehrer",
                        "structuredName": {
                            "firstName": "Elisabeth",
                            "lastName": "Umkehrer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Umkehrer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991412"
                        ],
                        "name": "S. Beinlich",
                        "slug": "S.-Beinlich",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Beinlich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Beinlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Zhu et al. [2006], and for this the R-HOG architecture is not optimal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 145
                            }
                        ],
                        "text": "\u2026on human eye-tracking, where it was inferred that gaze preferably fixates on image regions with corners and multiple superimposed orientations [Zetzsche et al. 1998, Barth et al. 1998], and that local spatial contrast is significantly higher at these points than at random locations, while\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60549164,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0561489aade2443f813c28342f7bc2ac08a27f79",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Investigation-of-a-sensorimotor-system-for-saccadic-Zetzsche-Schill",
            "title": {
                "fragments": [],
                "text": "Investigation of a sensorimotor system for saccadic scene analysis: an integrated approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66866489"
                        ],
                        "name": "Vincent Depoortere",
                        "slug": "Vincent-Depoortere",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Depoortere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Depoortere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103276008"
                        ],
                        "name": "J. Cant",
                        "slug": "J.-Cant",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057914555"
                        ],
                        "name": "B. Bosch",
                        "slug": "B.-Bosch",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Bosch",
                            "middleNames": [
                                "Van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47145879"
                        ],
                        "name": "J. D. Prins",
                        "slug": "J.-D.-Prins",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Prins",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Prins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2733505"
                        ],
                        "name": "R. Fransens",
                        "slug": "R.-Fransens",
                        "structuredName": {
                            "firstName": "Rik",
                            "lastName": "Fransens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fransens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58131327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0799401f5f29529c0c1498a93d6d447739a266f",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-pedestrian-detection-:-a-test-case-for-Depoortere-Cant",
            "title": {
                "fragments": [],
                "text": "Efficient pedestrian detection : a test case for SVM based categorization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1893129"
                        ],
                        "name": "M. Bichsel",
                        "slug": "M.-Bichsel",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Bichsel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bichsel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142804056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9373c1a890f9d4d1847d602d6e14df00e1bd0115",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Strategies-of-robust-object-recognition-for-the-of-Bichsel",
            "title": {
                "fragments": [],
                "text": "Strategies of robust object recognition for the automatic identification of human faces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4688361"
                        ],
                        "name": "E. Land",
                        "slug": "E.-Land",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Land",
                            "middleNames": [
                                "Herbert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Land"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 1722334,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "47b18c23002293913d317252bb142cc62b1a4f6b",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "COLOR-VISION-AND-THE-NATURAL-IMAGE-PART-II.-Land",
            "title": {
                "fragments": [],
                "text": "COLOR VISION AND THE NATURAL IMAGE PART II."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221303277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96cff156cdc8a479aa994eae875a0860d663d317",
            "isKey": false,
            "numCitedBy": 2181,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-with-kernels-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "tel-00390303, version 1 -1 Jun 2009 References B. SchSch\u00a8Sch\u00f6lkopf and A. Smola. Learning with Kernels"
            },
            "venue": {
                "fragments": [],
                "text": "tel-00390303, version 1 -1 Jun 2009 References B. SchSch\u00a8Sch\u00f6lkopf and A. Smola. Learning with Kernels"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A.5 Example images from the INRIA static person data set with corresponding original annotations marked on it"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "11 Visual cues encoded in the R2-HOG and C-HOG encoding algorithm . . . . . . . . . . . . 49"
            },
            "venue": {
                "fragments": [],
                "text": "11 Visual cues encoded in the R2-HOG and C-HOG encoding algorithm . . . . . . . . . . . . 49"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1 Some examples of cat images from the PASCAL 2006 Visual Object Challenge Our template based approach is unable to handle the large amount of within-class shape variation in these"
            },
            "venue": {
                "fragments": [],
                "text": "images"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Example images from the head and shoulders, torso and legs data sets and the corresponding average gradient images over all examples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Performance comparison of head and shoulders, torso, and legs detectors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "4 The feature extraction process for the motion"
            },
            "venue": {
                "fragments": [],
                "text": "channel"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonparametric Analysis of Visual Data: the Mean Shift Paradigm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graz 01 data set. On Web"
            },
            "venue": {
                "fragments": [],
                "text": "Graz 01 data set. On Web"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scan range for the head and shoulders, the torso and the leg detectors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonparametric Analysis of Visual Data: the Mean Shift Paradigm \" with Dorin Comaniciu Tentative publishing date: late"
            },
            "venue": {
                "fragments": [],
                "text": "Nonparametric Analysis of Visual Data: the Mean Shift Paradigm \" with Dorin Comaniciu Tentative publishing date: late"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The feature information that the R-HOG descriptor cues on for cars"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "List of Figures 1.1 Some images from a collection of personal digital photos. This collection, the INRIA static person detection data set, is the benchmark data for most of the analysis in this"
            },
            "venue": {
                "fragments": [],
                "text": "Other Activities @BULLET Peer reviewer for IEEE Transactions on Pattern Analysis and Machine Intelligence. @BULLET Peer reviewer for International Conference on Computer Vision, IEEE Conference on Computer Vision and Pattern Recognition, European Conference on Computer Vision"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 10
                            }
                        ],
                        "text": "Similarly Leibe [2006] report that their detector does not perform well for natural images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 20
                            }
                        ],
                        "text": "4, except that like Leibe et al. [2004], Leibe and Schiele [2004] we return all modes with values higher than a user supplied threshold as final detections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Informative features for profile-view pedestrians"
            },
            "venue": {
                "fragments": [],
                "text": "Personal Communication,"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recall-precision curves showing the effect of different non-maximum suppression parameters on overall performance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2 Variants of transformation functions t(w) used in non-maximum suppression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The performance of selected detectors on the MIT and INRIA static data sets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Method of and apparatus for pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Siemens, Eptron) developing image interpolation algorithms for rendering novel views"
            },
            "venue": {
                "fragments": [],
                "text": "Siemens, Eptron) developing image interpolation algorithms for rendering novel views"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "4 A pair of consecutive images and the estimated regularised and unregularised flow fields"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-scale Detection of Local Image Structures. US Serial No"
            },
            "venue": {
                "fragments": [],
                "text": "Multi-scale Detection of Local Image Structures. US Serial No"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-scale Detection of Local Image Structures. US Serial No"
            },
            "venue": {
                "fragments": [],
                "text": "Multi-scale Detection of Local Image Structures. US Serial No"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 28
                            }
                        ],
                        "text": "Land [1959a,b], Daw [1984], Ingle [1985]. Thus a robust object detector must handle colour changes and provide invariance to a broad range of illumination and lighting changes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The goldfish as a retinex"
            },
            "venue": {
                "fragments": [],
                "text": "animal. Science,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The effect of encoding at multiple-scales and comparison of different rectangular blocks sizes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An overview of the overall performance of our various motion detectors on different test data sets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projects \u2022 Scientific collaborator in aceMedia, an European Union sixth frame work project. Led cross-functional team on \"Person Detection and Identification\" distributed across"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sample images from MIT pedestrian database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "32 4.2 Generalised 1 st -and 2 nd -order 35 4.3 Gradient-strength shape context (G-ShapeC) and edge-presence"
            },
            "venue": {
                "fragments": [],
                "text": "1 Variants of proposed HOG descriptors based on rectangular or circular layouts"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inconsistent behaviour of key point detectors on humans"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An overview of Histogram of Oriented Gradient (HOG) feature extraction and object detection chain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complete window classifier learning algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Effect of detection window size and kernel SVM on the performance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Performance variations as a function of the C-HOG descriptor parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalised 1 st -and 2 nd -order Haar wavelet operators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of the various part detector fusion methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast operator for detection and precise location of distinct points, corners and centres of circular features"
            },
            "venue": {
                "fragments": [],
                "text": "Intercommission Conference on Fast Processing of Photogrammetric Data"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The complete IMHcd, IMHmd encoding algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Our multi-scale dense optical flow computation method. The steps are optimised for rapid flow computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "different rectangular blocks sizes"
            },
            "venue": {
                "fragments": [],
                "text": "different rectangular blocks sizes"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An overview of the steps of the non-maximum suppression algorithm used to fuse multiple detections during the detection phase"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The feature information that R-HOG descriptor cues on for motorbikes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Our template based approach is unable to handle the large amount of within-class shape variation in these images"
            },
            "venue": {
                "fragments": [],
                "text": "Some examples of cat images from the PASCAL 2006 Visual Object Challenge"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An overview of the steps of the non-maximum suppression algorithm used to fuse multiple detections during the detection phase"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The feature information that R-HOG descriptor cues on for motorbikes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some examples of detections on test images for the final person detector. . . . . . . . . . 66"
            },
            "venue": {
                "fragments": [],
                "text": "Some examples of detections on test images for the final person detector. . . . . . . . . . 66"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An overview of feature extraction for spatial histogram of classifiers approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2 An overview of Histogram of Oriented Gradient (HOG) feature extraction and object detection3 An overview of HOG feature extraction process for static images"
            },
            "venue": {
                "fragments": [],
                "text": "1 Overall object detection architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . chain"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1 The effect of window stride on average precision and maximum recall"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The performance of selected detectors on the INRIA static and moving data sets"
            },
            "venue": {
                "fragments": [],
                "text": "The performance of selected detectors on the INRIA static and moving data sets"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Example images from the INRIA static person data set with corresponding original annotations marked on it"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The feature extraction process for the motion channel"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Open source computer vision library"
            },
            "venue": {
                "fragments": [],
                "text": "Open source computer vision library"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classifier responses for a dense scan"
            },
            "venue": {
                "fragments": [],
                "text": "Classifier responses for a dense scan"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "5 An overview of the steps of the non-maximum suppression algorithm used to fuse multiple detections during the detection phase"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Informative features for profile-view pedestrians. Personal Communication"
            },
            "venue": {
                "fragments": [],
                "text": "Informative features for profile-view pedestrians. Personal Communication"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "tel-00390303, version 1 -1"
            },
            "venue": {
                "fragments": [],
                "text": "tel-00390303, version 1 -1"
            },
            "year": 131
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An illustration of the motion boundary histogram descriptor on a pair of consecutive images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cutting and Kozlowski 1977] or in the worst case determine his or her gender [Kozlowski and Cutting"
            },
            "venue": {
                "fragments": [],
                "text": "Cutting and Kozlowski 1977] or in the worst case determine his or her gender [Kozlowski and Cutting"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "4 A comparison of the various part detector"
            },
            "venue": {
                "fragments": [],
                "text": "fusion methods"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An overview of HOG feature extraction process for static images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object Detection Using Histograms of Oriented Gradients. In preparation for submission to Pattern Analysis and Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-maximum suppression for the fusion of overlapping detections"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some detection results for the contour fragment based framework"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual cues selected and learned by the HOG descriptors for the person detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some examples of detections on test images for the final person detector"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some false positive detection results for the contour fragment based framework"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2 Some examples of person detection in images where humans can use overall image context, high-level inference and logical reasoning to make accurate decisions about ambiguous object instances"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1 Performance comparison of head and shoulders, torso, and legs detectors. None of the part detectors perform as well as the full person"
            },
            "venue": {
                "fragments": [],
                "text": "detector"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "7 Overall recall-precision curves for the different object classes in the PASCAL"
            },
            "venue": {
                "fragments": [],
                "text": "The feature information that R-HOG descriptor cues on for motorbikes"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "7 An overview of the overall performance of our various motion detectors on different test data sets All detectors are trained on Motion Training Set 1 combined with the Static"
            },
            "venue": {
                "fragments": [],
                "text": "Test Set"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Overall object detection architecture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Overall recall-precision curves for the different object classes in the PASCAL 2006 VOC challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projects @BULLET Scientific collaborator in aceMedia, an European Union sixth frame work project. Led cross-functional team on \" Person Detection and Identification"
            },
            "venue": {
                "fragments": [],
                "text": "Projects @BULLET Scientific collaborator in aceMedia, an European Union sixth frame work project. Led cross-functional team on \" Person Detection and Identification"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "3 An overview of feature extraction for spatial histogram of classifiers approach"
            },
            "venue": {
                "fragments": [],
                "text": "3 An overview of feature extraction for spatial histogram of classifiers approach"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some normalised image windows from our INRIA static person detection data set"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast operator for detection and precise location of distinct points, corners and centres of circular features"
            },
            "venue": {
                "fragments": [],
                "text": "Intercommission Conference on Fast Processing of Photogrammetric Data"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2 Spatial smoothing proportional to the window stride gives the best results. Smoothing should be adapted to the window shape"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The performance of selected detectors on the MIT and INRIA static data sets. . . . . . 44 4.10 Visual cues selected and learned by the HOG descriptors for the person detection"
            },
            "venue": {
                "fragments": [],
                "text": "The performance of selected detectors on the MIT and INRIA static data sets. . . . . . 44 4.10 Visual cues selected and learned by the HOG descriptors for the person detection"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1 The miss rates of various detectors trained on Set 1 + Static images and tested on purely"
            },
            "venue": {
                "fragments": [],
                "text": "Static images"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Method of and apparatus for pattern recognition, January 1986. U.S"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Illustration of different coding schemes for internal motion histogram descriptors 78"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2 Scan range for the head and shoulders, the torso and the leg detectors"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "13 Complete window classifier learning algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "13 Complete window classifier learning algorithm"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classifier responses for a dense scan (at every pixel) of the person detector at a scale"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differences between the motion boundary histogram and motion boundary histogram descriptors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The complete object detection algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2 Key HOG parameters used for several different object classes"
            },
            "venue": {
                "fragments": [],
                "text": "."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IMHmd encoding algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "IMHmd encoding algorithm"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "8 Sample detections on Motion Test Set 2 from the combined R-HOG + IMHmd detector trained on Set 1"
            },
            "venue": {
                "fragments": [],
                "text": "+ Static"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 38,
            "methodology": 38,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 218,
        "totalPages": 22
    },
    "page_url": "https://www.semanticscholar.org/paper/Finding-People-in-Images-and-Videos-Dalal/47980c6e42f1a3381e6c5f3db7230e6a64c40218?sort=total-citations"
}