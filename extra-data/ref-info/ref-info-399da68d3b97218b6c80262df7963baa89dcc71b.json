{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066017394"
                        ],
                        "name": "P. Clarkson",
                        "slug": "P.-Clarkson",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Clarkson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Clarkson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As pointed out in [ 1 ], the main techniques for effective language modeling have been known for at least a decade, although one suspects that important advances are possible, and indeed needed, to bring about significant breakthroughs in the application areas cited above\u2014such breakthroughs just have been very hard to come by [2, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some additional statistics that also help gauge LM quality are the number of out-of-vocabulary (OOV) words and the \u201chit rates\u201d of various levels of N-grams (in LMs based on N-grams) [ 1 ]; these are either computed byngram itself or (as in the case of hit rates) tallied by auxiliary scripts that analyze the ngram output."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The CMU-Cambridge toolkit [ 1 ], and discussions with its original author, Roni Rosenfeld, served as a general inspiration and reference point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One such package, the CMU-Cambridge LM toolkit [ 1 ], has been in wide use in the research community and has greatly facilitated the construction of language models (LMs) for many practitioners."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13988648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87",
            "isKey": true,
            "numCitedBy": 707,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The CMU Statistical Language Modeling toolkit was re leased in in order to facilitate the construction and testing of bigram and trigram language models It is currently in use in over academic government and industrial laboratories in over countries This paper presents a new version of the toolkit We outline the con ventional language modeling technology as implemented in the toolkit and describe the extra e ciency and func tionality that the new toolkit provides as compared to previous software for this task Finally we give an exam ple of the use of the toolkit in constructing and testing a simple language model"
            },
            "slug": "Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Statistical language modeling using the CMU-cambridge toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The CMU Statistical Language Modeling toolkit was re leased in in order to facilitate the construction and testing of bigram and trigram language models and the technology as implemented in the toolkit is outlined."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Cache models \u2014 This well-known LM technique assigns nonzero probability to recent words, thus modeling the tendency of words to reoccur over short spans [ 12 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31924166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be1fed9544830df1137e72b1d2396c40d3e18365",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made. >"
            },
            "slug": "A-Cache-Based-Natural-Language-Model-for-Speech-Kuhn-Mori",
            "title": {
                "fragments": [],
                "text": "A Cache-Based Natural Language Model for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented and contains a 3g-gram component of the traditional type."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 151
                            }
                        ],
                        "text": "For lack of space we must refer to other publications for an introduction to language modeling and its role in speech recognition and other areas [3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60691216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54bcfca3fddc26b8889739a247a25e445818149",
            "isKey": false,
            "numCitedBy": 3827,
            "numCiting": 263,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing."
            },
            "slug": "Speech-and-language-processing-an-introduction-to-Jurafsky-Martin",
            "title": {
                "fragments": [],
                "text": "Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora, to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in artificial intelligence"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38232647"
                        ],
                        "name": "R. Iyer",
                        "slug": "R.-Iyer",
                        "structuredName": {
                            "firstName": "Rukmini",
                            "lastName": "Iyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803620"
                        ],
                        "name": "J. R. Rohlicek",
                        "slug": "J.-R.-Rohlicek",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Rohlicek",
                            "middleNames": [
                                "Robin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Rohlicek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026N-best generalization of the ROVER algorithm [20, 21].\nnbest-scripts \u2014 a collection of wrapper scripts that manipulate and rescore N-best lists.\npfsg-scripts \u2014 for converting LMs to word graphs.\nnbest-optimize \u2014 optimizes log linear score combination for word posterior-based (\u201csausage\u201d) decoding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16141899,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd646f7b2ceb85bf96e80d5c4a7a69e1ad4dfbe4",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a simple mixture language model that attempts to capture long distance constraints in a sentence or paragraph. The model is an m-component mixture of trigram models. The models were constructed using a 5K vocabulary and trained using a 76 million word Wall Street Journal text corpus. Using the BU recognition system, experiments show a 7% improvement in recognition accuracy with the mixture trigram models as compared to using a trigram model."
            },
            "slug": "Language-Modeling-with-Sentence-Level-Mixtures-Iyer-Ostendorf",
            "title": {
                "fragments": [],
                "text": "Language Modeling with Sentence-Level Mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper introduces a simple mixture language model that attempts to capture long distance constraints in a sentence or paragraph using an m-component mixture of trigram models."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70422141"
                        ],
                        "name": "Elizabeth Shriberg",
                        "slug": "Elizabeth-Shriberg",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Shriberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Shriberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A special type of hidden event LM can model speech disfluencies by allowing the hidden events to modify the word history; for example, a word deletion event would erase one or more words to model a false start [ 14 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 148263,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "766018bf388646e23bd53eb3692afb0f67915a14",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech disfluencies (such as filled pauses, repetitions, restarts) are among the characteristics distinguishing spontaneous speech from planned or read speech. We introduce a language model that predicts disfluencies probabilistically and uses an edited, fluent context to predict following words. The model is based on a generalization of the standard N-gram language model. It uses dynamic programming to compute the probability of a word sequence, taking into account possible hidden disfluency events. We analyze the model's performance for various disfluency types on the Switchboard corpus. We find that the model reduces the word perplexity in the neighborhood of disfluency events; however, overall differences are small and have no significant impact on the recognition accuracy. We also note that for modeling of the most frequent type of disfluency, filled pauses, a segmentation of utterances into linguistic (rather than acoustic) units is required. Our analysis illustrates a generally useful technique for language model evaluation based on local perplexity comparisons."
            },
            "slug": "Statistical-language-modeling-for-speech-Stolcke-Shriberg",
            "title": {
                "fragments": [],
                "text": "Statistical language modeling for speech disfluencies"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A language model is introduced that predicts disfluencies probabilistically and uses an edited, fluent context to predict following words and finds that the model reduces the word perplexity in the neighborhood of disfluency events; however, overall differences are small and have no significant impact on the recognition accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47825073"
                        ],
                        "name": "Weiqi Wang",
                        "slug": "Weiqi-Wang",
                        "structuredName": {
                            "firstName": "Weiqi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiqi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1614038854"
                        ],
                        "name": "Yang Liu",
                        "slug": "Yang-Liu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1999488"
                        ],
                        "name": "M. Harper",
                        "slug": "M.-Harper",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Harper",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Harper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One candidate for future addition is a more flexible class-based model, since refinements of class-based LMs seem to provide an effective and efficient way to incorporate grammatical information into the LM [ 23 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1340381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0770e7dd04120d5b2b87ed55d60eb5eb9775457c",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare the efficacy of a variety of language models (LMs) for rescoring word graphs and N-best lists generated by a large vocabulary continuous speech recognizer. These LMs differ based on the level of knowledge used (word, lexical features, syntax) and the type of integration of that knowledge (tight or loose). The trigram LM incorporates word level information; our Part-of-Speech (POS) LM uses word and lexical class information in a tightly coupled way; our new SuperARV LM tightly integrates word, a richer set of lexical features than POS, and syntactic dependency information; and the Parser LM integrates some limited word information, POS, and syntactic information. We also investigate LMs created using a linear interpolation of LM pairs. When comparing each LM on the task of rescoring word graphs or N-best lists for the Wall Street Journal (WSJ) 5k- and 20k- vocabulary test sets, the SuperARV LM always achieves the greatest reduction in word error rate (WER) and the greatest increase in sentence accuracy (SAC). On the 5k test sets, the SuperARV LM obtains more than a 10% relative reduction in WER compared to the trigram LM, and on the 20k test set more than 2%. Additionally, the SuperARV LM performs comparably to or better than the interpolated LMs. Hence, we conclude that the tight coupling of knowledge from all three levels is an effective method of constructing high quality LMs."
            },
            "slug": "Rescoring-effectiveness-of-language-models-using-of-Wang-Liu",
            "title": {
                "fragments": [],
                "text": "Rescoring effectiveness of language models using different levels of knowledge and their integration"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The tight coupling of knowledge from all three levels is an effective method of constructing high quality LMs for rescoring word graphs and N-best lists generated by a large vocabulary continuous speech recognizer."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Word classes may be defined manually or by a separate program, ngram-class, which induces classes from bigram statistics using the Brown algorithm [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746402"
                        ],
                        "name": "H. Bratt",
                        "slug": "H.-Bratt",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Bratt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bratt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148249"
                        ],
                        "name": "J. Butzberger",
                        "slug": "J.-Butzberger",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Butzberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Butzberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145660147"
                        ],
                        "name": "H. Franco",
                        "slug": "H.-Franco",
                        "structuredName": {
                            "firstName": "Horacio",
                            "lastName": "Franco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Franco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084801"
                        ],
                        "name": "V. R. Gadde",
                        "slug": "V.-R.-Gadde",
                        "structuredName": {
                            "firstName": "Venkata",
                            "lastName": "Gadde",
                            "middleNames": [
                                "Ramana",
                                "Rao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. R. Gadde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3220221"
                        ],
                        "name": "Colleen Richey",
                        "slug": "Colleen-Richey",
                        "structuredName": {
                            "firstName": "Colleen",
                            "lastName": "Richey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colleen Richey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70422141"
                        ],
                        "name": "Elizabeth Shriberg",
                        "slug": "Elizabeth-Shriberg",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Shriberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Shriberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807350"
                        ],
                        "name": "F. Weng",
                        "slug": "F.-Weng",
                        "structuredName": {
                            "firstName": "Fuliang",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Weng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143851659"
                        ],
                        "name": "Jing Zheng",
                        "slug": "Jing-Zheng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Together with a helper script, this tool also implements a word posterior-based N-best generalization of the ROVER algorithm [20,  21 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 168827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23318a16f8a049409be848be6e5fcdef22e78012",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe SRI\u2019s large vocabulary conversational speech r ecognition system as used in the March 2000 NIST Hub-5E evaluation. The system performs four recognition passes: (1) bigram recognition with phone-loop-adapted, within-word triphone acoustic models, (2) lattice generation with transcription-mod e-adapted models, (3) trigram lattice recognition with adapted cross -word triphone models, and (4) N-best rescoring and reranking with various additional knowledge sources. The system incorporates two new kinds of acoustic model: triphone models conditioned on speaking rate, and an explicit joint model of within-word phone durations. We also obtained an unusually large improvement from modeling crossword pronunciation variants in \u201cmultiword\u201d vocabulary items. The language model (LM) was enhanced with an \u201canti-LM\u201d representing acoustically confusable word sequences. Finally, we applied a generalized ROVER algorithm to combine the N-best hypotheses from several systems based on different acoustic models."
            },
            "slug": "THE-SRI-MARCH-2000-HUB-5-CONVERSATIONAL-SPEECH-Stolcke-Bratt",
            "title": {
                "fragments": [],
                "text": "THE SRI MARCH 2000 HUB-5 CONVERSATIONAL SPEECH TRANSCRIPTION SYSTEM"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "SRI\u2019s large vocabulary conversational speech r ecognition system as used in the March 2000 NIST Hub-5E evaluation is described and a generalized ROVER algorithm is applied to combine the N-best hypotheses from several systems based on different acoustic models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The decision to explore objectoriented design was based on a prior project, an implementation of various types of statistical grammars in the Common Lisp Object System [ 6 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2760798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccfcfe858592bd0554df7d17bd2c8b132c2ee3ef",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "The general topic of this thesis is the probabilistic modeling of language, in particular natural language. In probabilistic language modeling, one characterizes the strings of phonemes, words, etc. of a certain domain in terms of a probability distribution over all possible strings within the domain. Probabilistic language modeling has been applied to a wide range of problems in recent years, from the traditional uses in speech recognition to more recent applications in biological sequence modeling. \nThe main contribution of this thesis is a particular approach to the learning problem for probabilistic language models, known as Bayesian model merging. This approach can be characterized as follows. (1) Models are built either in batch mode or incrementally from samples, by incorporating individual samples into a working model. (2) A uniform, small number of simple operators works to gradually transform an instance-based model to a generalized model that abstracts from the data. (3) Instance-based parts of a model can coexist with generalized ones, depending on the degree of similarity among the observed samples, allowing the model to adapt to non-uniform coverage of the sample space. (4) The generalization process is driven and controlled by a uniform, probabilistic metric: the Bayesian posterior probability of a model, integrating both criteria of goodness-of-fit with respect to the data and a notion of model simplicity ('Occam's Razor'). \nThe Bayesian model merging framework is instantiated for three different classes of probabilistic models: Hidden Markov Models (HMMs), stochastic context-free grammars (SCFGs), and simple probabilistic attribute grammars (PAGs). Along with the theoretical background, various applications and case studies are presented, including the induction of multiple-pronunciation word models for speech recognition (with HMMs), data-driven learning of syntactic structures (with SCFGs), and the learning of simple sentence-meaning associations from examples (with PAGs). \nApart from language learning issues, a number of related computational problems involving probabilistic context-free grammars are discussed. A version of Earley's parser is presented that solves the standard problems associated with SCFGs efficiently, including the computation of sentence probabilities and sentence prefix probabilities, finding most likely parses, and the estimation of grammar parameters. \nFinally, we describe an algorithm that computes n-gram statistics from a given SCFG, based on solving linear systems derived from the grammar. This method can be an effective tool to transform part of the probabilistic knowledge from a structured language model into an unstructured low-level form for use in applications such as speech decoding. We show how this problem is just an instance of a larger class of related ones (such as average sentence length or derivation entropy) that are all solvable with the same computational technique. \nAn introductory chapter tries to present a unified view of the various model types and algorithms found in the literature, as well as issues of model learning and estimation."
            },
            "slug": "Bayesian-learning-of-probabilistic-language-models-Stolcke",
            "title": {
                "fragments": [],
                "text": "Bayesian learning of probabilistic language models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A version of Earley's parser is presented that solves the standard problems associated with SCFGs efficiently, including the computation of sentence probabilities and sentence prefix probabilities, finding most likely parses, and the estimation of grammar parameters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For lack of space we must refer to other publications for an introduction to language modeling and its role in speech recognition and other areas [ 3 , 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "As pointed out in [1], the main techniques for effective language modeling have been known for at least a decade, although one suspects that important advances are possible, and indeed needed, to bring about significant breakthroughs in the application areas cited above\u2014such breakthroughs just have been very hard to come by [2,  3 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10959945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6586e7c73cc1c9e9a251947425c54c5051be626",
            "isKey": false,
            "numCitedBy": 732,
            "numCiting": 205,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the first significant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data."
            },
            "slug": "Two-decades-of-statistical-language-modeling:-where-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Two decades of statistical language modeling: where do we go from here?"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A Bayesian approach to integration of linguistic theories with data is argued for inStatistical language models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781290"
                        ],
                        "name": "H. Murveit",
                        "slug": "H.-Murveit",
                        "structuredName": {
                            "firstName": "Hy",
                            "lastName": "Murveit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murveit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148249"
                        ],
                        "name": "J. Butzberger",
                        "slug": "J.-Butzberger",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Butzberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Butzberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121786"
                        ],
                        "name": "V. Digalakis",
                        "slug": "V.-Digalakis",
                        "structuredName": {
                            "firstName": "Vassilios",
                            "lastName": "Digalakis",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Digalakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744182"
                        ],
                        "name": "M. Weintraub",
                        "slug": "M.-Weintraub",
                        "structuredName": {
                            "firstName": "Mitch",
                            "lastName": "Weintraub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Weintraub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The software build system was borrowed from SRI\u2019s DecipherTM speech recognition system [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57374243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3492842d6ec502cd9d314ccf1080b0defdb7955f",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a technique called progressive search which is useful for developing and implementing speech recognition systems with high computational requirements. The scheme iteratively uses more and more complex recognition schemes, where each iteration constrains the speech space of the next. An algorithm called the forward-backward word-life algorithm is described. It can generate a word lattice in a progressive search that would be used as a language model embedded in a succeeding recognition pass to reduce computation requirements. It is shown that speed-ups of more than an order of magnitude are achievable with only minor costs in accuracy.<<ETX>>"
            },
            "slug": "Large-vocabulary-dictation-using-SRI's-DECIPHER-Murveit-Butzberger",
            "title": {
                "fragments": [],
                "text": "Large-vocabulary dictation using SRI's DECIPHER speech recognition system: progressive search techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The authors describe a technique called progressive search which is useful for developing and implementing speech recognition systems with high computational requirements and shows that speed-ups of more than an order of magnitude are achievable with only minor costs in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730059"
                        ],
                        "name": "K. Ma",
                        "slug": "K.-Ma",
                        "structuredName": {
                            "firstName": "Kristine",
                            "lastName": "Ma",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354787"
                        ],
                        "name": "G. Zavaliagkos",
                        "slug": "G.-Zavaliagkos",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zavaliagkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zavaliagkos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "HMMs of N-grams provide a general framework that can encode a variety of LM types proposed in the literature, such as sentence-level mixtures [15] and pivot LMs [ 16 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10800167,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "75d7a13f82da430171db6fe5b3765e485970ec7c",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "According to discourse theories in linguistics, conversational utterances possess an informational structure that partitions each sentence into two portions: a \"given\" and \"new\". We explore this idea by building sub-sentence discourse language models for conversational speech recognition. The internal sentence structure is captured in statistical language modeling by training multiple n-gram models using the expectation-maximization algorithm on the Switchboard corpus. The resulting model contributes to a 30% reduction in language model perplexity and a small gain in word error rate."
            },
            "slug": "Sub-sentence-discourse-models-for-conversational-Ma-Zavaliagkos",
            "title": {
                "fragments": [],
                "text": "Sub-sentence discourse models for conversational speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work builds sub-sentence discourse language models for conversational speech recognition by training multiple n-gram models using the expectation-maximization algorithm on the Switchboard corpus."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70422141"
                        ],
                        "name": "Elizabeth Shriberg",
                        "slug": "Elizabeth-Shriberg",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Shriberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Shriberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395813836"
                        ],
                        "name": "Dilek Z. Hakkani-T\u00fcr",
                        "slug": "Dilek-Z.-Hakkani-T\u00fcr",
                        "structuredName": {
                            "firstName": "Dilek",
                            "lastName": "Hakkani-T\u00fcr",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dilek Z. Hakkani-T\u00fcr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748051"
                        ],
                        "name": "G\u00f6khan T\u00fcr",
                        "slug": "G\u00f6khan-T\u00fcr",
                        "structuredName": {
                            "firstName": "G\u00f6khan",
                            "lastName": "T\u00fcr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G\u00f6khan T\u00fcr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 922042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d8b955ad56d1ffa52f39abab459a10d5790adb0",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a new approach for using speech prosody as a knowledge source for speech recognition. The idea is to penalize word hypotheses that are inconsistent with prosodic features such as duration and pitch. To model the interaction between words and prosody we modify the language model to represent hidden events such as sentence boundaries and various forms of disfluency, and combine with it decision trees that predict such events from prosodic features. N-best rescoring experiments on the Switchboard corpus show a small but consistent reduction of word error as a result of this modeling. We conclude with a preliminary analysis of the types of errors that are corrected by the prosodically informed model."
            },
            "slug": "Modeling-the-prosody-of-hidden-events-for-improved-Stolcke-Shriberg",
            "title": {
                "fragments": [],
                "text": "Modeling the prosody of hidden events for improved word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new approach to penalize word hypotheses that are inconsistent with prosodic features such as duration and pitch is investigated, and the language model is modified to represent hidden events such as sentence boundaries and various forms of disfluency."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Numerous helper and wrapper scripts perform miscellaneous tasks that are more conveniently implemented in the gawk and Bourne shell scripting languages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2622747,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "630679147c64c439d112fcfd7751e351397ae3db",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Language modeling, especially for spontaneous speech, often suffers from a mismatch of utterance segmentations between training and test conditions. In particular, training often uses linguistically-based segments, whereas testing occurs on acoustically determined segments, resulting in degraded performance. We present an N-best rescoring algorithm that removes the effect of segmentation mismatch. Furthermore, we show that explicit language modeling of hidden linguistic segment boundaries is improved by including turn-boundary events in the model. 1. THE SEGMENTATION PROBLEM IN LANGUAGE MODELING One of the problems encountered in speech recognition on continuous, spontaneous speech is the segmentation of long waveforms. Because current recognizers prefer short waveform segments for best performance and to limit computational resources, conversation-length waveforms are typically pre-segmented using simple acoustic criteria, such as locations of long pauses and turn switches. This creates several problems for language modeling: The segmentation algorithm used (including its parameters) influences the statistics embodied in the language model (LM), creating a potential mismatch between training and test set. Strictly speaking, one would have to resegment the training data, recreate the word-level transcriptions, and retrain the language model every time the segmentation process is modified. The acoustic segmentation typically yields units that are not linguistically coherent, and hence sub-optimal for language modeling. Language modeling research on spontaneous speech [10] shows that N-gram LMs based on complete utterance units give lower perplexity than those based only on acoustic segmentations. Furthermore, work reported in [12] showed that the word error rate on spontaneous speech can be reduced simply by resegmenting the speech at linguistic boundaries and using a language model based on the same segmentation. Explicit modeling of spontaneous speech phenomena such as disfluencies also requires modeling of linguistic (as opposed to acoustic) segment boundaries [15]. Similarly, sophisticated LMs modeling syntactic structure typically assume complete sentences as their input [12]. The following excerpt from the Switchboard corpus [2] illustrates the discrepancies between acoustic and linguistic segmentations. Linguistic segment boundaries are marked by , whereas acoustic boundaries are indicated by //. A subset of acoustic boundaries corresponds to turn boundaries, indicated by . B: Worried that they\u2019re not going to get enough attention?"
            },
            "slug": "Modeling-linguistic-segment-and-turn-boundaries-for-Stolcke",
            "title": {
                "fragments": [],
                "text": "Modeling linguistic segment and turn boundaries for n-best rescoring of spontaneous speech"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "An N-best rescoring algorithm is presented that removes the effect of segmentation mismatch between training and test conditions and shows that explicit language modeling of hidden linguistic segment boundaries is improved by including turn-boundary events in the model."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We summarize the characteristics of each implementation layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6135726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7d08c4fdf359f5cf0946a4a86db52d273a59ba4",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new framework for distilling information from word lattices to improve the accuracy of the speech recognition output and obtain a more perspicuous representation of a set of alternative hypotheses. In the standard MAP decoding approach the recognizer outputs the string of words corresponding to the path with the highest posterior probability given the acoustics and a language model. However, even given optimal models, the MAP decoder does not necessarily minimize the commonly used performance metric, word error rate (WER). We describe a method for explicitly minimizing WER by extracting word hypotheses with the highest posterior probabilities from word lattices. We change the standard problem formulation by replacing global search over a large set of sentence hypotheses with local search over a small set of word candidates. In addition to improving the accuracy of the recognizer, our method produces a new representation of a set of candidate hypotheses that specifies the sequence of word-level confusions in a compact lattice format. We study the properties of confusion networks and examine their use for other tasks, such as lattice compression, word spotting, confidence annotation, and reevaluation of recognition hypotheses using higher-level knowledge sources."
            },
            "slug": "Finding-consensus-in-speech-recognition:-word-error-Mangu-Brill",
            "title": {
                "fragments": [],
                "text": "Finding consensus in speech recognition: word error minimization and other applications of confusion networks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new framework for distilling information from word lattices is described to improve the accuracy of the speech recognition output and obtain a more perspicuous representation of a set of alternative hypotheses."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "prune N-gram parameters, using an entropy criterion [10]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8150809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29053eab305c2b585bcfbb713243b05646e7d62d",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance."
            },
            "slug": "Entropy-based-Pruning-of-Backoff-Language-Models-Stolcke",
            "title": {
                "fragments": [],
                "text": "Entropy-based Pruning of Backoff Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models and shown that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643845523"
                        ],
                        "name": "F. ChenStanley",
                        "slug": "F.-ChenStanley",
                        "structuredName": {
                            "firstName": "F",
                            "lastName": "ChenStanley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. ChenStanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643789739"
                        ],
                        "name": "GoodmanJoshua",
                        "slug": "GoodmanJoshua",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "GoodmanJoshua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "GoodmanJoshua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Currently supported methods include Good-Turing, absolute, Witten-Bel l, and modified Kneser-Ney [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 85
                            }
                        ],
                        "text": "Currently supported methods include Good-Turing, absolute, Witten-Bell, and modified Kneser-Ney [9]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 215842252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "isKey": false,
            "numCitedBy": 2861,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."
            },
            "slug": "An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A survey of the most widely-used algorithms for smoothing models for language n -gram modeling and an extensive empirical comparison of several of these smoothing techniques are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745906"
                        ],
                        "name": "Y. Konig",
                        "slug": "Y.-Konig",
                        "structuredName": {
                            "firstName": "Yochai",
                            "lastName": "Konig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Konig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744182"
                        ],
                        "name": "M. Weintraub",
                        "slug": "M.-Weintraub",
                        "structuredName": {
                            "firstName": "Mitch",
                            "lastName": "Weintraub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Weintraub"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "nbest-lattice \u2014 a tool to perform word error minimization on N-best lists [ 18 ] or construct confusion networks (\u201csausages\u201d) [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6874209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0765c1fef84f21b89d00a6c5035d73bbf8aaa9c6",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that the standard hypothesis scoring paradigm used in maximum-likelihood-based speech recognition systems is not optimal with regard to minimizing the word error rate, the commonly used performance metric in speech recognition. This can lead to sub-optimal performance, especially in high-error-rate environments where word error and sentence error are not necessarily monotonically related. To address this discrepancy, we developed a new algorithm that explicitly minimizes expected word error for recognition hypotheses. First, we approximate the posterior hypothesis probabilities using N-best lists. We then compute the expected word error for each hypothesis with respect to the posterior distribution, and choose the hypothesis with the lowest error. Experiments show improved recognition rates on two spontaneous speech corpora."
            },
            "slug": "Explicit-word-error-minimization-in-n-best-list-Stolcke-Konig",
            "title": {
                "fragments": [],
                "text": "Explicit word error minimization in n-best list rescoring"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new algorithm is developed that explicitly minimizes expected word error for recognition hypotheses, and approximate the posterior hypothesis probabilities using N-best lists and chooses the hypothesis with the lowest error."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3241934"
                        ],
                        "name": "J. Fiscus",
                        "slug": "J.-Fiscus",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Fiscus",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fiscus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Together with a helper script, this tool also implements a word posterior-based N-best generalization of the ROVER algorithm [ 20 , 21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18751160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0861015b3c89d68749548c19c6f056eee34eafc6",
            "isKey": false,
            "numCitedBy": 1153,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a system developed at NIST to produce a composite automatic speech recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has a lower error rate than any of the individual systems. The system implements a \"voting\" or rescoring process to reconcile differences in ASR system outputs. We refer to this system as the NIST Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources are added to an ASR system (e.g. acoustic and language models), error rates are typically decreased. This paper describes a post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate. To accomplish this, the outputs of multiple of ASR systems are combined into a single, minimal-cost word transition network (WTN) via iterative applications of dynamic programming (DP) alignments. The resulting network is searched by an automatic rescoring or \"voting\" process that selects the output sequence with the lowest score."
            },
            "slug": "A-post-processing-system-to-yield-reduced-word-Fiscus",
            "title": {
                "fragments": [],
                "text": "A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Currently supported methods include Good-Turing, absolute, Witten-Bell, and modified Kneser-Ney [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221583858,
            "fieldsOfStudy": [],
            "id": "769dbbe88801b57a9b44f89c5516264f16cbed60",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 329
                            }
                        ],
                        "text": "As pointed out in [1], the main techniques for effe ctive language modeling have been known for at least a decade, although one suspects that important advances are possible, a nd indeed needed, to bring about significant breakthroughs in the application areas cited above\u2014such breakthroughs just have been v ery hard to come by [2, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40671443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b336f9a030d0fb2983b34182b7333115c27b7712",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Up-from-trigrams!-the-struggle-for-improved-models-Jelinek",
            "title": {
                "fragments": [],
                "text": "Up from trigrams! - the struggle for improved language models"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "Word classes may be defined manually or by a separate program, ngram-class, which induces classes from bigram statistics using the Brown algorithm [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A cache-base natural language model for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE PAMI"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Word graphs use SRI\u2019s probabilistic finite-state grammar (PFSG) format, which ca n be converted to and from that used by AT&T\u2019s finite state machine toolkit [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FSM Library\u2014 general-purpose finite-state machine software tools, vers  ion 3.6"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.research.att.com/sw/tools/fsm/,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "A first implementation with minimal functionality for standard N-gram models was created prior to the 1995 Johns Hopkins Language Modeling Summer Workshop [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 198
                            }
                        ],
                        "text": "Dynamically interpolated LMs\u2014 Two or more LMs can be interpolated linearly at the word level such that the interpolation weights reflect the likelihoods of the models given the recent Ngram history [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LM95 Project Report: Fast training and portability\u201d, Research Note 1, Center for Language and Speech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "One candidate for future addition is a more flexible clas s-based model, since refinements of class-based LMs seem to provide a n effective and efficient way to incorporate grammatical info rmation into the LM [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rescoring effectiven  ess of language models using different levels of knowledge and the ir integration"
            },
            "venue": {
                "fragments": [],
                "text": "in Proc. ICASSP,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[2] F. Jelinek, \u201cUp from trigrams!"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FSM Library\u2014 general-purpose finite-state machine software tools, version 3"
            },
            "venue": {
                "fragments": [],
                "text": "FSM Library\u2014 general-purpose finite-state machine software tools, version 3"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "The software build system was borrowed from SRI\u2019 s DecipherTM speech recognition system [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weintrau  b, \u201cLargevocabulary dictation using SRI\u2019s DECIPHER speech recognit ion system: Progressive search techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICASSP,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "nbest-lattice \u2014 a tool to perform word error minimization on N-best lists [18] or construct confusion networks (\u201csausages\u201d) [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding consensus i n speech recognition: Word error minimization and other applicatio ns of confusion networks\u201d,Computer Speech and Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 200
                            }
                        ],
                        "text": "Dynamically interpolated LMs\u2014 Two or more LMs can be interpolated linearly at the word level such that the interp olation weights reflect the likelihoods of the models given the recen t Ngram history [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": "A first implementation with minimal functionality for stand ard N-gram models was created prior to the 1995 Johns Hopkins Lan guage Modeling Summer Workshop [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 130
                            }
                        ],
                        "text": "A first implementation with minimal functionality for standard N-gram models was created prior to the 1995 Johns Hopkins Language Modeling Summer Workshop [8]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LM95 Pro ject Report: Fast training and portability\u201d, Research Note 1, Ce  nter for Language and Speech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 264
                            }
                        ],
                        "text": "Besides computing test set log proba bilities from text or counts, it can renormalize a model (recomputing backoff weights) approximate a class-based or interpolated N-gram with a sta ndard word-based backoff LM prune N-gram parameters, using an entropy criterion [10] prepare LMs for conversion to finite-state graphs by removin g N-grams that would be superseded by backoffs generate random sentences from the distribution embodied b y the LM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Entropy-based pruning of backoff languag e models\u201d, in Proceedings DARPA Broadcast News Transcription and Unde  rstanding Workshop"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 147
                            }
                        ],
                        "text": "For lack of space we must refer to other publications for an introduction to language modelin g a d its role in speech recognition and other areas [3, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech and Language Processing: An Introduction to Natural Language Processing, Computation  al Linguistics, and Speech Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "The HTK Lattice Toolkit [5] (to w hich SRILM has an interface) provided many good ideas for a viable and efficient API for language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 4
                            }
                        ],
                        "text": "The HTK Lattice Toolkit [5] (to which SRILM has an interface) provided many good ideas for a viable and efficient API for language models."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Odell,Lattice and Language Model Toolkit Reference Manual"
            },
            "venue": {
                "fragments": [],
                "text": "Entropic Cambridge Research Laboratories,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "Cache models\u2014 This well-known LM technique assigns nonzero probability to recent words, thus modeling the tend ency of words to reoccur over short spans [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A cache-base natural language mo  del for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE PAMI,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Word graphs use SRI\u2019s probabilistic finite-state grammar (PFSG) format, which can be converted to and from that used by AT&T\u2019s finite state machine toolkit [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FSM Library\u2014 general-purpose finite-state machine software tools, version 3.6"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.research.att.com/sw/tools/fsm/,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A first implementation with minimal functionality for standard N-gram models was created prior to the 1995 Johns Hopkins Language Modeling Summer Workshop [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026N-best generalization of the ROVER algorithm [20, 21].\nnbest-scripts \u2014 a collection of wrapper scripts that manipulate and rescore N-best lists.\npfsg-scripts \u2014 for converting LMs to word graphs.\nnbest-optimize \u2014 optimizes log linear score combination for word posterior-based (\u201csausage\u201d) decoding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LM95 Project Report: Fast training and portability"
            },
            "venue": {
                "fragments": [],
                "text": "Research Note"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "The HTK Lattice Toolkit [5] (to which SRILM has an interface) provided many good ideas for a viable and efficient API for language models."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lattice and Language Model Toolkit Reference Manual"
            },
            "venue": {
                "fragments": [],
                "text": "Lattice and Language Model Toolkit Reference Manual"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "A special type of hidden event LM can model speech disfluencies by allowing the hidden events to modify t he word history; for example, a word deletion event would erase on or more words to model a false start [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical language mode ling for speech disfluencies"
            },
            "venue": {
                "fragments": [],
                "text": "in Proc. ICASSP,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 146
                            }
                        ],
                        "text": "For lack of space we must refer to other publications for an introduction to language modeling and its role in speech recognition and other areas [3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, Prentice-Hall"
            },
            "venue": {
                "fragments": [],
                "text": "Upper Saddle River, NJ,"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 25
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/SRILM-an-extensible-language-modeling-toolkit-Stolcke/399da68d3b97218b6c80262df7963baa89dcc71b?sort=total-citations"
}