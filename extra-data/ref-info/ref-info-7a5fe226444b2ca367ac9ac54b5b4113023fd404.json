{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712738"
                        ],
                        "name": "C. Sch\u00fcldt",
                        "slug": "C.-Sch\u00fcldt",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Sch\u00fcldt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sch\u00fcldt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033284"
                        ],
                        "name": "B. Caputo",
                        "slug": "B.-Caputo",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Caputo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caputo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "Spatial-temporal features have shown particular promise in motion understanding due to its rich descriptive power [3, 14, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8777811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b480f6a3750b4cebaf1db205692c8321d45926a2",
            "isKey": false,
            "numCitedBy": 3080,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Local space-time features capture local events in video and can be adapted to the size, the frequency and the velocity of moving patterns. In this paper, we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented results of action recognition justify the proposed method and demonstrate its advantage compared to other relative approaches for action recognition."
            },
            "slug": "Recognizing-human-actions:-a-local-SVM-approach-Sch\u00fcldt-Laptev",
            "title": {
                "fragments": [],
                "text": "Recognizing human actions: a local SVM approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition and presents the presented results of action recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089071"
                        ],
                        "name": "Lena Gorelick",
                        "slug": "Lena-Gorelick",
                        "structuredName": {
                            "firstName": "Lena",
                            "lastName": "Gorelick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lena Gorelick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50170517"
                        ],
                        "name": "M. Blank",
                        "slug": "M.-Blank",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Blank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760994"
                        ],
                        "name": "R. Basri",
                        "slug": "R.-Basri",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Basri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Basri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "We test our model using the human action dataset from [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "Specifically, certain actions, such as hand waving in [2], produce a small number of motion features since most body parts remain static."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "Based on the recent works in human motion categorization [2, 10, 14, 16], we make two key observations that will in turn influence the design of our model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 255
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "We test the model on a publicly available human action dataset [2] and show that our new method performs well on the classification task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Human actions dataset: Example frames from video sequences in the dataset from [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "The first reported classification results on this dataset appeared on [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52847824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "320caf8d8ae63331d48ed2d824aa0c13d0af3617",
            "isKey": true,
            "numCitedBy": 1383,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Human action in video sequences can be seen as silhouettes of a moving torso and protruding limbs undergoing articulated motion. We regard human actions as three-dimensional shapes induced by the silhouettes in the space-time volume. We adopt a recent approach [14] for analyzing 2D shapes and generalize it to deal with volumetric space-time action shapes. Our method utilizes properties of the solution to the Poisson equation to extract space-time features such as local space-time saliency, action dynamics, shape structure, and orientation. We show that these features are useful for action recognition, detection, and clustering. The method is fast, does not require video alignment, and is applicable in (but not limited to) many scenarios where the background is known. Moreover, we demonstrate the robustness of our method to partial occlusions, nonrigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action, and low-quality video."
            },
            "slug": "Actions-as-Space-Time-Shapes-Gorelick-Blank",
            "title": {
                "fragments": [],
                "text": "Actions as Space-Time Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The method is fast, does not require video alignment, and is applicable in many scenarios where the background is known, and the robustness of the method is demonstrated to partial occlusions, nonrigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action, and low-quality video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080929103"
                        ],
                        "name": "Claudio Fanti",
                        "slug": "Claudio-Fanti",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Fanti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claudio Fanti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398327241"
                        ],
                        "name": "Lihi Zelnik-Manor",
                        "slug": "Lihi-Zelnik-Manor",
                        "structuredName": {
                            "firstName": "Lihi",
                            "lastName": "Zelnik-Manor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihi Zelnik-Manor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "proposed in [10] that it is fruitful to utilize a mixture of both static and dynamic features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "Based on the recent works in human motion categorization [2, 10, 14, 16], we make two key observations that will in turn influence the design of our model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 191
                            }
                        ],
                        "text": "Unfortunately due to the computational complexity of the model, previous works can only use a very small number of features (typically 4 to 6) or approximate the connections by triangulation [10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] represent the human action model as a triangulated graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 43
                            }
                        ],
                        "text": "Constellation models offer such a solution [10, 21]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1765587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ae55802a906fadd51203ed41793e480ce4c5d23",
            "isKey": true,
            "numCitedBy": 129,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic models have been previously shown to be efficient and effective for modeling and recognition of human motion. In particular we focus on methods which represent the human motion model as a triangulated graph. Previous approaches learned models based just on positions and velocities of the body parts while ignoring their appearance. Moreover, a heuristic approach was commonly used to obtain translation invariance. In this paper we suggest an improved approach for learning such models and using them for human motion recognition. The suggested approach combines multiple cues, i.e., positions, velocities and appearance into both the learning and detection phases. Furthermore, we introduce global variables in the model, which can represent global properties such as translation, scale or view-point. The model is learned in an unsupervised manner from unlabelled data. We show that the suggested hybrid probabilistic model (which combines global variables, like translation, with local variables, like relative positions and appearances of body parts), leads to: (i) faster convergence of learning phase, (it) robustness to occlusions, and, (Hi) higher recognition rate."
            },
            "slug": "Hybrid-models-for-human-motion-recognition-Fanti-Zelnik-Manor",
            "title": {
                "fragments": [],
                "text": "Hybrid models for human motion recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper focuses on methods which represent the human motion model as a triangulated graph and introduces global variables in the model, which can represent global properties such as translation, scale or view-point, and shows that the suggested hybrid probabilistic model leads to faster convergence of learning phase and higher recognition rate."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684865"
                        ],
                        "name": "Guillaume Bouchard",
                        "slug": "Guillaume-Bouchard",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Bouchard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Bouchard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "Our model is partly inspired by a hierarchical model proposed by Bouchard and Triggs in [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6193293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3452d42804cbea13b8b252d08ec249b008e8df93",
            "isKey": true,
            "numCitedBy": 174,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a generative model that codes the geometry and appearance of generic visual object categories as a loose hierarchy of parts, with probabilistic spatial relations linking parts to subparts, soft assignment of subparts to parts, and scale invariant keypoint based local features at the lowest level of the hierarchy. The method is designed to efficiently handle categories containing hundreds of redundant local features, such as those returned by current key-point detectors. This robustness allows it to outperform constellation style models, despite their stronger spatial models. The model is initialized by robust bottom-up voting over location-scale pyramids, and optimized by expectation-maximization. Training is rapid, and objects do not need to be marked in the training images. Experiments on several popular datasets show the method's ability to capture complex natural object classes."
            },
            "slug": "Hierarchical-part-based-visual-object-Bouchard-Triggs",
            "title": {
                "fragments": [],
                "text": "Hierarchical part-based visual object categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A generative model that codes the geometry and appearance of generic visual object categories as a loose hierarchy of parts, with probabilistic spatial relations linking parts to subparts, soft assignment of subparts to parts, and scale invariant keypoint based local features at the lowest level of the hierarchy."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3254319"
                        ],
                        "name": "Hongcheng Wang",
                        "slug": "Hongcheng-Wang",
                        "structuredName": {
                            "firstName": "Hongcheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongcheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "Spatial-temporal features have shown particular promise in motion understanding due to its rich descriptive power [3, 14, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "Based on the recent works in human motion categorization [2, 10, 14, 16], we make two key observations that will in turn influence the design of our model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 103
                            }
                        ],
                        "text": "They have proven to be highly efficient and effective in classifying objects [12, 19] and human motion [8, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 85439346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "263442bab9faba9ad84c8a1dfb6ca9a0947bd4d4",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Imagine a video taken on a sunny beach, can a computer automatically tell what is happening in the scene? Can it identify different human activities in the video, such as water surfing, people walking and lying on the beach? To automatically classify or localize different actions in video sequences is very useful for a variety of tasks, such as video surveillance, objectlevel video summarization, video indexing, digital library organization, etc. However, it remains a challenging task for computers to achieve robust action recognition due to cluttered background, camera motion, occlusion, and geometric and photometric variances of objects. For example, in a live video of a skating competition, the skater moves rapidly across the rink, and the camera also moves to follow the skater. With moving camera, non-stationary background, and moving target, few vision algorithms could identify, categorize and localize such motions well. In addition, the challenge is even greater when there are multiple activities in a complex video sequence (Figure 1). We present a video demo for our novel unsupervised learning method for human action categories [1]. A video sequence is represented as a collection of spatial-temporal words by extracting space-time interest points. The algorithm learns the probability distributions of the spatial-temporal words and intermediate topics corresponding to human action categories automatically using a probabilistic Latent Semantic Analysis (pLSA) model [4]. The learned model is then used for human action categorization and localization in a novel video, by maximizing the posterior of action category (topic) distributions. The contributions of this work are as follows: \u2022 Unsupervised learning of actions using \u2018video words\u2019 representation. We deploy a pLSA model with \u2018bag of video words\u2019 representation for video analysis; \u2022 Multiple action localization and categorization. Our approach is not only able to classify different actions, but also to localize different actions simultaneously in a novel and complex video sequence."
            },
            "slug": "Unsupervised-Learning-of-Human-Action-Categories-Niebles-Wang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Human Action Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The approach is not only able to classify different actions, but also to localize different actions simultaneously in a novel and complex video sequence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 163
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1350374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804d86dd7ab3498266922244e73a88c1add5a6ab",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as two forms of data-based action synthesis \"do as I do\" and \"do as I say\". Results are demonstrated on ballet, tennis as well as football datasets."
            },
            "slug": "Recognizing-action-at-a-distance-Efros-Berg",
            "title": {
                "fragments": [],
                "text": "Recognizing action at a distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure is introduced, and an associated similarity measure to be used in a nearest-neighbor framework is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704879"
                        ],
                        "name": "H. Kjellstr\u00f6m",
                        "slug": "H.-Kjellstr\u00f6m",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Kjellstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kjellstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 163
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1255196,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c368eb34697350e2d9921a5354ddda76813408c3",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper address the problems of modeling the appearance of humans and distinguishing human appearance from the appearance of general scenes. We seek a model of appearance and motion that is generic in that it accounts for the ways in which people's appearance varies and, at the same time, is specific enough to be useful for tracking people in natural scenes. Given a 3D model of the person projected into an image we model the likelihood of observing various image cues conditioned on the predicted locations and orientations of the limbs. These cues are taken to be steered filter responses corresponding to edges, ridges, and motion-compensated temporal differences. Motivated by work on the statistics of natural scenes, the statistics of these filter responses for human limbs are learned from training images containing hand-labeled limb regions. Similarly, the statistics of the filter responses in general scenes are learned to define a \u201cbackground\u201d distribution. The likelihood of observing a scene given a predicted pose of a person is computed, for each limb, using the likelihood ratio between the learned foreground (person) and background distributions. Adopting a Bayesian formulation allows cues to be combined in a principled way. Furthermore, the use of learned distributions obviates the need for hand-tuned image noise models and thresholds. The paper provides a detailed analysis of the statistics of how people appear in scenes and provides a connection between work on natural image statistics and the Bayesian tracking of people."
            },
            "slug": "Learning-the-Statistics-of-People-in-Images-and-Kjellstr\u00f6m-Black",
            "title": {
                "fragments": [],
                "text": "Learning the Statistics of People in Images and Video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The paper provides a detailed analysis of the statistics of how people appear in scenes and provides a connection between work on natural image statistics and the Bayesian tracking of people."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 77
                            }
                        ],
                        "text": "They have proven to be highly efficient and effective in classifying objects [12, 19] and human motion [8, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a737c107623bcffefa0bac20f1b64677f6a1255a",
            "isKey": false,
            "numCitedBy": 1142,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing only one object per image. We also extend the bag-of-words vocabulary to include 'doublets' which encode spatially local co-occurring regions. It is demonstrated that this extended vocabulary gives a cleaner image segmentation. Finally, the classification and segmentation methods are applied to a set of images containing multiple objects per image. These results demonstrate that we can successfully build object class models from an unsupervised analysis of images."
            },
            "slug": "Discovering-objects-and-their-location-in-images-Sivic-Russell",
            "title": {
                "fragments": [],
                "text": "Discovering objects and their location in images"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work treats object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics, and develops a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA)."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404428"
                        ],
                        "name": "Yang Song",
                        "slug": "Yang-Song",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149680415"
                        ],
                        "name": "L. Goncalves",
                        "slug": "L.-Goncalves",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Goncalves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goncalves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 191
                            }
                        ],
                        "text": "Unfortunately due to the computational complexity of the model, previous works can only use a very small number of features (typically 4 to 6) or approximate the connections by triangulation [10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6822921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bc66914ab7d39e016e7d496e99f67f68199e241",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm that can obtain a probabilistic model of an object composed of a collection of parts (a moving human body in our examples) automatically from unlabeled training data is presented. The training data include both useful \"foreground\" features as well as features that arise from irrelevant background clutter - the correspondence between parts and detected features is unknown. The joint probability density function of the parts is represented by a mixture of decomposable triangulated graphs which allow for fast detection. To learn the model structure as well as model parameters, an EM-like algorithm is developed where the labeling of the data (part assignments) is treated as hidden variables. The unsupervised learning technique is not limited to decomposable triangulated graphs. The efficiency and effectiveness of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled image sequences, and testing the learned models on a variety of sequences."
            },
            "slug": "Unsupervised-Learning-of-Human-Motion-Song-Goncalves",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Human Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "An unsupervised learning algorithm that can obtain a probabilistic model of an object composed of a collection of parts automatically from unlabeled training data is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "Based on the recent works in human motion categorization [2, 10, 14, 16], we make two key observations that will in turn influence the design of our model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 702738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a03a9ad60525db2fe6afb29883174bb8ce937360",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a system that can annotate a video sequence with: a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view. The system does not require a fixed background, and is automatic. The system works by (1) tracking people in 2D and then, using an annotated motion capture dataset, (2) synthesizing an annotated 3D motion sequence matching the 2D tracks. The 3D motion capture data is manually annotated off-line using a class structure that describes everyday motions and allows motion annotations to be composed \u2014 one may jump while running, for example. Descriptions computed from video of real motions show that the method is accurate."
            },
            "slug": "Automatic-Annotation-of-Everyday-Movements-Ramanan-Forsyth",
            "title": {
                "fragments": [],
                "text": "Automatic Annotation of Everyday Movements"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A system that can annotate a video sequence with a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762649"
                        ],
                        "name": "V. Rabaud",
                        "slug": "V.-Rabaud",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Rabaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rabaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524603"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "Motion features are obtained using the separable linear filter method in [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 103
                            }
                        ],
                        "text": "They have proven to be highly efficient and effective in classifying objects [12, 19] and human motion [8, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] applied an SVM classifier to learn the differences among videos containing different human motions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 255
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "The motion features are obtained using the method in [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1956774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f1707caad72573633c2307fa26ec093e8f4bb03",
            "isKey": true,
            "numCitedBy": 2717,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior."
            },
            "slug": "Behavior-recognition-via-sparse-spatio-temporal-Doll\u00e1r-Rabaud",
            "title": {
                "fragments": [],
                "text": "Behavior recognition via sparse spatio-temporal features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and an alternative is proposed, and a recognition algorithm based on spatio-temporally windowed data is devised."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7285418"
                        ],
                        "name": "X. Feng",
                        "slug": "X.-Feng",
                        "structuredName": {
                            "firstName": "Xiaolin",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 110
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11845424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc3442c4dac375410e0501c9238082002f0a8856",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the recognition of human actions in image sequences is presented. The algorithm consists of 3 stages: background subtraction, body pose classification, and action recognition. A pose is represented in space-time, called 'movelet'. A movelet is a collection of the shape, motion and occlusion of image patches corresponding to the main parts of the body. The (infinite) set of all possible movelets is quantized into codewords obtained by the vector quantization. For every pair of frames each codeword is assigned a probability. Recognition is performed by simultaneously estimating the most likely sequence of codewords and the action that took place in a sequence. This is done using hidden Markov models. Experiments on the recognition of 3 periodic human actions, each under 3 different viewpoints, and 8 non-periodic human actions, are presented. training and testing are performed on different subjects with encouraging results. The influence of the number of codewords on the algorithm performance is studied."
            },
            "slug": "Human-action-recognition-by-sequence-of-movelet-Feng-Perona",
            "title": {
                "fragments": [],
                "text": "Human action recognition by sequence of movelet codewords"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An algorithm for the recognition of human actions in image sequences is presented that consists of 3 stages: background subtraction, body pose classification, and action recognition using hidden Markov models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. First International Symposium on 3D Data Processing Visualization and Transmission"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110604335"
                        ],
                        "name": "Markus Weber",
                        "slug": "Markus-Weber",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 43
                            }
                        ],
                        "text": "Constellation models offer such a solution [10, 21]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8970876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cf1527807ebb16020b04d4166e7ba8d27652302",
            "isKey": false,
            "numCitedBy": 757,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars."
            },
            "slug": "Unsupervised-Learning-of-Models-for-Recognition-Weber-Welling",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Models for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method to learn object class models from unlabeled and unsegmented cluttered cluttered scenes for the purpose of visual object recognition that achieves very good classification results on human faces and rear views of cars."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698689"
                        ],
                        "name": "N. Jojic",
                        "slug": "N.-Jojic",
                        "structuredName": {
                            "firstName": "Nebojsa",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jojic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14556010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba2d5d0407ed25a31751d1b3c4ea841313cfcdc3",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering is a simple, effective way to derive useful representations of data, such as images and videos. Clustering explains the input as one of several prototypes, plus noise. In situations where each input has been randomly transformed (e.g., by translation, rotation, and shearing in images and videos), clustering techniques tend to extract cluster centers that account for variations in the input due to transformations, instead of more interesting and potentially useful structure. For example, if images from a video sequence of a person walking across a cluttered background are clustered, it would be more useful for the different clusters to represent different poses and expressions, instead of different positions of the person and different configurations of the background clutter. We describe a way to add transformation invariance to mixture models, by approximating the nonlinear transformation manifold by a discrete set of points. We show how the expectation maximization algorithm can be used to jointly learn clusters, while at the same time inferring the transformation associated with each input. We compare this technique with other methods for filtering noisy images obtained from a scanning electron microscope, clustering images from videos of faces into different categories of identification and pose and removing foreground obstructions from video. We also demonstrate that the new technique is quite insensitive to initial conditions and works better than standard techniques, even when the standard techniques are provided with extra data."
            },
            "slug": "Transformation-Invariant-Clustering-Using-the-EM-Frey-Jojic",
            "title": {
                "fragments": [],
                "text": "Transformation-Invariant Clustering Using the EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how the expectation maximization algorithm can be used to jointly learn clusters, while at the same time inferring the transformation associated with each input, by approximating the nonlinear transformation manifold by a discrete set of points."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 110
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15] recently proposed a Conditional Random Field model"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8170470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd0597f8513dc100cd0bc1b493768cde45098a9",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database."
            },
            "slug": "Learning-to-parse-images-of-articulated-bodies-Ramanan",
            "title": {
                "fragments": [],
                "text": "Learning to parse images of articulated bodies"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work considers the machine vision task of pose estimation from static images, specifically for the case of articulated objects, and casts visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 77
                            }
                        ],
                        "text": "They have proven to be highly efficient and effective in classifying objects [12, 19] and human motion [8, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7783445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f2e2fee9ac508c8aefb59371fd982a4eb768058",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically learn object categories from unlabeled images. Each image is represented by an unordered set of local features, and all sets are embedded into a space where they cluster according to their partial-match feature correspondences. After efficiently computing the pairwise affinities between the input images in this space, a spectral clustering technique is used to recover the primary groupings among the images. We introduce an efficient means of refining these groupings according to intra-cluster statistics over the subsets of features selected by the partial matches between the images, and based on an optional, variable amount of user supervision. We compute the consistent subsets of feature correspondences within a grouping to infer category feature masks. The output of the algorithm is a partition of the data into a set of learned categories, and a set of classifiers trained from these ranked partitions that can recognize the categories in novel images."
            },
            "slug": "Unsupervised-Learning-of-Categories-from-Sets-of-Grauman-Darrell",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Categories from Sets of Partially Matching Image Features"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces an efficient means of refining groupings according to intra-cluster statistics over the subsets of features selected by the partial matches between the images, and based on an optional, variable amount of user supervision."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 110
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8729004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44f3ac3277c2eb6e5599739eb875888c46e21d4c",
            "isKey": false,
            "numCitedBy": 1776,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8% miss rate on our Test Set 1."
            },
            "slug": "Human-Detection-Using-Oriented-Histograms-of-Flow-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Human Detection Using Oriented Histograms of Flow and Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A detector for standing and moving people in videos with possibly moving cameras and backgrounds is developed, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060848835"
                        ],
                        "name": "Vincent Cheung",
                        "slug": "Vincent-Cheung",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Cheung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Cheung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698689"
                        ],
                        "name": "N. Jojic",
                        "slug": "N.-Jojic",
                        "structuredName": {
                            "firstName": "Nebojsa",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jojic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 255
                            }
                        ],
                        "text": "Generally speaking, there are three popular types of features: static features based on edges and limb shapes [7, 11, 15]; dynamic features based on optical flows [7, 9, 18], and spatialtemporal features that characterizes a space-time volume of the data [2, 6, 8, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10838917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c281a9029e13cabe0c4482879c99687eb68c461",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, \u201cepitomes\u201d were introduced as patch-based probability models that are learned by compiling together a large number of examples of patches from input images. In this paper, we describe how epitomes can be used to model video data and we describe significant computational speedups that can be incorporated into the epitome inference and learning algorithm. In the case of videos, epitomes are estimated so as to model most of the small space-time cubes from the input data. Then, the epitome can be used for various modeling and reconstruction tasks, of which we show results for video super-resolution, video interpolation, and object removal. Besides computational efficiency, an interesting advantage of the epitome as a representation is that it can be reliably estimated even from videos with large amounts of missing data. We illustrate this ability on the task of reconstructing the dropped frames in video broadcast using only the degraded video and also in denoising a severely corrupted video."
            },
            "slug": "Video-Epitomes-Cheung-Frey",
            "title": {
                "fragments": [],
                "text": "Video Epitomes"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is described how epitomes can be used to model video data and significant computational speedups that can be incorporated into the epitome inference and learning algorithm are described."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "Spatial-temporal features have shown particular promise in motion understanding due to its rich descriptive power [3, 14, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Boiman and Irani [3] recently propose to extract ensemble of local video patches to localize irregular action behavior in videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31339674,
            "fieldsOfStudy": [],
            "id": "b01f0b155b2448f6e974fff19d20c6143e0230b5",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detecting Irregularities in Images and in Video"
            },
            "venue": {
                "fragments": [],
                "text": "ICCV"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Hierarchical-Model-of-Shape-and-Appearance-for-Niebles-Fei-Fei/7a5fe226444b2ca367ac9ac54b5b4113023fd404?sort=total-citations"
}