{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2667773"
                        ],
                        "name": "A. Tsybakov",
                        "slug": "A.-Tsybakov",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Tsybakov",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsybakov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "\u2026of machines (Devroye & Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Tsybakov (1997) has studied an estimator based on piecewise polynomial approximation of C(\u03b1) and has shown it attains the asymptotically minimax rate for certain classes of densities p. Polonik (1997) has studied the estimation of C(\u03b1) by C`(\u03b1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121159147,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e665a933a408d39ba3a432ac537e6673f3b0cd1",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Let X 1 ,...,X n be independent identically distributed observations from an unknown probability density f(.). Consider the problem of estimating the level set G = G f (\u03bb) = {x \u2208 R 2 : f(x) \u2265 \u03bb} from the sample X 1 ,...,X n , under the assumption that the boundary of G has a certain smoothness. We propose piecewise-polynomial estimators of G based on the maximization of local empirical excess masses. We show that the estimators have optimal rates of convergence in the asymptotically minimax sense within the studied classes of densities. We find also the optimal convergence rates for estimation of convex level sets. A generalization to the N-dimensional case, where N > 2, is given."
            },
            "slug": "On-nonparametric-estimation-of-density-level-sets-Tsybakov",
            "title": {
                "fragments": [],
                "text": "On nonparametric estimation of density level sets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33157212,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "e848ec306b88545745a3f39484023716ddbc156f",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "It is known that the covering numbers of a function class on a double sample (length 2m) can be used to bound the generalization performance of a classifier by using a margin based analysis. In this paper we show that one can utilize an analogous argument in terms of the observed covering numbers on a single m-sample (being the actual observed data points). The significance of this is that for certain interesting classes of functions, such as support vector machines, there are new techniques which allow one to find good estimates for such covering numbers in terms of the speed of decay of the eigenvalues of a Gram matrix. These covering numbers can be much less than a priori bounds indicate in situations where the particular data received is \"easy\". The work can be considered an extension of previous results which provided generalization performance bounds in terms of the VC-dimension of the class of hypotheses restricted to the sample, with the considerable advantage that the covering numbers can be readily computed, and they often are small."
            },
            "slug": "Generalization-Performance-of-Classifiers-in-Terms-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Generalization Performance of Classifiers in Terms of Observed Covering Numbers"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "It is shown that one can utilize an analogous argument in terms of the observed covering numbers on a single m-sample (being the actual observed data points) to bound the generalization performance of a classifier by using a margin based analysis."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12790395"
                        ],
                        "name": "G. Wise",
                        "slug": "G.-Wise",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Wise",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wise"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "\u2026in medical diagnosis (Tarassenko, Hayton, Cerneaz, & Brady, 1995), marketing (Ben-David & Lindenbaum, 1997), condition monitoring of machines (Devroye & Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Devroye & Wise (1980) showed the asymptotic consistency of (A.1) with respect to the symmetric difference between C(1) and C\u0302`."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 109
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11576673,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9c346f8a40d31d884c8d5496d1d46a4a0b1848d7",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper two problems are considered, both involving the nonparametric estimation of the support of a random vector from a sequence of independent identically distributed observations. In the first problem, after observing n independent random vectors with a common unknown distribution $\\mu $, we are given one new measurement and we wish to know whether or not it belongs to the support of $\\mu $. In the second problem, after observing the n independent random vectors with a common unknown distribution $\\mu $, we then observe n additional independent random vectors with a common unknown distribution $\\nu $. In this case we wish to know whether or not the support of $\\nu $ is completely contained within the support of $\\mu $. Decision schemes are presented and then convergence properties are established."
            },
            "slug": "Detection-of-Abnormal-Behavior-Via-Nonparametric-of-Devroye-Wise",
            "title": {
                "fragments": [],
                "text": "Detection of Abnormal Behavior Via Nonparametric Estimation of the Support"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727935"
                        ],
                        "name": "M. Lindenbaum",
                        "slug": "M.-Lindenbaum",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lindenbaum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lindenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9328501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cda0324c7f815d53d75149ea97b122f15e5b2f1c",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a mathematical model for learning the high-density areas of an unknown distribution from (unlabeled) random points drawn according to this distribution. While this type of a learning task has not been previously addressed in the computational learnability literature, we believe that this it a rather basic problem that appears in many practical learning scenarios. From a statistical theory standpoint, our model may be viewed as a restricted instance of the fundamental issue of inferring information about a probability distribution from the random samples it generates. From a computational learning angle, what we propose is a few framework of unsupervised concept learning. The examples provided to the learner in our model are not labeled (and are not necessarily all positive or all negative). The only information about their membership is indirectly disclosed to the student through the sampling distribution. We investigate the basic features of the proposed model and provide lower and upper bounds on the sample complexity of such learning tasks. We prove that classes whose VC-dimension is finite are learnable in a very strong sense, while on the other hand,\ufffd-covering numbers of a concept class impose lower bounds on the sample size needed for learning in our models. One direction of the proof involves a reduction of the density-level learnability to PAC learning with respect to fixed distributions (as well as some fundamental statistical lower bounds), while the sufficiency condition is proved through the introduction of a generic learning algorithm."
            },
            "slug": "Learning-Distributions-by-Their-Density-Levels:-A-a-Ben-David-Lindenbaum",
            "title": {
                "fragments": [],
                "text": "Learning Distributions by Their Density Levels: A Paradigm for Learning without a Teacher"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that classes whose VC-dimension is finite are learnable in a very strong sense, while on the other hand,\ufffd-covering numbers of a concept class impose lower bounds on the sample size needed for learning in the authors' models."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6789514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces some generalizations of Vapnik's (1982) method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a \"large margin\". This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (1971) dimension measured on the sample."
            },
            "slug": "Structural-Risk-Minimization-Over-Data-Dependent-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization Over Data-Dependent Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A result is presented that allows one to trade off errors on the training sample against improved generalization performance, and a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938990"
                        ],
                        "name": "A. Cuevas",
                        "slug": "A.-Cuevas",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Cuevas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cuevas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100667744"
                        ],
                        "name": "R. Fraiman",
                        "slug": "R.-Fraiman",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Fraiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fraiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Cuevas and Fraiman (1997) studied the asymptotic consistency of a plug-in estimator of C(1): C\u0302plug-in = {x: p\u0302`(x) > 0} where p\u0302` is a kernel density estimator."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121458284,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8121205acdf0fb50bec8e54a3ba679463afe7a51",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We suggest a new approach, based on the use of density estimators, for the problem of estimating the (compact) support of a multivariate density. This subject (motivated in terms of pattern analysis by Grenander) has interesting connections with detection and clustering. A natural class of density-based estimators is defined. Universal consistency results and convergence rates are established for these estimators, with respect to the usual measure-based metric d \u03bc between sets. Further convergence rates (with respect to both d \u03bc and the Hausdorff metric d H ) are also obtained under some, fairly intuitive, shape restrictions."
            },
            "slug": "A-plug-in-approach-to-support-estimation-Cuevas-Fraiman",
            "title": {
                "fragments": [],
                "text": "A plug-in approach to support estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The technique was originally considered foranalyzingsoft margin classiecation in  Shawe-Taylor and Cristianini (1999) , but we will adapt it for the unsupervised problem we are considering here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44495969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02aa4e48f1c30798cf89bdff151bf521a07232e9",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of results have bounded generalization of a classi fier in terms of its margin on the training points. There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization. Freund and Schapire [6] have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound. We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size. We also derive an algorithm for optimizing the new measure for general kernel based learning machines. Some preliminary experiments are presented."
            },
            "slug": "Margin-Distribution-Bounds-on-Generalization-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Margin Distribution Bounds on Generalization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8901626,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "91d2951397d63b7fc47edbd534ae875f9d0eeb13",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive new bounds for the generalization error of feature space machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers. The proofs are based on a viewpoint that is apparently novel in the field of statistical learning theory. The hypothesis class is described in terms of a linear operator mapping from a possibly infinite dimensional unit ball in feature space into a finite dimensional space. The covering numbers of the class are then determined via the entropy numbers of the operator. These numbers, which characterize the degree of compactness of the operator, can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine. As a consequence we are able to theoretically explain the effect of the choice of kernel functions on the generalization performance of support vector machines."
            },
            "slug": "Entropy-Numbers,-Operators-and-Support-Vector-Williamson-Smola",
            "title": {
                "fragments": [],
                "text": "Entropy Numbers, Operators and Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "New bounds for the generalization error of feature space machines, such as support vector machines and related regularization networks, are derived by obtaining new bounds on their covering numbers by virtue of the eigenvalues of an integral operator induced by the kernel function used by the machine."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 777816,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ee177aacf6b3697d079579ce558cdb2ee58cee39",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive new bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers. The proofs make use of a viewpoint that is apparently novel in the field of statistical learning theory. The hypothesis class is described in terms of a linear operator mapping from a possibly infinite-dimensional unit ball in feature space into a finite-dimensional space. The covering numbers of the class are then determined via the entropy numbers of the operator. These numbers, which characterize the degree of compactness of the operator can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine. As a consequence, we are able to theoretically explain the effect of the choice of kernel function on the generalization performance of support vector machines."
            },
            "slug": "Generalization-performance-of-regularization-and-of-Williamson-Smola",
            "title": {
                "fragments": [],
                "text": "Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "New bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks, are derived by obtaining new bounds on their covering numbers by using the eigenvalues of an integral operator induced by the kernel function used by the machine."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145039030"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18223591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9b99b58bfbe7500ee71d62febc60bab6d4c0b575",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is proposed which computes a direction in a dataset such that a speci ed fraction of a particular class of all examples is separated from the overall mean by a maximal margin. The projector onto that direction can be used for class-speci c feature extraction. The algorithm is carried out in a feature space associated with a support vector kernel function, hence it can be used to construct a large class of nonlinear feature extractors. In the particular case where there exists only one class, the method can be thought of as a robust form of principal component analysis, where instead of variance we maximize percentile thresholds. Finally, we generalize it to also include the possibility of specifying negative examples."
            },
            "slug": "Kernel-method-for-percentile-feature-extraction-Sch\u00f6lkopf-Platt",
            "title": {
                "fragments": [],
                "text": "Kernel method for percentile feature extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A method is proposed which computes a direction in a dataset such that a speci ed fraction of a particular class of all examples is separated from the overall mean by a maximal margin, and this method can be thought of as a robust form of principal component analysis, where instead of variance the authors maximize percentile thresholds."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30545896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "356125478f5d06b564b420755a4944254045bbbe",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology."
            },
            "slug": "Support-vector-learning-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This book provides a comprehensive analysis of what can be done using Support vector Machines, achieving record results in real-life pattern recognition problems, and proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which it is considered as the most natural and elegant way for generalization of classical Principal Component analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140519721"
                        ],
                        "name": "Chao He",
                        "slug": "Chao-He",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao He"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5892886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eca01c0be691a2c5b769e0d80b10feaca39bc800",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "The requirement to reduce the computational cost of evaluating a point probability density estimate when employing a Parzen window estimator is a well-known problem. This paper presents the Reduced Set Density Estimator that provides a kernel-based density estimator which employs a small percentage of the available data sample and is optimal in the L/sub 2/ sense. While only requiring /spl Oscr/(N/sup 2/) optimization routines to estimate the required kernel weighting coefficients, the proposed method provides similar levels of performance accuracy and sparseness of representation as Support Vector Machine density estimation, which requires /spl Oscr/(N/sup 3/) optimization routines, and which has previously been shown to consistently outperform Gaussian Mixture Models. It is also demonstrated that the proposed density estimator consistently provides superior density estimates for similar levels of data reduction to that provided by the recently proposed Density-Based Multiscale Data Condensation algorithm and, in addition, has comparable computational scaling. The additional advantage of the proposed method is that no extra free parameters are introduced such as regularization, bin width, or condensation ratios, making this method a very simple and straightforward approach to providing a reduced set density estimator with comparable accuracy to that of the full sample Parzen density estimator."
            },
            "slug": "Probability-Density-Estimation-from-Optimally-Data-Girolami-He",
            "title": {
                "fragments": [],
                "text": "Probability Density Estimation from Optimally Condensed Data Samples"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The Reduced Set Density Estimator is presented, which provides a kernel-based density estimator which employs a small percentage of the available data sample and is optimal in the L/sub 2/ sense."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2963587"
                        ],
                        "name": "W. Polonik",
                        "slug": "W.-Polonik",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Polonik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Polonik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116335673,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0f64d67a51e5f682023280e570e10a972b6c022e",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a method for estimating a density f in Rd under assumptions which are of qualitative nature. The resulting density estimator can be considered as a generalization of the Grenander estimator for monotone densities. The assumptions on f are given in terms of shape restrictions of the density contour clusters ?(?) = (x : f(x) ? ?). We assume that for all ? ? 0 the sets ?(?) lie in a given class C of measurable subsets of Rd. By choosing C appropriately it is possible to model for example monotonicity, symmetry, or multimodality. The main mathematical tool for proving consistency and rates of convergence of the density estimator is empirical process theory. It turns out that the rates depend on the richness of C measured by metric entropy."
            },
            "slug": "Density-estimation-under-qualative-assumptions-Polonik",
            "title": {
                "fragments": [],
                "text": "Density estimation under qualative assumptions inhigher dimensions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 113
                            }
                        ],
                        "text": "More specifically, let us assume that is Green\u2019s function of for an operator mapping into some dot product space (Smola et al., 1998; Girosi, 1998), and take a look at the dual objective function that we minimize,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "\u2026let us assume that k is Green\u2019s function of P\u2217P for an operator P mapping into some inner product space (Smola, Scho\u0308lkopf, & Mu\u0308ller, 1998; Girosi, 1998), and take a look at the dual objective function that we minimize,\u2211\ni,j\n\u03b1i\u03b1jk ( xi, xj ) =\u2211 i,j \u03b1i\u03b1j ( k (xi, \u00b7) \u00b7 \u03b4xj (\u00b7) ) =\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6082464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d27c7569fdbcbb57ff511f5293e32b547acca7b3",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995). SVM is a technique that can be derived from the structural risk minimization principle (Vapnik, 1982) and can be used to estimate the parameters of several different approximation schemes, including radial basis functions, algebraic and trigonometric polynomials, B-splines, and some forms of multilayer perceptrons. Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary). We show that if the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem. In the appendix, we present a derivation of the SVM technique in the framework of regularization theory, rather than statistical learning theory, establishing a connection between SVM, sparse approximation, and regularization theory."
            },
            "slug": "An-Equivalence-Between-Sparse-Approximation-and-Girosi",
            "title": {
                "fragments": [],
                "text": "An Equivalence Between Sparse Approximation and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "If the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 116
                            }
                        ],
                        "text": "To this end, we use a result of Williamson, Smola, and Scho\u0308lkopf (2000) to bound logN (\u03b3 /2,F , 2`), and a result of Shawe-Taylor and Cristianini (2000) to bound logN (\u03b3 /2,LB(X ), 2`)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5145345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5437518f5c007b21d5b0e782640406831ffa7b4",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Generalization bounds depending on the margin of a classifier are a relatively new development. They provide an explanation of the performance of state-of-the-art learning systems such as support vector machines (SVMs) and Adaboost. The difficulty with these bounds has been either their lack of robustness or their looseness. The question of whether the generalization of a classifier can be more tightly bounded in terms of a robust measure of the distribution of margin values has remained open for some time. The paper answers this open question in the affirmative and, furthermore, the analysis leads to bounds that motivate the previously heuristic soft margin SVM algorithms as well as justifying the use of the quadratic loss in neural network training algorithms. The results are extended to give bounds for the probability of failing to achieve a target accuracy in regression prediction, with a statistical analysis of ridge regression and Gaussian processes as a special case. The analysis presented in the paper has also lead to new boosting algorithms described elsewhere."
            },
            "slug": "On-the-generalization-of-soft-margin-algorithms-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "On the generalization of soft margin algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The paper answers the open question of whether the generalization of a classifier can be more tightly bounded in terms of a robust measure of the distribution of margin values in the affirmative and leads to bounds that motivate the previously heuristic soft margin SVM algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10843,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 32
                            }
                        ],
                        "text": "To this end, we use a result of Williamson, Smola, and Scho\u0308lkopf (2000) to bound logN (\u03b3 /2,F , 2`), and a result of Shawe-Taylor and Cristianini (2000) to bound logN (\u03b3 /2,LB(X ), 2`)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2 Estimating High Probability Regions (\u03b1 6= 1). Polonik (1995b) has studied the use of the \u201cexcess mass approach\u201d (M\u00fcller, 1992) to construct an estimator of \u201cgeneralized \u03b1-clusters\u201d related to C(\u03b1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7051801,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ee6782fa23729abbab8eaa6c64822a96fd43cc23",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper collects together a miscellany of results originally motivated by the analysis of the generalization performance of the \u201cmaximum-margin\u201d algorithm due to Vapnik and others. The key feature of the paper is its operator-theoretic viewpoint. New bounds on covering numbers for classes related to Maximum Margin classes are derived directly without making use of a combinatorial dimension such as the VC-dimension. Specific contents of the paper include: a new and self-contained proof of Maurey\u2019s theorem and some generalizations with small explicit values of constants; bounds on the covering numbers of maximum margin classes suitable for the analysis of their generalization performance; the extension of such classes to those induced by balls in quasi-Banach spaces (such as norms with ). extension of results on the covering numbers of convex hulls of basis functions to -convex hulls ( ); an appendix containing the tightest known bounds on the entropy numbers of the identity operator between and ( )."
            },
            "slug": "Entropy-Numbers-of-Linear-Function-Classes-Williamson-Smola",
            "title": {
                "fragments": [],
                "text": "Entropy Numbers of Linear Function Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper collects together a miscellany of results originally motivated by the analysis of the generalization performance of the \u201cmaximum-margin\u201d algorithm due to Vapnik and others."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8431774"
                        ],
                        "name": "D. Chakraborty",
                        "slug": "D.-Chakraborty",
                        "structuredName": {
                            "firstName": "Debrup",
                            "lastName": "Chakraborty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chakraborty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708545"
                        ],
                        "name": "N. Pal",
                        "slug": "N.-Pal",
                        "structuredName": {
                            "firstName": "Nikhil",
                            "lastName": "Pal",
                            "middleNames": [
                                "Ranjan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60654807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7badd31cfcb529bccc45f887c228e60a32d322f",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The response of a multilayered perceptron (MLP) network on points far away from the training data is generally not reliable. Ideally a network should not respond to a data point which lies far away from the boundary of its training data. We propose a new training scheme for MLPs as classifiers, which ensures this. Our scheme trains subnets for each class. Each subnet can decide whether a data point belongs to a certain class or not. Training each subnet requires data from the class which the subnet represents along with some points outside the boundary of that class. For this purpose we propose an easy but approximate method to generate points outside the boundary of a pattern class. The trained subnets are then merged to solve the multi-class classification problem. We demonstrate that an MLP trained by our method does not respond to points which lie outside the boundary of its training sample. Also, our network can deal with overlapped classes in a better manner."
            },
            "slug": "Making-a-multilayered-perceptron-network-say-\"don't-Chakraborty-Pal",
            "title": {
                "fragments": [],
                "text": "Making a multilayered perceptron network say - \"don't know\" when it should"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated that an MLP trained by the proposed new training scheme does not respond to points which lie outside the boundary of its training sample, and the network can deal with overlapped classes in a better manner."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15659058"
                        ],
                        "name": "T. Sager",
                        "slug": "T.-Sager",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Sager",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sager"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123203138,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6181befd8db2bdeef3988b6bc017596b6d32377c",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An iterative procedure for estimating the mode and isopleths of a multivariate distribution is presented. The mode is estimated by selecting a point from the final set in a nested decreasing sequence of convex sets, each one of which is iteratively the smallest closed, convex subset containing a certain proportion of its predecessor's data points. An isopleth is estimated by the boundary of the convex subset corresponding to a fixed number of iterations (independent of n). The method gives rise to a natural density estimator. The estimators are shown to converge almost surely, and convergence rates for the univariate isopleth estimator are presented. Applications to air pollution and health sciences are noted."
            },
            "slug": "An-Iterative-Method-for-Estimating-a-Multivariate-Sager",
            "title": {
                "fragments": [],
                "text": "An Iterative Method for Estimating a Multivariate Mode and Isopleth"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2963587"
                        ],
                        "name": "W. Polonik",
                        "slug": "W.-Polonik",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Polonik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Polonik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119350444,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "46918f499b2c5dbd177eb9da124da6806060d6e4",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "By using empirical process theory, the so-called excess mass approach is studied. It can be applied to various statistical problems, especially in higher dimensions, such as testing for multimodality, estimating density contour clusters, estimating nonlinear functionals of a density, density estimation, regression problems and spectral analysis. We mainly consider the problems of testing for multimodality and estimating density contour clusters, but the other problems also are discussed. The excess mass (over C) is defined as a supremum of a certain functional defined on C, where C is a class of subsets of the d-dimensional Euclidean space. Comparing excess masses over different classes C yields information about the modality of the underlying probability measure F. This can be used to construct tests for multimodality. If F has a density f, the maximizing sets of the excess mass are level sets or density contour clusters of f, provided they lie in C. The excess mass and the density contour clusters can be estimated from the data. Asymptotic properties of these estimators and of the test statistics are studied for general classes C, including the classes of balls, ellipsoids and convex sets."
            },
            "slug": "Measuring-Mass-Concentrations-and-Estimating-Excess-Polonik",
            "title": {
                "fragments": [],
                "text": "Measuring Mass Concentrations and Estimating Density Contour Clusters-An Excess Mass Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This scan can be accelerated by not checking patterns which are on the correct side of the hyperplane by a large margin, using the method of Joachims (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 142
                            }
                        ],
                        "text": "2 This scan can be accelerated by not checking patterns that are on the correct side of the hyperplane by a large margin, using the method of Joachims (1999)\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089976601750264965 by guest on 09 July 2021"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60502770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb99668d4df98a3f6ff0b9fa3402e09008f22e2c",
            "isKey": true,
            "numCitedBy": 1838,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-support-vector-machine-learning-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large-scale support vector machine learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207673395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d73c0d0c92446102fdb6cc728b5d69674a1a387",
            "isKey": false,
            "numCitedBy": 2615,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of , and report experimental results."
            },
            "slug": "New-Support-Vector-Algorithms-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "New Support Vector Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new class of support vector algorithms for regression and classification that eliminates one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 730432,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "3d7c0d2f32e85e4335d634dd7b62243362528bb9",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Many settings of unsupervised learning can be viewed as quantization problems - the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning. This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding.We explore this connection in two ways: (1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm. In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators. Experimental results demonstrate the feasibility of the approach."
            },
            "slug": "Regularized-Principal-Manifolds-Smola-Mika",
            "title": {
                "fragments": [],
                "text": "Regularized Principal Manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An algorithm for finding principal manifolds that can be regularized in a variety of ways is proposed and bounds on the covering numbers are given which allows us to obtain nearly optimal learning rates for certain types of regularization operators."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 92
                            }
                        ],
                        "text": "a function such that most of the data will live in the region where the function is nonzero (Sch\u00f6lkopf et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5544,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144336939"
                        ],
                        "name": "D. Nolan",
                        "slug": "D.-Nolan",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Nolan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "beingthe classofclosed convexsetsinX . (Theyactuallyconsidereddensity contour clusters; cf. appendix A for a deenition.)  Nolan (1991)  considered higher dimensions with C being the class of ellipsoids."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Nolan (1991) considered higher dimensions with C being the class of ellipsoids."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121876340,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "03fba3a4bd62fe591e36626f73e505027aa01f2c",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-excess-mass-ellipsoid-Nolan",
            "title": {
                "fragments": [],
                "text": "The excess-mass ellipsoid"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144813423"
                        ],
                        "name": "Arnulf B. A. Graf",
                        "slug": "Arnulf-B.-A.-Graf",
                        "structuredName": {
                            "firstName": "Arnulf",
                            "lastName": "Graf",
                            "middleNames": [
                                "B.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnulf B. A. Graf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2724554"
                        ],
                        "name": "S. Borer",
                        "slug": "S.-Borer",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Borer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Borer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 633145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5af0e69ff389f3355bf0d95570dd2791449200c3",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses classification using support vector machines in a normalized feature space. We consider both normalization in input space and in feature space. Exploiting the fact that in this setting all points lie on the surface of a unit hypersphere we replace the optimal separating hyperplane by one that is symmetric in its angles, leading to an improved estimator. Evaluation of these considerations is done in numerical experiments on two real-world datasets. The stability to noise of this offset correction is subsequently investigated as well as its optimality."
            },
            "slug": "Classification-in-a-normalized-feature-space-using-Graf-Smola",
            "title": {
                "fragments": [],
                "text": "Classification in a normalized feature space using support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper discusses classification using support vector machines in a normalized feature space using the fact that in this setting all points lie on the surface of a unit hypersphere to replace the optimal separating hyperplane by one that is symmetric in its angles, leading to an improved estimator."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This scan can be accelerated by not checking patterns which are on the correct side of the hyperplane by a large margin, using the method of Joachims (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 142
                            }
                        ],
                        "text": "2 This scan can be accelerated by not checking patterns that are on the correct side of the hyperplane by a large margin, using the method of Joachims (1999)\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089976601750264965 by guest on 09 July 2021"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61116019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "isKey": true,
            "numCitedBy": 5454,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-SVM-learning-practical-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large scale SVM learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144959271"
                        ],
                        "name": "J. Hartigan",
                        "slug": "J.-Hartigan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hartigan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hartigan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121619993,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b1aa7b8c4794d06aae4a968b102926c55a3f0ebd",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract If a density in two dimensions has a convex contour containing probability \u03b1, the contour may be estimated from a sample by finding the convex polygon of smallest area containing a proportion \u03b1 of the sample points. An algorithm for finding a particular contour is given that takes O(n 2) space and O(n 3) time for n sample points."
            },
            "slug": "Estimation-of-a-Convex-Density-Contour-in-Two-Hartigan",
            "title": {
                "fragments": [],
                "text": "Estimation of a Convex Density Contour in Two Dimensions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101872040"
                        ],
                        "name": "A. Korostelev",
                        "slug": "A.-Korostelev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Korostelev",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Korostelev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2667773"
                        ],
                        "name": "A. Tsybakov",
                        "slug": "A.-Tsybakov",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Tsybakov",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsybakov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They include problems in medical diagnosis (Tarassenko, Hayton, Cerneaz, & Brady, 1995), marketing (Ben-David & Lindenbaum, 1997), condition monitoring of machines (Devroye & Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997;  Korostelev & Tsybakov, 1993 ), regression and spectral analysis (Polonik, 1997), tests for multimodality and ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 10
                            }
                        ],
                        "text": "(See also Korostelev & Tsybakov, 1993, chap."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 149
                            }
                        ],
                        "text": "\u2026(Devroye & Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(See also  Korostelev & Tsybakov, 1993,  chap."
                    },
                    "intents": []
                }
            ],
            "corpusId": 119006222,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f8de7df259908769a23e94f7016e9462c222e217",
            "isKey": true,
            "numCitedBy": 513,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Image processing is an increasingly important area of research and there exists a large variety of image reconstruction methods proposed by different authors. This book is concerned with a technique for image reconstruction known as the asymptotic minimax approach, which is based on non-parametric regression and non-parametric change-point analysis. In effect, the central idea is to assume that the image under analysis belongs to a certain functional class and the method finds the image estimators which achieve the best order of accuracy for the worst images in that class. The first two chapters present the basic ideas required from non-parametric regression and change-point analysis whilst the subsequent chapters develop the main theory and examples of applications. In order to provide a relatively simple account of this method, the authors' emphasis is to present results under the simplest assumptions which still allow the main features of a particular problem. As a result the book is essentially self-contained, although it does assume a firm grounding in functional analysis, statistics and image processing fundamentals."
            },
            "slug": "Minimax-theory-of-image-reconstruction-Korostelev-Tsybakov",
            "title": {
                "fragments": [],
                "text": "Minimax theory of image reconstruction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 135
                            }
                        ],
                        "text": "The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 164
                            }
                        ],
                        "text": "However, the general problem of estimating the measure for a large class of sets, say, the sets measureable in Borel\u2019s sense, is not solvable (for a discussion, see Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34628173"
                        ],
                        "name": "K. Tsuda",
                        "slug": "K.-Tsuda",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Tsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tsuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5894296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fcbefeb0beae4470cf40df74cd116b1d4bdcae4",
            "isKey": false,
            "numCitedBy": 3519,
            "numCiting": 211,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis."
            },
            "slug": "An-introduction-to-kernel-based-learning-algorithms-M\u00fcller-Mika",
            "title": {
                "fragments": [],
                "text": "An introduction to kernel-based learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143923114"
                        ],
                        "name": "L. Tarassenko",
                        "slug": "L.-Tarassenko",
                        "structuredName": {
                            "firstName": "Lionel",
                            "lastName": "Tarassenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tarassenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482710"
                        ],
                        "name": "P. Hayton",
                        "slug": "P.-Hayton",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Hayton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hayton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2562173"
                        ],
                        "name": "N. Cerneaz",
                        "slug": "N.-Cerneaz",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Cerneaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cerneaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144431498"
                        ],
                        "name": "M. Brady",
                        "slug": "M.-Brady",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brady"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58441702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f02218771a8faa4b2d59588a46d0c40bfec95176",
            "isKey": false,
            "numCitedBy": 380,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Breast cancer is the major cause of death amongst women in the 35 to 55 age group. Mammography is the only feasible imaging modality for screening large numbers of women. With the present screening policy, there are three million mammograms to be analysed each year in the UK; there is therefore a need (as yet unmet) for an automated analysis system which could highlight areas of interest. In the first instance, the areas of interest might simply be any mass-like structures and this is indeed the approach reported on in this paper. Mammography is typical of many problems in medicine: the class of real interest is under-represented in the database of available examples and hence its prior probability will be very low. As a result of this, there are very few examples of abnormalities in any of the existing databases. If a neural network classifier is trained using the standard approach of minimising the mean-squared error (MSE) at the output, the under-represented class will be ignored. We have been exploring an alternative approach in which we attempt to learn a description of normality using the large number of available mammograms which do not show any evidence of mass-like structures. The idea is then to test for novelty against this description in order to try and identify candidate masses in previously unseen images analysis and interpretation and present a sample of the results which we have so far obtained on a standard database."
            },
            "slug": "Novelty-detection-for-the-identification-of-masses-Tarassenko-Hayton",
            "title": {
                "fragments": [],
                "text": "Novelty detection for the identification of masses in mammograms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative approach is explored in which a description of normality is attempted using the large number of available mammograms which do not show any evidence of mass-like structures to try and identify candidate masses in previously unseen images analysis and interpretation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2963587"
                        ],
                        "name": "W. Polonik",
                        "slug": "W.-Polonik",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Polonik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Polonik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 196
                            }
                        ],
                        "text": "\u20261980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others (Mu\u0308ller, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 185
                            }
                        ],
                        "text": "Tsybakov (1997) has studied an estimator based on piecewise polynomial approximation of C(\u03b1) and has shown it attains the asymptotically minimax rate for certain classes of densities p. Polonik (1997) has studied the estimation of C(\u03b1) by C`(\u03b1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123651633,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "beb7348757e0fdd2c2134886434031c8b6f2963a",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Minimum-volume-sets-and-generalized-quantile-Polonik",
            "title": {
                "fragments": [],
                "text": "Minimum volume sets and generalized quantile processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 601110,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8985a9637540daa0b7b8295f8a5bbda3a3be1dea",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-connection-between-regularization-operators-and-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "The connection between regularization operators and support vector kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103058452"
                        ],
                        "name": "J. Einmahl",
                        "slug": "J.-Einmahl",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Einmahl",
                            "middleNames": [
                                "H.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Einmahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150981361"
                        ],
                        "name": "D. Mason",
                        "slug": "D.-Mason",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mason",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mason"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to summarize the methods, it is convenient to introduce the following definition of a (multi-dimensional) quantile function (introduced by Einmal and Mason (1992))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123086826,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "110435692de1c40db5220e8dc23ac648820dcb1b",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "For random vectors taking values in $\\mathbb{R}^d$ we introduce a notion of multivariate quantiles defined in terms of a class of sets and study an associated process which we call the generalized quantile process. This process specializes to the well known univariate quantile process. We obtain functional central limit theorems for our generalized quantile process and show that both Gaussian and non-Gaussian limiting processes can arise. A number of interesting example are included."
            },
            "slug": "Generalized-quantile-processes-Einmahl-Mason",
            "title": {
                "fragments": [],
                "text": "Generalized quantile processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108548875"
                        ],
                        "name": "A. Cuevas",
                        "slug": "A.-Cuevas",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Cuevas",
                            "middleNames": [
                                "Burgos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cuevas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Devroye and Wise (1980) showed the asymptotic consistency of (1) with respect to the symmetric difference between and ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121076467,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "61647051602420937fa7c750707b95f3ea9409e1",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of estimating a k\u2010dimensional body from a random sample of points is discussed and a natural estimate, first proposed by Grenander, is considered. Two consistency results together with a convergence rate for this estimate are given, and some open problems are proposed."
            },
            "slug": "On-Pattern-Analysis-in-the-Non\u2010Convex-Case-Cuevas",
            "title": {
                "fragments": [],
                "text": "On Pattern Analysis in the Non\u2010Convex Case"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Cover and Thomas, (1991) show that for all 2 , d < (1)2 , then"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42795,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145882527"
                        ],
                        "name": "A. Belousov",
                        "slug": "A.-Belousov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Belousov",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belousov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2721911"
                        ],
                        "name": "S. Verzakov",
                        "slug": "S.-Verzakov",
                        "structuredName": {
                            "firstName": "Serguei",
                            "lastName": "Verzakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Verzakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932185069"
                        ],
                        "name": "J. von Frese",
                        "slug": "J.-von-Frese",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "von Frese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. von Frese"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120740707,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "4334d85e8b18ac69dc72bb010df2149d8de3160b",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The special emphasis of support vector machines (SVMs) on generalization ability makes this approach particularly interesting for real\u2010world applications with limited amounts of training data. In this paper we analyse the applicational aspects of SVMs, illustrating them with the step\u2010by\u2010step construction of a classifier for polymers by means of their mid\u2010infrared spectra. With this example we show how the main difficulties of a typical industrial classification task can be addressed using SVMs. Copyright \u00a9 2002 John Wiley & Sons, Ltd."
            },
            "slug": "Applicational-aspects-of-support-vector-machines-Belousov-Verzakov",
            "title": {
                "fragments": [],
                "text": "Applicational aspects of support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7831590,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1ad15c08556c8f8e3739703857ea01077ce738c5",
            "isKey": false,
            "numCitedBy": 2055,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Kernel-Principal-Component-Analysis-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Kernel Principal Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of Principal Component Analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403113247"
                        ],
                        "name": "R. Santiago-Mozos",
                        "slug": "R.-Santiago-Mozos",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Santiago-Mozos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Santiago-Mozos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403833704"
                        ],
                        "name": "J. Leiva-Murillo",
                        "slug": "J.-Leiva-Murillo",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Leiva-Murillo",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Leiva-Murillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388508441"
                        ],
                        "name": "F. P\u00e9rez-Cruz",
                        "slug": "F.-P\u00e9rez-Cruz",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "P\u00e9rez-Cruz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. P\u00e9rez-Cruz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398903195"
                        ],
                        "name": "Antonio Art\u00e9s-Rodr\u00edguez",
                        "slug": "Antonio-Art\u00e9s-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Art\u00e9s-Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Art\u00e9s-Rodr\u00edguez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15770005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8eb27e170fb4f5a43992c2df2fb10f17da07625e",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We tackle the problem of detecting sources of combustion in high definition multispectral medium wavelength infrared (MWIR) (3-5 /spl mu/m) images. We present a novel approach to this problem consisting of processing the images block-wise using a new technique that we call supervised principal component analysis (SPCA) to get the components of these blocks. This outperforms state-of-the-art methods with a significant reduction in the complexity of the whole scheme. As a classifier, we propose the use of a support vector machine (SVM) comparing the results from both its novelty-detection and binary non-linear versions. High performance is achieved from a small set of components."
            },
            "slug": "Supervised-PCA-and-SVM-classifiers-for-object-in-Santiago-Mozos-Leiva-Murillo",
            "title": {
                "fragments": [],
                "text": "Supervised-PCA and SVM classifiers for object detection in infrared images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This work presents a novel approach to the problem of detecting sources of combustion in high definition multispectral medium wavelength infrared (MWIR) images by processing the images block-wise using a new technique that is called supervised principal component analysis (SPCA) to get the components of these blocks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE Conference on Advanced Video and Signal Based Surveillance, 2003."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772326"
                        ],
                        "name": "S. Shevade",
                        "slug": "S.-Shevade",
                        "structuredName": {
                            "firstName": "Shirish",
                            "lastName": "Shevade",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shevade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145880755"
                        ],
                        "name": "C. Bhattacharyya",
                        "slug": "C.-Bhattacharyya",
                        "structuredName": {
                            "firstName": "Chiranjib",
                            "lastName": "Bhattacharyya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bhattacharyya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38445965"
                        ],
                        "name": "K. Murthy",
                        "slug": "K.-Murthy",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Murthy",
                            "middleNames": [
                                "R.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1536643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d95b96d71669f3f4edfcc95cacd428b62b3fcde",
            "isKey": false,
            "numCitedBy": 1804,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This article points out an important source of inefficiency in Platt's sequential minimal optimization (SMO) algorithm that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO. These modified algorithms perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "slug": "Improvements-to-Platt's-SMO-Algorithm-for-SVM-Keerthi-Shevade",
            "title": {
                "fragments": [],
                "text": "Improvements to Platt's SMO Algorithm for SVM Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO that perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053956"
                        ],
                        "name": "Susanne Still",
                        "slug": "Susanne-Still",
                        "structuredName": {
                            "firstName": "Susanne",
                            "lastName": "Still",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susanne Still"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34937451"
                        ],
                        "name": "K. Hepp",
                        "slug": "K.-Hepp",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Hepp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742758"
                        ],
                        "name": "R. Douglas",
                        "slug": "R.-Douglas",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Douglas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Douglas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11362978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8639a365f477821787e7f66421064e312ddfb123",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior."
            },
            "slug": "Four-legged-Walking-Gait-Control-Using-a-Chip-to-a-Still-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel neuromorphic VLSI chip is presented that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 111
                            }
                        ],
                        "text": "Most good learning algorithms achieve error rates of 3 to 5% on the USPS benchmark (for a list of results, cf. Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 96
                            }
                        ],
                        "text": "During recent years, a new set of kernel techniques for supervised learning has been developed (Vapnik, 1995; Scho\u0308lkopf, Burges, & Smola, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 95
                            }
                        ],
                        "text": "During recent years, a new set of kernel techniques for supervised learning has been developed (Vapnik, 1995; Sch\u00f6lkopf, Burges, & Smola, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 184
                            }
                        ],
                        "text": "It is folklore in the community that the USPS test set (see Figure 4) contains a number of patterns that are hard or impossible to classify, due to segmentation errors or mislabeling (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 161
                            }
                        ],
                        "text": "To conclude this section, we note that one can also use balls to describe the data in feature space, close in spirit to the algorithms of Scho\u0308lkopf, Burges, and Vapnik (1995), with hard boundaries, and Tax and Duin (1999), with \u201csoft margins.\u201d"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 187
                            }
                        ],
                        "text": "Let W be a feature map X ! F, that is, a map into an inner product space F such that the inner product in the image of W can be computed by evaluating some simple kernel (Boser, Guyon, & Vapnik, (1992), Vapnik, (1995); Sch\u00f6lkopf, Burges, et al., (1999))"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 187
                            }
                        ],
                        "text": "Let W be a feature map X ! F, that is, a map into an inner product space F such that the inner product in the image of W can be computed by evaluating some simple kernel (Boser, Guyon, & Vapnik, (1992), Vapnik, (1995); Sch\u00f6lkopf, Burges, et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 284
                            }
                        ],
                        "text": "\u2026obtain another\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089976601750264965 by guest on 09 July 2021\noptimal separating hyperplane by reflecting the first one with respect to the origin this would contradict the uniqueness of the optimal separating hyperplane (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "optimal separating hyperplane by re\u008fecting the \u008erst one with respect to the origin this would contradict the uniqueness of the optimal separating hyperplane (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98356364"
                        ],
                        "name": "D. Stoneking",
                        "slug": "D.-Stoneking",
                        "structuredName": {
                            "firstName": "Danny",
                            "lastName": "Stoneking",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stoneking"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 111165777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b82ee88f169363d031a67b64fcd633faf2482935",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of statistical circuit design techniques can prevent problems of low yield in manufacture of circuits. Statistical circuit design techniques analyze the yields of circuit designs whose underlying components exhibit random fluctuations. These techniques can help produce more robust designs by calling attention to areas where statistical variations are likely to combine in such a way as to cause circuit failure. Nonparametric boundary analysis (NBA), a technique introduced in Hewlett Packard EEsof's IC-CAP 5.0, permits yield analysis when the random fluctuations result from an arbitrary stochastic process, in addition to well-studied processes such as the Gaussian."
            },
            "slug": "Improving-the-manufacturability-of-electronic-Stoneking",
            "title": {
                "fragments": [],
                "text": "Improving the manufacturability of electronic designs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Nonparametric boundary analysis (NBA), a technique introduced in Hewlett Packard EEsof's IC-CAP 5.0, permits yield analysis when the random fluctuations result from an arbitrary stochastic process, in addition to well-studied processes such as the Gaussian."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145039030"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "(Platt, 1999), and have been found to well in our experiments to be reported below."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "The algorithm is a modified version of SMO (Sequential Minimal Optimization), an SV training algorithm originally proposed for classification (Platt, 1999), and subsequently adapted to regression estimation (Smola and Sch\u00f6lkopf, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 79
                            }
                        ],
                        "text": "These other heuristics are the same as in the case of pattern recognition (cf. Platt, 1999), and have been found to work well in the experiments we report below."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 205
                            }
                        ],
                        "text": "In this section, we describe an algorithm that takes advantage of these features and empirically scales better to large data set sizes than a standard QP solver with time complexity of order O ( `3 )\n(cf. Platt, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 143
                            }
                        ],
                        "text": "The algorithm is a modified version of SMO (sequential minimal optimization), an SV training algorithm originally proposed for classification (Platt, 1999), and subsequently adapted to regression estimation (Smola & Scho\u0308lkopf, in press)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58153557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5434adb46efd796915d1ec1b5adf97d651badc0",
            "isKey": true,
            "numCitedBy": 217,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-training-of-svms-using-sequential-minimal-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of svms using sequential minimal optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482710"
                        ],
                        "name": "P. Hayton",
                        "slug": "P.-Hayton",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Hayton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hayton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143923114"
                        ],
                        "name": "L. Tarassenko",
                        "slug": "L.-Tarassenko",
                        "structuredName": {
                            "firstName": "Lionel",
                            "lastName": "Tarassenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tarassenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2655487"
                        ],
                        "name": "P. Anuzis",
                        "slug": "P.-Anuzis",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Anuzis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anuzis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7548605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa16e86a2e6bb487d4b0c419f6a745f57170059b",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A system has been developed to extract diagnostic information from jet engine carcass vibration data. Support Vector Machines applied to novelty detection provide a measure of how unusual the shape of a vibration signature is, by learning a representation of normality. We describe a novel method for Support Vector Machines of including information from a second class for novelty detection and give results from the application to Jet Engine vibration analysis."
            },
            "slug": "Support-Vector-Novelty-Detection-Applied-to-Jet-Hayton-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel method for Support Vector Machines of including information from a second class for novelty detection is described and results from the application to Jet Engine vibration analysis are given."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 99
                            }
                        ],
                        "text": "The existence and uniqueness of the hyperplane then follow from the supporting hyperplane theorem (Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "We scan over the entire data set2 until we \u008end a variable violating a Karush-kuhn-Tucker (KKT) condition (Bertsekas, 1995), that is, a point such that (Oi \u00a1r ) \u00a2 ai > 0 or (r \u00a1Oi) \u00a2 (1/ (o )\u0300 \u00a1 ai) > 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "Suppose xo is an outlier, that is, jo > 0, hence by the KKT conditions (Bertsekas, 1995) ao D 1/ (o )\u0300 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 107
                            }
                        ],
                        "text": "We scan over the entire data set2 until we find a variable violating a Karush-kuhn-Tucker (KKT) condition (Bertsekas, 1995), that is, a point such that (Oi\u2212\u03c1) \u00b7\u03b1i > 0 or (\u03c1\u2212Oi) \u00b7 (1/(\u03bd`)\u2212\u03b1i) > 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 6
                            }
                        ],
                        "text": "Thus (Bertsekas, 1995),\u03b1 is still the optimal solution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 5
                            }
                        ],
                        "text": "Thus (Bertsekas, 1995), \u00ae is still the optimal solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Suppose xo is an outlier, that is, \u03beo > 0, hence by the KKT conditions (Bertsekas, 1995) \u03b1o = 1/(\u03bd`)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": true,
            "numCitedBy": 6359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 551,
                                "start": 51
                            }
                        ],
                        "text": "Observe that forC being all Borel measurable sets, C(1) is the supportof the densityp corresponding toP , assuming it exists. (Note that C(1) is well defined even whenp does not exist.) For smaller classes C, C(1) is the minimum volumeC 2 C containing the support of p. Turning to the case where < 1, it seems the first work was reported by Sager (1979) and then Hartigan (1987) who considered X = R2 with C being the class of closed convex sets in X. (They actually considered density contour clusters; cf. Appendix A for a definition.) Nolan (1991) considered higher dimensions with C being the class of ellipsoids."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 51
                            }
                        ],
                        "text": "Observe that forC being all Borel measurable sets, C(1) is the supportof the densityp corresponding toP , assuming it exists. (Note that C(1) is well defined even whenp does not exist.) For smaller classes C, C(1) is the minimum volumeC 2 C containing the support of p. Turning to the case where < 1, it seems the first work was reported by Sager (1979) and then Hartigan (1987) who considered X = R2 with C being the class of closed convex sets in X."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 634,
                                "start": 51
                            }
                        ],
                        "text": "Observe that forC being all Borel measurable sets, C(1) is the supportof the densityp corresponding toP , assuming it exists. (Note that C(1) is well defined even whenp does not exist.) For smaller classes C, C(1) is the minimum volumeC 2 C containing the support of p. Turning to the case where < 1, it seems the first work was reported by Sager (1979) and then Hartigan (1987) who considered X = R2 with C being the class of closed convex sets in X. (They actually considered density contour clusters; cf. Appendix A for a definition.) Nolan (1991) considered higher dimensions with C being the class of ellipsoids. Tsybakov (1997) has studied an estimator based on piecewise polynomial approximation of C( ) and has shown it attains the asymptotically minimax rate for certain classes of densities p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1930,
                                "start": 51
                            }
                        ],
                        "text": "Observe that forC being all Borel measurable sets, C(1) is the supportof the densityp corresponding toP , assuming it exists. (Note that C(1) is well defined even whenp does not exist.) For smaller classes C, C(1) is the minimum volumeC 2 C containing the support of p. Turning to the case where < 1, it seems the first work was reported by Sager (1979) and then Hartigan (1987) who considered X = R2 with C being the class of closed convex sets in X. (They actually considered density contour clusters; cf. Appendix A for a definition.) Nolan (1991) considered higher dimensions with C being the class of ellipsoids. Tsybakov (1997) has studied an estimator based on piecewise polynomial approximation of C( ) and has shown it attains the asymptotically minimax rate for certain classes of densities p. Polonik (1997) has studied the estimation of C( ) by C`( ). He derived asymptotic rates of convergence in terms of various measures of richness of C. He considered both VC classes and classes with a log covering number with bracketing of order O( r) for r > 0. More information on minimum volume estimators can be found in that work, and in Appendix A. A number of applications have been suggested for these techniques. They include problems in medical diagnosis (Tarassenko et al., 1995), marketing (Ben-David and Lindenbaum, 1997), condition monitoring of machines (Devroye and Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev and Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b) and others (M \u0308 uller, 1992). Most of these papers, in particular those with a theoretical slant, do not go all the way in devising practical algorithms that work on high-dimensional real-worldproblems. A notable exception to this is the work of Tarassenko et al. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 819,
                                "start": 51
                            }
                        ],
                        "text": "Observe that forC being all Borel measurable sets, C(1) is the supportof the densityp corresponding toP , assuming it exists. (Note that C(1) is well defined even whenp does not exist.) For smaller classes C, C(1) is the minimum volumeC 2 C containing the support of p. Turning to the case where < 1, it seems the first work was reported by Sager (1979) and then Hartigan (1987) who considered X = R2 with C being the class of closed convex sets in X. (They actually considered density contour clusters; cf. Appendix A for a definition.) Nolan (1991) considered higher dimensions with C being the class of ellipsoids. Tsybakov (1997) has studied an estimator based on piecewise polynomial approximation of C( ) and has shown it attains the asymptotically minimax rate for certain classes of densities p. Polonik (1997) has studied the estimation of C( ) by C`( )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two examples arevolC(1) or the center ofC(1)"
            },
            "venue": {
                "fragments": [],
                "text": "(See also (Korostelev and Tsybakov,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 281
                            }
                        ],
                        "text": "\u20261980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others (Mu\u0308ller, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": "Polonik (1995b) has also studied the use of the \u201cexcess mass approach\u201d (M\u00fcller, 1992) to construct an estimator of \u201cgeneralized -clusters\u201d which are related to \"D ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "Polonik (1995b) has studied the use of the \u201cexcess mass approach\u201d (Mu\u0308ller, 1992) to construct an estimator of \u201cgeneralized \u03b1-clusters\u201d related to C(\u03b1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 399,
                                "start": 385
                            }
                        ],
                        "text": ", 1995), marketing (Ben-David and Lindenbaum, 1997), condition monitoring of machines (Devroye and Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev and Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b) and others (M\u00fcller, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The excess mass approach in statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Beitra\u0308ge zur Statistik, Universita\u0308t Heidelberg,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 162
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 139
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabelled data points by separating it from the origin using a hyperplane (Vapnik and Lerner, 1963; Vapnik and Chervonenkis, 1974)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 35
                            }
                        ],
                        "text": "Firstly, the original algorithm in Vapnik and Chervonenkis (1974) was limited to linear decision rules in input space; second, there was no way of dealing with outliers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 35
                            }
                        ],
                        "text": "Firstly, the original algorithm in (Vapnik and Chervonenkis, 1974) was limited to linear decision rules in input space, secondly, there was no way of dealing with outliers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition [in Russian"
            },
            "venue": {
                "fragments": [],
                "text": "Nauka, Moscow,"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70184938"
                        ],
                        "name": "Num\u00e9risation de documents anciens math\u00e9matiques",
                        "slug": "Num\u00e9risation-de-documents-anciens-math\u00e9matiques",
                        "structuredName": {
                            "firstName": "Num\u00e9risation",
                            "lastName": "math\u00e9matiques",
                            "middleNames": [
                                "de",
                                "documents",
                                "anciens"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Num\u00e9risation de documents anciens math\u00e9matiques"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125110486,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "8640a639984deecfe78000b9ebbeb9244e9adc4c",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Annales-de-l'Institut-Henri-Poincar\u00e9.-Section-B,-et-math\u00e9matiques",
            "title": {
                "fragments": [],
                "text": "Annales de l'Institut Henri Poincar\u00e9. Section B, Calcul des probabilit\u00e9s et statistique"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125202514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5efc26183090fa5b042ffbcb9e6d811a79a14067",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bounds-on-Error-Expectation-for-SVM-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Bounds on Error Expectation for SVM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 91
                            }
                        ],
                        "text": "It is our hope that theoretical results as the one we have briefly outlined and the one of Vapnik and Chapelle (2000) will provide a solid foundation for this formidable task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115626129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc541af488410b6558c02386665a698e0bf93618",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bounds-on-error-expectation-for-SVM-Vapnik-Chapelle",
            "title": {
                "fragments": [],
                "text": "Bounds on error expectation for SVM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101872040"
                        ],
                        "name": "A. Korostelev",
                        "slug": "A.-Korostelev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Korostelev",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Korostelev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2667773"
                        ],
                        "name": "A. Tsybakov",
                        "slug": "A.-Tsybakov",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Tsybakov",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsybakov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 10
                            }
                        ],
                        "text": "(See also Korostelev & Tsybakov, 1993, chap."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 149
                            }
                        ],
                        "text": "\u2026(Devroye & Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116404230,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2a789b40864a553550915891325fc3c184c5ccfc",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MiniMax-Methods-for-Image-Reconstruction-Korostelev-Tsybakov",
            "title": {
                "fragments": [],
                "text": "MiniMax Methods for Image Reconstruction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115205884,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7cabbdf6a7288d15e26fa6ea504009bab3d1edf4",
            "isKey": false,
            "numCitedBy": 1137,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-recognition-using-generalized-portrait-Vapnik",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ARTICLE Communicated by Vladimir Vapnik"
                    },
                    "intents": []
                }
            ],
            "corpusId": 196129709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b2e42d8510dca89629a61bb52f19f92fb3d23d2",
            "isKey": false,
            "numCitedBy": 924,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Advances-in-Kernel-Methods-Support-Vector-Learning-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Advances in Kernel Methods - Support Vector Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1419672012"
                        ],
                        "name": "Smola Aj Williamson R",
                        "slug": "Smola-Aj-Williamson-R",
                        "structuredName": {
                            "firstName": "Smola",
                            "lastName": "Williamson R",
                            "middleNames": [
                                "Aj"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Smola Aj Williamson R"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247052"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052120020"
                        ],
                        "name": "H. Ritter",
                        "slug": "H.-Ritter",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ritter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60120277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8177c99c40b23f9fd5f4ebd12be72d82323fb347",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Single-class-Support-Vector-Machines-Sch\u00f6lkopf-SmolaAjWilliamson",
            "title": {
                "fragments": [],
                "text": "Single-class Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 162
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 35
                            }
                        ],
                        "text": "Firstly, the original algorithm in Vapnik and Chervonenkis (1974) was limited to linear decision rules in input space; second, there was no way of dealing with outliers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61480753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edb62d05f8eeaa7e1921c6c25c544935a2b6b131",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Pattern-Recognition-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating the support of a high-dimensional distribution Available online at: http://www.research.microsoft.com/scripts/pubs/view"
            },
            "venue": {
                "fragments": [],
                "text": "Estimating the support of a high-dimensional distribution Available online at: http://www.research.microsoft.com/scripts/pubs/view"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation du support et du contour du support d \u2019 une loi de probabilit \u00c2 e"
            },
            "venue": {
                "fragments": [],
                "text": "Annales de l \u2019 InstitutHenri Poincar \u00c2 e . SectionB . Calcul des Probabilit \u00c2 es et Statistique . Nouvelle S \u00c2 erie"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cristianini . Margin distribution bounds on generalization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "Note that this result also applies to the soft margin ball algorithm of Tax and Duin (1999), provided that it is stated in the \u03bd-parameterization given in section 3."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 202
                            }
                        ],
                        "text": "To conclude this section, we note that one can also use balls to describe the data in feature space, close in spirit to the algorithms of Scho\u0308lkopf, Burges, and Vapnik (1995), with hard boundaries, and Tax and Duin (1999), with \u201csoft margins.\u201d"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Data domain description by support vectors"
            },
            "venue": {
                "fragments": [],
                "text": "M. Verleysen , editor, Proceedings ESANN"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 10
                            }
                        ],
                        "text": "(See also Korostelev & Tsybakov, 1993, chap."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 149
                            }
                        ],
                        "text": "\u2026(Devroye & Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1993).Minimax theory of image reconstruction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cristianini . Margin distribution bounds on generalization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "\u2026of machines (Devroye & Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997; Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 338,
                                "start": 294
                            }
                        ],
                        "text": "They include problems in medical diagnosis (Tarassenko, Hayton, Cerneaz, & Brady, 1995), marketing (Ben-David & Lindenbaum, 1997), condition monitoring of machines (Devroye& Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997;Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others (M\u00fcller, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Tsybakov (1997) has studied an estimator based on piecewise polynomial approximation of C(\u03b1) and has shown it attains the asymptotically minimax rate for certain classes of densities p. Polonik (1997) has studied the estimation of C(\u03b1) by C`(\u03b1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1997).On nonparametric estimation of density level"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Data domain description by support vectors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 153
                            }
                        ],
                        "text": "In order to describe some previous work, it is convenient to introduce the following definition of a (multidimensional) quantile function, introduced by Einmal and Mason (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized quantile processes. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Generalized quantile processes. The Annals of Statistics"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation du support et du contour du support d\u2019une loi de probabilit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1976).Estimation du support et du contour du support d\u2019une loi de probabilit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "A notable exception to this is the work of Tarassenko et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Novelty detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "Chevalier (1976); Devroye and Wise (1980); Cuevas (1990); see (Gayraud, 1997) for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 62
                            }
                        ],
                        "text": "The most recent work relating to the estimation of C(1) is by Gayraud (1997), who has made an asymptotic minimax study of estimators of functionals of C(1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 135
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of functional of density support"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematical Methods of Statistics"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "Note that this result also applies to the soft margin ball algorithm of Tax and Duin (1999), provided that it is stated in the \u03bd-parameterization given in section 3."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 202
                            }
                        ],
                        "text": "To conclude this section, we note that one can also use balls to describe the data in feature space, close in spirit to the algorithms of Scho\u0308lkopf, Burges, and Vapnik (1995), with hard boundaries, and Tax and Duin (1999), with \u201csoft margins.\u201d"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Datadomaindescription by support vectors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik . Extracting supportda a for a given task"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portraits. Avtomatika i Telemekhanika"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern recognition using generalized portraits. Avtomatika i Telemekhanika"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel method for percentile feature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 99
                            }
                        ],
                        "text": "The regularization operators of common kernels can be shown to correspond to derivative operators (Poggio & Girosi, 1990); therefore, minimizing the dual objective function corresponds to maximizing the smoothness of the function f (which is, up to a thresholding operation, the function we\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning. Proceedings of the IEEE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 139
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabelled data points by separating it from the origin using a hyperplane (Vapnik and Lerner, 1963; Vapnik and Chervonenkis, 1974)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portraits"
            },
            "venue": {
                "fragments": [],
                "text": "Avtomatika i Telemekhanika,"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 99
                            }
                        ],
                        "text": "The regularization operators of common kernels can be shown to correspond to derivative operators (Poggio & Girosi, 1990); therefore, minimizing the dual objective function corresponds to maximizing the smoothness of the function f (which is, up to a thresholding operation, the function we\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning. Proceedings of the IEEE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "Note that this result also applies to the soft margin ball algorithm of Tax and Duin (1999), provided that it is stated in the \u03bd-parameterization given in section 3."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 202
                            }
                        ],
                        "text": "To conclude this section, we note that one can also use balls to describe the data in feature space, close in spirit to the algorithms of Scho\u0308lkopf, Burges, and Vapnik (1995), with hard boundaries, and Tax and Duin (1999), with \u201csoft margins.\u201d"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1999).Data domain description by support vectors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 162
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 35
                            }
                        ],
                        "text": "Firstly, the original algorithm in Vapnik and Chervonenkis (1974) was limited to linear decision rules in input space; second, there was no way of dealing with outliers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of pattern recognition. Nauka, Moscow. [In Russian] (German translation: W"
            },
            "venue": {
                "fragments": [],
                "text": "Wapnik & A. Tscherwonenkis, Theorie der Zeichenerkennung, Akademie\u2013Verlag, Berlin,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "A notable exception to this is the work of Tarassenko et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Novelty detection for the identication of masses in mammograms"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEE International Conference on Articial Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation du support et du contour du support d'une loi de probabilit\u00c2 e"
            },
            "venue": {
                "fragments": [],
                "text": "Annales de l'Institut Henri Poincar\u00c2 e. Section B. Calcul des Probabilit\u00c2 es et Statistique. Nouvelle S\u00c2 erie"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation du support et du contour du support d'une loi de probabilit\u00e9. Annales de l'Institut Henri Poincar\u00e9. Section B"
            },
            "venue": {
                "fragments": [],
                "text": "Calcul des Probabilit\u00e9s et Statistique . Nouvelle S\u00e9rie"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method. Automation and Remote Control"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method. Automation and Remote Control"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The connection between regu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 69
                            }
                        ],
                        "text": "The problem of estimating C(1) appears to have first been studied by Geffroy (1964) who considered X = R2 with piecewise constant estimators."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sur unprob\u00ec eme d'estimation g\u00e9om\u00e9trique. Publications de l'Institut de Statistique de l"
            },
            "venue": {
                "fragments": [],
                "text": "Sur unprob\u00ec eme d'estimation g\u00e9om\u00e9trique. Publications de l'Institut de Statistique de l"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 69
                            }
                        ],
                        "text": "The problem of estimating C(1) appears to have first been studied by Geffroy (1964) who considered X = R2 with piecewise constant estimators."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sur unprob\u00ec eme d'estimation g\u00c2 eom\u00c2 etrique. Publications de l"
            },
            "venue": {
                "fragments": [],
                "text": "Sur unprob\u00ec eme d'estimation g\u00c2 eom\u00c2 etrique. Publications de l"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "A notable exception to this is the work of Tarassenko et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Novelty detection for the identi \u008e cationofmasses inmammograms"
            },
            "venue": {
                "fragments": [],
                "text": "ProceedingsFourth IEE International Conference on Arti \u008e cial Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularized principal manifolds . Machine Learning ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Margin distribution bounds on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sch  \u0308 olkopf . A tutorial on support vector regression"
            },
            "venue": {
                "fragments": [],
                "text": "NeuroColt 2TR"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel principal component"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 116
                            }
                        ],
                        "text": "To this end, we use a result of Williamson, Smola, and Scho\u0308lkopf (2000) to bound logN (\u03b3 /2,F , 2`), and a result of Shawe-Taylor and Cristianini (2000) to bound logN (\u03b3 /2,LB(X ), 2`)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the generalisation of soft margin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating the support of a high-dimensional distribution"
            },
            "venue": {
                "fragments": [],
                "text": "Microsoft Research. Available online at"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 713,
                                "start": 459
                            }
                        ],
                        "text": "They include problems in medical diagnosis (Tarassenko, Hayton, Cerneaz, & Brady, 1995), marketing (Ben-David & Lindenbaum, 1997), condition monitoring of machines (Devroye& Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997;Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others (M\u00fcller, 1992). Most of this work, in particular that with a theoretical slant, does not go all the way in devising practical algorithms that work on high-dimensional real-world-problems.A notable exception to this is the work of Tarassenko et al. (1995). Polonik, (1995a) has shown how one can use estimators of C(a) to construct density estimators."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 472,
                                "start": 458
                            }
                        ],
                        "text": "They include problems in medical diagnosis (Tarassenko, Hayton, Cerneaz, & Brady, 1995), marketing (Ben-David & Lindenbaum, 1997), condition monitoring of machines (Devroye& Wise, 1980), estimating manufacturing yields (Stoneking, 1999), econometrics and generalized nonlinear principal curves (Tsybakov, 1997;Korostelev & Tsybakov, 1993), regression and spectral analysis (Polonik, 1997), tests for multimodality and clustering (Polonik, 1995b), and others (M\u00fcller, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "Polonik (1995b) has studied the use of the \u201cexcess mass approach\u201d (M\u00fcller, 1992) to construct an estimator of \u201cgeneralized a-clusters\u201d related to C(a) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The excess mass approach in statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 69
                            }
                        ],
                        "text": "The problem of estimating C(1) appears to have first been studied by Geffroy (1964) who considered X = R2 with piecewise constant estimators."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sur un probl\u00e8me d\u2019estimation g\u00e9om\u00e9trique"
            },
            "venue": {
                "fragments": [],
                "text": "Publications de l\u2019Institut de Statistique de l\u2019Universite\u0301 de Paris,"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Microsoft Research Available online at: http://www.research.microsoft.com/scripts/pubs/view"
            },
            "venue": {
                "fragments": [],
                "text": "Microsoft Research Available online at: http://www.research.microsoft.com/scripts/pubs/view"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ARTICLE Communicated by Vladimir Vapnik"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularized principal manifolds Forthcoming"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "studied the problem of estimatingcp( ). They too made use of VC classes but stated their results in a stronger form which is meaningful for finite sample sizes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received November"
            },
            "venue": {
                "fragments": [],
                "text": "Received November"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "A notable exception to this is the work of Tarassenko et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Novelty detection for the identiication of masses in mammograms"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEE International Conference on Artiicial Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 92
                            }
                        ],
                        "text": "There have been a number of works studying a natural nonparametric estimator of C(1) (e.g., Chevalier, 1976; Devroye & Wise, 1980; see Gayraud, 1997 for further references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation du support et du contour du support d\u2019une loi de probabilit\u00e9"
            },
            "venue": {
                "fragments": [],
                "text": "Annales de l\u2019Institut Henri Poincare\u0301. Section B. Calcul des Probabilite\u0301s et Statistique. Nouvelle Se\u0301rie,"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating the support of a high-dimensional distribution (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. No. 87)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 162
                            }
                        ],
                        "text": "In 1962, they proposed an algorithm for characterizing a set of unlabeled data points by separating it from the origin using a hyperplane (Vapnik & Lerner, 1963; Vapnik & Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 35
                            }
                        ],
                        "text": "Firstly, the original algorithm in Vapnik and Chervonenkis (1974) was limited to linear decision rules in input space; second, there was no way of dealing with outliers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of pattern recognition. Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The excess mass approach in statistics"
            },
            "venue": {
                "fragments": [],
                "text": "The excess mass approach in statistics"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "A notable exception to this is the work of Tarassenko et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Novelty detection for the identi\u008ecation of masses in mammograms"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings Fourth IEE International Conference on Arti\u008ecial Neural Networks (pp. 442\u2013447)"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 39,
            "methodology": 21,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 110,
        "totalPages": 11
    },
    "page_url": "https://www.semanticscholar.org/paper/Estimating-the-Support-of-a-High-Dimensional-Sch\u00f6lkopf-Platt/9cc912ae25797e5f7c0d73300d3968ad8339b411?sort=total-citations"
}