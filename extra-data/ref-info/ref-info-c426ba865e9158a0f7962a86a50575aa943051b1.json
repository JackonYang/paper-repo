{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860351"
                        ],
                        "name": "Will Y. Zou",
                        "slug": "Will-Y.-Zou",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Zou",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Y. Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682028"
                        ],
                        "name": "Shenghuo Zhu",
                        "slug": "Shenghuo-Zhu",
                        "structuredName": {
                            "firstName": "Shenghuo",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenghuo Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Temporal coherence has been explored for unsupervised feature learning with CNNs [22, 37, 9, 3, 19], with applications to dimensionality reduction [10], object recognition [22, 37], and metric learning [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7782960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aefc7c708269b874182a5c877fb6dae06da210d4",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply salient feature detection and tracking in videos to simulate fixations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classification accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset."
            },
            "slug": "Deep-Learning-of-Invariant-Features-via-Simulated-Zou-Ng",
            "title": {
                "fragments": [],
                "text": "Deep Learning of Invariant Features via Simulated Fixations in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work applies salient feature detection and tracking in videos to simulate fixations and smooth pursuit in human vision, and achieves state-of-the-art recognition accuracy 61% on STL-10 dataset."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710604"
                        ],
                        "name": "R. Memisevic",
                        "slug": "R.-Memisevic",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Memisevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Memisevic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "In [20], bilinear models with multiplicative interactions are used to learn contentindependent \u201cmotion features\u201d that encode only the transformation between image pairs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7197251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f282338fa4cd5516cdcd33cd4b6034f9739c45f4",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. Recently, there has been increasing interest in learning to infer correspondences from data using relational, spatiotemporal, and bilinear variants of deep learning methods. These methods use multiplicative interactions between pixels or between features to represent correlation patterns across multiple images. In this paper, we review the recent work on relational feature learning, and we provide an analysis of the role that multiplicative interactions play in learning to encode relations. We also discuss how square-pooling and complex cell models can be viewed as a way to represent multiplicative interactions and thereby as a way to encode relations."
            },
            "slug": "Learning-to-Relate-Images-Memisevic",
            "title": {
                "fragments": [],
                "text": "Learning to Relate Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper reviews the recent work on relational feature learning, and provides an analysis of the role that multiplicative interactions play in learning to encode relations, and discusses how square-pooling and complex cell models can be viewed as a way to representmultiplicative interactions and thereby as a ways to encoded relations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33932184"
                        ],
                        "name": "Pulkit Agrawal",
                        "slug": "Pulkit-Agrawal",
                        "structuredName": {
                            "firstName": "Pulkit",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pulkit Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1637703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfbfaaec46d38392f61d683c340ee92a0a66e5d9",
            "isKey": false,
            "numCitedBy": 487,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "The current dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it also possible to learn features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigated if the awareness of egomotion(i.e. self motion) can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching."
            },
            "slug": "Learning-to-See-by-Moving-Agrawal-Carreira",
            "title": {
                "fragments": [],
                "text": "Learning to See by Moving"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt with class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47002659"
                        ],
                        "name": "Yin Li",
                        "slug": "Yin-Li",
                        "structuredName": {
                            "firstName": "Yin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50706340"
                        ],
                        "name": "A. Fathi",
                        "slug": "A.-Fathi",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Fathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15749797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd6f8f20f549bd9d6711c1c7e89c3350ed48f8c1",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model for gaze prediction in egocentric video by leveraging the implicit cues that exist in camera wearer's behaviors. Specifically, we compute the camera wearer's head motion and hand location from the video and combine them to estimate where the eyes look. We further model the dynamic behavior of the gaze, in particular fixations, as latent variables to improve the gaze prediction. Our gaze prediction results outperform the state-of-the-art algorithms by a large margin on publicly available egocentric vision datasets. In addition, we demonstrate that we get a significant performance boost in recognizing daily actions and segmenting foreground objects by plugging in our gaze predictions into state-of-the-art methods."
            },
            "slug": "Learning-to-Predict-Gaze-in-Egocentric-Video-Li-Fathi",
            "title": {
                "fragments": [],
                "text": "Learning to Predict Gaze in Egocentric Video"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A model for gaze prediction in egocentric video is presented by leveraging the implicit cues that exist in camera wearer's behaviors and model the dynamic behavior of the gaze, in particular fixations, as latent variables to improve the gaze prediction."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115473544"
                        ],
                        "name": "Changhai Xu",
                        "slug": "Changhai-Xu",
                        "structuredName": {
                            "firstName": "Changhai",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changhai Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800425"
                        ],
                        "name": "Jingen Liu",
                        "slug": "Jingen-Liu",
                        "structuredName": {
                            "firstName": "Jingen",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingen Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145585296"
                        ],
                        "name": "B. Kuipers",
                        "slug": "B.-Kuipers",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Kuipers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kuipers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1080382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99b75de6386f30971ba2aece836bc2ae2cb482f8",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Moving object segmentation from an image sequence is essential for a robot to interact with its environment. Traditional vision approaches appeal to pure motion analysis on videos without exploiting the source of the background motion. We observe, however, that the background motion (from the robot's egocentric view) has stronger correlation to the robot's motor signals than the foreground motion. We propose a novel approach to detecting moving objects by clustering features into background and foreground according to their motion consistency with motor signals. Specifically, our approach learns homography and fundamental matrices as functions of motor signals, and predict sparse feature locations from the learned matrices. The errors between the predictions and their actual tracked locations are used to label them into background and foreground. The labels of the sparse features are then propagated to all pixels. Our approach does not require building a dense mosaic background or searching for affine, homography, or fundamental matrix parameters for foreground separation. In addition, it does not need to explicitly model the intrinsic and extrinsic calibration parameters hence requires much less prior geometry knowledge. It works completely in 2D image space, and does not involve any complex analysis or computation in 3D space."
            },
            "slug": "Moving-Object-Segmentation-Using-Motor-Signals-Xu-Liu",
            "title": {
                "fragments": [],
                "text": "Moving Object Segmentation Using Motor Signals"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel approach to detecting moving objects by clustering features into background and foreground according to their motion consistency with motor signals, which works completely in 2D image space, and does not involve any complex analysis or computation in 3D space."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2558463"
                        ],
                        "name": "Ross Goroshin",
                        "slug": "Ross-Goroshin",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Goroshin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross Goroshin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143627859"
                        ],
                        "name": "Joan Bruna",
                        "slug": "Joan-Bruna",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Bruna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Bruna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704494"
                        ],
                        "name": "Jonathan Tompson",
                        "slug": "Jonathan-Tompson",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Tompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Tompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ed features vary slowly over continuous video, since visual stimuli can only gradually change between adjacent frames. Temporal coherence has been explored for unsupervised feature learning with CNNs [22, 37, 9, 3, 19], with applications to dimensionality reduction [10], object recognition [22, 37], and metric learning [9]. Temporal coherence of inferred body poses in unlabeled video is exploited for invariant reco"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "larization ([10]). TEMPORAL and DRLIM are the most pertinent baselines because they, like us, use contrastive loss-based formulations, but represent the popular \u201cslowness\u201d-based family of techniques ([37, 3, 9, 19]) for unsupervised feature learning from video, which, unlike our approach, are passive. 4.1. Experimental setup details Recall that in the fully unsupervised mode, our method trains with pairs of vid"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "hat couples ego-motor signals and video. To our knowledge, ours is the \ufb01rst attempt to ground feature learning in physical activity. The limited prior work on unsupervised feature learning with video [22, 24, 21, 9] learns only passively from observed scene dynamics, uninformed by explicit motor sensory cues. Furthermore, while equivariance is explored in some recent work, unlike our idea, it typically focuses o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 98541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53e14bb909ef71388c8ca189c9db84b52af2db44",
            "isKey": true,
            "numCitedBy": 119,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define \"temporal coherence\" -- a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels."
            },
            "slug": "Unsupervised-Learning-of-Spatiotemporally-Coherent-Goroshin-Bruna",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Spatiotemporally Coherent Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work focuses on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information, and establishes a connection between slow feature learning and metric learning."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295846"
                        ],
                        "name": "K. Yamada",
                        "slug": "K.-Yamada",
                        "structuredName": {
                            "firstName": "Kentaro",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751242"
                        ],
                        "name": "Yusuke Sugano",
                        "slug": "Yusuke-Sugano",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Sugano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Sugano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1943600"
                        ],
                        "name": "Takahiro Okabe",
                        "slug": "Takahiro-Okabe",
                        "structuredName": {
                            "firstName": "Takahiro",
                            "lastName": "Okabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takahiro Okabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9467266"
                        ],
                        "name": "Yoichi Sato",
                        "slug": "Yoichi-Sato",
                        "structuredName": {
                            "firstName": "Yoichi",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoichi Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143993575"
                        ],
                        "name": "A. Sugimoto",
                        "slug": "A.-Sugimoto",
                        "structuredName": {
                            "firstName": "Akihiro",
                            "lastName": "Sugimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sugimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805464"
                        ],
                        "name": "K. Hiraki",
                        "slug": "K.-Hiraki",
                        "structuredName": {
                            "firstName": "Kazuo",
                            "lastName": "Hiraki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hiraki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "While most work relies solely on apparent image motion, the method of [35] exploits a robot\u2019s motor signals to detect moving objects and [23] uses reinforcement learning to form robot movement policies by exploiting correlations between motor commands and observed motion cues."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10643792,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f2e5b460fbc692e63e160cdef26141d236f5211f",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method of predicting human egocentric visual attention using bottom-up visual saliency and egomotion information. Computational models of visual saliency are often employed to predict human attention; however, its mechanism and effectiveness have not been fully explored in egocentric vision. The purpose of our framework is to compute attention maps from an egocentric video that can be used to infer a person's visual attention. In addition to a standard visual saliency model, two kinds of attention maps are computed based on a camera's rotation velocity and direction of movement. These rotation-based and translation-based attention maps are aggregated with a bottom-up saliency map to enhance the accuracy with which the person's gaze positions can be predicted. The efficiency of the proposed framework was examined in real environments by using a head-mounted gaze tracker, and we found that the egomotion-based attention maps contributed to accurately predicting human visual attention."
            },
            "slug": "Attention-Prediction-in-Egocentric-Video-Using-and-Yamada-Sugano",
            "title": {
                "fragments": [],
                "text": "Attention Prediction in Egocentric Video Using Motion and Visual Saliency"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The efficiency of the proposed framework was examined in real environments by using a head-mounted gaze tracker, and it was found that the egomotion-based attention maps contributed to accurately predicting human visual attention."
            },
            "venue": {
                "fragments": [],
                "text": "PSIVT"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3197570"
                        ],
                        "name": "Chao-Yeh Chen",
                        "slug": "Chao-Yeh-Chen",
                        "structuredName": {
                            "firstName": "Chao-Yeh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao-Yeh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7696761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db0b7b99bf25fda8673ab169ce8c1d7cb70ff8a6",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process. Using unlabeled video containing various human activities, the system first learns how body pose tends to change locally in time. Then, given a small number of labeled static images, it uses that model to extrapolate beyond the given exemplars and generate \"synthetic\" training examples-poses that could link the observed images and/or immediately precede or follow them in time. In this way, we expand the training set without requiring additional manually labeled examples. We explore both example-based and manifold-based methods to implement our idea. Applying our approach to recognize actions in both images and video, we show it enhances a state-of-the-art technique when very few labeled training examples are available."
            },
            "slug": "Watching-Unlabeled-Video-Helps-Learn-New-Human-from-Chen-Grauman",
            "title": {
                "fragments": [],
                "text": "Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Applying an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process, it enhances a state-of-the-art technique when very few labeled training examples are available."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149531"
                        ],
                        "name": "Arthur D. Szlam",
                        "slug": "Arthur-D.-Szlam",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Szlam",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arthur D. Szlam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143627859"
                        ],
                        "name": "Joan Bruna",
                        "slug": "Joan-Bruna",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Bruna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Bruna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "hat couples ego-motor signals and video. To our knowledge, ours is the \ufb01rst attempt to ground feature learning in physical activity. The limited prior work on unsupervised feature learning with video [20,22,19,7] learns only passively from observed scene dynamics, uninformed by explicit motor sensory cues. Furthermore, while equivariance is explored in some recent work, unlike our idea, it typically focuses o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the \u201cgated autoencoder\u201d is extended to perform sequence prediction for video in [19]. Recurrent neural networks combined with a grammar model of scene dynamics can also predict future frames in video [22]. Whereas these methods learn a representation for image pairs (or tuples) related by some transformation, we learn a representation for individual images in which the behavior under transformations i"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17572062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c",
            "isKey": false,
            "numCitedBy": 379,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences."
            },
            "slug": "Video-(language)-modeling:-a-baseline-for-models-of-Ranzato-Szlam",
            "title": {
                "fragments": [],
                "text": "Video (language) modeling: a baseline for generative models of natural videos"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "For the first time, it is shown that a strong baseline model for unsupervised feature learning using video data can predict non-trivial motions over short video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152732433"
                        ],
                        "name": "Uwe Schmidt",
                        "slug": "Uwe-Schmidt",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 17
                            }
                        ],
                        "text": "While prior work [14, 26] focuses on equivariance where g is a 2D image warp, we explore the case where g \u2208 P is an ego-motion pattern (cf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 96
                            }
                        ],
                        "text": "Recent work explores ways to learn descriptors with in-plane translation/rotation equ ivariance [14, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14624413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23593d08d7fe3b87e59343f194cf72a092d36304",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Identifying suitable image features is a central challenge in computer vision, ranging from representations for low-level to high-level vision. Due to the difficulty of this task, techniques for learning features directly from example data have recently gained attention. Despite significant benefits, these learned features often have many fewer of the desired invariances or equivariances than their hand-crafted counterparts. While translation in-/equivariance has been addressed, the issue of learning rotation-invariant or equivariant representations is hardly explored. In this paper we describe a general framework for incorporating invariance to linear image transformations into product models for feature learning. A particular benefit is that our approach induces transformation-aware feature learning, i.e. it yields features that have a notion with which specific image transformation they are used. We focus our study on rotation in-/equivariance and show the advantages of our approach in learning rotation-invariant image priors and in building rotation-equivariant and invariant descriptors of learned features, which result in state-of-the-art performance for rotation-invariant object detection."
            },
            "slug": "Learning-rotation-aware-features:-From-invariant-to-Schmidt-Roth",
            "title": {
                "fragments": [],
                "text": "Learning rotation-aware features: From invariant priors to equivariant descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper describes a general framework for incorporating invariance to linear image transformations into product models for feature learning and shows the advantages of this approach in learning rotation-invariant image priors and in building rotation-equivariant and invariant descriptors of learned features."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8729431"
                        ],
                        "name": "Sida I. Wang",
                        "slug": "Sida-I.-Wang",
                        "structuredName": {
                            "firstName": "Sida",
                            "lastName": "Wang",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sida I. Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Methods to learn representations with disentangled latent factors [11, 14] aim to sort properties like pose, illumination etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, the transforming auto-encoder learns to explicitly represent instantiation parameters of object parts in equivariant hidden layer units [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, whereas existing methods that learn from image transformations focus on view synthesis applications [11, 14, 20], we explore recognition applications of learning jointly equivariant and discriminative feature maps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6138085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20f0357688876fa4662f806f985779dce6e24f3c",
            "isKey": false,
            "numCitedBy": 838,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain."
            },
            "slug": "Transforming-Auto-Encoders-Hinton-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "Transforming Auto-Encoders"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that neural networks can be used to learn features that output a whole vector of instantiation parameters and this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1954876"
                        ],
                        "name": "Tejas D. Kulkarni",
                        "slug": "Tejas-D.-Kulkarni",
                        "structuredName": {
                            "firstName": "Tejas",
                            "lastName": "Kulkarni",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tejas D. Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3376546"
                        ],
                        "name": "William F. Whitney",
                        "slug": "William-F.-Whitney",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Whitney",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William F. Whitney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Methods to learn representations with disentangled latent factors [11, 14] aim to sort properties like pose, illumination etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, whereas existing methods that learn from image transformations focus on view synthesis applications [11, 14, 20], we explore recognition applications of learning jointly equivariant and discriminative feature maps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14020873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "687e80eb70c7bbad6001006d9269b202650a3354",
            "isKey": false,
            "numCitedBy": 825,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs."
            },
            "slug": "Deep-Convolutional-Inverse-Graphics-Network-Kulkarni-Whitney",
            "title": {
                "fragments": [],
                "text": "Deep Convolutional Inverse Graphics Network"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232655"
                        ],
                        "name": "H. Mobahi",
                        "slug": "H.-Mobahi",
                        "structuredName": {
                            "firstName": "Hossein",
                            "lastName": "Mobahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mobahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "We want to highlight the important distinctions between our objective and the \u201ctemporal coherence\u201d objective of [22] for slow feature analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 111
                            }
                        ],
                        "text": "Most relevant to our work are feature learning methods based on temporal coherence and \u201cslow feature analysis\u201d [32, 10, 22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Written in our notation, the objective of [22] may be stated as:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "\u2022 TEMPORAL: The temporal coherenceapproach of [22], which regularizes the classification loss with Eq (7) setting the distance measure d(."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1883779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2c477de72bb7718f5304c6f38457fda9c8334b1",
            "isKey": true,
            "numCitedBy": 335,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks."
            },
            "slug": "Deep-learning-from-temporal-coherence-in-video-Mobahi-Collobert",
            "title": {
                "fragments": [],
                "text": "Deep learning from temporal coherence in video"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings, and is used to improve the performance on a supervised task of interest."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39599498"
                        ],
                        "name": "Chunhui Gu",
                        "slug": "Chunhui-Gu",
                        "structuredName": {
                            "firstName": "Chunhui",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhui Gu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Recent methods use ego-motion cues to separate foreground and background [25, 35] or infer the firstperson gaze [36, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14825223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04b16a1a19ee2128c663326b1e87a2d8ec368450",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Identifying handled objects, i.e. objects being manipulated by a user, is essential for recognizing the person's activities. An egocentric camera as worn on the body enjoys many advantages such as having a natural first-person view and not needing to instrument the environment. It is also a challenging setting, where background clutter is known to be a major source of problems and is difficult to handle with the camera constantly and arbitrarily moving. In this work we develop a bottom-up motion-based approach to robustly segment out foreground objects in egocentric video and show that it greatly improves object recognition accuracy. Our key insight is that egocentric video of object manipulation is a special domain and many domain-specific cues can readily help. We compute dense optical flow and fit it into multiple affine layers. We then use a max-margin classifier to combine motion with empirical knowledge of object location and background movement as well as temporal cues of support region and color appearance. We evaluate our segmentation algorithm on the large Intel Egocentric Object Recognition dataset with 42 objects and 100K frames. We show that, when combined with temporal integration, figure-ground segmentation improves the accuracy of a SIFT-based recognition system from 33% to 60%, and that of a latent-HOG system from 64% to 86%."
            },
            "slug": "Figure-ground-segmentation-improves-handled-object-Ren-Gu",
            "title": {
                "fragments": [],
                "text": "Figure-ground segmentation improves handled object recognition in egocentric video"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work develops a bottom-up motion-based approach to robustly segment out foreground objects in egocentric video and shows that it greatly improves object recognition accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202046"
                        ],
                        "name": "C. Cadieu",
                        "slug": "C.-Cadieu",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Cadieu",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cadieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16079623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75547e0e9d06b52188aed0eed6b665d99d58c59b",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present a model of intermediate-level visual representation that is based on learning invariances from movies of the natural environment. The model is composed of two stages of processing: an early feature representation layer and a second layer in which invariances are explicitly represented. Invariances are learned as the result of factoring apart the temporally stable and dynamic components embedded in the early feature representation. The structure contained in these components is made explicit in the activities of second-layer units that capture invariances in both form and motion. When trained on natural movies, the first layer produces a factorization, or separation, of image content into a temporally persistent part representing local edge structure and a dynamic part representing local motion structure, consistent with known response properties in early visual cortex (area V1). This factorization linearizes statistical dependencies among the first-layer units, making them learnable by the second layer. The second-layer units are split into two populations according to the factorization in the first layer. The form-selective units receive their input from the temporally persistent part (local edge structure) and after training result in a diverse set of higher-order shape features consisting of extended contours, multiscale edges, textures, and texture boundaries. The motion-selective units receive their input from the dynamic part (local motion structure) and after training result in a representation of image translation over different spatial scales and directions, in addition to more complex deformations. These representations provide a rich description of dynamic natural images and testable hypotheses regarding intermediate-level representation in visual cortex."
            },
            "slug": "Learning-Intermediate-Level-Representations-of-Form-Cadieu-Olshausen",
            "title": {
                "fragments": [],
                "text": "Learning Intermediate-Level Representations of Form and Motion from Natural Movies"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A model of intermediate-level visual representation that is based on learning invariances from movies of the natural environment and composed of an early feature representation layer and a second layer in whichinvariances are explicitly represented is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729571"
                        ],
                        "name": "Kihyuk Sohn",
                        "slug": "Kihyuk-Sohn",
                        "structuredName": {
                            "firstName": "Kihyuk",
                            "lastName": "Sohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kihyuk Sohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "Most relevant to our work are feature learning methods based on temporal coherence and \u201cslow feature analysis\u201d [32, 10, 22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 75
                            }
                        ],
                        "text": "Feature learning work aims tolearn invariances from data [27, 28, 31, 29, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14884315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f7214bafbed6d4dfd397c35315c7275d5608f61",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains."
            },
            "slug": "Learning-Invariant-Representations-with-Local-Sohn-Lee",
            "title": {
                "fragments": [],
                "text": "Learning Invariant Representations with Local Transformations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3257286"
                        ],
                        "name": "Karel Lenc",
                        "slug": "Karel-Lenc",
                        "structuredName": {
                            "firstName": "Karel",
                            "lastName": "Lenc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karel Lenc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "The work of [17] quantifies the invariance/equivariance of various standard representations, including CNN features, in terms of their responses to specified in-plane 2D image transformations (affine warps, flips of the image)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "Without leveraging the accompanying motor signals initiated by the videographer, learning from video data doesnot escape the passive kitten\u2019s predicament."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9044418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "371400e61632592146f40b621fb3dbb6971721be",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too."
            },
            "slug": "Understanding-image-representations-by-measuring-Lenc-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Understanding image representations by measuring their equivariance and equivalence"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Three key mathematical properties of representations: equivariance, invariance, and equivalence are investigated and applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841331"
                        ],
                        "name": "A. Dosovitskiy",
                        "slug": "A.-Dosovitskiy",
                        "structuredName": {
                            "firstName": "Alexey",
                            "lastName": "Dosovitskiy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dosovitskiy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060551"
                        ],
                        "name": "Jost Tobias Springenberg",
                        "slug": "Jost-Tobias-Springenberg",
                        "structuredName": {
                            "firstName": "Jost",
                            "lastName": "Springenberg",
                            "middleNames": [
                                "Tobias"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jost Tobias Springenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137672"
                        ],
                        "name": "Martin A. Riedmiller",
                        "slug": "Martin-A.-Riedmiller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Riedmiller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Riedmiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3244218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6fd96a900d4130940b488863b71fd09ad41ccb9",
            "isKey": false,
            "numCitedBy": 722,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101)."
            },
            "slug": "Discriminative-Unsupervised-Feature-Learning-with-Dosovitskiy-Springenberg",
            "title": {
                "fragments": [],
                "text": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper presents an approach for training a convolutional neural network using only unlabeled data and trains the network to discriminate between a set of surrogate classes, finding that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37108776"
                        ],
                        "name": "Philip Lenz",
                        "slug": "Philip-Lenz",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "To this end, we cast the problem in terms of unsupervised equivariant feature learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6724907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42",
            "isKey": false,
            "numCitedBy": 7189,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti."
            },
            "slug": "Are-we-ready-for-autonomous-driving-The-KITTI-suite-Geiger-Lenz",
            "title": {
                "fragments": [],
                "text": "Are we ready for autonomous driving? The KITTI vision benchmark suite"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The autonomous driving platform is used to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection, revealing that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207168299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843959ffdccf31c6694d135fad07425924f785b1",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite."
            },
            "slug": "Extracting-and-composing-robust-features-with-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Extracting and composing robust features with denoising autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47511804"
                        ],
                        "name": "J\u00f6rn-Philipp Lies",
                        "slug": "J\u00f6rn-Philipp-Lies",
                        "structuredName": {
                            "firstName": "J\u00f6rn-Philipp",
                            "lastName": "Lies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00f6rn-Philipp Lies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065814303"
                        ],
                        "name": "Ralf M. H\u00e4fner",
                        "slug": "Ralf-M.-H\u00e4fner",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "H\u00e4fner",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ralf M. H\u00e4fner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731199"
                        ],
                        "name": "M. Bethge",
                        "slug": "M.-Bethge",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Bethge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bethge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Temporal coherence has been explored for unsupervised feature learning with CNNs [22, 37, 9, 3, 19], with applications to dimensionality reduction [10], object recognition [22, 37], and metric learning [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2733046,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "328c248e996b349627eebc30b9c3469d83f4701c",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Following earlier studies which showed that a sparse coding principle may explain the receptive field properties of complex cells in primary visual cortex, it has been concluded that the same properties may be equally derived from a slowness principle. In contrast to this claim, we here show that slowness and sparsity drive the representations towards substantially different receptive field properties. To do so, we present complete sets of basis functions learned with slow subspace analysis (SSA) in case of natural movies as well as translations, rotations, and scalings of natural images. SSA directly parallels independent subspace analysis (ISA) with the only difference that SSA maximizes slowness instead of sparsity. We find a large discrepancy between the filter shapes learned with SSA and ISA. We argue that SSA can be understood as a generalization of the Fourier transform where the power spectrum corresponds to the maximally slow subspace energies in SSA. Finally, we investigate the trade-off between slowness and sparseness when combined in one objective function."
            },
            "slug": "Slowness-and-Sparseness-Have-Diverging-Effects-on-Lies-H\u00e4fner",
            "title": {
                "fragments": [],
                "text": "Slowness and Sparseness Have Diverging Effects on Complex Cell Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that slowness and sparsity drive the representations towards substantially different receptive field properties, and it is argued that SSA can be understood as a generalization of the Fourier transform where the power spectrum corresponds to the maximally slow subspace energies in SSA."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "The KITTI data itself is vastly more challenging than NORB due to its noisy ego-poses from inertial sensors, dynamic scenes with moving traffic, depth variations, occlusions, and objects that enter and exit the scene."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "We now describe our method for next-best view selection for recognition on NORB."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Next we evaluate equivariance for all methods using features optimized for the NORB recognition task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "On a single Tesla K-40 GPU machine, NORB-NORB training tasks took\u224815 minutes, KITTI-KITTI tasks took\u224830 minutes, and KITTI-SUN tasks took\u22482 hours."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Consider the first azimuthal rotation NORB query in row 2, where pixel distance, perhaps dominated by the lighting, identifies a wrong ego-motion match, whereas our approach finds a correct match, despite the changed object identity, starting azimuth, lightingetc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "4 on 3 recognition tasks: NORB-NORB, KITTI-KITTI, and KITTI-SUN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Given one view of a NORB object, the task is to tell a hypothetical robot how to move next to help recognize the object, i.e., which neighboring view would best reduce object prediction uncertainty."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Unsupervised datasets We consider two unsupervised datasets, NORB and KITTI: (1) NORB [15]: This dataset has 24,300 96\u00d796-pixel images of 25 toys captured by systematically varying camera pose."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Our EQUIV method\u2019s average\u03c1g error is 0.0304 and 0.0394 for atomic and composite ego-motions in NORB, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Given one view of a NORB object, the task is to tell a hypothetical robot how to move next to help recognize the objecti.e. which neighboring view would best reduce object prediction uncertainty."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Supervised datasets In our recognition experiments, we consider 3 supervised datasetsL: (1) NORB: We select 6 images from each of theC = 25 object training splits at random to create instance recognition training data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "Unsupervised datasets We consider two unsupervised datasets, NORB and KITTI: (1) NORB [16]: This dataset has 24,300 96\u00d796-pixel images of 25 toys captured by systematically varying camera pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "Batch sizes (for both the classification\nstack and the Siamese networks) were set to 16 (found to have\nno major difference from 4 or 64) for NORB-NORB and KITTIKITTI, and to 128 (selected from 4, 16, 64, 128) for KITTI-SUN, where we found it necessary to increase batch size so that meaningful classification loss gradients were computed in each SGD iteration, and training loss began to fall, despite the large number (397) of classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "So,Cg([z\u03b8(x), z\u03b8(gx)]) returns class likelihood probabilities for all 25 NORB classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "For NORB, we use a fully connected architecture: 20 full-ReLU\u2192 D =100 full feature units."
                    },
                    "intents": []
                }
            ],
            "corpusId": 712708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f354310098e09c1e1dc88758fca36767fd9d084d",
            "isKey": true,
            "numCitedBy": 1306,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."
            },
            "slug": "Learning-methods-for-generic-object-recognition-to-LeCun-Huang",
            "title": {
                "fragments": [],
                "text": "Learning methods for generic object recognition with invariance to pose and lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second and proved impractical, while convolutional nets yielded 16/7% error."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37108776"
                        ],
                        "name": "Philip Lenz",
                        "slug": "Philip-Lenz",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760556"
                        ],
                        "name": "C. Stiller",
                        "slug": "C.-Stiller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Stiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "(2) KITTI [6, 7]: This dataset contains videos with registered GPS/IMU sensor streams captured on a car driving around 4 types of areas (location classes): \u201ccampus\u201d, \u201ccity\u201d, \u201cresidential\u201d, \u201croad\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9455111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79b949d9b35c3f51dd20fb5c746cc81fc87147eb",
            "isKey": false,
            "numCitedBy": 4623,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10\u2013100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide."
            },
            "slug": "Vision-meets-robotics:-The-KITTI-dataset-Geiger-Lenz",
            "title": {
                "fragments": [],
                "text": "Vision meets robotics: The KITTI dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research, using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras and a high-precision GPS/IMU inertial navigation system."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736245"
                        ],
                        "name": "Laurenz Wiskott",
                        "slug": "Laurenz-Wiskott",
                        "structuredName": {
                            "firstName": "Laurenz",
                            "lastName": "Wiskott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurenz Wiskott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 111
                            }
                        ],
                        "text": "Most relevant to our work are feature learning methods based on temporal coherence and \u201cslow feature analysis\u201d [32, 10, 22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12366835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5127759530ce213f488af2859190697770f557f3",
            "isKey": false,
            "numCitedBy": 1188,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Invariant features of temporally varying signals are useful for analysis and classification. Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal. It is based on a nonlinear expansion of the input signal and application of principal component analysis to this expanded signal and its time derivative. It is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decor-related features, which are ordered by their degree of invariance. SFA can be applied hierarchically to process high-dimensional input signals and extract complex features. SFA is applied first to complex cell tuning properties based on simple cell output, including disparity and motion. Then more complicated input-output functions are learned by repeated application of SFA. Finally, a hierarchical network of SFA modules is presented as a simple model of the visual system. The same unstructured network can learn translation, size, rotation, contrast, or, to a lesser degree, illumination invariance for one-dimensional objects, depending on only the training stimulus. Surprisingly, only a few training objects suffice to achieve good generalization to new objects. The generated representation is suitable for object recognition. Performance degrades if the network is trained to learn multiple invariances simultaneously."
            },
            "slug": "Slow-Feature-Analysis:-Unsupervised-Learning-of-Wiskott-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Slow Feature Analysis: Unsupervised Learning of Invariances"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal that is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decor-related features, which are ordered by their degree of invariance."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865091"
                        ],
                        "name": "Krista A. Ehinger",
                        "slug": "Krista-A.-Ehinger",
                        "structuredName": {
                            "firstName": "Krista",
                            "lastName": "Ehinger",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krista A. Ehinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1309931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "908091b4a8757c3b2f7d9cfa2c4f616ee12c5157",
            "isKey": false,
            "numCitedBy": 2354,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes."
            },
            "slug": "SUN-database:-Large-scale-scene-recognition-from-to-Xiao-Hays",
            "title": {
                "fragments": [],
                "text": "SUN database: Large-scale scene recognition from abbey to zoo"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images and uses 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152247501"
                        ],
                        "name": "Zhirong Wu",
                        "slug": "Zhirong-Wu",
                        "structuredName": {
                            "firstName": "Zhirong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhirong Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3340170"
                        ],
                        "name": "S. Song",
                        "slug": "S.-Song",
                        "structuredName": {
                            "firstName": "Shuran",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31088232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2d9f92bf374c08409fe50213f0e80f1f1fcc243",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "3D shape is a crucial but heavily underutilized cue in object recognition, mostly due to the lack of a good generic shape representation. With the recent boost of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is even more urgent to have a useful 3D shape model in an object recognition pipeline. Furthermore, when the recognition has low confidence, it is important to have a fail-safe mode for object recognition systems to intelligently choose the best view to obtain extra observation from another viewpoint, in order to reduce the uncertainty as much as possible. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model naturally supports object recognition from 2.5D depth map and also view planning for object recognition. We construct a large-scale 3D computer graphics dataset to train our model, and conduct extensive experiments to study this new representation."
            },
            "slug": "3D-ShapeNets-for-2.5D-Object-Recognition-and-Wu-Song",
            "title": {
                "fragments": [],
                "text": "3D ShapeNets for 2.5D Object Recognition and Next-Best-View Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This work proposes to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network, and naturally supports object recognition from 2.5D depth map and also view planning for object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13757,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6245351"
                        ],
                        "name": "J. Kivinen",
                        "slug": "J.-Kivinen",
                        "structuredName": {
                            "firstName": "Jyri",
                            "lastName": "Kivinen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "o studied in [17], ensures that the structure of the mapping has a simple form, and is convenient for learning since M g can be encoded as a fully connected layer in a neural network. While priorwork [14, 26] focuses on equivariancewhere gis a 2Dimagewarp, we explorethecase whereg \u2208 P is an ego-motionpattern(cf. Sec3.1)re\ufb02ectingtheobserver\u2019s3D movementin the world. Intheory,appearancechangesofan image in "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "informed by explicit motor sensory cues. Furthermore, while equivariance is explored in some recent work, unlike our idea, it typically focuses on 2D image transformations as opposed to 3D ego-motion [14, 26] and considers existing features [30, 17]. Finally, whereas existing methods that learn from image transformations focus on view synthesis applications [12, 15, 21], we explore recognition application"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "or example, equivariant or \u201cco-variant\u201d operators are designed to detect repeatable interest points [30]. Recent work explores ways to learn descriptors with in-plane translation/rotationequivariance [14, 26]. While the latter does perform feature learning, its equivariance properties are crafted for speci\ufb01c 2D image transformations. In contrast, we target more complex equivariances arising from natural o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13996839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6a9f4569368969cca84988a7d823b7bb94b4f35",
            "isKey": true,
            "numCitedBy": 99,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a novel modeling framework for Boltzmann machines, augmenting each hidden unit with a latent transformation assignment variable which describes the selection of the transformed view of the canonical connection weights associated with the unit. This enables the inferences of the model to transform in response to transformed input data in a stable and predictable way, and avoids learning multiple features differing only with respect to the set of transformations. Extending prior work on translation equivariant (convolutional) models, we develop translation and rotation equivariant restricted Boltzmann machines (RBMs) and deep belief nets (DBNs), and demonstrate their effectiveness in learning frequently occurring statistical structure from artificial and natural images."
            },
            "slug": "Transformation-Equivariant-Boltzmann-Machines-Kivinen-Williams",
            "title": {
                "fragments": [],
                "text": "Transformation Equivariant Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A novel modeling framework for Boltzmann machines is developed, augmenting each hidden unit with a latent transformation assignment variable which describes the selection of the transformed view of the canonical connection weights associated with the unit."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5575601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "isKey": false,
            "numCitedBy": 12434,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact)."
            },
            "slug": "Understanding-the-difficulty-of-training-deep-Glorot-Bengio",
            "title": {
                "fragments": [],
                "text": "Understanding the difficulty of training deep feedforward neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748421"
                        ],
                        "name": "Vincent Michalski",
                        "slug": "Vincent-Michalski",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Michalski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Michalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710604"
                        ],
                        "name": "R. Memisevic",
                        "slug": "R.-Memisevic",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Memisevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Memisevic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35254095"
                        ],
                        "name": "K. Konda",
                        "slug": "K.-Konda",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Konda",
                            "middleNames": [
                                "Reddy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Konda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Ever since statistical learning methods emerged as the dominant paradigm in the recognition literature, the norm has been to treat images as i.i.d. draws from an underlying distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Recurrent neural networks combined with a grammar model of scene dynamics can also predict future frames in video [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6528211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "544b80ef13bb0d4288c6a1f50c504c03d3a14d37",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the \"syntax\" of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks."
            },
            "slug": "Modeling-Deep-Temporal-Dependencies-with-Recurrent-Michalski-Memisevic",
            "title": {
                "fragments": [],
                "text": "Modeling Deep Temporal Dependencies with Recurrent \"Grammar Cells\""
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work shows how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315504"
                        ],
                        "name": "R. Hadsell",
                        "slug": "R.-Hadsell",
                        "structuredName": {
                            "firstName": "Raia",
                            "lastName": "Hadsell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hadsell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "90 DRLIM [9] 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2022 EQUIV+DRLIM: Our approach augmented with temporal coherence regularization ([9])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ods based on temporal coherence and \u201cslow feature analysis\u201d [31, 9, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Pairs within one step (elevation and/or azimuth) are treated as \u201ctemporal neighbors\u201d, as in the turntable results of [9, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The loss Le of Eq (5) is optimized by sharing the weight parameters \u03b8 among two identical stacks of layers in a \u201cSiamese\u201d network [2, 9, 21], as shown in the top two rows of Fig 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2022 DRLIM: The approach of [9], which also regularizes the classification loss with Eq (7), but setting d(."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Temporal coherence has been explored for unsupervised feature learning with CNNs [21, 36, 8, 3, 18], with applications to dimensionality reduction [9], object recognition [21, 36], and metric learning [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ") is a \u201ccontrastive loss\u201d [9] specific to motion pattern g:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8281592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46f30e94dd3d5902141c5fbe58d0bc9189545c76",
            "isKey": true,
            "numCitedBy": 2966,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar\" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE."
            },
            "slug": "Dimensionality-Reduction-by-Learning-an-Invariant-Hadsell-Chopra",
            "title": {
                "fragments": [],
                "text": "Dimensionality Reduction by Learning an Invariant Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work presents a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38767254"
                        ],
                        "name": "David Steinkraus",
                        "slug": "David-Steinkraus",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steinkraus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steinkraus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4659176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5562a56da3a96dae82add7de705e2bd841eb00fc",
            "isKey": false,
            "numCitedBy": 2433,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages."
            },
            "slug": "Best-practices-for-convolutional-neural-networks-to-Simard-Steinkraus",
            "title": {
                "fragments": [],
                "text": "Best practices for convolutional neural networks applied to visual document analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A set of concrete bestpractices that document analysis researchers can use to get good results with neural networks, including a simple \"do-it-yourself\" implementation of convolution with a flexible architecture suitable for many visual document problems."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887277"
                        ],
                        "name": "R. Held",
                        "slug": "R.-Held",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Held",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Held"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47209111"
                        ],
                        "name": "A. Hein",
                        "slug": "A.-Hein",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2439192,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "8adc062b73f0e9c26f9b5ae706528b690a15780d",
            "isKey": false,
            "numCitedBy": 1039,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Full and exact adaptation to sensory rearrangement in adult human Ss requires movement-produced sensory feedback. Riesen's work suggested that this factor also operates in the development of higher mammals but he proposed that sensory-sensory associations are the prerequisite. To test these alternatives, visual stimulation o f the active member (A) o f each of 10 pairs of neonatal kittens was allowed to vary with its locomotor movements while equivalent stimulation o f the second member (P) resulted from passive motion. Subsequent tests of visually guided paw placement, discrimination on a visual cliff, and the blink response were normal for A but failing in P. When other alternative explanations are excluded, this result extends the conclusions of studies of adult rearrangement to neonatal development. Hebb's writing (1949) has stirred interest"
            },
            "slug": "MOVEMENT-PRODUCED-STIMULATION-IN-THE-DEVELOPMENT-OF-Held-Hein",
            "title": {
                "fragments": [],
                "text": "MOVEMENT-PRODUCED STIMULATION IN THE DEVELOPMENT OF VISUALLY GUIDED BEHAVIOR."
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Full and exact adaptation to sensory rearrangement in adult human Ss requires movement-produced sensory feedback and this result extends the conclusions of studies of adult rearrangements to neonatal development."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of comparative and physiological psychology"
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16067356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "153f64ab7c1c24b1b136d8da2f36c6333b8dbfdd",
            "isKey": false,
            "numCitedBy": 385,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In pattern recognition, statistical modeling, or regression, the amount of data is the most critical factor affecting the performance. If the amount of data and computational resources are near infinite, many algorithmes will probably converge to the optimal solution. When this is not the case, one has to introduce regularizers and a-priori knowledge to supplement the available data in order to boost the performance. Invariance (or known dependance) with respect to transformation of the input is a frequent occurrence of such an a-priori knowledge. In this chapter, we introduce the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, \u201cTangent distance\u201d and \u2018Tangent propagation\u201d, which make use of these invariances to improve performance."
            },
            "slug": "Transformation-Invariance-in-Pattern-Distance-and-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This chapter introduces the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, \u201cTangent distance\u201d and \u2018Tangent propagation\u201d, which make use of theseinvariances to improve performance."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803520"
                        ],
                        "name": "L. V. D. Maaten",
                        "slug": "L.-V.-D.-Maaten",
                        "structuredName": {
                            "firstName": "Laurens",
                            "lastName": "Maaten",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. D. Maaten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5855042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c46943103bd7b7a2c7be86859995a4144d1938b",
            "isKey": false,
            "numCitedBy": 22361,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets."
            },
            "slug": "Visualizing-Data-using-t-SNE-Maaten-Hinton",
            "title": {
                "fragments": [],
                "text": "Visualizing Data using t-SNE"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new technique called t-SNE that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map, a variation of Stochastic Neighbor Embedding that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116470384"
                        ],
                        "name": "Takayuki Nakamura",
                        "slug": "Takayuki-Nakamura",
                        "structuredName": {
                            "firstName": "Takayuki",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayuki Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144657032"
                        ],
                        "name": "M. Asada",
                        "slug": "M.-Asada",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Asada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Asada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14599770,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ec1e01237d1067ba893ee0e9d8606ed954e3aa06",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "\u8996\u899a\u3092\u6301\u3064\u79fb\u52d5\u30ed\u30dc\u30c3\u30c8\u304c,\u8907\u96d1\u306a\u74b0\u5883\u5185\u3092\u81ea\u5f8b\u7684\u306b\u8d70 \u884c\u3059\u308b\u305f\u3081\u306b\u306f,\u74b0\u5883\u306b\u9069\u5fdc\u3057\u3066\u884c\u52d5\u3092\u6c7a\u5b9a\u3059\u308b\u3053\u3068\u304c\u91cd\u8981 \u3067\u3042\u308b.\u305d\u306e\u305f\u3081\u306b\u306f,\u30ed\u30dc\u30c3\u30c8\u81ea\u8eab\u304c\u74b0\u5883\u306b\u95a2\u3059\u308b\u4f55\u3089\u304b \u306e\u30e2\u30c7\u30eb\u3092\u6301\u3063\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308b. \u5f93\u6765\u304b\u3089\u306e\u7814\u7a76\u306b\u304a\u3044\u3066\u306f,\u30ed\u30dc\u30c3\u30c8\u4e0a\u306e\u8996\u899a\u30bb\u30f3\u30b5\u304b \u3089\u306e2\u6b21\u5143\u753b\u50cf\u60c5\u5831\u3092\u51e6\u7406\u3057\u3066\u8a73\u7d30\u306a3\u6b21\u5143\u5e7e\u4f55\u60c5\u5831\u3092\u518d\u69cb \u6210\u3057,\u3053\u306e3\u6b21\u5143\u60c5\u5831\u3092\u7528\u3044\u3066\u30ed\u30dc\u30c3\u30c8\u306e\u53d6\u308a\u5dfb\u304f\u74b0\u5883\u3092\u8868 \u73fe\u3057\u3066\u3044\u308b\u7814\u7a76\u4f8b\u304c\u591a\u3044.\u3053\u306e\u69d8\u306a\u624b\u6cd5\u306f,\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf \u30d3\u30b8\u30e7\u30f3\u306e\u5206\u91ce\u306b\u304a\u3044\u3066,Ullman1)\u306b\u3088\u3063\u3066\u201dshape from motion\u201d\u306e\u554f\u984c\u304c\u5b9a\u5f0f\u5316\u3055\u308c\u3066\u4ee5\u6765,\u591a\u304f\u306e\u7814\u7a76\u304c\u306a\u3055\u308c\u3066 \u3044\u308b.\u6700\u8fd1\u306e\u7814\u7a76\u3067\u306f,2\u6b21\u5143\u753b\u50cf\u7cfb\u5217\u304b\u30892, 3),\u30a2\u30d5\u30a3\u30f3 \u307e\u305f\u306f\u6295\u5f71\u4e0d\u5909\u91cf\u304b\u30894, 5, 6, 7),\u30ab\u30e1\u30e9\u30ad\u30e3\u30ea\u30d6\u30ec\u30fc\u30b7\u30e7 \u30f3\u304b\u30898),\u305d\u308c\u305e\u308c3\u6b21\u5143\u60c5\u5831\u3092\u6c42\u3081\u308b\u7814\u7a76\u304c\u3042\u308b.\u3053\u308c\u3089 \u306e\u7814\u7a76\u3067\u306f,3\u6b21\u5143\u60c5\u5831\u3084\u305d\u306e\u4e0d\u5909\u91cf\u3092\u3067\u304d\u308b\u3060\u3051\u6b63\u78ba\u306b\u6c42 \u3081\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u308b.\u3057\u304b\u3057\u306a\u304c\u3089,\u3069\u308c\u304f\u3089\u3044\u306e\u518d \u69cb\u6210\u306e\u7cbe\u5ea6\u304c\u5fc5\u8981\u3067\u3042\u308b\u304b\u3092\u6c7a\u3081\u308b\u3053\u3068\u306f\u96e3\u3057\u3044.3\u6b21\u5143\u60c5 \u5831\u306f\u4e00\u822c\u7684\u306a\u5f62\u5f0f\u3092\u3057\u3066\u304a\u308a\u305d\u306e\u60c5\u5831\u3092\u4f7f\u7528\u3059\u308b\u969b\u306b\u7c21\u5358\u306b \u5909\u63db\u304c\u53ef\u80fd\u3067\u3042\u308b\u3068\u3057\u3066,3\u6b21\u5143\u60c5\u5831\u3092\u3082\u3068\u306b\u3057\u305f\u74b0\u5883\u306e\u8868 \u73fe\u304c,\u30ed\u30dc\u30c3\u30c8\u306e\u30ca\u30d3\u30b2\u30fc\u30b7\u30e7\u30f3\u7b49\u306b\u6709\u52b9\u3067\u3042\u308b\u3068\u4e00\u822c\u7684\u306b \u8a00\u308f\u308c\u3066\u3044\u308b.\u3057\u304b\u3057\u306a\u304c\u3089,\u30ed\u30dc\u30c3\u30c8\u3092\u4f7f\u7528\u3059\u308b\u969b\u306b\u306f, \u30bb\u30f3\u30b5\u60c5\u5831\u3084\u30e2\u30fc\u30bf\u30fc\u5236\u5fa1\u306b\u5bfe\u3057\u3066\u5b9f\u6642\u9593\u51e6\u7406\u3057\u306a\u3051\u308c\u3070 \u306a\u3089\u306a\u3044.\u3055\u3089\u306b,\u76ee\u7684\u3068\u3059\u308b\u30bf\u30b9\u30af\u306b\u4f9d\u5b58\u3057\u3066\u30ed\u30dc\u30c3\u30c8\u3092 \u53d6\u308a\u5dfb\u304f\u74b0\u5883\u5168\u4f53\u3092\u8868\u73fe\u3059\u308b\u5fc5\u8981\u306f\u306a\u304f,\u307e\u305f\u898b\u3048\u65b9\u306e\u7570\u306a \u308b\u74b0\u5883\u3092\u5fc5\u305a\u3057\u3082\u533a\u5225\u3059\u308b\u5fc5\u8981\u3082\u306a\u3044.\u4f8b\u3048\u3070, \u305f\u3068\u3048\u3070, \u969c\u5bb3\u7269\u767a\u898b\u3084\u56de\u907f\u3092\u30bf\u30b9\u30af\u3068\u3057\u305f\u5834\u5408,\u673a\u3084\u6905\u5b50\u304c\u4e71\u96d1\u306b\u914d \u7f6e\u3055\u308c\u305f\u5c4b\u5185\u74b0\u5883\u3082,\u5ca9\u306a\u3069\u3092\u542b\u3080\u5c4b\u5916\u74b0\u5883\u3082\u8b58\u5225\u3059\u308b\u5fc5\u8981 \u304c\u306a\u3044.\u3053\u308c\u307e\u3067\u306e3\u6b21\u5143\u5e7e\u4f55\u60c5\u5831\u306e\u518d\u69cb\u6210\u3092\u4e3b\u773c\u3068\u3057\u3066\u6765 \u305f\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f,\u3053\u308c\u3089\u3092\u540c\u4e00\u8996\u3059\u308b\u3053\u3068\u306f\u56f0\u96e3\u3067\u3042\u308b\u3068 \u8003\u3048\u3089\u308c\u308b. \u305d\u3053\u3067,\u8996\u899a\u60c5\u5831\u3092\u7528\u3044\u3066\u4e0e\u3048\u3089\u308c\u305f\u30bf\u30b9\u30af\u3092\u9054\u6210\u3059\u308b \u81ea\u5f8b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306b\u3068\u3063\u3066,\u3069\u306e\u3088\u3046\u306a\u74b0\u5883\u8868\u73fe\u304c\u9069\u5f53\u3067\u3042 \u308b\u304b\u306b\u3064\u3044\u3066\u8003\u3048\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044.\u3053\u306e\u554f\u984c\u306b\u5bfe\u3057\u3066,\u30ed \u30dc\u30c3\u30c8\u5b66\u7fd2\u306e\u7814\u7a76\u8005\u306f,\u30ed\u30dc\u30c3\u30c8\u306b\u5bfe\u3057\u3066,\u5916\u754c\u304b\u3089\u77e5\u899a\u3055 \u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u884c\u52d5\u3059\u308b\u3053\u3068,\u3059\u306a\u308f\u3061,\u74b0\u5883\u72b6\u614b\u3068\u30ed \u30dc\u30c3\u30c8\u81ea\u8eab\u306e\u884c\u52d5\u306e\u6700\u9069\u306a\u95a2\u4fc2\u3092\u5b66\u7fd2\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u304d\u305f 9).\u3053\u306e\u72b6\u614b\u3068\u884c\u52d5\u306e\u6700\u9069\u306a\u95a2\u4fc2\u304c,\u30bf\u30b9\u30af\u3082\u3057\u304f\u306f\u30ed\u30dc\u30c3 \u30c8\u306e\u884c\u52d5\u306b\u57fa\u3065\u304f\u74b0\u5883\u8868\u73fe\u3068\u8003\u3048\u3089\u308c\u308b.\u3053\u308c\u306b\u3088\u3063\u3066,\u30bf \u30b9\u30af\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306b\u8a73\u7d30\u306a3\u6b21\u5143\u60c5\u5831\u3092\u518d\u69cb\u6210\u3059\u308b\u5fc5\u8981\u304c \u306a\u304f\u306a\u308b.\u3057\u304b\u3057\u306a\u304c\u3089,\u3053\u308c\u307e\u3067\u306e\u30ed\u30dc\u30c3\u30c8\u5b66\u7fd2\u306e\u7814\u7a76\u306b \u304a\u3044\u3066\u306f,\u7279\u5b9a\u306e\u30bf\u30b9\u30af\u306e\u884c\u52d5\u5b66\u7fd2\u3092\u884c\u306a\u3046\u305f\u3081\u306b,\u74b0\u5883\u3092 \u8a18\u8ff0\u3059\u308b\u8a18\u8ff0\u5b50\u306b\u5bfe\u3059\u308b\u5019\u88dc\u306f,\u77e5\u899a\u3055\u308c\u305f\u83ab\u5927\u306a\u30c7\u30fc\u30bf\u306e \u4e2d\u304b\u3089\u524d\u3082\u3063\u3066\u9078\u629e\u3055\u308c\u3066\u3044\u308b.\u307e\u305f\u3053\u308c\u3089\u306e\u8a18\u8ff0\u5b50\u306e\u5019\u88dc \u306f,\u74b0\u5883\u30b7\u30fc\u30f3\u306e\u69cb\u6210\u8981\u7d20\u3084\u7279\u5b9a\u306e\u72b6\u6cc1\u3084\u30bf\u30b9\u30af\u306b\u4f9d\u5b58\u3057\u3066 \u6c7a\u5b9a\u3055\u308c\u3066\u3044\u308b. \u305d\u3053\u3067,\u3053\u306e\u5831\u544a\u3067\u306f,\u74b0\u5883\u30b7\u30fc\u30f3\u306e\u69cb\u6210\u8981\u7d20\u306b\u72ec\u7acb\u3067, \u30e2\u30fc\u30bf\u30b3\u30de\u30f3\u30c9\u3068\u5bc6\u63a5\u306b\u95a2\u9023\u3057\u305f\u30ed\u30d0\u30b9\u30c8\u306a\u8a18\u8ff0\u5b50\u3067\u3042\u308b\u753b \u50cf\u904b\u52d5\u60c5\u5831\u3092\u5229\u7528\u3057\u3066,\u5b9f\u30ed\u30dc\u30c3\u30c8\u306b,\u76ee\u7684\u306e\u30bf\u30b9\u30af\u3092\u9054\u6210 \u3055\u305b\u308b\u305f\u3081\u306e\u884c\u52d5\u306b\u57fa\u3065\u304f\u74b0\u5883\u8868\u73fe\u3092\u7372\u5f97\u3055\u305b\u308b\u624b\u6cd5\u306b\u3064\u3044 \u3066\u8ff0\u3079\u308b.\u3053\u3053\u3067\u306f,\u76ee\u7684\u306e\u30bf\u30b9\u30af\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306e\u884c\u52d5\u306b \u57fa\u3065\u304f\u74b0\u5883\u8868\u73fe\u3092\u904b\u52d5\u30b9\u30b1\u30c3\u30c1\u3068\u547c\u3093\u3067\u3044\u308b.\u30ed\u30dc\u30c3\u30c8\u306e\u30bf \u30b9\u30af\u3068\u3057\u3066\u306f,\u52d5\u7684\u74b0\u5883\u5185\u3067\u969c\u5bb3\u7269\u3092\u56de\u907f\u3057\u306a\u304c\u3089\u5bfe\u8c61\u7269\u4f53 \u3092\u8ffd\u8de1\u3059\u308b\u3068\u3044\u3046\u30bf\u30b9\u30af\u3092\u60f3\u5b9a\u3057\u3066\u3044\u308b."
            },
            "slug": "Motion-Sketch:-Acquisition-of-Visual-Motion-Guided-Nakamura-Asada",
            "title": {
                "fragments": [],
                "text": "Motion Sketch: Acquisition of Visual Motion Guided Behaviors"
            },
            "tldr": {
                "abstractSimilarityScore": 0,
                "text": "It is confirmed that 3.3% of the population believes in reincarnation, but not all believe in it in the same way."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120242409"
                        ],
                        "name": "J. Bromley",
                        "slug": "J.-Bromley",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Bromley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bromley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058232056"
                        ],
                        "name": "James W. Bentz",
                        "slug": "James-W.-Bentz",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bentz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Bentz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49015421"
                        ],
                        "name": "C. Moore",
                        "slug": "C.-Moore",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776424"
                        ],
                        "name": "Eduard S\u00e4ckinger",
                        "slug": "Eduard-S\u00e4ckinger",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "S\u00e4ckinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduard S\u00e4ckinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105573840"
                        ],
                        "name": "Roopak Shah",
                        "slug": "Roopak-Shah",
                        "structuredName": {
                            "firstName": "Roopak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roopak Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 130
                            }
                        ],
                        "text": "The loss Le of Eq (5) is optimized by sharing the weight parameters \u03b8 among two identical stacks of layers in a \u201cSiamese\u201d network [2, 9, 21], as shown in the top two rows of Fig 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16394033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "997dc5d9a058753f034422afe7bd0cc0b8ad808b",
            "isKey": false,
            "numCitedBy": 2617,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a \"Siamese\" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries."
            },
            "slug": "Signature-Verification-Using-A-\"Siamese\"-Time-Delay-Bromley-Bentz",
            "title": {
                "fragments": [],
                "text": "Signature Verification Using A \"Siamese\" Time Delay Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An algorithm for verification of signatures written on a pen-input tablet based on a novel, artificial neural network called a \"Siamese\" neural network, which consists of two identical sub-networks joined at their outputs."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67162290"
                        ],
                        "name": "W. Marsden",
                        "slug": "W.-Marsden",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Marsden",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Marsden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124633341,
            "fieldsOfStudy": [],
            "id": "3d2218b17e7898a222e5fc2079a3f1531990708f",
            "isKey": false,
            "numCitedBy": 151431,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "I-and-J-Marsden",
            "title": {
                "fragments": [],
                "text": "I and J"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84120004"
                        ],
                        "name": "\uc131\uae30\uc775",
                        "slug": "\uc131\uae30\uc775",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uc131\uae30\uc775",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uc131\uae30\uc775"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52064104"
                        ],
                        "name": "\uc774\uc601\ud0c1",
                        "slug": "\uc774\uc601\ud0c1",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uc774\uc601\ud0c1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uc774\uc601\ud0c1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66797895"
                        ],
                        "name": "\ubc15\uacc4\ud604",
                        "slug": "\ubc15\uacc4\ud604",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\ubc15\uacc4\ud604",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\ubc15\uacc4\ud604"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66930011"
                        ],
                        "name": "\uc804\ud0dc\uad6d",
                        "slug": "\uc804\ud0dc\uad6d",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uc804\ud0dc\uad6d",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uc804\ud0dc\uad6d"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65818414"
                        ],
                        "name": "\ubc15\ud45c\uc6d0",
                        "slug": "\ubc15\ud45c\uc6d0",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\ubc15\ud45c\uc6d0",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\ubc15\ud45c\uc6d0"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66373723"
                        ],
                        "name": "\ud55c\uc77c\uc6a9",
                        "slug": "\ud55c\uc77c\uc6a9",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\ud55c\uc77c\uc6a9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\ud55c\uc77c\uc6a9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79743017"
                        ],
                        "name": "\uc7a5\uc724\ud76c",
                        "slug": "\uc7a5\uc724\ud76c",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uc7a5\uc724\ud76c",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uc7a5\uc724\ud76c"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 73741649,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "07b056a23b225fa4fc54cda80a8e6c2c74760541",
            "isKey": false,
            "numCitedBy": 46282,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Y.-\uc131\uae30\uc775-\uc774\uc601\ud0c1",
            "title": {
                "fragments": [],
                "text": "Y."
            },
            "venue": {
                "fragments": [],
                "text": "Industrial and Labor Relations Terms"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Methods to learn representations with disentangled latent factors [12, 15] aim to sort properties like pose, illuminationetc. into distinct portions of the feature space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Without leveraging the accompanying motor signals initiated by the videographer, learning from video data doesnot escape the passive kitten\u2019s predicament."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep convolutional inverse graphics network. arXiv"
            },
            "venue": {
                "fragments": [],
                "text": "Deep convolutional inverse graphics network. arXiv"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Methods to learn representations with disentangled latent factors [12, 15] aim to sort properties like pose, illuminationetc. into distinct portions of the feature space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Without leveraging the accompanying motor signals initiated by the videographer, learning from video data doesnot escape the passive kitten\u2019s predicament."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transforming Auto-Encoders. ICANN"
            },
            "venue": {
                "fragments": [],
                "text": "Transforming Auto-Encoders. ICANN"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, equivariant or \u201cco-variant\u201d operators are designed to detect repeatable interest points [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Furthermore, while equivariance is explored in some recent work, unlike our idea, it typically focuses on 2D image transformations as opposed to 3D ego-motion [13, 25] and considers existing features [29, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local Invariant Feature Detectors: A Survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local invariant feat ure detectors: a survey.Foundations and trends in computer graphics and vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Recent work explores ways to learn descriptors with in-plane translation/rotation equivariance [14, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Only partially."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local invariant feature detectors: a survey. Foundations and trends in computer graphics and vision"
            },
            "venue": {
                "fragments": [],
                "text": "Local invariant feature detectors: a survey. Foundations and trends in computer graphics and vision"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vision meets robotics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 17
                            }
                        ],
                        "text": "While prior work [14, 26] focuses on equivariance where g is a 2D image warp, we explore the case where g \u2208 P is an ego-motion pattern (cf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 96
                            }
                        ],
                        "text": "Recent work explores ways to learn descriptors with in-plane translation/rotation equ ivariance [14, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transformation equiva r ant boltzmann machines"
            },
            "venue": {
                "fragments": [],
                "text": " ICANN"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to See by Moving , ICCV 2015 3 . Laurens van derMaaten and Geoffrey Hinton , Visualizing Data using t - SNE 4"
            },
            "venue": {
                "fragments": [],
                "text": "Learning Image Representations Tied to Ego - motion , ICCV"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local invariant feature detectors : a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Foundations and trends in computer graphics and vision"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "https://code.google.com/p/cuda-convnet"
            },
            "venue": {
                "fragments": [],
                "text": "https://code.google.com/p/cuda-convnet"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning intermediatelevel representations of form and motion from natural movies.Neural computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Image Representations Tied to Ego-motion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cuda-convnet. https://code.google.com/p/ cuda-convnet"
            },
            "venue": {
                "fragments": [],
                "text": "Cuda-convnet. https://code.google.com/p/ cuda-convnet"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 52,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Image-Representations-Tied-to-Ego-Motion-Jayaraman-Grauman/c426ba865e9158a0f7962a86a50575aa943051b1?sort=total-citations"
}