{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944655894"
                        ],
                        "name": "Jacob Walker",
                        "slug": "Jacob-Walker",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Walker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Prediction is an inherently ambiguous problem."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1303771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0bb8f933e514dd9441e3082a34a9f129e35500",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a conceptually simple but surprisingly powerful method for visual prediction which combines the effectiveness of mid-level visual elements with temporal modeling. Our framework can be learned in a completely unsupervised manner from a large collection of videos. However, more importantly, because our approach models the prediction framework on these mid-level elements, we can not only predict the possible motion in the scene but also predict visual appearances - how are appearances going to change with time. This yields a visual \"hallucination\" of probable events on top of the scene. We show that our method is able to accurately predict and visualize simple future events, we also show that our approach is comparable to supervised methods for event prediction."
            },
            "slug": "Patch-to-the-Future:-Unsupervised-Visual-Prediction-Walker-Gupta",
            "title": {
                "fragments": [],
                "text": "Patch to the Future: Unsupervised Visual Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents a conceptually simple but surprisingly powerful method for visual prediction which combines the effectiveness of mid-level visual elements with temporal modeling and shows that it is comparable to supervised methods for event prediction."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038685"
                        ],
                        "name": "S. Divvala",
                        "slug": "S.-Divvala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Divvala",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Divvala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "To capture temporal information, we learn a model of the absolute motion of the objects, e.g., a child is more likely to move in the direction they are facing (Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6495092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07ac2e342db42589322b28ef291c2702f4a793a8",
            "isKey": false,
            "numCitedBy": 463,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an empirical evaluation of the role of context in a contemporary, challenging object detection task - the PASCAL VOC 2008. Previous experiments with context have mostly been done on home-grown datasets, often with non-standard baselines, making it difficult to isolate the contribution of contextual information. In this work, we present our analysis on a standard dataset, using top-performing local appearance detectors as baseline. We evaluate several different sources of context and ways to utilize it. While we employ many contextual cues that have been used before, we also propose a few novel ones including the use of geographic context and a new approach for using object spatial support."
            },
            "slug": "An-empirical-study-of-context-in-object-detection-Divvala-Hoiem",
            "title": {
                "fragments": [],
                "text": "An empirical study of context in object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper presents an empirical evaluation of the role of context in a contemporary, challenging object detection task - the PASCAL VOC 2008, using top-performing local appearance detectors as baseline and evaluates several different sources of context and ways to utilize it."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "To capture temporal information, we learn a model of the absolute motion of the objects, e.g., a child is more likely to move in the direction they are facing (Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6152006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4081e007d7eced95cc618164e976a80d44ff5f4e",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
            },
            "slug": "Putting-Objects-in-Perspective-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Putting Objects in Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper provides a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint by allowing probabilistic object hypotheses to refine geometry and vice-versa."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37991449"
                        ],
                        "name": "Kris M. Kitani",
                        "slug": "Kris-M.-Kitani",
                        "structuredName": {
                            "firstName": "Kris",
                            "lastName": "Kitani",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kris M. Kitani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753269"
                        ],
                        "name": "Brian D. Ziebart",
                        "slug": "Brian-D.-Ziebart",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ziebart",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian D. Ziebart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d8a5addbd17d2c7c8043d8877234675da19938a",
            "isKey": false,
            "numCitedBy": 653,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the task of inferring the future actions of people from noisy visual input. We denote this task activity forecasting. To achieve accurate activity forecasting, our approach models the effect of the physical environment on the choice of human actions. This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from optimal control theory. Our unified model also integrates several other key elements of activity analysis, namely, destination forecasting, sequence smoothing and transfer learning. As proof-of-concept, we focus on the domain of trajectory-based activity analysis from visual input. Experimental results demonstrate that our model accurately predicts distributions over future actions of individuals. We show how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories."
            },
            "slug": "Activity-Forecasting-Kitani-Ziebart",
            "title": {
                "fragments": [],
                "text": "Activity Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The unified model uses state-of-the-art semantic scene understanding combined with ideas from optimal control theory to achieve accurate activity forecasting and shows how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2(e), it is more likely for the boy and the baseball cap to move in the same direction than not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "We show that our model outperforms a number of alternate baseline approaches to modeling scene dynamics on both abstract and natural images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10554419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "051830b0ea58d1568f19ec3297e301d9789c9a76",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity."
            },
            "slug": "Bringing-Semantics-into-Focus-Using-Visual-Zitnick-Parikh",
            "title": {
                "fragments": [],
                "text": "Bringing Semantics into Focus Using Visual Abstraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper creates 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions and thoroughly analyzes this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143738177"
                        ],
                        "name": "Jenny Yuen",
                        "slug": "Jenny-Yuen",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Yuen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenny Yuen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "We model four aspects of scene composition and transition: the position and motion of each object in either absolute terms or relative to other objects (Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9066351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed01c2706c1dd05de8664bee1e42a628a49480ad",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When given a single static picture, humans can not only interpret the instantaneous content captured by the image, but also they are able to infer the chain of dynamic events that are likely to happen in the near future. Similarly, when a human observes a short video, it is easy to decide if the event taking place in the video is normal or unexpected, even if the video depicts a an unfamiliar place for the viewer. This is in contrast with work in surveillance and outlier event detection, where the models rely on thousands of hours of video recorded at a single place in order to identify what constitutes an unusual event. In this work we present a simple method to identify videos with unusual events in a large collection of short video clips. The algorithm is inspired by recent approaches in computer vision that rely on large databases. In this work we show how, relying on large collections of videos, we can retrieve other videos similar to the query to build a simple model of the distribution of expected motions for the query. Consequently, the model can evaluate how unusual is the video as well as make event predictions. We show how a very simple retrieval model is able to provide reliable results."
            },
            "slug": "A-Data-Driven-Approach-for-Event-Prediction-Yuen-Torralba",
            "title": {
                "fragments": [],
                "text": "A Data-Driven Approach for Event Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a simple method to identify videos with unusual events in a large collection of short video clips, inspired by recent approaches in computer vision that rely on large databases and shows how a very simple retrieval model is able to provide reliable results."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949240"
                        ],
                        "name": "Scott Satkin",
                        "slug": "Scott-Satkin",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Satkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Satkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Analogously in the affordance literature [8], common sense takes the form of geometric knowledge of where humans can perform various actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6106986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e700356dae7a0a54c91567801c8c5f09bdd8c05",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a human-centric paradigm for scene understanding. Our approach goes beyond estimating 3D scene geometry and predicts the \"workspace\" of a human which is represented by a data-driven vocabulary of human interactions. Our method builds upon the recent work in indoor scene understanding and the availability of motion capture data to create a joint space of human poses and scene geometry by modeling the physical interactions between the two. This joint space can then be used to predict potential human poses and joint locations from a single image. In a way, this work revisits the principle of Gibsonian affor-dances, reinterpreting it for the modern, data-driven era."
            },
            "slug": "From-3D-scene-geometry-to-human-workspace-Gupta-Satkin",
            "title": {
                "fragments": [],
                "text": "From 3D scene geometry to human workspace"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work builds upon the recent work in indoor scene understanding and the availability of motion capture data to create a joint space of human poses and scene geometry by modeling the physical interactions between the two."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909300"
                        ],
                        "name": "Lucy Vanderwende",
                        "slug": "Lucy-Vanderwende",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vanderwende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vanderwende"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2(e), it is more likely for the boy and the baseball cap to move in the same direction than not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "We show that our model outperforms a number of alternate baseline approaches to modeling scene dynamics on both abstract and natural images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5642345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c39f56c3c21c3972c362f8e752be57a50c41f4f",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences' visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches."
            },
            "slug": "Learning-the-Visual-Interpretation-of-Sentences-Zitnick-Parikh",
            "title": {
                "fragments": [],
                "text": "Learning the Visual Interpretation of Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper extracts predicate tuples that contain two nouns and a relation from sentences to generate novel scenes depicting the sentences' visual meaning by sampling from the Conditional Random Field (CRF)."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31458284"
                        ],
                        "name": "S. Pellegrini",
                        "slug": "S.-Pellegrini",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Pellegrini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pellegrini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433494"
                        ],
                        "name": "Andreas Ess",
                        "slug": "Andreas-Ess",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144810819"
                        ],
                        "name": "K. Schindler",
                        "slug": "K.-Schindler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schindler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7065547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d7a98a81715e2d3747071722b2deeed8937d122",
            "isKey": false,
            "numCitedBy": 1042,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Object tracking typically relies on a dynamic model to predict the object's location from its past trajectory. In crowded scenarios a strong dynamic model is particularly important, because more accurate predictions allow for smaller search regions, which greatly simplifies data association. Traditional dynamic models predict the location for each target solely based on its own history, without taking into account the remaining scene objects. Collisions are resolved only when they happen. Such an approach ignores important aspects of human behavior: people are driven by their future destination, take into account their environment, anticipate collisions, and adjust their trajectories at an early stage in order to avoid them. In this work, we introduce a model of dynamic social behavior, inspired by models developed for crowd simulation. The model is trained with videos recorded from birds-eye view at busy locations, and applied as a motion model for multi-people tracking from a vehicle-mounted camera. Experiments on real sequences show that accounting for social interactions and scene knowledge improves tracking performance, especially during occlusions."
            },
            "slug": "You'll-never-walk-alone:-Modeling-social-behavior-Pellegrini-Ess",
            "title": {
                "fragments": [],
                "text": "You'll never walk alone: Modeling social behavior for multi-target tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A model of dynamic social behavior, inspired by models developed for crowd simulation, is introduced, trained with videos recorded from birds-eye view at busy locations, and applied as a motion model for multi-people tracking from a vehicle-mounted camera."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723948"
                        ],
                        "name": "H. Koppula",
                        "slug": "H.-Koppula",
                        "structuredName": {
                            "firstName": "Hema",
                            "lastName": "Koppula",
                            "middleNames": [
                                "Swetha"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Koppula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681995"
                        ],
                        "name": "Ashutosh Saxena",
                        "slug": "Ashutosh-Saxena",
                        "structuredName": {
                            "firstName": "Ashutosh",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashutosh Saxena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Xie et al. [18] learn functional parts of scenes for predicting human paths, and Koppula et al. [11] learn to predict human actions given the presence of other objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1121245,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50a00d4fa9bf2e7bff37bc944ac48b403f5eb097",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "An important aspect of human perception is anticipation, which we use extensively in our day-to-day activities when interacting with other humans as well as with our surroundings. Anticipating which activities will a human do next (and how) can enable an assistive robot to plan ahead for reactive responses. Furthermore, anticipation can even improve the detection accuracy of past activities. The challenge, however, is two-fold: We need to capture the rich context for modeling the activities and object affordances, and we need to anticipate the distribution over a large space of future human activities. In this work, we represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles. In extensive evaluation on CAD-120 human activity RGB-D dataset, we first show that anticipation improves the state-of-the-art detection results. We then show that for new subjects (not seen in the training set), we obtain an activity anticipation accuracy (defined as whether one of top three predictions actually happened) of 84.1, 74.4 and 62.2 percent for an anticipation time of 1, 3 and 10 seconds respectively. Finally, we also show a robot using our algorithm for performing a few reactive responses."
            },
            "slug": "Anticipating-Human-Activities-Using-Object-for-Koppula-Saxena",
            "title": {
                "fragments": [],
                "text": "Anticipating Human Activities Using Object Affordances for Reactive Robotic Response"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work represents each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances and represents each ATCRF as a particle and represents the distribution over the potential futures using a set of particles."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760137"
                        ],
                        "name": "Haifeng Gong",
                        "slug": "Haifeng-Gong",
                        "structuredName": {
                            "firstName": "Haifeng",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haifeng Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1978562"
                        ],
                        "name": "Jack Sim",
                        "slug": "Jack-Sim",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Sim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jack Sim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145371551"
                        ],
                        "name": "M. Likhachev",
                        "slug": "M.-Likhachev",
                        "structuredName": {
                            "firstName": "Maxim",
                            "lastName": "Likhachev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Likhachev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "In the tracking literature, this has been done by treating humans as privileged objects and reasoning about their motion around defined obstacles [6] and other pedestrians [14, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9857913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73f84bfad5d3d6cd548ac43702802e2d244ad38d",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a long-term motion model for visual object tracking. In crowded street scenes, persistent occlusions are a frequent challenge for tracking algorithm and a robust, long-term motion model could help in these situations. Motivated by progresses in robot motion planning, we propose to construct a set of \u2018plausible\u2019 plans for each person, which are composed of multiple long-term motion prediction hypotheses that do not include redundancies, unnecessary loops or collisions with other objects. Constructing plausible plan is the key step in utilizing motion planning in object tracking, which has not been fully investigate in robot motion planning. We propose a novel method of efficiently constructing disjoint plans in different homotopy classes, based on winding numbers and winding angles of planned paths around all obstacles. As the goals can be specified by winding numbers and winding angles, we can avoid redundant plans in the same homotopy class and multiple whirls or loops around a single obstacle. We test our algorithm on a challenging, real-world dataset, and compare our algorithm with Linear Trajectory Avoidance and a simplified linear planning model. We find that our algorithm outperforms both algorithms in most sequences."
            },
            "slug": "Multi-hypothesis-motion-planning-for-visual-object-Gong-Sim",
            "title": {
                "fragments": [],
                "text": "Multi-hypothesis motion planning for visual object tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel method of efficiently constructing disjoint plans in differenthomotopy classes, based on winding numbers and winding angles of planned paths around all obstacles, which can avoid redundant plans in the same homotopy class and multiple whirls or loops around a single obstacle."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48811777"
                        ],
                        "name": "Dan Xie",
                        "slug": "Dan-Xie",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Xie et al. [18] learn functional parts of scenes for predicting human paths, and Koppula et al. [11] learn to predict human actions given the presence of other objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15150618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4a3be1de7ea7b07cf6ac98398e8b9bea6cb2dfe",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they \"attract\" people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or \"repel\" people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as \"dark matter\", emanating \"dark energy\" that affects people's trajectories in the video. To detect \"dark matter\" and infer their \"dark energy\" field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach \"dark matter\" and thus satisfy their needs, where their motions are subject to a composite \"dark energy\" field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended \"dark matter\" while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people's trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people's trajectories in unobserved parts of the video footage."
            },
            "slug": "Inferring-\"Dark-Matter\"-and-\"Dark-Energy\"-from-Xie-Todorovic",
            "title": {
                "fragments": [],
                "text": "Inferring \"Dark Matter\" and \"Dark Energy\" from Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene by extending the Lagrangian mechanics to probabilistically model people's trajectories and intents, constraint map of the scene, and locations of functional objects."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113225771"
                        ],
                        "name": "Tian Lan",
                        "slug": "Tian-Lan",
                        "structuredName": {
                            "firstName": "Tian",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tian Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "For instance, Lan et al. [12] learn social relationships from video data that are informative of human actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2053069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ce28767572a3db6d9681e08b5b5ad54d7533c03",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a hierarchical model for human activity recognition in entire multi-person scenes. Our model describes human behaviour at multiple levels of detail, ranging from low-level actions through to high-level events. We also include a model of social roles, the expected behaviours of certain people, or groups of people, in a scene. The hierarchical model includes these varied representations, and various forms of interactions between people present in a scene. The model is trained in a discriminative max-margin framework. Experimental results demonstrate that this model can improve performance at all considered levels of detail, on two challenging datasets."
            },
            "slug": "Social-roles-in-hierarchical-models-for-human-Lan-Sigal",
            "title": {
                "fragments": [],
                "text": "Social roles in hierarchical models for human activity recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A hierarchical model for human activity recognition in entire multi-person scenes, trained in a discriminative max-margin framework, that can improve performance at all considered levels of detail, on two challenging datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39717886"
                        ],
                        "name": "Xinlei Chen",
                        "slug": "Xinlei-Chen",
                        "structuredName": {
                            "firstName": "Xinlei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinlei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "This subtle yet important factor ensures that objects move coherently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12350611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53e4ab9730e983242a3409c7bf1af945041a6563",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose NEIL (Never Ending Image Learner), a computer program that runs 24 hours per day and 7 days per week to automatically extract visual knowledge from Internet data. NEIL uses a semi-supervised learning algorithm that jointly discovers common sense relationships (e.g., \"Corolla is a kind of/looks similar to Car\", \"Wheel is a part of Car\") and labels instances of the given visual categories. It is an attempt to develop the world's largest visual structured knowledge base with minimum human labeling effort. As of 10th October 2013, NEIL has been continuously running for 2.5 months on 200 core cluster (more than 350K CPU hours) and has an ontology of 1152 object categories, 1034 scene categories and 87 attributes. During this period, NEIL has discovered more than 1700 relationships and has labeled more than 400K visual instances."
            },
            "slug": "NEIL:-Extracting-Visual-Knowledge-from-Web-Data-Chen-Shrivastava",
            "title": {
                "fragments": [],
                "text": "NEIL: Extracting Visual Knowledge from Web Data"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "NEIL (Never Ending Image Learner), a computer program that runs 24 hours per day and 7 days per week to automatically extract visual knowledge from Internet data, is proposed in an attempt to develop the world's largest visual structured knowledge base with minimum human labeling effort."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794917"
                        ],
                        "name": "M. Choi",
                        "slug": "M.-Choi",
                        "structuredName": {
                            "firstName": "Myung",
                            "lastName": "Choi",
                            "middleNames": [
                                "Jin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Finally, we model the relative motion of pairs of objects in scene transitions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8354810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28aa465c3af7e5ccf1b10ae9cf76e83aab3ee34f",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Context-models-and-out-of-context-objects-Choi-Torralba",
            "title": {
                "fragments": [],
                "text": "Context models and out-of-context objects"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Finally, we model the relative motion of pairs of objects in scene transitions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13251789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e523721feebeaee18e487607b7d0920ac6cd3b4",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual classifiers for object recognition from weakly labeled data requires determining correspondence between image regions and semantic object classes. Most approaches use co-occurrence of \"nouns\" and image features over large datasets to determine the correspondence, but many correspondence ambiguities remain. We further constrain the correspondence problem by exploiting additional language constructs to improve the learning process from weakly labeled data. We consider both \"prepositions\" and \"comparative adjectives\" which are used to express relationships between objects. If the models of such relationships can be determined, they help resolve correspondence ambiguities. However, learning models of these relationships requires solving the correspondence problem. We simultaneously learn the visual features defining \"nouns\" and the differential visual features defining such \"binary-relationships\" using an EM-based approach."
            },
            "slug": "Beyond-Nouns:-Exploiting-Prepositions-and-for-Gupta-Davis",
            "title": {
                "fragments": [],
                "text": "Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning Visual Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work simultaneously learns the visual features defining \"nouns\" and the differentialVisual features defining such \"binary-relationships\" using an EM-based approach and constrain the correspondence problem by exploiting additional language constructs to improve the learning process from weakly labeled data."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35422941"
                        ],
                        "name": "Ramin Mehran",
                        "slug": "Ramin-Mehran",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Mehran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ramin Mehran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064015218"
                        ],
                        "name": "Alexis Oyama",
                        "slug": "Alexis-Oyama",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Oyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Oyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "In the tracking literature, this has been done by treating humans as privileged objects and reasoning about their motion around defined obstacles [6] and other pedestrians [14, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2430392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3113eaad326a955ba96c11b7b65d0c065fb2054",
            "isKey": false,
            "numCitedBy": 727,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a novel method to detect and localize abnormal behaviors in crowd videos using Social Force model. For this purpose, a grid of particles is placed over the image and it is advected with the space-time average of optical flow. By treating the moving particles as individuals, their interaction forces are estimated using social force model. The interaction force is then mapped into the image plane to obtain Force Flow for every pixel in every frame. Randomly selected spatio-temporal volumes of Force Flow are used to model the normal behavior of the crowd. We classify frames as normal and abnormal by using a bag of words approach. The regions of anomalies in the abnormal frames are localized using interaction forces. The experiments are conducted on a publicly available dataset from University of Minnesota for escape panic scenarios and a challenging dataset of crowd videos taken from the web. The experiments show that the proposed method captures the dynamics of the crowd behavior successfully. In addition, we have shown that the social force approach outperforms similar approaches based on pure optical flow."
            },
            "slug": "Abnormal-crowd-behavior-detection-using-social-Mehran-Oyama",
            "title": {
                "fragments": [],
                "text": "Abnormal crowd behavior detection using social force model"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A novel method to detect and localize abnormal behaviors in crowd videos using Social Force model and it is shown that the social force approach outperforms similar approaches based on pure optical flow."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910174"
                        ],
                        "name": "Haonan Yu",
                        "slug": "Haonan-Yu",
                        "structuredName": {
                            "firstName": "Haonan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haonan Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2(e), it is more likely for the boy and the baseball cap to move in the same direction than not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5061155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a0320ef14877038906947b684011cf7378c440",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video."
            },
            "slug": "Grounded-Language-Learning-from-Video-Described-Yu-Siskind",
            "title": {
                "fragments": [],
                "text": "Grounded Language Learning from Video Described with Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A method that learns representations for word meanings from short video clips paired with sentences that can be subsequently used to automatically generate description of new video."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2408872"
                        ],
                        "name": "P. Talukdar",
                        "slug": "P.-Talukdar",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Talukdar",
                            "middleNames": [
                                "Pratim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Talukdar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129412"
                        ],
                        "name": "D. Wijaya",
                        "slug": "D.-Wijaya",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Wijaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wijaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2(e), it is more likely for the boy and the baseball cap to move in the same direction than not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16301762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad49e345860e602df58359b11c915b39acf901d9",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of automatically acquiring knowledge about the typical temporal orderings among relations (e.g., actedIn(person, film) typically occurs before wonPrize (film, award)), given only a database of known facts (relation instances) without time information, and a large document collection. Our approach is based on the conjecture that the narrative order of verb mentions within documents correlates with the temporal order of the relations they represent. We propose a family of algorithms based on this conjecture, utilizing a corpus of 890m dependency parsed sentences to obtain verbs that represent relations of interest, and utilizing Wikipedia documents to gather statistics on narrative order of verb mentions. Our proposed algorithm, GraphOrder, is a novel and scalable graph-based label propagation algorithm that takes transitivity of temporal order into account, as well as these statistics on narrative order of verb mentions. This algorithm achieves as high as 38.4% absolute improvement in F1 over a random baseline. Finally, we demonstrate the utility of this learned general knowledge about typical temporal orderings among relations, by showing that these temporal constraints can be successfully used by a joint inference framework to assign specific temporal scopes to individual facts."
            },
            "slug": "Acquiring-temporal-constraints-between-relations-Talukdar-Wijaya",
            "title": {
                "fragments": [],
                "text": "Acquiring temporal constraints between relations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed algorithm, GraphOrder, is a novel and scalable graph-based label propagation algorithm that takes transitivity of temporal order into account, as well as these statistics on narrative order of verb mentions, and achieves as high as 38.4% absolute improvement in F1 over a random baseline."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143818235"
                        ],
                        "name": "Andrew Carlson",
                        "slug": "Andrew-Carlson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Carlson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Carlson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31779043"
                        ],
                        "name": "J. Betteridge",
                        "slug": "J.-Betteridge",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Betteridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Betteridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16411658"
                        ],
                        "name": "B. Kisiel",
                        "slug": "B.-Kisiel",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Kisiel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kisiel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717452"
                        ],
                        "name": "Burr Settles",
                        "slug": "Burr-Settles",
                        "structuredName": {
                            "firstName": "Burr",
                            "lastName": "Settles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Burr Settles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1842532"
                        ],
                        "name": "Estevam Hruschka",
                        "slug": "Estevam-Hruschka",
                        "structuredName": {
                            "firstName": "Estevam",
                            "lastName": "Hruschka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Estevam Hruschka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2(e), it is more likely for the boy and the baseball cap to move in the same direction than not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8423494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7312b8568d63bbbb239583ed282f46cdc40978d",
            "isKey": false,
            "numCitedBy": 1739,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent."
            },
            "slug": "Toward-an-Architecture-for-Never-Ending-Language-Carlson-Betteridge",
            "title": {
                "fragments": [],
                "text": "Toward an Architecture for Never-Ending Language Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work proposes an approach and a set of design principles for an intelligent computer agent that runs forever and describes a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153642390"
                        ],
                        "name": "David L. Chen",
                        "slug": "David-L.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David L. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2(e), it is more likely for the boy and the baseball cap to move in the same direction than not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2488088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0008076a1968d9fb590c9013ab27b824849a4e80",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries."
            },
            "slug": "Learning-to-sportscast:-a-test-of-grounded-language-Chen-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to sportscast: a test of grounded language acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A novel commentator system that learns language from sportscasts of simulated soccer games and uses a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704635"
                        ],
                        "name": "D. Lenat",
                        "slug": "D.-Lenat",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Lenat",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lenat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145615125"
                        ],
                        "name": "R. Guha",
                        "slug": "R.-Guha",
                        "structuredName": {
                            "firstName": "Ramanathan",
                            "lastName": "Guha",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Guha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076089767"
                        ],
                        "name": "K. Pittman",
                        "slug": "K.-Pittman",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Pittman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pittman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144019683"
                        ],
                        "name": "Dexter Pratt",
                        "slug": "Dexter-Pratt",
                        "structuredName": {
                            "firstName": "Dexter",
                            "lastName": "Pratt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dexter Pratt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052381298"
                        ],
                        "name": "M. Shepherd",
                        "slug": "M.-Shepherd",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Shepherd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shepherd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2(e), it is more likely for the boy and the baseball cap to move in the same direction than not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7296269,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "909273dc44ce4c4f2cf4adfe5d60c3d421635909",
            "isKey": false,
            "numCitedBy": 536,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Cyc is a bold attempt to assemble a massive knowledge base (on the order of 108 axioms) spanning human consensus knowledge. This article examines the need for such an undertaking and reviews the authos' efforts over the past five years to begin its construction. The methodology and history of the project are briefly discussed, followed by a more developed treatment of the current state of the representation language used (epistemological level), techniques for efficient inferencing and default reasoning (heuristic level), and the content and organization of the knowledge base."
            },
            "slug": "Cyc:-toward-programs-with-common-sense-Lenat-Guha",
            "title": {
                "fragments": [],
                "text": "Cyc: toward programs with common sense"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The need for a massive knowledge base spanning human consensus knowledge is examined and the authos' efforts over the past five years to begin its construction are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103579533"
                        ],
                        "name": "Q. V. Soranus",
                        "slug": "Q.-V.-Soranus",
                        "structuredName": {
                            "firstName": "Q.",
                            "lastName": "Soranus",
                            "middleNames": [
                                "Valerius"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. V. Soranus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46288330"
                        ],
                        "name": "Edward Courtney",
                        "slug": "Edward-Courtney",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Courtney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Courtney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125268083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1000dfbfef78c593c401c1f15974a409885f2b86",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "2-=-4-M-Soranus-Courtney",
            "title": {
                "fragments": [],
                "text": "2 = 4 M"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 178
                            }
                        ],
                        "text": "In the tracking literature, this has been done by treating humans as privileged objects and reasoning about their motion around defined obstacles [6] and other pedestrians [14, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Youll never walk alone: Modeling social behavior for multi-target tracking"
            },
            "venue": {
                "fragments": [],
                "text": "ICCV"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "A more detailed analysis appears in the full-length version of this work [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Predicting object dynamics in scenes"
            },
            "venue": {
                "fragments": [],
                "text": "In CVPR,"
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Predicting-Object-Dynamics-in-Scenes-Fouhey-Zitnick/c99798fce885b41ab1de66bbacf04b7de7274f85?sort=total-citations"
}