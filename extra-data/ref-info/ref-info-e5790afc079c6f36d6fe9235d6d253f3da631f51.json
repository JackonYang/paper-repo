{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113242192"
                        ],
                        "name": "Abhishek Das",
                        "slug": "Abhishek-Das",
                        "structuredName": {
                            "firstName": "Abhishek",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhishek Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19200118"
                        ],
                        "name": "Samyak Datta",
                        "slug": "Samyak-Datta",
                        "structuredName": {
                            "firstName": "Samyak",
                            "lastName": "Datta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samyak Datta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082991"
                        ],
                        "name": "Georgia Gkioxari",
                        "slug": "Georgia-Gkioxari",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Gkioxari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgia Gkioxari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121944615"
                        ],
                        "name": "Stefan Lee",
                        "slug": "Stefan-Lee",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 58822200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6868c233c2d0fe01ecf0eda01099f6c7a0f98fb9",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new AI task - Embodied Question Answering(EmbodiedQA) - where an agent is spawned at a random location in a 3D environment and asked a question ('What color is the car?'). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question ('orange'). EmbodiedQA requires a range of AI skills - language understanding, visual recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, and grounding language into actions. In this work, we develop a dataset of questions and answers in House3D environments [1], evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning."
            },
            "slug": "Embodied-Question-Answering-Das-Datta",
            "title": {
                "fragments": [],
                "text": "Embodied Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A new AI task where an agent is spawned at a random location in a 3D environment and asked a question ('What color is the car?'), and the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152462964"
                        ],
                        "name": "Daniel Gordon",
                        "slug": "Daniel-Gordon",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gordon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684226"
                        ],
                        "name": "Aniruddha Kembhavi",
                        "slug": "Aniruddha-Kembhavi",
                        "structuredName": {
                            "firstName": "Aniruddha",
                            "lastName": "Kembhavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aniruddha Kembhavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143887493"
                        ],
                        "name": "Mohammad Rastegari",
                        "slug": "Mohammad-Rastegari",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Rastegari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammad Rastegari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197953"
                        ],
                        "name": "D. Fox",
                        "slug": "D.-Fox",
                        "structuredName": {
                            "firstName": "Dieter",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] studied the task of EmbodiedQA in the AI2-THOR environment [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4670339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0cd469a06fb2eae3a5cc0c860aa592f71b13f6d",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: \"Are there any apples in the fridge?\" The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR [35], a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98."
            },
            "slug": "IQA:-Visual-Question-Answering-in-Interactive-Gordon-Kembhavi",
            "title": {
                "fragments": [],
                "text": "IQA: Visual Question Answering in Interactive Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction, is proposed, and outperforms popular single controller based methods on IQUAD V1."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910174"
                        ],
                        "name": "Haonan Yu",
                        "slug": "Haonan-Yu",
                        "structuredName": {
                            "firstName": "Haonan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haonan Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108640124"
                        ],
                        "name": "Haichao Zhang",
                        "slug": "Haichao-Zhang",
                        "structuredName": {
                            "firstName": "Haichao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haichao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738410"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "XWORLD [27]), to 3D game-like environments with limited realism (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "There are several interactive environments commonly used in the community, ranging from simple 2D grid-worlds (e.g. XWORLD [27]), to 3D game-like environments with limited realism (e.g. DeepMind Lab [34] or Doom [16]), to more complex, realistic environments (e.g. AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 119
                            }
                        ],
                        "text": "The form and structure of these goal specifications range from declarative programs [26], to simple templated commands [27,28], to free-form natural language instructions [29, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7624474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c1a24169be56e01b0e36e260f49025260a5c7e7",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We tackle a task where an agent learns to navigate in a 2D maze-like environment called XWORLD. In each session, the agent perceives a sequence of raw-pixel frames, a natural language command issued by a teacher, and a set of rewards. The agent learns the teacher's language from scratch in a grounded and compositional manner, such that after training it is able to correctly execute zero-shot commands: 1) the combination of words in the command never appeared before, and/or 2) the command contains new object concepts that are learned from another task but never learned from navigation. Our deep framework for the agent is trained end to end: it learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and the action module that outputs actions. The zero-shot learning capability of our framework results from its compositionality and modularity with parameter tying. We visualize the intermediate outputs of the framework, demonstrating that the agent truly understands how to solve the problem. We believe that our results provide some preliminary insights on how to train an agent with similar abilities in a 3D environment."
            },
            "slug": "A-Deep-Compositional-Framework-for-Human-like-in-Yu-Zhang",
            "title": {
                "fragments": [],
                "text": "A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work tackles a task where an agent learns to navigate in a 2D maze-like environment called XWORLD and provides some preliminary insights on how to train an agent with similar abilities in a 3D environment."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117748"
                        ],
                        "name": "Yuke Zhu",
                        "slug": "Yuke-Zhu",
                        "structuredName": {
                            "firstName": "Yuke",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuke Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152462964"
                        ],
                        "name": "Daniel Gordon",
                        "slug": "Daniel-Gordon",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gordon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3386570"
                        ],
                        "name": "Eric Kolve",
                        "slug": "Eric-Kolve",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Kolve",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Kolve"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197953"
                        ],
                        "name": "D. Fox",
                        "slug": "D.-Fox",
                        "structuredName": {
                            "firstName": "Dieter",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3012475"
                        ],
                        "name": "Roozbeh Mottaghi",
                        "slug": "Roozbeh-Mottaghi",
                        "structuredName": {
                            "firstName": "Roozbeh",
                            "lastName": "Mottaghi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roozbeh Mottaghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 265
                            }
                        ],
                        "text": "There are several interactive environments commonly used in the community, ranging from simple 2D grid-worlds (e.g. XWORLD [27]), to 3D game-like environments with limited realism (e.g. DeepMind Lab [34] or Doom [16]), to more complex, realistic environments (e.g. AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "Visual navigation typically specifies agent goals either implicitly via the reward function [18, 19] (thus training a separate policy for each goal/reward), or explicitly by conditioning on goal state representations [23] like images of target objects [17, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 200
                            }
                        ],
                        "text": "Concurrent with our work, Anderson et al. [31] proposed goal-driven navigation from natural language instructions in real 3D environments, and Gordon et al. [32] studied the task of EmbodiedQA in the AI2-THOR environment [33]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5370136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4558b932075a862c72bb98bbce5f08590f563b14",
            "isKey": true,
            "numCitedBy": 111,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment."
            },
            "slug": "Visual-Semantic-Planning-Using-Deep-Successor-Zhu-Gordon",
            "title": {
                "fragments": [],
                "text": "Visual Semantic Planning Using Deep Successor Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work addresses the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state, and develops a deep predictive model based on successor representations."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6965856"
                        ],
                        "name": "Peter Anderson",
                        "slug": "Peter-Anderson",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144663765"
                        ],
                        "name": "Qi Wu",
                        "slug": "Qi-Wu",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406263"
                        ],
                        "name": "Damien Teney",
                        "slug": "Damien-Teney",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Teney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Teney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12139064"
                        ],
                        "name": "Jake Bruce",
                        "slug": "Jake-Bruce",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Bruce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jake Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771913"
                        ],
                        "name": "Niko S\u00fcnderhauf",
                        "slug": "Niko-S\u00fcnderhauf",
                        "structuredName": {
                            "firstName": "Niko",
                            "lastName": "S\u00fcnderhauf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niko S\u00fcnderhauf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 280
                            }
                        ],
                        "text": "There are several interactive environments commonly used in the community, ranging from simple 2D grid-worlds (e.g. XWORLD [27]), to 3D game-like environments with limited realism (e.g. DeepMind Lab [34] or Doom [16]), to more complex, realistic environments (e.g. AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[31] proposed goal-driven navigation from natural language instructions in real 3D environments, and Gordon et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4673790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bd9642470ff8c2089427f7a6392cd17d213a334",
            "isKey": false,
            "numCitedBy": 564,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matter-port3D Simulator - a large-scale reinforcement learning environment based on real imagery [11]. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings - the Room-to-Room (R2R) dataset1."
            },
            "slug": "Vision-and-Language-Navigation:-Interpreting-in-Anderson-Wu",
            "title": {
                "fragments": [],
                "text": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work provides the first benchmark dataset for visually-grounded natural language navigation in real buildings - the Room-to-Room (R2R) dataset and presents the Matter-port3D Simulator - a large-scale reinforcement learning environment based on real imagery."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2338742"
                        ],
                        "name": "Y. Jang",
                        "slug": "Y.-Jang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Jang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Jang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2317183"
                        ],
                        "name": "Yale Song",
                        "slug": "Yale-Song",
                        "structuredName": {
                            "firstName": "Yale",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yale Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7877122"
                        ],
                        "name": "Youngjae Yu",
                        "slug": "Youngjae-Yu",
                        "structuredName": {
                            "firstName": "Youngjae",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngjae Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108281205"
                        ],
                        "name": "Youngjin Kim",
                        "slug": "Youngjin-Kim",
                        "structuredName": {
                            "firstName": "Youngjin",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngjin Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743920"
                        ],
                        "name": "Gunhee Kim",
                        "slug": "Gunhee-Kim",
                        "structuredName": {
                            "firstName": "Gunhee",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunhee Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3030826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2f521c02c6ed3080c5fe123e938cdf4555e6fd2",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations."
            },
            "slug": "TGIF-QA:-Toward-Spatio-Temporal-Reasoning-in-Visual-Jang-Song",
            "title": {
                "fragments": [],
                "text": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly and introduces a new large-scale dataset for videoVQA named TGIF-QA that extends existing VQ a work with its new tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2328602"
                        ],
                        "name": "Devendra Singh Chaplot",
                        "slug": "Devendra-Singh-Chaplot",
                        "structuredName": {
                            "firstName": "Devendra",
                            "lastName": "Chaplot",
                            "middleNames": [
                                "Singh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devendra Singh Chaplot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443185"
                        ],
                        "name": "Kanthashree Mysore Sathyendra",
                        "slug": "Kanthashree-Mysore-Sathyendra",
                        "structuredName": {
                            "firstName": "Kanthashree",
                            "lastName": "Sathyendra",
                            "middleNames": [
                                "Mysore"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kanthashree Mysore Sathyendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096732"
                        ],
                        "name": "Rama Kumar Pasumarthi",
                        "slug": "Rama-Kumar-Pasumarthi",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Pasumarthi",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rama Kumar Pasumarthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801149"
                        ],
                        "name": "Dheeraj Rajagopal",
                        "slug": "Dheeraj-Rajagopal",
                        "structuredName": {
                            "firstName": "Dheeraj",
                            "lastName": "Rajagopal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dheeraj Rajagopal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "DeepMind Lab [34] or Doom [16]), to more complex, realistic environments (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] both develop embodied agents in simple game-like environments to understand simple \u2018go to X\u2019 or \u2018pick up X\u2019 style commands where X could specify an object (and possibly some of its attributes)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "There are several interactive environments commonly used in the community, ranging from simple 2D grid-worlds (e.g. XWORLD [27]), to 3D game-like environments with limited realism (e.g. DeepMind Lab [34] or Doom [16]), to more complex, realistic environments (e.g. AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2921786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88347f9f12b50590f50aefce4cf71b3a3f0bd138",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.\n \n"
            },
            "slug": "Gated-Attention-Architectures-for-Task-Oriented-Chaplot-Sathyendra",
            "title": {
                "fragments": [],
                "text": "Gated-Attention Architectures for Task-Oriented Language Grounding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801949"
                        ],
                        "name": "Aishwarya Agrawal",
                        "slug": "Aishwarya-Agrawal",
                        "structuredName": {
                            "firstName": "Aishwarya",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aishwarya Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963421"
                        ],
                        "name": "Stanislaw Antol",
                        "slug": "Stanislaw-Antol",
                        "structuredName": {
                            "firstName": "Stanislaw",
                            "lastName": "Antol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanislaw Antol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067793408"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3180429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
            "isKey": false,
            "numCitedBy": 2887,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing $$\\sim $$\u223c0.25\u00a0M images, $$\\sim $$\u223c0.76\u00a0M questions, and $$\\sim $$\u223c10\u00a0M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa)."
            },
            "slug": "VQA:-Visual-Question-Answering-Agrawal-Lu",
            "title": {
                "fragments": [],
                "text": "VQA: Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The task of free-form and open-ended Visual Question Answering (VQA) is proposed, given an image and a natural language question about the image, the task is to provide an accurate natural language answer."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117748"
                        ],
                        "name": "Yuke Zhu",
                        "slug": "Yuke-Zhu",
                        "structuredName": {
                            "firstName": "Yuke",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuke Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3012475"
                        ],
                        "name": "Roozbeh Mottaghi",
                        "slug": "Roozbeh-Mottaghi",
                        "structuredName": {
                            "firstName": "Roozbeh",
                            "lastName": "Mottaghi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roozbeh Mottaghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3386570"
                        ],
                        "name": "Eric Kolve",
                        "slug": "Eric-Kolve",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Kolve",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Kolve"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35198686"
                        ],
                        "name": "Joseph J. Lim",
                        "slug": "Joseph-J.-Lim",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Lim",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph J. Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 252
                            }
                        ],
                        "text": "Visual navigation typically specifies agent goals either implicitly via the reward function [18, 19] (thus training a separate policy for each goal/reward), or explicitly by conditioning on goal state representations [23] like images of target objects [17, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "approach of [17], with the difference that the goal is specified via a question encoding instead of a target image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2305273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7af7f2f539cd3479faae4c66bbef49b0f66202fa",
            "isKey": false,
            "numCitedBy": 991,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment."
            },
            "slug": "Target-driven-visual-navigation-in-indoor-scenes-Zhu-Mottaghi",
            "title": {
                "fragments": [],
                "text": "Target-driven visual navigation in indoor scenes using deep reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization and proposes the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Robotics and Automation (ICRA)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144157872"
                        ],
                        "name": "Saurabh Gupta",
                        "slug": "Saurabh-Gupta",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saurabh Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "78708388"
                        ],
                        "name": "Varun Tolani",
                        "slug": "Varun-Tolani",
                        "structuredName": {
                            "firstName": "Varun",
                            "lastName": "Tolani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varun Tolani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068894907"
                        ],
                        "name": "James Davidson",
                        "slug": "James-Davidson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davidson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Davidson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736651"
                        ],
                        "name": "S. Levine",
                        "slug": "S.-Levine",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "Visual navigation typically specifies agent goals either implicitly via the reward function [18, 19] (thus training a separate policy for each goal/reward), or explicitly by conditioning on goal state representations [23] like images of target objects [17, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 203658640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c47d54e783012f9e9936d8c5fbf204e95135f949",
            "isKey": false,
            "numCitedBy": 465,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: (a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and (b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as \u201cgoing to a chair\u201d. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation."
            },
            "slug": "Cognitive-Mapping-and-Planning-for-Visual-Gupta-Tolani",
            "title": {
                "fragments": [],
                "text": "Cognitive Mapping and Planning for Visual Navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Cognitive Mapper and Planner is based on a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and a spatial memory with the ability to plan given an incomplete set of observations about the world."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3386570"
                        ],
                        "name": "Eric Kolve",
                        "slug": "Eric-Kolve",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Kolve",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Kolve"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3012475"
                        ],
                        "name": "Roozbeh Mottaghi",
                        "slug": "Roozbeh-Mottaghi",
                        "structuredName": {
                            "firstName": "Roozbeh",
                            "lastName": "Mottaghi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roozbeh Mottaghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1443358534"
                        ],
                        "name": "Winson Han",
                        "slug": "Winson-Han",
                        "structuredName": {
                            "firstName": "Winson",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Winson Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1632920625"
                        ],
                        "name": "Eli VanderBilt",
                        "slug": "Eli-VanderBilt",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "VanderBilt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eli VanderBilt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20745881"
                        ],
                        "name": "Luca Weihs",
                        "slug": "Luca-Weihs",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Weihs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luca Weihs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3360135"
                        ],
                        "name": "Alvaro Herrasti",
                        "slug": "Alvaro-Herrasti",
                        "structuredName": {
                            "firstName": "Alvaro",
                            "lastName": "Herrasti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alvaro Herrasti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152462964"
                        ],
                        "name": "Daniel Gordon",
                        "slug": "Daniel-Gordon",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gordon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117748"
                        ],
                        "name": "Yuke Zhu",
                        "slug": "Yuke-Zhu",
                        "structuredName": {
                            "firstName": "Yuke",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuke Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "[32] studied the task of EmbodiedQA in the AI2-THOR environment [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28328610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89c8aad71433f7638d2e2c009e1ea20e039f832d",
            "isKey": false,
            "numCitedBy": 455,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain."
            },
            "slug": "AI2-THOR:-An-Interactive-3D-Environment-for-Visual-Kolve-Mottaghi",
            "title": {
                "fragments": [],
                "text": "AI2-THOR: An Interactive 3D Environment for Visual AI"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks and facilitate building visually intelligent models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151333468"
                        ],
                        "name": "Peng Zhang",
                        "slug": "Peng-Zhang",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37226164"
                        ],
                        "name": "Yash Goyal",
                        "slug": "Yash-Goyal",
                        "structuredName": {
                            "firstName": "Yash",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yash Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403432120"
                        ],
                        "name": "Douglas Summers-Stay",
                        "slug": "Douglas-Summers-Stay",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Summers-Stay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Summers-Stay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 121
                            }
                        ],
                        "text": "This enables us to control the distribution of question-types and answers, deter algorithms from exploiting dataset bias [4, 9], and provide fine-grained breakdown of performance by skill."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6733279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fa973b8d284145bf0ced9acf2913a74674260f6",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is \"yes\", and otherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on the highlevel semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is \"yes\" for one scene, and \"no\" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset."
            },
            "slug": "Yin-and-Yang:-Balancing-and-Answering-Binary-Visual-Zhang-Goyal",
            "title": {
                "fragments": [],
                "text": "Yin and Yang: Balancing and Answering Binary Visual Questions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper addresses binary Visual Question Answering on abstract scenes as visual verification of concepts inquired in the questions by converting the question to a tuple that concisely summarizes the visual concept to be detected in the image."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2313517"
                        ],
                        "name": "Abhishek Das",
                        "slug": "Abhishek-Das",
                        "structuredName": {
                            "firstName": "Abhishek",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhishek Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37825612"
                        ],
                        "name": "Harsh Agrawal",
                        "slug": "Harsh-Agrawal",
                        "structuredName": {
                            "firstName": "Harsh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harsh Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 315
                            }
                        ],
                        "text": "Language Grounding: One commonly noted shortcoming of modern vision-and-language models is their lack of grounding \u2013 these models often fail to associate entities in text with corresponding image pixels, relying instead on dataset biases to respond seemingly intelligently even when attending to irrelevant regions [3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 220553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58cb0c24c936b8a14ca7b2d56ba80de733c545b3",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans."
            },
            "slug": "Human-Attention-in-Visual-Question-Answering:-Do-at-Das-Agrawal",
            "title": {
                "fragments": [],
                "text": "Human Attention in Visual Question Answering: Do Humans and Deep Networks look at the same regions?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The VQA-HAT (Human ATtention) dataset is introduced and attention maps generated by state-of-the-art V QA models are evaluated against human attention both qualitatively and quantitatively."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31498163"
                        ],
                        "name": "Dipendra Kumar Misra",
                        "slug": "Dipendra-Kumar-Misra",
                        "structuredName": {
                            "firstName": "Dipendra",
                            "lastName": "Misra",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dipendra Kumar Misra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3167681"
                        ],
                        "name": "Yoav Artzi",
                        "slug": "Yoav-Artzi",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Artzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Artzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10458880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc9f3c466c6f6b386f4ef1195853d498cf3c182e",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent\u2019s exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants."
            },
            "slug": "Mapping-Instructions-and-Visual-Observations-to-Misra-Langford",
            "title": {
                "fragments": [],
                "text": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work learns a single model to jointly reason about linguistic and visual input in a contextual bandit setting to train a neural network agent and shows significant improvements over supervised learning and common reinforcement learning variants."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070088720"
                        ],
                        "name": "Simon Green",
                        "slug": "Simon-Green",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Green",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115096249"
                        ],
                        "name": "Fumin Wang",
                        "slug": "Fumin-Wang",
                        "structuredName": {
                            "firstName": "Fumin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fumin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48627702"
                        ],
                        "name": "R. Faulkner",
                        "slug": "R.-Faulkner",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Faulkner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Faulkner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2794457"
                        ],
                        "name": "Hubert Soyer",
                        "slug": "Hubert-Soyer",
                        "structuredName": {
                            "firstName": "Hubert",
                            "lastName": "Soyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hubert Soyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7635903"
                        ],
                        "name": "David Szepesvari",
                        "slug": "David-Szepesvari",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Szepesvari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Szepesvari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144792148"
                        ],
                        "name": "Wojciech M. Czarnecki",
                        "slug": "Wojciech-M.-Czarnecki",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Czarnecki",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech M. Czarnecki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035073"
                        ],
                        "name": "Denis Teplyashin",
                        "slug": "Denis-Teplyashin",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Teplyashin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Teplyashin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47969243"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Wainwright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17937882"
                        ],
                        "name": "C. Apps",
                        "slug": "C.-Apps",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Apps",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Apps"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27228816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "019923afa86036b69c0e423f3c2188bfa7050923",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world."
            },
            "slug": "Grounded-Language-Learning-in-a-Simulated-3D-World-Hermann-Hill",
            "title": {
                "fragments": [],
                "text": "Grounded Language Learning in a Simulated 3D World"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An agent is presented that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions and its comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73710317"
                        ],
                        "name": "B. Hariharan",
                        "slug": "B.-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803520"
                        ],
                        "name": "L. V. D. Maaten",
                        "slug": "L.-V.-D.-Maaten",
                        "structuredName": {
                            "firstName": "Laurens",
                            "lastName": "Maaten",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. D. Maaten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "We draw inspiration from the CLEVR [39] dataset, and programmatically generate a dataset (EQA) of grounded questions and answers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15458100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03eb382e04cca8cca743f7799070869954f1402a",
            "isKey": false,
            "numCitedBy": 1223,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."
            },
            "slug": "CLEVR:-A-Diagnostic-Dataset-for-Compositional-and-Johnson-Hariharan",
            "title": {
                "fragments": [],
                "text": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a diagnostic dataset that tests a range of visual reasoning abilities and uses this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31613801"
                        ],
                        "name": "Yi Wu",
                        "slug": "Yi-Wu",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98264506"
                        ],
                        "name": "Yuxin Wu",
                        "slug": "Yuxin-Wu",
                        "structuredName": {
                            "firstName": "Yuxin",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxin Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082991"
                        ],
                        "name": "Georgia Gkioxari",
                        "slug": "Georgia-Gkioxari",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Gkioxari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgia Gkioxari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39402399"
                        ],
                        "name": "Yuandong Tian",
                        "slug": "Yuandong-Tian",
                        "structuredName": {
                            "firstName": "Yuandong",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuandong Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "In the descriptions that follow, an \u2018entity\u2019 can refer to either a queryable room or a queryable object from the House3D [8] environment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "We have an encoder network that transforms the egocentric RGB image from the House3D renderer [8] to a fixed-size representation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "In this work, we use the House3D [8] environment as it strikes a useful middle-ground between simple synthetic and realistic environments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "\u2022 We evaluate our agents in House3D [8], a rich, interactive environment based on human-designed 3D indoor scenes from the SUNCG dataset [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "We instantiate EmbodiedQA in House3D [8], a recently introduced rich, interactive environment based on 3D indoor scenes from the SUNCG dataset [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38294295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4adfa52793b0f6f70915383bf12114f4824897e1",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Towards bridging the gap between machine and human intelligence, it is of utmost importance to introduce environments that are visually realistic and rich in content. In such environments, one can evaluate and improve a crucial property of practical intelligent systems, namely \\emph{generalization}. In this work, we build \\emph{House3D}, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of houses, ranging from single-room studios to multi-storeyed houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). With an emphasis on semantic-level generalization, we study the task of concept-driven navigation, \\emph{RoomNav}, using a subset of houses in House3D. In RoomNav, an agent navigates towards a target specified by a semantic concept. To succeed, the agent learns to comprehend the scene it lives in by developing perception, understand the concept by mapping it to the correct semantics, and navigate to the target by obeying the underlying physical rules. We train RL agents with both continuous and discrete action spaces and show their ability to generalize in new unseen environments. In particular, we observe that (1) training is substantially harder on large house sets but results in better generalization, (2) using semantic signals (e.g., segmentation mask) boosts the generalization performance, and (3) gated networks on semantic input signal lead to improved training performance and generalization. We hope House3D, including the analysis of the RoomNav task, serves as a building block towards designing practical intelligent systems and we wish it to be broadly adopted by the community."
            },
            "slug": "Building-Generalizable-Agents-with-a-Realistic-and-Wu-Wu",
            "title": {
                "fragments": [],
                "text": "Building Generalizable Agents with a Realistic and Rich 3D Environment"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "House3D is built, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset and an emphasis on semantic-level generalization."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2894414"
                        ],
                        "name": "Junhyuk Oh",
                        "slug": "Junhyuk-Oh",
                        "structuredName": {
                            "firstName": "Junhyuk",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junhyuk Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 80
                            }
                        ],
                        "text": "Hierarchical modeling has recently shown promise in deep reinforcement learning [22,28,36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "\u2019), and is reminiscent of hierarchical RL [22, 28, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22] develop agents in a simple maze world and task them to follow a series of templated instructions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11974467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30834ae1497c35d362eea14857d93c28d2d12b57",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions."
            },
            "slug": "Zero-Shot-Task-Generalization-with-Multi-Task-Deep-Oh-Singh",
            "title": {
                "fragments": [],
                "text": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks is introduced and a new neural architecture in the meta controller that learns when to update the subtask is proposed, which makes learning more efficient."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144578435"
                        ],
                        "name": "Mark Pfeiffer",
                        "slug": "Mark-Pfeiffer",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Pfeiffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Pfeiffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065780860"
                        ],
                        "name": "Michaela Schaeuble",
                        "slug": "Michaela-Schaeuble",
                        "structuredName": {
                            "firstName": "Michaela",
                            "lastName": "Schaeuble",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michaela Schaeuble"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144147879"
                        ],
                        "name": "Juan I. Nieto",
                        "slug": "Juan-I.-Nieto",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Nieto",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan I. Nieto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720483"
                        ],
                        "name": "R. Siegwart",
                        "slug": "R.-Siegwart",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Siegwart",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Siegwart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586813"
                        ],
                        "name": "C\u00e9sar Cadena",
                        "slug": "C\u00e9sar-Cadena",
                        "structuredName": {
                            "firstName": "C\u00e9sar",
                            "lastName": "Cadena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C\u00e9sar Cadena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "Visual navigation typically specifies agent goals either implicitly via the reward function [18, 19] (thus training a separate policy for each goal/reward), or explicitly by conditioning on goal state representations [23] like images of target objects [17, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206852465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa0b2517c1555fc5b3885723959f7ac950ba1626",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning from demonstration for motion planning is an ongoing research topic. In this paper we present a model that is able to learn the complex mapping from raw 2D-laser range findings and a target position to the required steering commands for the robot. To our best knowledge, this work presents the first approach that learns a target-oriented end-to-end navigation model for a robotic platform. The supervised model training is based on expert demonstrations generated in simulation with an existing motion planner. We demonstrate that the learned navigation model is directly transferable to previously unseen virtual and, more interestingly, real-world environments. It can safely navigate the robot through obstacle-cluttered environments to reach the provided targets. We present an extensive qualitative and quantitative evaluation of the neural network-based motion planner, and compare it to a grid-based global approach, both in simulation and in real-world experiments."
            },
            "slug": "From-perception-to-decision:-A-data-driven-approach-Pfeiffer-Schaeuble",
            "title": {
                "fragments": [],
                "text": "From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents the first approach that learns a target-oriented end-to-end navigation model for a robotic platform, and demonstrates that the learned navigation model is directly transferable to previously unseen virtual and, more interestingly, real-world environments."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Robotics and Automation (ICRA)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417003"
                        ],
                        "name": "Nikolay Savinov",
                        "slug": "Nikolay-Savinov",
                        "structuredName": {
                            "firstName": "Nikolay",
                            "lastName": "Savinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolay Savinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841331"
                        ],
                        "name": "A. Dosovitskiy",
                        "slug": "A.-Dosovitskiy",
                        "structuredName": {
                            "firstName": "Alexey",
                            "lastName": "Dosovitskiy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dosovitskiy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145231047"
                        ],
                        "name": "V. Koltun",
                        "slug": "V.-Koltun",
                        "structuredName": {
                            "firstName": "Vladlen",
                            "lastName": "Koltun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3687922,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1064bb7a5e1cfe0004192b0db300d1c5f4a300c3",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semi-parametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three. A video of the agent is available at this https URL"
            },
            "slug": "Semi-parametric-Topological-Memory-for-Navigation-Savinov-Dosovitskiy",
            "title": {
                "fragments": [],
                "text": "Semi-parametric Topological Memory for Navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals, that consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a deep network capable of retrieving nodes from the graph based on observations."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736651"
                        ],
                        "name": "S. Levine",
                        "slug": "S.-Levine",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46881670"
                        ],
                        "name": "Chelsea Finn",
                        "slug": "Chelsea-Finn",
                        "structuredName": {
                            "firstName": "Chelsea",
                            "lastName": "Finn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chelsea Finn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 81
                            }
                        ],
                        "text": "Specifically, our approach follows the recent paradigm from robotics and deep RL (Levine et al., 2016; Misra et al., 2017) \u2013 the training environments are sufficiently instrumented, and provide access to the agent location, RGB, depth & semantic annotations of the visual environment, and allow for computing obstacle-avoiding shortest navigable paths from the agent to any target."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7242892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6b8a1b80891c96c28cc6340267b58186157e536",
            "isKey": false,
            "numCitedBy": 2506,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods."
            },
            "slug": "End-to-End-Training-of-Deep-Visuomotor-Policies-Levine-Finn",
            "title": {
                "fragments": [],
                "text": "End-to-End Training of Deep Visuomotor Policies"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper develops a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors, trained using a partially observed guided policy search method, with supervision provided by a simple trajectory-centric reinforcement learning method."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112400"
                        ],
                        "name": "Jacob Andreas",
                        "slug": "Jacob-Andreas",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Andreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Andreas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736651"
                        ],
                        "name": "S. Levine",
                        "slug": "S.-Levine",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 80
                            }
                        ],
                        "text": "Hierarchical modeling has recently shown promise in deep reinforcement learning [22,28,36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "\u2019), and is reminiscent of hierarchical RL [22, 28, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 119
                            }
                        ],
                        "text": "The form and structure of these goal specifications range from declarative programs [26], to simple templated commands [27,28], to free-form natural language instructions [29, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14711954,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them\u2014specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level sub-goals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks."
            },
            "slug": "Modular-Multitask-Reinforcement-Learning-with-Andreas-Klein",
            "title": {
                "fragments": [],
                "text": "Modular Multitask Reinforcement Learning with Policy Sketches"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments show that using the approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37226164"
                        ],
                        "name": "Yash Goyal",
                        "slug": "Yash-Goyal",
                        "structuredName": {
                            "firstName": "Yash",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yash Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7595427"
                        ],
                        "name": "Tejas Khot",
                        "slug": "Tejas-Khot",
                        "structuredName": {
                            "firstName": "Tejas",
                            "lastName": "Khot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tejas Khot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403432120"
                        ],
                        "name": "Douglas Summers-Stay",
                        "slug": "Douglas-Summers-Stay",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Summers-Stay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Summers-Stay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 121
                            }
                        ],
                        "text": "This enables us to control the distribution of question-types and answers, deter algorithms from exploiting dataset bias [4, 9], and provide fine-grained breakdown of performance by skill."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 315
                            }
                        ],
                        "text": "Language Grounding: One commonly noted shortcoming of modern vision-and-language models is their lack of grounding \u2013 these models often fail to associate entities in text with corresponding image pixels, relying instead on dataset biases to respond seemingly intelligently even when attending to irrelevant regions [3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8081284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c",
            "isKey": false,
            "numCitedBy": 1162,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of visual question answering (VQA) is of significant importance both as a challenging research question and for the rich set of applications it enables. In this context, however, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in VQA models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of VQA and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., in: ICCV, 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the VQA Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. We also present interesting insights from analysis of the participant entries in VQA Challenge 2017, organized by us on the proposed VQA v2.0 dataset. The results of the challenge were announced in the 2nd VQA Challenge Workshop at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users."
            },
            "slug": "Making-the-V-in-VQA-Matter:-Elevating-the-Role-of-Goyal-Khot",
            "title": {
                "fragments": [],
                "text": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work balances the popular VQA dataset by collecting complementary images such that every question in the authors' balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135128997"
                        ],
                        "name": "Akira Fukui",
                        "slug": "Akira-Fukui",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Fukui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akira Fukui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422202"
                        ],
                        "name": "Dong Huk Park",
                        "slug": "Dong-Huk-Park",
                        "structuredName": {
                            "firstName": "Dong Huk",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Huk Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422876"
                        ],
                        "name": "Daylen Yang",
                        "slug": "Daylen-Yang",
                        "structuredName": {
                            "firstName": "Daylen",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daylen Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34721166"
                        ],
                        "name": "Anna Rohrbach",
                        "slug": "Anna-Rohrbach",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2840197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fddc15480d086629b960be5bff96232f967f2252",
            "isKey": false,
            "numCitedBy": 1086,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge."
            },
            "slug": "Multimodal-Compact-Bilinear-Pooling-for-Visual-and-Fukui-Park",
            "title": {
                "fragments": [],
                "text": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work extensively evaluates Multimodal Compact Bilinear pooling (MCB) on the visual question answering and grounding tasks and consistently shows the benefit of MCB over ablations without MCB."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8729431"
                        ],
                        "name": "Sida I. Wang",
                        "slug": "Sida-I.-Wang",
                        "structuredName": {
                            "firstName": "Sida",
                            "lastName": "Wang",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sida I. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 171
                            }
                        ],
                        "text": "The form and structure of these goal specifications range from declarative programs [26], to simple templated commands [27,28], to free-form natural language instructions [29, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2705742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13d156f37ecea807706fd117547ac1b805d5c5aa",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players."
            },
            "slug": "Learning-Language-Games-through-Interaction-Wang-Liang",
            "title": {
                "fragments": [],
                "text": "Learning Language Games through Interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new language learning setting relevant to building adaptive natural language interfaces inspired by Wittgenstein's language games is introduced, showing that using compositionality and avoiding synonyms correlates positively with task performance and that modeling pragmatics further accelerates learning for successful players."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6965856"
                        ],
                        "name": "Peter Anderson",
                        "slug": "Peter-Anderson",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31790073"
                        ],
                        "name": "Chris Buehler",
                        "slug": "Chris-Buehler",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Buehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406263"
                        ],
                        "name": "Damien Teney",
                        "slug": "Damien-Teney",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Teney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Teney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3753452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
            "isKey": false,
            "numCitedBy": 2275,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge."
            },
            "slug": "Bottom-Up-and-Top-Down-Attention-for-Image-and-Anderson-He",
            "title": {
                "fragments": [],
                "text": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions is proposed, demonstrating the broad applicability of this approach to VQA."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928511"
                        ],
                        "name": "Samarth Brahmbhatt",
                        "slug": "Samarth-Brahmbhatt",
                        "structuredName": {
                            "firstName": "Samarth",
                            "lastName": "Brahmbhatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samarth Brahmbhatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12396215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9019286143f89561509506c3164f36f0e7e3a364",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present DeepNav, a Convolutional Neural Network (CNN) based algorithm for navigating large cities using locally visible street-view images. The DeepNav agent learns to reach its destination quickly by making the correct navigation decisions at intersections. We collect a large-scale dataset of street-view images organized in a graph where nodes are connected by roads. This dataset contains 10 city graphs and a total of more than 1 million street-view images. We propose 3 supervised learning approaches for the navigation task, and show how A* search in the city graph can be used to generate labels for the images. Our annotation process is fully automated using publicly available mapping services, and requires no human input. We evaluate the proposed DeepNav models on 4 held-out cities for navigating to 5 different types of destinations and show that our algorithms outperform previous work that uses hand-crafted features and Support Vector Regression (SVR) [19]."
            },
            "slug": "DeepNav:-Learning-to-Navigate-Large-Cities-Brahmbhatt-Hays",
            "title": {
                "fragments": [],
                "text": "DeepNav: Learning to Navigate Large Cities"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed DeepNav models are evaluated on 4 held-out cities for navigating to 5 different types of destinations and it is shown that the algorithms outperform previous work that uses hand-crafted features and Support Vector Regression (SVR)."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715051"
                        ],
                        "name": "Misha Denil",
                        "slug": "Misha-Denil",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Denil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Misha Denil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016840"
                        ],
                        "name": "Sergio Gomez Colmenarejo",
                        "slug": "Sergio-Gomez-Colmenarejo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Colmenarejo",
                            "middleNames": [
                                "Gomez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergio Gomez Colmenarejo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12159303"
                        ],
                        "name": "Serkan Cabi",
                        "slug": "Serkan-Cabi",
                        "structuredName": {
                            "firstName": "Serkan",
                            "lastName": "Cabi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serkan Cabi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143810408"
                        ],
                        "name": "D. Saxton",
                        "slug": "D.-Saxton",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Saxton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Saxton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "The form and structure of these goal specifications range from declarative programs [26], to simple templated commands [27,28], to free-form natural language instructions [29, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1844702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "046c4ff0b08361b1ee84fecf1292d385d7bca277",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We build deep RL agents that execute declarative programs expressed in formal language. The agents learn to ground the terms in this language in their environment, and can generalize their behavior at test time to execute new programs that refer to objects that were not referenced during training. The agents develop disentangled interpretable representations that allow them to generalize to a wide variety of zero-shot semantic tasks."
            },
            "slug": "Programmable-Agents-Denil-Colmenarejo",
            "title": {
                "fragments": [],
                "text": "Programmable Agents"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work builds deep RL agents that execute declarative programs expressed in formal language that develop disentangled interpretable representations that allow them to generalize to a wide variety of zero-shot semantic tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699245"
                        ],
                        "name": "T. Winograd",
                        "slug": "T.-Winograd",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Winograd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Winograd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 34
                            }
                        ],
                        "text": "Inspired by the classical work of Winograd [25], a number of recent works have revisited grounded language learning by situating agents in simple globally-perceived environments and tasking them with goals specified in natural\nlanguage."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "Inspired by the classical work of Winograd [25], a number of recent works have revisited grounded language learning by situating agents in simple globally-perceived environments and tasking them with goals specified in natural language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56798209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb20f121c979b535bbeade5ac06676d627d4ad7d",
            "isKey": false,
            "numCitedBy": 2455,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system."
            },
            "slug": "Understanding-natural-language-Winograd",
            "title": {
                "fragments": [],
                "text": "Understanding natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A computer system for understanding English that contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system based on the belief that in modeling language understanding, it must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103464"
                        ],
                        "name": "Makarand Tapaswi",
                        "slug": "Makarand-Tapaswi",
                        "structuredName": {
                            "firstName": "Makarand",
                            "lastName": "Tapaswi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makarand Tapaswi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844940337"
                        ],
                        "name": "Yukun Zhu",
                        "slug": "Yukun-Zhu",
                        "structuredName": {
                            "firstName": "Yukun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yukun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742325"
                        ],
                        "name": "R. Stiefelhagen",
                        "slug": "R.-Stiefelhagen",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Stiefelhagen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stiefelhagen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1017389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7",
            "isKey": false,
            "numCitedBy": 478,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events occurred. Each question comes with a set of five possible answers, a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information - video clips, plots, subtitles, scripts, and DVS. We analyze our data through various statistics and methods. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain."
            },
            "slug": "MovieQA:-Understanding-Stories-in-Movies-through-Tapaswi-Zhu",
            "title": {
                "fragments": [],
                "text": "MovieQA: Understanding Stories in Movies through Question-Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The MovieQA dataset, which aims to evaluate automatic story comprehension from both video and text, is introduced and existing QA techniques are extended to show that question-answering with such open-ended semantics is hard."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3393407"
                        ],
                        "name": "Chen Tessler",
                        "slug": "Chen-Tessler",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Tessler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Tessler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39791491"
                        ],
                        "name": "Shahar Givony",
                        "slug": "Shahar-Givony",
                        "structuredName": {
                            "firstName": "Shahar",
                            "lastName": "Givony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shahar Givony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331540"
                        ],
                        "name": "Tom Zahavy",
                        "slug": "Tom-Zahavy",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Zahavy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Zahavy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3187297"
                        ],
                        "name": "D. Mankowitz",
                        "slug": "D.-Mankowitz",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Mankowitz",
                            "middleNames": [
                                "Jaymin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mankowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712535"
                        ],
                        "name": "Shie Mannor",
                        "slug": "Shie-Mannor",
                        "structuredName": {
                            "firstName": "Shie",
                            "lastName": "Mannor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shie Mannor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 80
                            }
                        ],
                        "text": "Hierarchical modeling has recently shown promise in deep reinforcement learning [22,28,36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "\u2019), and is reminiscent of hierarchical RL [22, 28, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c3861c607fb79f3fbf79552018724617fc8ba1b",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the H-DRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.\n \n"
            },
            "slug": "A-Deep-Hierarchical-Approach-to-Lifelong-Learning-Tessler-Givony",
            "title": {
                "fragments": [],
                "text": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50388928"
                        ],
                        "name": "Charlie Beattie",
                        "slug": "Charlie-Beattie",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Beattie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charlie Beattie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700356"
                        ],
                        "name": "Joel Z. Leibo",
                        "slug": "Joel-Z.-Leibo",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Leibo",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Z. Leibo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035073"
                        ],
                        "name": "Denis Teplyashin",
                        "slug": "Denis-Teplyashin",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Teplyashin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Teplyashin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056968992"
                        ],
                        "name": "Tom Ward",
                        "slug": "Tom-Ward",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47969243"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Wainwright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3373807"
                        ],
                        "name": "Heinrich K\u00fcttler",
                        "slug": "Heinrich-K\u00fcttler",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "K\u00fcttler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heinrich K\u00fcttler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8455031"
                        ],
                        "name": "Andrew Lefrancq",
                        "slug": "Andrew-Lefrancq",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Lefrancq",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Lefrancq"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070088720"
                        ],
                        "name": "Simon Green",
                        "slug": "Simon-Green",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Green",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093272753"
                        ],
                        "name": "V\u00edctor Vald\u00e9s",
                        "slug": "V\u00edctor-Vald\u00e9s",
                        "structuredName": {
                            "firstName": "V\u00edctor",
                            "lastName": "Vald\u00e9s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V\u00edctor Vald\u00e9s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49813280"
                        ],
                        "name": "A. Sadik",
                        "slug": "A.-Sadik",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Sadik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sadik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4337102"
                        ],
                        "name": "Julian Schrittwieser",
                        "slug": "Julian-Schrittwieser",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Schrittwieser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Schrittwieser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152450665"
                        ],
                        "name": "Keith Anderson",
                        "slug": "Keith-Anderson",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keith Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143981350"
                        ],
                        "name": "Sarah York",
                        "slug": "Sarah-York",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "York",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sarah York"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38974617"
                        ],
                        "name": "Max Cant",
                        "slug": "Max-Cant",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Cant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Cant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055913310"
                        ],
                        "name": "Adam Cain",
                        "slug": "Adam-Cain",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Cain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Cain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34848283"
                        ],
                        "name": "A. Bolton",
                        "slug": "A.-Bolton",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Bolton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bolton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50768854"
                        ],
                        "name": "Stephen Gaffney",
                        "slug": "Stephen-Gaffney",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gaffney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gaffney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143776287"
                        ],
                        "name": "Helen King",
                        "slug": "Helen-King",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helen King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34313265"
                        ],
                        "name": "S. Legg",
                        "slug": "S.-Legg",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Legg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Legg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48348688"
                        ],
                        "name": "Stig Petersen",
                        "slug": "Stig-Petersen",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Petersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stig Petersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "DeepMind Lab [34] or Doom [16]), to more complex, realistic environments (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Collapsing Object Labels: Object types that are visually very similar (e.g. \u2018teapot\u2019 and \u2018coffee_kettle\u2019) or semantically hierarchical in relation (e.g. \u2018bread\u2019 and \u2018food\u2019) introduce unwanted ambiguity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 195
                            }
                        ],
                        "text": "There are several interactive environments commonly used in the community, ranging from simple 2D grid-worlds (e.g. XWORLD [27]), to 3D game-like environments with limited realism (e.g. DeepMind Lab [34] or Doom [16]), to more complex, realistic environments (e.g. AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3221395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5c68d0d01ace4a7a5f4d3e0a3fccc42a7d1f354",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community."
            },
            "slug": "DeepMind-Lab-Beattie-Leibo",
            "title": {
                "fragments": [],
                "text": "DeepMind Lab"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems that is powered by a fast and widely recognised game engine, and tailored for effective use by the research community."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3340170"
                        ],
                        "name": "S. Song",
                        "slug": "S.-Song",
                        "structuredName": {
                            "firstName": "Shuran",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807197"
                        ],
                        "name": "F. Yu",
                        "slug": "F.-Yu",
                        "structuredName": {
                            "firstName": "Fisher",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38591293"
                        ],
                        "name": "Andy Zeng",
                        "slug": "Andy-Zeng",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Zeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145830541"
                        ],
                        "name": "Angel X. Chang",
                        "slug": "Angel-X.-Chang",
                        "structuredName": {
                            "firstName": "Angel",
                            "lastName": "Chang",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angel X. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295141"
                        ],
                        "name": "M. Savva",
                        "slug": "M.-Savva",
                        "structuredName": {
                            "firstName": "Manolis",
                            "lastName": "Savva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Savva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807080"
                        ],
                        "name": "T. Funkhouser",
                        "slug": "T.-Funkhouser",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Funkhouser",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Funkhouser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "House3D converts SUNCG from a static 3D dataset to a set of simulated environments, where an agent (approximated as a 1 meter high cylinder) may navigate under simple physical constraints (not being able to pass through walls/objects)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 24
                            }
                        ],
                        "text": "(a) Sample environments [1, 8] garage kitchen elevator office"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 49
                            }
                        ],
                        "text": "First, we only consider environments where all 3 SUNCG annotators consider the scene layout realistic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "We instantiate EmbodiedQA in House3D [1], a recently introduced rich, simulated environment based on 3D indoor scenes from the SUNCG dataset [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 34
                            }
                        ],
                        "text": "We exclude objects and rooms from SUNCG that are obscure (e.g. \u2018loggia\u2019 rooms) or difficult to visually resolve (e.g. tiny objects like \u2018light switches\u2019)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "In total, SUNCG contains over 45k environments with 49k valid floors, 404k rooms containing 5 million object instances of 2644 unique objects from 80 different categories."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 186
                            }
                        ],
                        "text": "In theory, we now have the ability to automatically generate all valid questions with associated answers by executing the functional programs on the environment\u2019s annotations provided by SUNCG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "\u2022 We evaluate our agents in House3D [1], a rich, interactive 3D environment based on human-designed indoor scenes from SUNCG [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "Figure 2: The EQA dataset is built on a subset of House3D environments [1] and objects from the SUNCG [8] dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "Concretely, SUNCG consists of synthetic 3D scenes with realistic room and furniture layouts, manually designed and crowdsourced using an online interior design interface (Planner5D [38])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20416090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a05db7a75c65ee61c3ca7a6e5401b946166290d",
            "isKey": true,
            "numCitedBy": 812,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created largescale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task. The dataset and code is available at http://sscnet.cs.princeton.edu."
            },
            "slug": "Semantic-Scene-Completion-from-a-Single-Depth-Image-Song-Yu",
            "title": {
                "fragments": [],
                "text": "Semantic Scene Completion from a Single Depth Image"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The semantic scene completion network (SSCNet) is introduced, an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836466"
                        ],
                        "name": "Linda B. Smith",
                        "slug": "Linda-B.-Smith",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Smith",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda B. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2740806"
                        ],
                        "name": "M. Gasser",
                        "slug": "M.-Gasser",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gasser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gasser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7107473,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "25f8e9e35cafd7fb686d939f274111bcffeafd6b",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity. We offer six lessons for developing embodied intelligent agents suggested by research in developmental psychology. We argue that starting as a baby grounded in a physical, social, and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind."
            },
            "slug": "The-Development-of-Embodied-Cognition:-Six-Lessons-Smith-Gasser",
            "title": {
                "fragments": [],
                "text": "The Development of Embodied Cognition: Six Lessons from Babies"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is argued that starting as a baby grounded in a physical, social, and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8224916,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04cca8e341a5da42b29b0bc831cb25a0f784fa01",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data."
            },
            "slug": "Adaptive-Computation-Time-for-Recurrent-Neural-Graves",
            "title": {
                "fragments": [],
                "text": "Adaptive Computation Time for Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Performance is dramatically improved and insight is provided into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences, which suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 14
                            }
                        ],
                        "text": "We train with REINFORCE [40] with a running average of\nthe reward as baseline."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "We train with REINFORCE [40] with a running average of Figure 5: Sample trajectories from PACMAN-RL projected on a floor plan (white areas are unoccupiable) and on-path egocentric views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2332513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "isKey": false,
            "numCitedBy": 5180,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms."
            },
            "slug": "Simple-statistical-gradient-following-algorithms-Williams",
            "title": {
                "fragments": [],
                "text": "Simple statistical gradient-following algorithms for connectionist reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units that are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reInforcement tasks, and they do this without explicitly computing gradient estimates."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1868677"
                        ],
                        "name": "D. Harada",
                        "slug": "D.-Harada",
                        "structuredName": {
                            "firstName": "Daishi",
                            "lastName": "Harada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5730166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94066dc12fe31e96af7557838159bde598cb4f10",
            "isKey": false,
            "numCitedBy": 1602,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates conditions under which modi cations to the reward function of a Markov decision process preserve the op timal policy It is shown that besides the positive linear transformation familiar from utility theory one can add a reward for tran sitions between states that is expressible as the di erence in value of an arbitrary poten tial function applied to those states Further more this is shown to be a necessary con dition for invariance in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP These results shed light on the practice of reward shap ing a method used in reinforcement learn ing whereby additional training rewards are used to guide the learning agent In par ticular some well known bugs in reward shaping procedures are shown to arise from non potential based rewards and methods are given for constructing shaping potentials corresponding to distance based and subgoal based heuristics We show that such po tentials can lead to substantial reductions in learning time"
            },
            "slug": "Policy-Invariance-Under-Reward-Transformations:-and-Ng-Harada",
            "title": {
                "fragments": [],
                "text": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Conditions under which modi cations to the reward function of a Markov decision process preserve the op timal policy are investigated to shed light on the practice of reward shap ing a method used in reinforcement learn ing whereby additional training rewards are used to guide the learning agent."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "We use Adam [42] with a learning rate of 10 \u03013 and a batch size of 20."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "We use Adam [42] with a learning rate of 10\u00b43 and a batch size of 20."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90052,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798372"
                        ],
                        "name": "Iro Armeni",
                        "slug": "Iro-Armeni",
                        "structuredName": {
                            "firstName": "Iro",
                            "lastName": "Armeni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iro Armeni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47247217"
                        ],
                        "name": "S. Sax",
                        "slug": "S.-Sax",
                        "structuredName": {
                            "firstName": "Sasha",
                            "lastName": "Sax",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sax"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40029556"
                        ],
                        "name": "A. Zamir",
                        "slug": "A.-Zamir",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Zamir",
                            "middleNames": [
                                "Roshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zamir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702137"
                        ],
                        "name": "S. Savarese",
                        "slug": "S.-Savarese",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Savarese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Savarese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "AI2-THOR [18], Matterport3D [31], Stanford 2D-3D-S [35])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2730848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42b1cb0030e174ba4395d987df77cfa6d112d221",
            "isKey": false,
            "numCitedBy": 476,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\\deg} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: this http URL"
            },
            "slug": "Joint-2D-3D-Semantic-Data-for-Indoor-Scene-Armeni-Sax",
            "title": {
                "fragments": [],
                "text": "Joint 2D-3D-Semantic Data for Indoor Scene Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations, enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large- scale indoor spaces."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "input that will maximize answer con\ufb01dence. Visual Navigation: Vision + Action. The problem of navigating in an environment based on visual perception has long been studied in vision and robotics (see [16] for an extensive survey). Classical techniques divide navigation into two distinct phases \u2013 mapping (where visual observations are used to construct a 3D model of the environment), and planning (whic"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14552983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f8d7bdfc3ed0ff793f1236730486b3d5cf946aa",
            "isKey": false,
            "numCitedBy": 5097,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Planning and navigation algorithms exploit statistics gleaned from uncertain, imperfect real-world environments to guide robots toward their goals and around obstacles."
            },
            "slug": "Probabilistic-robotics-Thrun",
            "title": {
                "fragments": [],
                "text": "Probabilistic robotics"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This research presents a novel approach to planning and navigation algorithms that exploit statistics gleaned from uncertain, imperfect real-world environments to guide robots toward their goals and around obstacles."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "House3D converts SUNCG from a static 3D dataset to a set of simulated environments, where an agent (approximated as a 1 meter high cylinder) may navigate under simple physical constraints (not being able to pass through walls/objects)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "We have an encoder network that transforms the egocentric RGB image from the House3D renderer [1] to a fixed-size representation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "\u2022 We integrated House3D with Amazon Mechanical Turk (AMT), allowing humans to remotely operate the agent in real time, and collected expert demonstrations of questionguided navigation for EmbodiedQA that serve as a benchmark to compare our proposed and other future models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 104
                            }
                        ],
                        "text": "We thank the developers of PyTorch [41] for building an excellent framework, and Yuxin Wu for help with House3D environments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 25
                            }
                        ],
                        "text": "In this work, we use the House3D [1] environment as it strikes a useful middle-ground between being sufficiently realistic and providing access to a large and diverse set of room layouts and object categories."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 42
                            }
                        ],
                        "text": "We develop EQA, a dataset of questions in House3D environments, and a novel hierarchical neural model, trained via imitation and reinforcement learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "In the descriptions that follow, an \u2018entity\u2019 can refer to either a queryable room or a queryable object from the House3D [1] environment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 29
                            }
                        ],
                        "text": "We instantiate EmbodiedQA in House3D [1], a recently introduced rich, simulated environment based on 3D indoor scenes from the SUNCG dataset [8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 54
                            }
                        ],
                        "text": "Our agent takes egocentric 224\u02c6224 RGB images from the House3D renderer as input, which we process with a CNN consisting of 4 t5\u02c65 Conv, BatchNorm, ReLU, 2\u02c62 MaxPoolu blocks, producing a fixed-size representation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 73
                            }
                        ],
                        "text": "\u2022 We introduce EQA, a dataset of visual questions and answers grounded in House3D."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 27
                            }
                        ],
                        "text": "\u2022 We evaluate our agents in House3D [1], a rich, interactive 3D environment based on human-designed indoor scenes from SUNCG [8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 63
                            }
                        ],
                        "text": "In this work, we develop a dataset of questions and answers in House3D environments [1], evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 53
                            }
                        ],
                        "text": "We build EQA on a pruned subset of environments from House3D."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 212
                            }
                        ],
                        "text": "Treating the above CNN as a shared base encoder network, we train multiple decoder heads for 1) RGB reconstruction, 2) semantic segmentation, and 3) depth estimation (annotations for which are available from the House3D renderer)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Y"
            },
            "venue": {
                "fragments": [],
                "text": "Tian, \u201cBuilding generalizable agents with a realistic and rich 3D environment,\u201d arXiv preprint arXiv:1801.02209"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 80
                            }
                        ],
                        "text": "Hierarchical modeling has recently shown promise in deep reinforcement learning [22,28,36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "\u2019), and is reminiscent of hierarchical RL [22, 28, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A deep hierarchical approach to lifelong learn- 2143  ing in minecraft"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI, 2017. 3, 6"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Planner5d"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 42
                            }
                        ],
                        "text": "\u2019), and is reminiscent of hierarchical RL (Andreas et al., 2017; Oh et al., 2017; Tessler et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Zeroshot task generalization with multi-task deep reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML."
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 80
                            }
                        ],
                        "text": "Hierarchical modeling has recently shown promise in deep reinforcement learning [22,28,36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "\u2019), and is reminiscent of hierarchical RL [22, 28, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A deep hierarchical approach to lifelong learn- 2175  ing in minecraft"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI, 2017. 3, 6"
            },
            "year": 2017
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 26,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 45,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Embodied-Question-Answering-Das-Datta/e5790afc079c6f36d6fe9235d6d253f3da631f51?sort=total-citations"
}