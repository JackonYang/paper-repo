{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 199
                            }
                        ],
                        "text": "In practice, this maximization has been performed by a dual-loop approach in which the outer loop optimizes \u03b8 by a convex optimization algorithm, and the inner loop optimizes q by belief propagation [15, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Large graphical models are becoming increasingly common in applications including computer vision, relational learning [15], and natural language processing [18, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2282762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc36397e1fef5c922d64e88211a7e08ecc64759",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies."
            },
            "slug": "Discriminative-Probabilistic-Models-for-Relational-Taskar-Abbeel",
            "title": {
                "fragments": [],
                "text": "Discriminative Probabilistic Models for Relational Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach is presented, showing how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274348"
                        ],
                        "name": "Khashayar Rohanimanesh",
                        "slug": "Khashayar-Rohanimanesh",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Rohanimanesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khashayar Rohanimanesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 68
                            }
                        ],
                        "text": "The first loopy model we consider is thefactorial CRF introduced by Sutton, Rohanimanesh, and McCallum [13]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 199
                            }
                        ],
                        "text": "In practice, this maximization has been performed by a dual-loop approach in which the outer loop optimizes \u03b8 by a convex optimization algorithm, and the inner loop optimizes q by belief propagation [15, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "The first loopy model we consider is the factorial CRF introduced by Sutton, Rohanimanesh, and McCallum [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "We evaluate piecewise training on three models used in previous work: a linear-chain CRF [9], a factorial CRF [14], and a skip-chain CRF [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6038991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0329663498462521483612649c0dffc85d9d9419",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "slug": "Dynamic-conditional-random-fields:-factorized-for-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "On a natural-language chunking task, it is shown that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Wainwright, Jaakkola, and Willsky [16] introduce a class of upper bounds onA(\u03b8) that arise immediately from its convexity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 0
                            }
                        ],
                        "text": "Wainwright, Jaakkola, and Willsky [15] introduce a class of upper bounds onA(\u03b8) that arise immediately from its convexity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 71
                            }
                        ],
                        "text": "To collapse the dimensionality of the optimization problem, therefore, Wainwright, Jaakkola, and Willsky use the Lagrange dual of Equation 16, which can then be optimized using either standard optimization techniques, or a message passing algorithm similar to to BP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 109
                            }
                        ],
                        "text": "For concreteness, think ofT as the set of all spanning trees of G; this is in fact the special case to which Wainwright, Jaakkola, and Willsky devote their attention."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 114
                            }
                        ],
                        "text": "Although this bound can be proved directly, it can also be derived from the variational upper bounds presented by Wainwright, Jaakkola, and Willsky [15], a connection which motivates several generalizations of the basic piecewise procedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Our bound is derived from the upper bounds introduced by Wainwright, Jaakkola, and Willsky [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 106
                            }
                        ],
                        "text": "In this section, we sketch another proof of Proposition 1, deriving it from the tree-reweighted bounds of Wainwright, Jaakkola, and Willsky [15], a connection which suggests generalizations of the simple piecewise training procedure."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5749684,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f178383deb578992b2a62844a08a6451cbad16ed",
            "isKey": true,
            "numCitedBy": 453,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum-product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum-product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants."
            },
            "slug": "A-new-class-of-upper-bounds-on-the-log-partition-Wainwright-Jaakkola",
            "title": {
                "fragments": [],
                "text": "A new class of upper bounds on the log partition function"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new class of upper bounds on the log partition function of a Markov random field (MRF) is introduced, based on concepts from convex duality and information geometry, and the Legendre mapping between exponential and mean parameters is exploited."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " 5.2 FACTORIAL CRF The \ufb01rst loopy model we consider is the factorial CRF introduced by Sutton, Rohanimanesh, and McCallum [13]. Factorial CRFs are the conditionally-trained analogue of factorial HMMs [6]; it consists of a series of undirected linear chains with connections between cotemporal labels. This is a natural model for jointly performing multiple dependent sequence labeling tasks. speech tags"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 519313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78e510627d3f28601e370212bf063bbfa539ebed",
            "isKey": true,
            "numCitedBy": 1200,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable\u2014the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward\u2013backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot."
            },
            "slug": "Factorial-Hidden-Markov-Models-Ghahramani-Jordan",
            "title": {
                "fragments": [],
                "text": "Factorial Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner, and a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": "This allows inclusion of rich, overlapping features ofx without needing to model their distribution, which can greatly improve performance [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "We evaluate piecewise training on three models used in previous work: a linear-chain CRF [9], a factorial CRF [14], and a skip-chain CRF [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13408,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798719"
                        ],
                        "name": "Yuhong Guo",
                        "slug": "Yuhong-Guo",
                        "structuredName": {
                            "firstName": "Yuhong",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhong Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686063"
                        ],
                        "name": "R. Greiner",
                        "slug": "R.-Greiner",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Greiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Greiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Because the piecewise estimator is such an intuitively appealing method, it has been used in several scattered places in the literature, for tasks such as information extraction [18], collective classification [8], and computer vision [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1884202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "400b8ce909d235f61e96f595a21f6487b91b3715",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach to ensemble classification that requires learning only a single base classifier. The idea is to learn a classifier that simultaneously predicts pairs of test labels--as opposed to learning multiple predictors for single test labels-- then coordinating the assignment of individual labels by propagating beliefs on a graph over the data. We argue that the approach is statistically well motivated, even for independent identically distributed (iid) data. In fact, we present experimental results that show improvements in classification accuracy over single-example classifiers, across a range of iid data sets and over a set of base classifiers. Like boosting, the technique increases representational capacity while controlling variance through a principled form of classifier combination."
            },
            "slug": "Learning-Coordination-Classifiers-Guo-Greiner",
            "title": {
                "fragments": [],
                "text": "Learning Coordination Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new approach to ensemble classification that requires learning only a single base classifier that simultaneously predicts pairs of test labels and increases representational capacity while controlling variance through a principled form of classifier combination is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214361"
                        ],
                        "name": "B. Gidas",
                        "slug": "B.-Gidas",
                        "structuredName": {
                            "firstName": "Basilis",
                            "lastName": "Gidas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gidas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "An interesting question is whether the proofs of consistency for pseudolikelihood [7] can be adapted to the piecewise estimator."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120080803,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "610ad8066e3874f4201a8c69a83f1ce25273c10c",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove that the Maximum Likelihood and Pseudo-likelihood estimators for the parameters of Gibbs distributions (equivalently Markov Random Fields) over \u2124d, d\u2265l, are consistent even at points of \u201cfirst\u201d or \u201chigher-order\u201d phase transitions. The distributions are parametrized by points in a finite-dimensional Euclidean space \u211dm, m\u2265l, and the single spin state space is either a finite set or a compact metric space. Also, the underlying interactions need not be of finite range."
            },
            "slug": "Consistency-of-Maximum-Likelihood-and-Estimators-Gidas",
            "title": {
                "fragments": [],
                "text": "Consistency of Maximum Likelihood and Pseudo-Likelihood Estimators for Gibbs Distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40400230"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "We use a linear-chain CRF, whose features are described elsewhere [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11664683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b",
            "isKey": false,
            "numCitedBy": 1233,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character n-grams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998)."
            },
            "slug": "Early-results-for-Named-Entity-Recognition-with-and-McCallum-Li",
            "title": {
                "fragments": [],
                "text": "Early results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that conditionally-trained models, such as conditional maximum entropy models, handle inter-dependent features of greedy sequence modeling in NLP well."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "On three natural-language data sets, piecewise training is more accurate than pseudolikelihood, and often performs comparably to global training using belief propagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "For each tractable graphTR, let \u03b8(TR) be an exponential parameter vector that has the same dimensionality as\u03b8, but respects the structureof TR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "If the training data isD = {(x(i),y(i))}, then the piecewise CRF objective function is\n`PW(\u039b) = \u2211\ni \u2211 st \u2211 k \u03bbkfk(y(i)s , y (i) t ,x (i))\n\u2212 \u2211\ni \u2211 st A ( x(i); \u039b ) , (13)\nwhere the local normalization factors are\nA ( x(i); \u039b ) = log \u2211 ys,yt exp {\u2211 k \u03bbkfk(ys, yt,x(i)) } ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116757950,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1406b6d771c270aff4dcb1c96e4f5c62c02c00a5",
            "isKey": true,
            "numCitedBy": 1657,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In rather formal terms, the situation with which this paper is concerned may be described as follows. We are given a fixed system of n sites, labelled by the first n positive integers, and an associated vector x of observations, Xi, . . ., Xn, which, in turn, is presumed to be a realization of a vector X of (dependent) random variables, Xi, . . ., X.. In practice, the sites may represent points or regions in space and the random variables may be either continuous or discrete. The main statistical objectives are the following: firstly, to provide a means of using the available concomitant information, particularly the configuration of the sites, to attach a plausible probability distribution to the random vector X; secondly, to estimate any unknown parameters in the distribution from the realization x; thirdly, where possible, to quantify the extent of disagreement between hypothesis and observation."
            },
            "slug": "Statistical-Analysis-of-Non-Lattice-Data-Besag",
            "title": {
                "fragments": [],
                "text": "Statistical Analysis of Non-Lattice Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891751"
                        ],
                        "name": "E. Pasztor",
                        "slug": "E.-Pasztor",
                        "structuredName": {
                            "firstName": "Egon",
                            "lastName": "Pasztor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Pasztor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575670"
                        ],
                        "name": "Owen Carmichael",
                        "slug": "Owen-Carmichael",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Carmichael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Carmichael"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 235
                            }
                        ],
                        "text": "Because the piecewise estimator is such an intuitively appealing method, it has been used in several scattered places in the literature, for tasks such as information extraction [18], collective classification [8], and computer vision [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1414109,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "861897df39716877fb1e03a7d09a234faca076e9",
            "isKey": false,
            "numCitedBy": 1218,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a learning-based method for low-level vision problems\u2014estimating scenes from images. We generate a synthetic world of scenes and their corresponding rendered images, modeling their relationships with a Markov network. Bayesian belief propagation allows us to efficiently find a local maximum of the posterior probability for the scene, given an image. We call this approach VISTA\u2014Vision by Image/Scene TrAining.We apply VISTA to the \u201csuper-resolution\u201d problem (estimating high frequency details from a low-resolution image), showing good results. To illustrate the potential breadth of the technique, we also apply it in two other problem domains, both simplified. We learn to distinguish shading from reflectance variations in a single image under particular lighting conditions. For the motion estimation problem in a \u201cblobs world\u201d, we show figure/ground discrimination, solution of the aperture problem, and filling-in arising from application of the same probabilistic machinery."
            },
            "slug": "Learning-Low-Level-Vision-Freeman-Pasztor",
            "title": {
                "fragments": [],
                "text": "Learning Low-Level Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A learning-based method for low-level vision problems\u2014estimating scenes from images with Bayesian belief propagation, applied to the \u201csuper-resolution\u201d problem (estimating high frequency details from a low-resolution image), showing good results."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "Finally, we consider a model with many irregular loops, which is the skip chain model introduced by Sutton and McCallum [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 100
                            }
                        ],
                        "text": "Finally, we consider a model with many irregular loops, which is the skip chain model introduced by Sutton and McCallum [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "We evaluate piecewise training on three models used in previous work: a linear-chain CRF [9], a factorial CRF [14], and a skip-chain CRF [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4488e3309517d119080e143f3729106e4bb6d444",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In information extraction, we often wish to identify all mentions of an entity, such as a person or organization. Traditionally, a group of words is labeled as an entity based only on local information. But information from throughout a document can be useful; for example, if the same word is used multiple times, it is likely to have the same label each time. We present a CRF that explicitly represents dependencies between the labels of pairs of similar words in a document. On a standard information extraction data set, we show that learning these dependencies leads to a 13.7% reduction in error on the field that had caused the most repetition errors."
            },
            "slug": "Collective-Segmentation-and-Labeling-of-Distant-in-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Collective Segmentation and Labeling of Distant Entities in Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents a CRF that explicitly represents dependencies between the labels of pairs of similar words in a document, and shows that learning these dependencies leads to a 13.7% reduction in error on the field that had caused the most repetition errors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139133"
                        ],
                        "name": "Razvan C. Bunescu",
                        "slug": "Razvan-C.-Bunescu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Bunescu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan C. Bunescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14182377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6f9cfb31ab8178456dbc897c812b2d7ad2986f7",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for \"collective information extraction\" that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach."
            },
            "slug": "Collective-Information-Extraction-with-Relational-Bunescu-Mooney",
            "title": {
                "fragments": [],
                "text": "Collective Information Extraction with Relational Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new IE method is presented that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions, which allows for \"collective information extraction\" that exploits the mutual influence between possible extractions."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "This data set is due to Dayne Freitag [5], and has been used in much previous work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8618254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58de638505046e7de5fe7cc0660b4c6d79247488",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems."
            },
            "slug": "Machine-Learning-for-Information-Extraction-in-Freitag",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Information Extraction in Informal Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multistrategy approach which combines these learners and yields performance competitive with or better than the best of them is described, which is modular and flexible, and could find application in other machine learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To simplify the exposition, in this section we assume that the pieces correspond to edges in a graphical model, but the ideas extend readily to more general pieces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8175,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782178"
                        ],
                        "name": "S. Buchholz",
                        "slug": "S.-Buchholz",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Buchholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Buchholz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To simplify the exposition, in this section we assume that the pieces correspond to edges in a graphical model, but the ideas extend readily to more general pieces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8940645,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9e85832b04cc3700c2c26d6ba93fdeae39cac04a",
            "isKey": false,
            "numCitedBy": 871,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance."
            },
            "slug": "Introduction-to-the-CoNLL-2000-Shared-Task-Chunking-Sang-Buchholz",
            "title": {
                "fragments": [],
                "text": "Introduction to the CoNLL-2000 Shared Task Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking is described."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107124"
                        ],
                        "name": "Ben Wellner",
                        "slug": "Ben-Wellner",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Wellner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Wellner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682136"
                        ],
                        "name": "Fuchun Peng",
                        "slug": "Fuchun-Peng",
                        "structuredName": {
                            "firstName": "Fuchun",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuchun Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145256244"
                        ],
                        "name": "Michael Hay",
                        "slug": "Michael-Hay",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Hay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 79054,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "3a58d9b2f82b86bbf65d8a20911e73b5c296ac30",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In a method and apparatus for making a pile-surfaced thermoplastic material by a tack-spin technique the laminate of backing web and adhered pile is hauled off from the heated drawing surface over a rod or bar which fixes the point at which the laminate separates from the heated drawing surface and which directs cooling fluid from an aperture in the rod or bar and onto the back of the backing web and into the fibril-forming area."
            },
            "slug": "An-Integrated,-Conditional-Model-of-Information-and-Wellner-McCallum",
            "title": {
                "fragments": [],
                "text": "An Integrated, Conditional Model of Information Extraction and Coreference with Appli"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "In a method and apparatus for making a pile-surfaced thermoplastic material by a tack-spin technique the laminated of backing web and adhered pile is hauled off from the heated drawing surface over a rod or bar which fixes the point at which the laminate separates from the heating surface."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 113
                            }
                        ],
                        "text": "As mentioned earlier, the most closely related procedure that has been studied statistically is pseudolikelihood [1, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "The piecewise estimator is also closely related to pseudolikelihood [1, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120503853,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ed91ce023b6500c586802de7d23d8f8f01e5aa1b",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficiency-of-pseudolikelihood-estimation-for-Besag",
            "title": {
                "fragments": [],
                "text": "Efficiency of pseudolikelihood estimation for simple Gaussian fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Jordan . Factorial hidden Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz . Building a large annotated corpus of English : The Penn Treebank . Computational Linguistics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Jordan . Factorial hidden Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tjong Kim Sang and Sabine Buchholz"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tjong Kim Sang and Sabine Buchholz"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Jordan . Factorial hidden Markov models"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Generative piecewise training is likely to be less useful in practice, because in later work [17], Wainwright, Jaakkola, and Willsky propose an approximate maximum-likelihood estimator based on Method Overall F1 Piecewise 91."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Treereweighted belief propagation and approximate ML estimation by pseudo-moment matching"
            },
            "venue": {
                "fragments": [],
                "text": "In Ninth Workshop on Artificial Intelligence and Statistics"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Piecewise-Training-for-Undirected-Models-Sutton-McCallum/beb0766d6233836ac1203b224930dc037e2b1dff?sort=total-citations"
}