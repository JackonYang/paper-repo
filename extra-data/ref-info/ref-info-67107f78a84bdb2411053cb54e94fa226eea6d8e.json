{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 89
                            }
                        ],
                        "text": ", 2006), and more generally the idea of initializing each layer by unsupervised learning (Bengio et al., 2007; Ranzato et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14201947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "isKey": false,
            "numCitedBy": 3433,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is biologically implausible and hurts gradient-based optimization (LeCun et al., 1998; Bengio and Glorot, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2010) while others investigated why the original training procedure for deep neural networks failed (Bengio and Glorot, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hence, our work proposes a new contribution to the trend of understanding and merging the performance gap between deep networks learnt with and without unsupervised pre-training (Erhan et al., 2010; Bengio and Glorot, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The hyperbolic tangent has a steady state at 0, and is therefore preferred from the optimization standpoint (LeCun et al., 1998; Bengio and Glorot, 2010), but it forces an antisymmetry around 0 which is absent in biological neurons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5575601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "isKey": false,
            "numCitedBy": 12431,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact)."
            },
            "slug": "Understanding-the-difficulty-of-training-deep-Glorot-Bengio",
            "title": {
                "fragments": [],
                "text": "Understanding the difficulty of training deep feedforward neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073603971"
                        ],
                        "name": "Vinod Nair",
                        "slug": "Vinod-Nair",
                        "structuredName": {
                            "firstName": "Vinod",
                            "lastName": "Nair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinod Nair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 85
                            }
                        ],
                        "text": "We can see the model as an exponential number of linear models that share parameters (Nair and Hinton, 2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 66
                            }
                        ],
                        "text": "This last conclusion is slightly different from those reported in (Nair and Hinton, 2010) in which is demonstrated that unsupervised pre-training with Restricted Boltzmann Machines and using rectifier units is beneficial."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 44
                            }
                        ],
                        "text": "The data has been preprocessed similarly to (Nair and Hinton, 2010): we subsampled the original 2\u00d7 108\u00d7 108 stereo-pair images to 2 \u00d7 32 \u00d7 32 and scaled linearly the image in the range [\u22121,1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15539264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "isKey": true,
            "numCitedBy": 12806,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors."
            },
            "slug": "Rectified-Linear-Units-Improve-Restricted-Boltzmann-Nair-Hinton",
            "title": {
                "fragments": [],
                "text": "Rectified Linear Units Improve Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units that learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391278"
                        ],
                        "name": "Richard Hans Robert Hahnloser",
                        "slug": "Richard-Hans-Robert-Hahnloser",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hahnloser",
                            "middleNames": [
                                "Hans",
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Hans Robert Hahnloser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 111
                            }
                        ],
                        "text": "Most previous studies of neural networks involving a rectifying activation function concern recurrent networks (Salinas and Abbott, 1996; Hahnloser, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8510723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a09ef3a49654fdbc7c446884369f8c9ca542b012",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-piecewise-analysis-of-networks-of-linear-Hahnloser",
            "title": {
                "fragments": [],
                "text": "On the piecewise analysis of networks of linear threshold neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 153
                            }
                        ],
                        "text": "Regarding the training of deep networks, something that can be considered a breakthrough happened in 2006, with the introduction of Deep Belief Networks (Hinton et al., 2006), and more generally the idea of initializing each layer by unsupervised learning (Bengio et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Deep Belief Networks\u306b\u95a2\u3057\u3066 [5]Hinton, G. E., Osindero, S., and Teh, Y. \u201dA fast learning algorithm for deep belief nets\u201d."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13408,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90841478"
                        ],
                        "name": "Y-Lan Boureau",
                        "slug": "Y-Lan-Boureau",
                        "structuredName": {
                            "firstName": "Y-Lan",
                            "lastName": "Boureau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y-Lan Boureau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5867279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41fef1a197fab9684a4608b725d3ae72e1ab4b39",
            "isKey": false,
            "numCitedBy": 818,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured."
            },
            "slug": "Sparse-Feature-Learning-for-Deep-Belief-Networks-Ranzato-Boureau",
            "title": {
                "fragments": [],
                "text": "Sparse Feature Learning for Deep Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation, and describes a novel and efficient algorithm to learn sparse representations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 137
                            }
                        ],
                        "text": "Dense distributed representations are the richest representations, being potentially exponentially more efficient than purely local ones (Bengio, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "One of the claimed objectives of deep learning algorithms (Bengio, 2009) is to disentangle the factors explaining the variations in the data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 188
                            }
                        ],
                        "text": "This paper is particularly inspired by the sparse representations learned in the context of auto-encoder variants, as they have been found to be very useful in training deep architectures (Bengio, 2009), especially for unsupervised pre-training of neural networks (Erhan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207178999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60ff004dde5c13ec53087872cfcdd12e85beb57",
            "isKey": false,
            "numCitedBy": 7558,
            "numCiting": 345,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks."
            },
            "slug": "Learning-Deep-Architectures-for-AI-Bengio",
            "title": {
                "fragments": [],
                "text": "Learning Deep Architectures for AI"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer modelssuch as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060013"
                        ],
                        "name": "Christopher S. Poultney",
                        "slug": "Christopher-S.-Poultney",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Poultney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher S. Poultney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 819006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932c2a02d462abd75af018125413b1ceaa1ee3f4",
            "isKey": false,
            "numCitedBy": 1182,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps."
            },
            "slug": "Efficient-Learning-of-Sparse-Representations-with-Ranzato-Poultney",
            "title": {
                "fragments": [],
                "text": "Efficient Learning of Sparse Representations with an Energy-Based Model"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A novel unsupervised method for learning sparse, overcomplete features using a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120495009"
                        ],
                        "name": "R. Douglas",
                        "slug": "R.-Douglas",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Douglas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Douglas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38086808"
                        ],
                        "name": "M. Mahowald",
                        "slug": "M.-Mahowald",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Mahowald",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mahowald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069280553"
                        ],
                        "name": "K. Martin",
                        "slug": "K.-Martin",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Martin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117311704"
                        ],
                        "name": "H. Suarez",
                        "slug": "H.-Suarez",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Suarez",
                            "middleNames": [
                                "E.",
                                "Ortiz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Suarez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 28
                            }
                        ],
                        "text": "The neuroscience literature (Bush and Sejnowski, 1995; Douglas and al., 2003) indicates that cortical neurons are rarely in their maximum saturation regime, and suggests that their activation function can be approximated by a rectifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1003907,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c0d8919c4ce74fe4ba0c1d7b051ab568cff8f409",
            "isKey": false,
            "numCitedBy": 952,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "The majority of synapses in the mammalian cortex originate from cortical neurons. Indeed, the largest input to cortical cells comes from neighboring excitatory cells. However, most models of cortical development and processing do not reflect the anatomy and physiology of feedback excitation and are restricted to serial feedforward excitation. This report describes how populations of neurons in cat visual cortex can use excitatory feedback, characterized as an effective \"network conductance\", to amplify their feedforward input signals and demonstrates how neuronal discharge can be kept proportional to stimulus strength despite strong, recurrent connections that threaten to cause runaway excitation. These principles are incorporated into models of cortical direction and orientation selectivity that emphasize the basic design principles of cortical architectures."
            },
            "slug": "Recurrent-excitation-in-neocortical-circuits-Douglas-Koch",
            "title": {
                "fragments": [],
                "text": "Recurrent excitation in neocortical circuits"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "How populations of neurons in cat visual cortex can use excitatory feedback, characterized as an effective \"network conductance\", to amplify their feedforward input signals is described and how neuronal discharge can be kept proportional to stimulus strength despite strong, recurrent connections that threaten to cause runaway excitation is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145437578"
                        ],
                        "name": "E. Salinas",
                        "slug": "E.-Salinas",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Salinas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Salinas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293717"
                        ],
                        "name": "L. Abbott",
                        "slug": "L.-Abbott",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Abbott",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Abbott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 111
                            }
                        ],
                        "text": "Most previous studies of neural networks involving a rectifying activation function concern recurrent networks (Salinas and Abbott, 1996; Hahnloser, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16049376,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1fd00cabd152d148f1b243f099f9ee9964bd008a",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual responses of neurons in parietal area 7a are modulated by a combined eye and head position signal in a multiplicative manner. Neurons with multiplicative responses can act as powerful computational elements in neural networks. In the case of parietal cortex, multiplicative gain modulation appears to play a crucial role in the transformation of object locations from retinal to body-centered coordinates. It has proven difficult to uncover single-neuron mechanisms that account for neuronal multiplication. Here we show that multiplicative responses can arise in a network model through population effects. Specifically, neurons in a recurrently connected network with excitatory connections between similarly tuned neurons and inhibitory connections between differently tuned neurons can perform a product operation on additive synaptic inputs. The results suggest that parietal responses may be based on this architecture."
            },
            "slug": "A-model-of-multiplicative-neural-responses-in-Salinas-Abbott",
            "title": {
                "fragments": [],
                "text": "A model of multiplicative neural responses in parietal cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that multiplicative responses can arise in a network model through population effects and suggest that parietal responses may be based on this architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779306"
                        ],
                        "name": "Chaitanya Ekanadham",
                        "slug": "Chaitanya-Ekanadham",
                        "structuredName": {
                            "firstName": "Chaitanya",
                            "lastName": "Ekanadham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaitanya Ekanadham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12589862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "202cbbf671743aefd380d2f23987bd46b9caaf97",
            "isKey": false,
            "numCitedBy": 1028,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or \"deep,\" structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both colinear (\"contour\") features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "slug": "Sparse-deep-belief-net-model-for-visual-area-V2-Lee-Ekanadham",
            "title": {
                "fragments": [],
                "text": "Sparse deep belief net model for visual area V2"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An unsupervised learning model is presented that faithfully mimics certain properties of visual area V2 and the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses, suggesting that this sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4433870"
                        ],
                        "name": "P. Lennie",
                        "slug": "P.-Lennie",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Lennie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lennie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 220
                            }
                        ],
                        "text": "\u2022 Studies on brain energy expense suggest that neurons encode information in a sparse and distributed way (Attwell and Laughlin, 2001), estimating the percentage of neurons active at the same time to be between 1 and 4% (Lennie, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11194811,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c2f75b6609210a7f33762653acb5b1e58b0c3779",
            "isKey": false,
            "numCitedBy": 904,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Cost-of-Cortical-Computation-Lennie",
            "title": {
                "fragments": [],
                "text": "The Cost of Cortical Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Current Biology"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 10
                            }
                        ],
                        "text": "\u2022 CIFAR10 (Krizhevsky and Hinton, 2009): 50k/5k/5k, 32\u00d7 32\u00d7 3 RGB images, 10 classes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18268744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "isKey": false,
            "numCitedBy": 17095,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images."
            },
            "slug": "Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Layers of Features from Tiny Images"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 107
                            }
                        ],
                        "text": "It was first introduced in computational neuroscience in the context of sparse coding in the visual system (Olshausen and Field, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 71
                            }
                        ],
                        "text": "A sparsity penalty has been used in several computational neuroscience (Olshausen and Field, 1997; Doi et al., 2006) and machine learning models (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 36
                            }
                        ],
                        "text": ", 2006) and machine learning models (Lee et al., 2007; Mairal et al., 2009), in particular for deep architectures (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e64a9960734215e2b1866ea3cb723ffa5585ac14",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "slug": "Efficient-sparse-coding-algorithms-Lee-Battle",
            "title": {
                "fragments": [],
                "text": "Efficient sparse coding algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These algorithms are applied to natural images and it is demonstrated that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34927843"
                        ],
                        "name": "Andrew M. Saxe",
                        "slug": "Andrew-M.-Saxe",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Saxe",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew M. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 120
                            }
                        ],
                        "text": ", 2008), and that they become increasingly invariant to factors of variation (such as camera movement) in higher layers (Goodfellow et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1808153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3137bc367c61c0e507a5e3c1f8caeb26f292d79f",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We find that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of \"deep\" vs. \"shallower\" representations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation metrics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms."
            },
            "slug": "Measuring-Invariances-in-Deep-Networks-Goodfellow-Le",
            "title": {
                "fragments": [],
                "text": "Measuring Invariances in Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A number of empirical tests are proposed that directly measure the degree to which these learned features are invariant to different input transformations and find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images and convolutional deep belief networks learn substantially more invariant Features in each layer."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 22
                            }
                        ],
                        "text": "A zero masking (as in (Vincent et al., 2008)) is used for the higher layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 127
                            }
                        ],
                        "text": "Nonetheless, certain difficulties arise when one wants to introduce rectifier activations into stacked denoising auto-encoders (Vincent et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 64
                            }
                        ],
                        "text": ", 2004), the models we used are stacked denoising auto-encoders (Vincent et al., 2008) with three hidden layers and 1000 units per layer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "Our work extends this for the case of pre-training using denoising autoencoders (Vincent et al., 2008) and provides an extensive empirical comparison of the rectifying activation function against the hyperbolic tangent on image classification benchmarks as well as an original derivation for the text application of sentiment analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207168299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843959ffdccf31c6694d135fad07425924f785b1",
            "isKey": false,
            "numCitedBy": 5468,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite."
            },
            "slug": "Extracting-and-composing-robust-features-with-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Extracting and composing robust features with denoising autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 19
                            }
                        ],
                        "text": "In ICML \u201908, pages 1096-1103."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ACM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15796526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
            "isKey": false,
            "numCitedBy": 1726,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training."
            },
            "slug": "Why-Does-Unsupervised-Pre-training-Help-Deep-Erhan-Courville",
            "title": {
                "fragments": [],
                "text": "Why Does Unsupervised Pre-training Help Deep Learning?"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre- training."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2701388"
                        ],
                        "name": "Charles Dugas",
                        "slug": "Charles-Dugas",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Dugas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Dugas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083403739"
                        ],
                        "name": "F. B\u00e9lisle",
                        "slug": "F.-B\u00e9lisle",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "B\u00e9lisle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. B\u00e9lisle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081209689"
                        ],
                        "name": "C. Nadeau",
                        "slug": "C.-Nadeau",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Nadeau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nadeau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109840802"
                        ],
                        "name": "Ren\u00e9 Garcia",
                        "slug": "Ren\u00e9-Garcia",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren\u00e9 Garcia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 116
                            }
                        ],
                        "text": "To evaluate the potential impact of this effect we also investigate the softplus activation: softplus(x) = log(1+e) (Dugas et al., 2001), a smooth version of the rectifying non-linearity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13568249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbd9a15f75ab055b458bf4511b47b446d3b1328d",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints."
            },
            "slug": "Incorporating-Second-Order-Functional-Knowledge-for-Dugas-Bengio",
            "title": {
                "fragments": [],
                "text": "Incorporating Second-Order Functional Knowledge for Better Option Pricing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A class of functions similar to multi-layer neural networks but that has those properties of a universal approximator of continuous functions with these and other properties is proposed and applied to the task of modeling the price of call options."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35256,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415053"
                        ],
                        "name": "Shusen Zhou",
                        "slug": "Shusen-Zhou",
                        "structuredName": {
                            "firstName": "Shusen",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shusen Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159781"
                        ],
                        "name": "Qingcai Chen",
                        "slug": "Qingcai-Chen",
                        "structuredName": {
                            "firstName": "Qingcai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingcai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709719"
                        ],
                        "name": "Xiaolong Wang",
                        "slug": "Xiaolong-Wang",
                        "structuredName": {
                            "firstName": "Xiaolong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolong Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 15
                            }
                        ],
                        "text": "A recent study (Zhou et al., 2010) shows that Deep Belief Networks with binary units are competitive with the state-of-the-art methods for sentiment analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14908610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbb4c100d1ff3eceb84b3f18c974303eaa5e29fa",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel semi-supervised learning algorithm called Active Deep Networks (ADN), to address the semi-supervised sentiment classification problem with active learning. First, we propose the semi-supervised learning method of ADN. ADN is constructed by Restricted Boltzmann Machines (RBM) with unsupervised learning using labeled data and abundant of unlabeled data. Then the constructed structure is fine-tuned by gradient-descent based supervised learning with an exponential loss function. Second, we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data. Then ADN architecture is trained by the selected labeled data and all unlabeled data. Experiments on five sentiment classification datasets show that ADN outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification."
            },
            "slug": "Active-Deep-Networks-for-Semi-Supervised-Sentiment-Zhou-Chen",
            "title": {
                "fragments": [],
                "text": "Active Deep Networks for Semi-Supervised Sentiment Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Experiments show that ADN outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227028"
                        ],
                        "name": "Fr\u00e9d\u00e9ric Bastien",
                        "slug": "Fr\u00e9d\u00e9ric-Bastien",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bastien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fr\u00e9d\u00e9ric Bastien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47944877"
                        ],
                        "name": "Arnaud Bergeron",
                        "slug": "Arnaud-Bergeron",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Bergeron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnaud Bergeron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395619597"
                        ],
                        "name": "Nicolas Boulanger-Lewandowski",
                        "slug": "Nicolas-Boulanger-Lewandowski",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Boulanger-Lewandowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Boulanger-Lewandowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327616"
                        ],
                        "name": "Y. Chherawala",
                        "slug": "Y.-Chherawala",
                        "structuredName": {
                            "firstName": "Youssouf",
                            "lastName": "Chherawala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chherawala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5723508"
                        ],
                        "name": "Moustapha Ciss\u00e9",
                        "slug": "Moustapha-Ciss\u00e9",
                        "structuredName": {
                            "firstName": "Moustapha",
                            "lastName": "Ciss\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moustapha Ciss\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39977229"
                        ],
                        "name": "Myriam C\u00f4t\u00e9",
                        "slug": "Myriam-C\u00f4t\u00e9",
                        "structuredName": {
                            "firstName": "Myriam",
                            "lastName": "C\u00f4t\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Myriam C\u00f4t\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35215951"
                        ],
                        "name": "Jeremy Eustache",
                        "slug": "Jeremy-Eustache",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Eustache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremy Eustache"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090922238"
                        ],
                        "name": "X. Muller",
                        "slug": "X.-Muller",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2619783"
                        ],
                        "name": "Sylvain Pannetier Lebeuf",
                        "slug": "Sylvain-Pannetier-Lebeuf",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Lebeuf",
                            "middleNames": [
                                "Pannetier"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sylvain Pannetier Lebeuf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425018"
                        ],
                        "name": "S. Rifai",
                        "slug": "S.-Rifai",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Rifai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rifai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918629"
                        ],
                        "name": "F. Savard",
                        "slug": "F.-Savard",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Savard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Savard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057963758"
                        ],
                        "name": "Guillaume Sicard",
                        "slug": "Guillaume-Sicard",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Sicard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Sicard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7425863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9724027a887fef5462600902ab83ad73403e73d",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent theoretical and empirical work in statistical machine learning has demonstrated the importance of learning algorithms for deep architectures, i.e., function classes obtained by composing multiple non-linear transformations. Self-taught learning (exploiting unlabeled examples or examples from other distributions) has already been applied to deep learners, but mostly to show the advantage of unlabeled examples. Here we explore the advantage brought by out-of-distribution examples. For this purpose we developed a powerful generator of stochastic variations and noise processes for character images, including not only affine transformations but also slant, local elastic deformations, changes in thickness, background images, grey level changes, contrast, occlusion, and various types of noise. The out-of-distribution examples are obtained from these highly distorted images or by including examples of object classes different from those in the target test set. We show that deep learners benefit more from out-of-distribution examples than a corresponding shallow learner, at least in the area of handwritten character recognition. In fact, we show that they beat previously published results and reach human-level performance on both handwritten digit classification and 62-class handwritten character recognition."
            },
            "slug": "Deep-Self-Taught-Learning-for-Handwritten-Character-Bastien-Bengio",
            "title": {
                "fragments": [],
                "text": "Deep Self-Taught Learning for Handwritten Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that deep learners benefit more from out-of-distribution examples than a corresponding shallow learner, at least in the area of handwritten character recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2599292"
                        ],
                        "name": "J. Mairal",
                        "slug": "J.-Mairal",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Mairal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mairal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699339"
                        ],
                        "name": "G. Sapiro",
                        "slug": "G.-Sapiro",
                        "structuredName": {
                            "firstName": "Guillermo",
                            "lastName": "Sapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 36
                            }
                        ],
                        "text": ", 2006) and machine learning models (Lee et al., 2007; Mairal et al., 2009), in particular for deep architectures (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 95
                            }
                        ],
                        "text": "It has been a key element of deep convolutional networks exploiting a variant of auto-encoders (Ranzato et al., 2007, 2008; Mairal et al., 2009) with a sparse distributed representation, and has also become a key ingredient in Deep Belief Networks (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 682971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d65ba8bb20ae6dd001b9833c525c279dfe18916",
            "isKey": false,
            "numCitedBy": 1126,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "It is now well established that sparse signal models are well suited for restoration tasks and can be effectively learned from audio, image, and video data. Recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones. This paper proposes a new step in that direction, with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and discriminative class models. The linear version of the proposed model admits a simple probabilistic interpretation, while its most general variant admits an interpretation in terms of kernels. An optimization framework for learning all the components of the proposed model is presented, along with experimental results on standard handwritten digit and texture classification tasks."
            },
            "slug": "Supervised-Dictionary-Learning-Mairal-Bach",
            "title": {
                "fragments": [],
                "text": "Supervised Dictionary Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel sparse representation for signals belonging to different classes in terms of a shared dictionary and discriminative class models is proposed, with results on standard handwritten digit and texture classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001451"
                        ],
                        "name": "M. Gutnick",
                        "slug": "M.-Gutnick",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gutnick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gutnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612375"
                        ],
                        "name": "I. M\u00f3dy",
                        "slug": "I.-M\u00f3dy",
                        "structuredName": {
                            "firstName": "Istv\u00e1n",
                            "lastName": "M\u00f3dy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. M\u00f3dy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 84789945,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "44c70887fe5f27cbe2b0456d9b631893bf73f36e",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "PART I: GENERAL OVERVIEW 1. Thirty Years Among Cortical Neurons PART II: A CELLULAR AND SYNAPTIC PERSPECTIVE OF THE CORTICAL NEURON 2. Cortical Synaptic Transmission: An Overview 3. The Cortical Neuron as an Electrophysiological Unit 4. Signal Transduction and Protein Phosphorylation 5. Structure and Function of Ligand-Gated Channels 6. Synaptic Actions of Amino Acid Neurotransmitters 7. Synaptic Plasticity in Hippocampus and Neocortex: A Comparison PART III: THE CORTICAL NEURON AS PART OF A NETWORK 8. Overview: Basic Elements of the Cortical Network 9. Functions of Local Circuits in Neocortex: Synchrony and Laminae 10. Inhibition in the Cortical Network 11. Thalamocortical Interactions 12. Models of Cortical Networks PART IV: THE DEVELOPING CORTICAL NEURON 13. Determination of Cellular Phenotype and the Fundamental Organization of Cortical Layering: An Overview 14. Neurotransmitter Signaling Before the Birth of Neurons 15. Gene Control of Cortical Excitability 16. Development of Cortical Excitation and Inhibition 17. Cortical Epileptogenesis in the Developing Human Brain PART V. THE VULNERABLE CORTICAL NEURON 18. Glutamate Receptors and Neuronal Death: A Beginning 19. Measurement of Cortical Neurotransmitter Receptors with Radioligand Binding: Insights into the Mechanisms of Kindling-Induced Epilepsy 20. Pathophysiology of Cortical Synapses and Circuits 21. Pathophysiology of Cortical Areas from Human Epileptics 22. Clinical Strategies for Neuronal Protection"
            },
            "slug": "The-Cortical-Neuron-Gutnick-M\u00f3dy",
            "title": {
                "fragments": [],
                "text": "The Cortical Neuron"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Cortical Neuron as an Electrophysiological Unit and Cortical Epileptogenesis in the Developing Human Brain are studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077257730"
                        ],
                        "name": "Kevin Jarrett",
                        "slug": "Kevin-Jarrett",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Jarrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Jarrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206769720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f88427d7aa8225e47f946ac41a0667d7b69ac52",
            "isKey": false,
            "numCitedBy": 2085,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%)."
            },
            "slug": "What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "What is the best multi-stage architecture for object recognition?"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks and that two stages of feature extraction yield better accuracy than one."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852992"
                        ],
                        "name": "G. Kreiman",
                        "slug": "G.-Kreiman",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kreiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kreiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876834"
                        ],
                        "name": "Minjoon Kouh",
                        "slug": "Minjoon-Kouh",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Kouh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Kouh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202046"
                        ],
                        "name": "C. Cadieu",
                        "slug": "C.-Cadieu",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Cadieu",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cadieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3067508"
                        ],
                        "name": "U. Knoblich",
                        "slug": "U.-Knoblich",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Knoblich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Knoblich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 56
                            }
                        ],
                        "text": "This is particularly clear in the primate visual system (Serre et al., 2007), with its sequence of processing stages: detection of edges, primitive shapes, and moving up to gradually more complex visual shapes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10008231,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "8e11f636c9ac8d41a39e62d2dfe7c11ac31a0490",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-quantitative-theory-of-immediate-visual-Serre-Kreiman",
            "title": {
                "fragments": [],
                "text": "A quantitative theory of immediate visual recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Progress in brain research"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40386543"
                        ],
                        "name": "E. Doi",
                        "slug": "E.-Doi",
                        "structuredName": {
                            "firstName": "Eizaburo",
                            "lastName": "Doi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Doi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20968637"
                        ],
                        "name": "Doru-Cristian Balcan",
                        "slug": "Doru-Cristian-Balcan",
                        "structuredName": {
                            "firstName": "Doru-Cristian",
                            "lastName": "Balcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doru-Cristian Balcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 71
                            }
                        ],
                        "text": "A sparsity penalty has been used in several computational neuroscience (Olshausen and Field, 1997; Doi et al., 2006) and machine learning models (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 623629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1d428e32ede50c4d4854fd2f712eee44c96d676",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Biological sensory systems are faced with the problem of encoding a high-fidelity sensory signal with a population of noisy, low-fidelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and two-dimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of high-dimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets."
            },
            "slug": "A-Theoretical-Analysis-of-Robust-Coding-over-Noisy-Doi-Balcan",
            "title": {
                "fragments": [],
                "text": "A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A theoretical analysis is presented that characterizes the optimal linear coder and decoder for one- and two-dimensional data and shows how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2933783"
                        ],
                        "name": "D. Attwell",
                        "slug": "D.-Attwell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Attwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Attwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161090"
                        ],
                        "name": "S. Laughlin",
                        "slug": "S.-Laughlin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Laughlin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Laughlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 106
                            }
                        ],
                        "text": "\u2022 Studies on brain energy expense suggest that neurons encode information in a sparse and distributed way (Attwell and Laughlin, 2001), estimating the percentage of neurons active at the same time to be between 1 and 4% (Lennie, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9030676,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "bd7cc437f8e3ea9279061b7937b3b29dc5978452",
            "isKey": false,
            "numCitedBy": 2793,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Anatomic and physiologic data are used to analyze the energy expenditure on different components of excitatory signaling in the grey matter of rodent brain. Action potentials and postsynaptic effects of glutamate are predicted to consume much of the energy (47% and 34%, respectively), with the resting potential consuming a smaller amount (13%), and glutamate recycling using only 3%. Energy usage depends strongly on action potential rate\u2014an increase in activity of 1 action potential/cortical neuron/s will raise oxygen consumption by 145 mL/100 g grey matter/h. The energy expended on signaling is a large fraction of the total energy used by the brain; this favors the use of energy efficient neural codes and wiring patterns. Our estimates of energy usage predict the use of distributed codes, with \u226415% of neurons simultaneously active, to reduce energy consumption and allow greater computing power from a fixed number of neurons. Functional magnetic resonance imaging signals are likely to be dominated by changes in energy usage associated with synaptic currents and action potential propagation."
            },
            "slug": "An-Energy-Budget-for-Signaling-in-the-Grey-Matter-Attwell-Laughlin",
            "title": {
                "fragments": [],
                "text": "An Energy Budget for Signaling in the Grey Matter of the Brain"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The estimates of energy usage predict the use of distributed codes, with \u226415% of neurons simultaneously active, to reduce energy consumption and allow greater computing power from a fixed number of neurons."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of cerebral blood flow and metabolism : official journal of the International Society of Cerebral Blood Flow and Metabolism"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "For all experiments except on the NORB data (LeCun et al., 2004), the models we used are stacked denoising auto-encoders (Vincent et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 66
                            }
                        ],
                        "text": "\u2022 NORB: 233,172/58,428/58,320, taken from Jittered-Cluttered NORB (LeCun et al., 2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 712708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f354310098e09c1e1dc88758fca36767fd9d084d",
            "isKey": false,
            "numCitedBy": 1306,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."
            },
            "slug": "Learning-methods-for-generic-object-recognition-to-LeCun-Huang",
            "title": {
                "fragments": [],
                "text": "Learning methods for generic object recognition with invariance to pose and lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second and proved impractical, while convolutional nets yielded 16/7% error."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116927"
                        ],
                        "name": "John Blitzer",
                        "slug": "John-Blitzer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Blitzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Blitzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782853"
                        ],
                        "name": "Mark Dredze",
                        "slug": "Mark-Dredze",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Dredze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Dredze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 127
                            }
                        ],
                        "text": "Since no result has ever been published on the OpenTable data, we applied our model on the Amazon sentiment analysis benchmark (Blitzer et al., 2007) in order to assess the quality of our network with respect to literature methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14688775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d895647b4a80861703851ef55930a2627fe19492",
            "isKey": false,
            "numCitedBy": 2088,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains."
            },
            "slug": "Biographies,-Bollywood,-Boom-boxes-and-Blenders:-Blitzer-Dredze",
            "title": {
                "fragments": [],
                "text": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work extends to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50383069"
                        ],
                        "name": "T. Tao",
                        "slug": "T.-Tao",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "Sparsity has become a concept of interest, not only in computational neuroscience and machine learning but also in statistics and signal processing (Candes and Tao, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12605120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5e853572b2f3134acafa76d5ae80b9f28c7dca8",
            "isKey": false,
            "numCitedBy": 6682,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/|x/sub i/|) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:=|{i:e/sub i/ /spl ne/ 0}|/spl les//spl rho//spl middot/m for some /spl rho/>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail."
            },
            "slug": "Decoding-by-linear-programming-Cand\u00e8s-Tao",
            "title": {
                "fragments": [],
                "text": "Decoding by linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "F can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program) and numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144163566"
                        ],
                        "name": "Benjamin Snyder",
                        "slug": "Benjamin-Snyder",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Snyder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Snyder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 76
                            }
                        ],
                        "text": "Even though our tasks are identical, our database is larger than the one of (Snyder and Barzilay, 2007)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2279432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ab8faaa1ee1791e6da911e408dee97681cfa58e",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further conrm the strength of the model: the algorithm provides signicant improvement over both individual rankers and a state-of-the-art joint ranking model."
            },
            "slug": "Multiple-Aspect-Ranking-Using-the-Good-Grief-Snyder-Barzilay",
            "title": {
                "fragments": [],
                "text": "Multiple Aspect Ranking Using the Good Grief Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An algorithm is presented that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks, and it is proved that the agreementbased joint model is more expressive than individual ranking models."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145767633"
                        ],
                        "name": "D. Johnston",
                        "slug": "D.-Johnston",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Johnston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Johnston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 28
                            }
                        ],
                        "text": "The neuroscience literature (Bush and Sejnowski, 1995; Douglas and al., 2003) indicates that cortical neurons are rarely in their maximum saturation regime, and suggests that their activation function can be approximated by a rectifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 84509075,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5dc5ba0df6a4c6f1a7edfec52ffabfd7620ce476",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Cortical-Neuron-Johnston",
            "title": {
                "fragments": [],
                "text": "The Cortical Neuron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34782608"
                        ],
                        "name": "G. Orr",
                        "slug": "G.-Orr",
                        "structuredName": {
                            "firstName": "Genevieve",
                            "lastName": "Orr",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 71
                            }
                        ],
                        "text": "This is biologically implausible and hurts gradient-based optimization (LeCun et al., 1998; Bengio and Glorot, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 108
                            }
                        ],
                        "text": "The hyperbolic tangent has a steady state at 0, and is therefore preferred from the optimization standpoint (LeCun et al., 1998; Bengio and Glorot, 2010), but it forces an antisymmetry around 0 which is absent in biological neurons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 8
                            }
                        ],
                        "text": "\u2022 MNIST (LeCun et al., 1998): 50k/10k/10k, 28\u00d7 28 digit images, 10 classes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20158889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
            "isKey": false,
            "numCitedBy": 2630,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-BackProp-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Efficient BackProp"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 70
                            }
                        ],
                        "text": "This dataset is much larger and more difficult than the original NIST (Grother, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handprinted forms and character database, NIST special database 19"
            },
            "venue": {
                "fragments": [],
                "text": "National Institute of Standards and Technology (NIST) Intelligent Systems Division (NISTIR)."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "suring invariances in deep networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Opinion mining and sentiment analysis Also published as a book"
            },
            "venue": {
                "fragments": [],
                "text": "Foundations and Trends in Information Retrieval"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 99
                            }
                        ],
                        "text": "[7]Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. \u201dEfficient learning of sparse representations with an energy-based model\u201d."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Measuring invariances in deep networks. In NIPS'09"
            },
            "venue": {
                "fragments": [],
                "text": "Measuring invariances in deep networks. In NIPS'09"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning deep architectures for AI. Foundations and Trends in Machine Learning Also published as a book"
            },
            "venue": {
                "fragments": [],
                "text": "Learning deep architectures for AI. Foundations and Trends in Machine Learning Also published as a book"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 103
                            }
                        ],
                        "text": "\u2022 NISTP: 81920k/80k/20k, 32\u00d732 character images from the NIST database 19, with randomized distortions (Anonymous, 2010), 62 classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Anonymous title"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Anonymous University."
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 7,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 40,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-Sparse-Rectifier-Neural-Networks-Glorot-Bordes/67107f78a84bdb2411053cb54e94fa226eea6d8e?sort=total-citations"
}