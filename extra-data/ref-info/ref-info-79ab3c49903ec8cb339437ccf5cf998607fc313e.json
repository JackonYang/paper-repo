{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700433"
                        ],
                        "name": "St\u00e9phane Ross",
                        "slug": "St\u00e9phane-Ross",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "St\u00e9phane Ross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Ross and Bagnell (2010) provided an imitation learning example where J(\u03c0\u0302sup) = (1\u2212 T )J(\u03c0\u2217) + T 2 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "We here follow a similar proof to Ross and Bagnell (2010)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 264
                            }
                        ],
                        "text": "In particular, a classifier that makes a mistake with probability under the distribution of states/observations encountered by the expert can make as many as T 2 mistakes in expectation over T -steps under the distribution of states the classifier itself induces (Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 115
                            }
                        ],
                        "text": "Note that this is not as strong as the general error or regret reductions considered in (Beygelzimer et al., 2005; Ross and Bagnell, 2010; Daum\u00e9 III et al., 2009) which require only classification: we require a no-regret method or strongly convex surrogate loss function, a stronger (albeit common)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 168
                            }
                        ],
                        "text": "strations of good behavior are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 88
                            }
                        ],
                        "text": "Note that this is not as strong as the general error or regret reductions considered in (Beygelzimer et al., 2005; Ross and Bagnell, 2010; Daum\u00e9 III et al., 2009) which require only classification: we require a no-regret method or strongly convex surrogate loss function, a stronger (albeit common) assumption."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 73
                            }
                        ],
                        "text": "We here provide a theorem slightly more general than the one provided by Ross and Bagnell (2010) that applies to any policy \u03c0 that can guarantee surrogate loss under its own distribution of states."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 31
                            }
                        ],
                        "text": "Another approach called SMILe (Ross and Bagnell, 2010), similar to SEARN (Daum\u00e9 III et al., 2009) and CPI (Kakade and Langford, 2002), trains a stationary stochastic policy (a finite mixture of policies) by adding a new policy to the mixture at each iteration of training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 42
                            }
                        ],
                        "text": "For SMILe we choose parameter \u03b1 = 0.1 as in Ross and Bagnell (2010), and for DAGGER the parameter \u03b2i = I(i = 1) for I the indicator function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 1
                            }
                        ],
                        "text": "(Ross and Bagnell, 2010) Let Es\u223cd\u03c0\u2217 [`(s, \u03c0)] = , then J(\u03c0) \u2264 J(\u03c0\u2217) + T 2 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 75
                            }
                        ],
                        "text": "Ignoring this issue leads to poor performance both in theory and practice (Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 19
                            }
                        ],
                        "text": "SMILe, proposed by Ross and Bagnell (2010), alleviates this problem and can be applied in practice when T is large or undefined by adopting an approach similar to SEARN (Daum\u00e9 III et al., 2009) where a stochastic stationary policy is trained over several iterations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "The following two approaches from Ross and Bagnell (2010) achieve this on some classes of imitation learning problems, including all those where surrogate loss ` upper bounds C."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 45
                            }
                        ],
                        "text": "The forward training algorithm introduced by Ross and Bagnell (2010) trains a non-stationary policy (one policy \u03c0t for each time step t) iteratively over T iterations, where at iteration t, \u03c0t is trained to mimic \u03c0\u2217 on the distribution of states at time t induced by the previously trained policies\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 23
                            }
                        ],
                        "text": "Follows from result in Ross and Bagnell (2010) since is an upper bound on the 0-1 loss of \u03c0 in d\u03c0\u2217 ."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 30
                            }
                        ],
                        "text": "Another approach called SMILe (Ross and Bagnell, 2010), similar to SEARN (Daum\u00e9 III et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "Recent approaches (Ross and Bagnell, 2010) can guarantee an expected number of mistakes linear (or nearly so) in the task horizon T and error by training over several iterations and allowing the learner to influence the input states where expert demonstration is provided (through execution of its own controls in the system)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 14
                            }
                        ],
                        "text": "One approach (Ross and Bagnell, 2010) learns a non-stationary policy by training a different policy for each time step in sequence, starting from the first step."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Ross and Bagnell (2010) showed that choosing \u03b1 in O( 1T 2 ) and N in O(T\n2 log T ) guarantees near-linear regret in T and for some class of problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 19
                            }
                        ],
                        "text": "Recent approaches (Ross and Bagnell, 2010) can guarantee an expected number of mistakes linear (or nearly so) in the task horizon T and error by training over several iterations and allowing the learner to influence the input states where expert demonstration is provided (through execution of its\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 48
                            }
                        ],
                        "text": "Some recent approaches (Daum\u00e9 III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 269
                            }
                        ],
                        "text": "\u2026are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8498625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70e10a5459c6f1aaf346ee4f2dcc837151fbe75c",
            "isKey": false,
            "numCitedBy": 535,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d.. This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner\u2019s policy is slowly modified from executing the expert\u2019s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively."
            },
            "slug": "Efficient-Reductions-for-Imitation-Learning-Ross-Bagnell",
            "title": {
                "fragments": [],
                "text": "Efficient Reductions for Imitation Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes two alternative algorithms for imitation learning where training occurs over several episodes of interaction and shows that this leads to stronger performance guarantees and improved performance on two challenging problems: training a learner to play a 3D racing game and Mario Bros."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 197
                            }
                        ],
                        "text": "In future work, we will consider more sophisticated strategies than simple greedy forward decoding for structured prediction, as well as using base classifiers that rely on Inverse Optimal Control (Abbeel and Ng, 2004; Ratliff et al., 2006) techniques to learn a cost function for a planner to aid prediction in imitation learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 166
                            }
                        ],
                        "text": "\u2026more sophisticated strategies than simple greedy forward decoding for structured prediction, as well as using base classifiers that rely on Inverse Optimal Control (Abbeel and Ng, 2004; Ratliff et al., 2006) techniques to learn a cost function for a planner to aid prediction in imitation learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 168
                            }
                        ],
                        "text": "strations of good behavior are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 157
                            }
                        ],
                        "text": "\u2026are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207155342,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
            "isKey": true,
            "numCitedBy": 2545,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using \"inverse reinforcement learning\" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function."
            },
            "slug": "Apprenticeship-learning-via-inverse-reinforcement-Abbeel-Ng",
            "title": {
                "fragments": [],
                "text": "Apprenticeship learning via inverse reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work thinks of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and gives an algorithm for learning the task demonstrated by the expert, based on using \"inverse reinforcement learning\" to try to recover the unknown reward function."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13693897"
                        ],
                        "name": "Nathan D. Ratliff",
                        "slug": "Nathan-D.-Ratliff",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Ratliff",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan D. Ratliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061322266"
                        ],
                        "name": "David M. Bradley",
                        "slug": "David-M.-Bradley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bradley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Bradley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751288"
                        ],
                        "name": "J. Chestnutt",
                        "slug": "J.-Chestnutt",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Chestnutt",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Chestnutt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In future work, we will consider more sophisticated strategies than simple greedy forward decoding for structured prediction, as well as using base classifiers that rely on Inverse Optimal Control (Abbeel and Ng, 2004; Ratliff et al., 2006) techniques to learn a cost function for a planner to aid prediction in imitation learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "strations of good behavior are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1528918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9452a000f05bc6c52bf8d2e34e086fc60fa1999",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The Maximum Margin Planning (MMP) (Ratliff et al., 2006) algorithm solves imitation learning problems by learning linear mappings from features to cost functions in a planning domain. The learned policy is the result of minimum-cost planning using these cost functions. These mappings are chosen so that example policies (or trajectories) given by a teacher appear to be lower cost (with a loss-scaled margin) than any other policy for a given planning domain. We provide a novel approach, MMPBOOST, based on the functional gradient descent view of boosting (Mason et al., 1999; Friedman, 1999a) that extends MMP by \"boosting\" in new features. This approach uses simple binary classification or regression to improve performance of MMP imitation learning, and naturally extends to the class of structured maximum margin prediction problems. (Taskar et al., 2005) Our technique is applied to navigation and planning problems for outdoor mobile robots and robotic legged locomotion."
            },
            "slug": "Boosting-Structured-Prediction-for-Imitation-Ratliff-Bradley",
            "title": {
                "fragments": [],
                "text": "Boosting Structured Prediction for Imitation Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel approach, MMPBOOST, is provided, based on the functional gradient descent view of boosting, that extends MMP by \"boosting\" in new features by using simple binary classification or regression to improve performance of MMP imitation learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13693897"
                        ],
                        "name": "Nathan D. Ratliff",
                        "slug": "Nathan-D.-Ratliff",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Ratliff",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan D. Ratliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8195063"
                        ],
                        "name": "Martin A. Zinkevich",
                        "slug": "Martin-A.-Zinkevich",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zinkevich",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Zinkevich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 154
                            }
                        ],
                        "text": "The k output binary variable \u0177k = I(w k x + bk > 0), where wk, bk optimizes the SVM objective with regularizer \u03bb = 10\u22124 using stochastic gradient descent (Ratliff et al., 2007; Bottou, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 212
                            }
                        ],
                        "text": "\u2026prediction as a degenerate imitation learning problem, we apply DAGGER to the OCR (Taskar et al., 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al., 2003; Ratliff et al., 2007; Daum\u00e9 III et al., 2009) using only single-pass, greedy prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 154
                            }
                        ],
                        "text": "The kth output binary variable y\u0302k = I(wTk x + bk > 0), where wk, bk optimizes the SVM objective with regularizer \u03bb = 10\u22124 using stochastic gradient descent (Ratliff et al., 2007; Bottou, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 93
                            }
                        ],
                        "text": ", 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al., 2003; Ratliff et al., 2007; Daum\u00e9 III et al., 2009) using only single-pass, greedy prediction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5929174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45372f73a0e40da428595597816ac4cae1469cec",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Promising approaches to structured learning problems have recently been developed in the maximum margin framework. Unfortunately, algorithms that are computationally and memory efficient enough to solve large scale problems have lagged behind. We propose using simple subgradient-based techniques for optimizing a regularized risk formulation of these problems in both online and batch settings, and analyze the theoretical convergence, generalization, and robustness properties of the resulting techniques. These algorithms are are simple, memory efficient, fast to converge, and have small regret in the online setting. We also investigate a novel convex regression formulation of structured learning. Finally, we demonstrate the benefits of the subgradient approach on three structured prediction problems."
            },
            "slug": "Online)-Subgradient-Methods-for-Structured-Ratliff-Bagnell",
            "title": {
                "fragments": [],
                "text": "Online) Subgradient Methods for Structured Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes using simple subgradient-based techniques for optimizing a regularized risk formulation of structured learning problems in both online and batch settings, and analyzes the theoretical convergence, generalization, and robustness properties of the resulting techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078528382"
                        ],
                        "name": "A. Agarwal",
                        "slug": "A.-Agarwal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144055676"
                        ],
                        "name": "Satyen Kale",
                        "slug": "Satyen-Kale",
                        "structuredName": {
                            "firstName": "Satyen",
                            "lastName": "Kale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satyen Kale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 100
                            }
                        ],
                        "text": "Our approach is closely related to no regret online learning algorithms (Cesa-Bianchi et al., 2004; Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008) (in particular Follow-The-Leader) but better leverages the expert in our setting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 86
                            }
                        ],
                        "text": "Many no-regret algorithms guarantee that \u03b3N is O\u0303( 1N ) (e.g. when ` is strongly convex) (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008; Kakade and Tewari, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11569359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c883f38d202548c1d89ef5de8892d53227842092",
            "isKey": false,
            "numCitedBy": 938,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nIn an online convex optimization problem a decision-maker makes a sequence of decisions, i.e., chooses a sequence of points in Euclidean space, from a fixed feasible set. After each point is chosen, it encounters a sequence of (possibly unrelated) convex cost functions. Zinkevich (ICML 2003) introduced this framework, which models many natural repeated decision-making problems and generalizes many existing problems such as Prediction from Expert Advice and Cover\u2019s Universal Portfolios. Zinkevich showed that a simple online gradient descent algorithm achieves additive regret$O(\\sqrt{T})$\n, for an arbitrary sequence of T convex cost functions (of bounded gradients), with respect to the best single decision in hindsight.\n\nIn this paper, we give algorithms that achieve regret O(log\u2009(T)) for an arbitrary sequence of strictly convex functions (with bounded first and second derivatives). This mirrors what has been done for the special cases of prediction from expert advice by Kivinen and Warmuth (EuroCOLT 1999), and Universal Portfolios by Cover (Math. Finance 1:1\u201319, 1991). We propose several algorithms achieving logarithmic regret, which besides being more general are also much more efficient to implement.\n\nThe main new ideas give rise to an efficient algorithm based on the Newton method for optimization, a new tool in the field. Our analysis shows a surprising connection between the natural follow-the-leader approach and the Newton method. We also analyze other algorithms, which tie together several different previous approaches including follow-the-leader, exponential weighting, Cover\u2019s algorithm and gradient descent.\n"
            },
            "slug": "Logarithmic-regret-algorithms-for-online-convex-Hazan-Agarwal",
            "title": {
                "fragments": [],
                "text": "Logarithmic regret algorithms for online convex optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Several algorithms achieving logarithmic regret are proposed, which besides being more general are also much more efficient to implement, and give rise to an efficient algorithm based on the Newton method for optimization, a new tool in the field."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445783"
                        ],
                        "name": "A. Conconi",
                        "slug": "A.-Conconi",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Conconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 73
                            }
                        ],
                        "text": "Our approach is closely related to no regret online learning algorithms (Cesa-Bianchi et al., 2004; Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008) (in particular Follow-The-Leader) but better leverages the expert in our setting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 32
                            }
                        ],
                        "text": "Following a similar analysis to Cesa-Bianchi et al. (2004), we obtain: Theorem 4.2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1 N P N i=1 E s\u02d8D i (\u2018(s;\u02c7 i)) min \u02c72 1 N P N i=1 E s\u02d8D i (\u2018(s;\u02c7))  N. Let ^ N = min \u02c72 1 N P i=1 E s\u02d8D i [\u2018(s;\u02c7)] the training loss of the best policy in hindsight. Following a similar analysis to Cesa-Bianchi et al. (2004), we obtain: Theorem4.2. For DAGGER, with probability at least 1 , there exists a policy \u02c7^ 2^\u02c7 1:N s.t. E s\u02d8d \u02c7^ [\u2018(s;\u02c7^)] ^ N+ N + 2\u2018 max N [n + T P N i=n +1 i] + \u2018 max q 2log(1=) mN , for Nthe "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "es nearly linearly with the effective horizon of the problem. It naturally handles continuous as well as discrete predictions. Our approach is closely related to no regret online learning algorithms (Cesa-Bianchi et al., 2004; Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008) (in particular Follow-The-Leader) but better leverages the expert in our setting. Additionally, we show that any no-regret learner can be used in"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 437093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78396e535101308d4431c08f0e85b18c920ee44f",
            "isKey": true,
            "numCitedBy": 522,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, it is shown how to extract a hypothesis with small risk from the ensemble of hypotheses generated by an arbitrary on-line learning algorithm run on an independent and identically distributed (i.i.d.) sample of data. Using a simple large deviation argument, we prove tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M/sub n/ associated with the on-line performance of the ensemble. Via sharp pointwise bounds on M/sub n/, we then obtain risk tail bounds for kernel perceptron algorithms in terms of the spectrum of the empirical kernel matrix. These bounds reveal that the linear hypotheses found via our approach achieve optimal tradeoffs between hinge loss and margin size over the class of all linear functions, an issue that was left open by previous results. A distinctive feature of our approach is that the key tools for our analysis come from the model of prediction of individual sequences; i.e., a model making no probabilistic assumptions on the source generating the data. In fact, these tools turn out to be so powerful that we only need very elementary statistical facts to obtain our final risk bounds."
            },
            "slug": "On-the-generalization-ability-of-on-line-learning-Cesa-Bianchi-Conconi",
            "title": {
                "fragments": [],
                "text": "On the generalization ability of on-line learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proves tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M/sub n/ associated with the on-line performance of the ensemble, and obtains risk tail bounds for kernel perceptron algorithms interms of the spectrum of the empirical kernel matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064914"
                        ],
                        "name": "Ambuj Tewari",
                        "slug": "Ambuj-Tewari",
                        "structuredName": {
                            "firstName": "Ambuj",
                            "lastName": "Tewari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ambuj Tewari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 42
                            }
                        ],
                        "text": "Leveraging the strong convexity of ` as in (Kakade and Tewari, 2009) may lead to a tighter bound requiring only O(T log(T/\u03b4)) trajectories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 86
                            }
                        ],
                        "text": "A more refined analysis taking advantage of the strong convexity of the loss function (Kakade and Tewari, 2009) may lead to tighter generalization bounds that requireN only of order \u00d5(T log(1/\u03b4))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 170
                            }
                        ],
                        "text": "\u2026with probability at least 1\u2212 \u03b4 there exists a policy \u03c0\u0302 \u2208 \u03c0\u03021:N s.t. Es\u223cd\u03c0\u0302 [`(s, \u03c0\u0302)] \u2264 \u0302N +O(1/T )\nA more refined analysis taking advantage of the strong convexity of the loss function (Kakade and Tewari, 2009) may lead to tighter generalization bounds that requireN only of order O\u0303(T log(1/\u03b4))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "The theoretical analysis of DAGGER only relies on the noregret property of the underlying Follow-The-Leader algorithm on strongly convex losses (Kakade and Tewari, 2009) which picks the sequence of policies \u03c0\u03021:N ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 139
                            }
                        ],
                        "text": "Many no-regret algorithms guarantee that \u03b3N is O\u0303( 1N ) (e.g. when ` is strongly convex) (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008; Kakade and Tewari, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2891410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "825f3932a53fd25628ab74c9faac19810dc27545",
            "isKey": true,
            "numCitedBy": 140,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of PEGASOS (with high probability), a recently proposed method for solving the SVM optimization problem."
            },
            "slug": "On-the-Generalization-Ability-of-Online-Strongly-Kakade-Tewari",
            "title": {
                "fragments": [],
                "text": "On the Generalization Ability of Online Strongly Convex Programming Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A sharp bound is held on the excess risk of the output of an online algorithm in terms of the average regret, that allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excessrisk with high probability."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144753437"
                        ],
                        "name": "S. Chernova",
                        "slug": "S.-Chernova",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Chernova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chernova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1956361"
                        ],
                        "name": "M. Veloso",
                        "slug": "M.-Veloso",
                        "structuredName": {
                            "firstName": "Manuela",
                            "lastName": "Veloso",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Veloso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 168
                            }
                        ],
                        "text": "strations of good behavior are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 242
                            }
                        ],
                        "text": "\u2026are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7498454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e40a0969aac73d50485c05de7f1c0ab081d77028",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Confidence-Based Autonomy (CBA), an interactive algorithm for policy learning from demonstration. The CBA algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents. The first component, Confident Execution, enables the agent to identify states in which demonstration is required, to request a demonstration from the human teacher and to learn a policy based on the acquired data. The algorithm selects demonstrations based on a measure of action selection confidence, and our results show that using Confident Execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher. The second algorithmic component, Corrective Demonstration, enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance. CBA and its individual components are compared and evaluated in a complex simulated driving domain. The complete CBA algorithm results in the best overall learning performance, successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning."
            },
            "slug": "Interactive-Policy-Learning-through-Autonomy-Chernova-Veloso",
            "title": {
                "fragments": [],
                "text": "Interactive Policy Learning through Confidence-Based Autonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The algorithm selects demonstrations based on a measure of action selection confidence, and results show that using Confident Execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836885"
                        ],
                        "name": "B. Argall",
                        "slug": "B.-Argall",
                        "structuredName": {
                            "firstName": "Brenna",
                            "lastName": "Argall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Argall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144753437"
                        ],
                        "name": "S. Chernova",
                        "slug": "S.-Chernova",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Chernova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chernova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1956361"
                        ],
                        "name": "M. Veloso",
                        "slug": "M.-Veloso",
                        "structuredName": {
                            "firstName": "Manuela",
                            "lastName": "Veloso",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Veloso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699032"
                        ],
                        "name": "B. Browning",
                        "slug": "B.-Browning",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Browning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Browning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 168
                            }
                        ],
                        "text": "strations of good behavior are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 221
                            }
                        ],
                        "text": "\u2026are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1045325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e5dfb0b1e54412e799eb0e86d552956cc3a5f54",
            "isKey": false,
            "numCitedBy": 2960,
            "numCiting": 135,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-of-robot-learning-from-demonstration-Argall-Chernova",
            "title": {
                "fragments": [],
                "text": "A survey of robot learning from demonstration"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37564609"
                        ],
                        "name": "Karthik Sridharan",
                        "slug": "Karthik-Sridharan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Sridharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Sridharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7065301,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a4cf35b6772b57aa838f7d6d0daad83d3e36cbb6",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We study convergence properties of empirical minimization of a stochastic strongly convex objective, where the stochastic component is linear. We show that the value attained by the empirical minimizer converges to the optimal value with rate 1/n. The result applies, in particular, to the SVM objective. Thus, we obtain a rate of 1/n on the convergence of the SVM objective (with fixed regularization parameter) to its infinite data limit. We demonstrate how this is essential for obtaining certain type of oracle inequalities for SVMs. The results extend also to approximate minimization as well as to strong convexity with respect to an arbitrary norm, and so also to objectives regularized using other lp norms."
            },
            "slug": "Fast-Rates-for-Regularized-Objectives-Sridharan-Shalev-Shwartz",
            "title": {
                "fragments": [],
                "text": "Fast Rates for Regularized Objectives"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the value attained by the empirical minimizer converges to the optimal value with rate 1/n, which is essential for obtaining certain type of oracle inequalities for SVMs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our approach is closely related to no regret online learning algorithms (Cesa-Bianchi et al., 2004; Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008) (in particular Follow-The-Leader) but better leverages the expert in our setting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "when ` is strongly convex) (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008; Kakade and Tewari, 2009)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15285212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db1ebbf72e6b3097ec5cc1f853ab9c975454ea39",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in Hazan et al. [2006]. We then show that one can interpolate between these two extreme cases. In particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally, we further extend our framework for generalized strongly convex functions."
            },
            "slug": "Mind-the-Duality-Gap:-Logarithmic-regret-algorithms-Shalev-Shwartz-Kakade",
            "title": {
                "fragments": [],
                "text": "Mind the Duality Gap: Logarithmic regret algorithms for online optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A primal-dual framework for the design and analysis of online strongly convex optimization algorithms is described and a new algorithm is derived that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059473735"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722938"
                        ],
                        "name": "A. Stentz",
                        "slug": "A.-Stentz",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Stentz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stentz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 168
                            }
                        ],
                        "text": "strations of good behavior are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 200
                            }
                        ],
                        "text": "\u2026are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14305634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "251635bcb60d730d17391d54f418e01c787791bb",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "High performance, long-distance autonomous navigation is a central problem for field robotics. Efficient navigation relies not only upon intelligent onboard systems for perception and planning, but also the effective use of prior maps and knowledge. While the availability and quality of low cost, high resolution satellite and aerial terrain data continues to rapidly improve, automated interpretation appropriate for robot planning and navigation remains difficult. Recently, a class of machine learning techniques have been developed that rely upon expert human demonstration to develop a function mapping overhead data to traversal cost. These algorithms choose the cost function so that planner behavior mimics an expert\u2019s demonstration as closely as possible. In this work, we extend these methods to automate interpretation of overhead data. We address key challenges, including interpolation-based planners, non-linear approximation techniques, and imperfect expert demonstration, necessary to apply these methods for learning to search for effective terrain interpretations. We validate our approach on a large scale outdoor robot during over 300 kilometers of autonomous traversal through complex natural environments."
            },
            "slug": "High-Performance-Outdoor-Navigation-from-Overhead-Silver-Bagnell",
            "title": {
                "fragments": [],
                "text": "High Performance Outdoor Navigation from Overhead Data using Imitation Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work addresses key challenges, including interpolation-based planners, non-linear approximation techniques, and imperfect expert demonstration, necessary to apply these methods for learning to search for effective terrain interpretations."
            },
            "venue": {
                "fragments": [],
                "text": "Robotics: Science and Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 112
                            }
                        ],
                        "text": "(2009) in treating structured prediction as a degenerate imitation learning problem, we apply DAGGER to the OCR (Taskar et al., 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 140
                            }
                        ],
                        "text": "Following Daum\u00e9 III et al. (2009) in treating structured prediction as a degenerate imitation learning problem, we apply DAGGER to the OCR (Taskar et al., 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al., 2003; Ratliff et al., 2007; Daum\u00e9 III\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 22
                            }
                        ],
                        "text": "We use the dataset of Taskar et al. (2003) which has been used extensively in the literature to compare several structured prediction approaches."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 93
                            }
                        ],
                        "text": ", 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al., 2003; Ratliff et al., 2007; Daum\u00e9 III et al., 2009) using only single-pass, greedy prediction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "Following Daum\u00e9 III et al. (2009) in treating structured prediction as a degenerate imitation learning problem, we apply DAGGER to the OCR (Taskar et al., 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al., 2003; Ratliff et al., 2007; Daum\u00e9 III et al., 2009) using only single-pass, greedy prediction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 191
                            }
                        ],
                        "text": "\u2026prediction as a degenerate imitation learning problem, we apply DAGGER to the OCR (Taskar et al., 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al., 2003; Ratliff et al., 2007; Daum\u00e9 III et al., 2009) using only single-pass, greedy prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": true,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 168
                            }
                        ],
                        "text": "strations of good behavior are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "\u2026are used to learn a controller, have proven very useful in practice and have led to stateof-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7124120,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "f69e05a32fd1541bb41d981bdb013366c9150a85",
            "isKey": false,
            "numCitedBy": 1361,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Is-imitation-learning-the-route-to-humanoid-robots-Schaal",
            "title": {
                "fragments": [],
                "text": "Is imitation learning the route to humanoid robots?"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624289"
                        ],
                        "name": "A. Beygelzimer",
                        "slug": "A.-Beygelzimer",
                        "structuredName": {
                            "firstName": "Alina",
                            "lastName": "Beygelzimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Beygelzimer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714811"
                        ],
                        "name": "Varsha Dani",
                        "slug": "Varsha-Dani",
                        "structuredName": {
                            "firstName": "Varsha",
                            "lastName": "Dani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varsha Dani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806117"
                        ],
                        "name": "Thomas P. Hayes",
                        "slug": "Thomas-P.-Hayes",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hayes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas P. Hayes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 89
                            }
                        ],
                        "text": "Note that this is not as strong as the general error or regret reductions considered in (Beygelzimer et al., 2005; Ross and Bagnell, 2010; Daum\u00e9 III et al., 2009) which require only classification: we require a no-regret method or strongly convex surrogate loss function, a stronger (albeit common)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 84
                            }
                        ],
                        "text": "We train the multiclass SVM using the all-pairs reduction to binary classification (Beygelzimer et al., 2005).\npredicted character feature) this makes this approach not as unstable as in general reinforcement/imitation learning problems (as we saw in the previous experiment)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 88
                            }
                        ],
                        "text": "Note that this is not as strong as the general error or regret reductions considered in (Beygelzimer et al., 2005; Ross and Bagnell, 2010; Daum\u00e9 III et al., 2009) which require only classification: we require a no-regret method or strongly convex surrogate loss function, a stronger (albeit common) assumption."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 83
                            }
                        ],
                        "text": "We train the multiclass SVM using the all-pairs reduction to binary classification (Beygelzimer et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 36
                            }
                        ],
                        "text": "We take a reduction-based approach (Beygelzimer et al., 2005) that enables reusing existing supervised learning algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 68
                            }
                        ],
                        "text": "We analyze this approach using a noregret and a reduction approach (Beygelzimer et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6478578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1521e475bc6d8ed07c95a46ec099377a8584ba7d",
            "isKey": true,
            "numCitedBy": 83,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a reduction-based model for analyzing supervised learning tasks. We use this model to devise a new reduction from multi-class cost-sensitive classification to binary classification with the following guarantee: If the learned binary classifier has error rate at most \u03b5 then the cost-sensitive classifier has cost at most 2\u03b5 times the expected sum of costs of all possible lables. Since cost-sensitive classification can embed any bounded loss finite choice supervised learning task, this result shows that any such task can be solved using a binary classification oracle. Finally, we present experimental results showing that our new reduction outperforms existing algorithms for multi-class cost-sensitive learning."
            },
            "slug": "Error-limiting-reductions-between-classification-Beygelzimer-Dani",
            "title": {
                "fragments": [],
                "text": "Error limiting reductions between classification tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work introduces a reduction-based model for analyzing supervised learning tasks and designs a new reduction from multi-class cost-sensitive classification to binary classification with the following guarantee: if the learned binary classifier has error rate at most \u03b5 then the cost- sensitive classifiers has cost at most 2\u03b5 times the expected sum of costs of all possible lables."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Many no-regret algorithms guarantee that \u03b3N is O\u0303( 1N ) (e.g. when ` is strongly convex) (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008; Kakade and Tewari, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 704519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c9d9f3c6f7508f4e29730924529dc993c27cddc",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Searn, an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. Searn is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, Searn is able to learn prediction functions for any loss function and any class of features. Moreover, Searn comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem."
            },
            "slug": "Search-based-structured-prediction-Daum\u00e9-Langford",
            "title": {
                "fragments": [],
                "text": "Search-based structured prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Searn is an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision and comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies goodperformance on the structured prediction problem."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810053"
                        ],
                        "name": "J. Togelius",
                        "slug": "J.-Togelius",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Togelius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Togelius"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 64
                            }
                        ],
                        "text": "We used the simulator from a recent Mario Bros. AI competition (Togelius and Karakovskiy, 2009) which can randomly generate stages of varying difficulty (more difficult gaps and types of enemies)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1862257,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "0a63cca6749d5a6f20e779c315a0b07d91c3c977",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This competition is about learning, or otherwise developing, the best controller (agent) for a version of Super Mario Bros."
            },
            "slug": "Mario-AI-competition-Togelius",
            "title": {
                "fragments": [],
                "text": "Mario AI competition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This competition is about learning, or otherwise developing, the best controller (agent) for a version of Super Mario Bros."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Symposium on Computational Intelligence and Games"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 16
                            }
                        ],
                        "text": ", 2009) and CPI (Kakade and Langford, 2002), trains a stationary stochastic policy (a finite mixture of policies) by adding a new policy to the mixture at each iteration of training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 107
                            }
                        ],
                        "text": "Another approach called SMILe (Ross and Bagnell, 2010), similar to SEARN (Daum\u00e9 III et al., 2009) and CPI (Kakade and Langford, 2002), trains a stationary stochastic policy (a finite mixture of policies) by adding a new policy to the mixture at each iteration of training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31442909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximately-Optimal-Approximate-Reinforcement-Kakade-Langford",
            "title": {
                "fragments": [],
                "text": "Approximately Optimal Approximate Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "K\u00e4\u00e4ri\u00e4inen (2006) demonstrated this in a sequence prediction setting1 and\n1In their example, an error rate of > 0 when trained to predict the next output in sequence with the previous correct output as input can lead to an expected number of mistakes of T 2 \u2212 1\u2212(1\u22122 ) T+1 4 + 1 2 over sequences of\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lower bounds for reductions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 30
                            }
                        ],
                        "text": "A video available on YouTube (Ross, 2010b) also shows a qualitative comparison of the behavior obtained with each method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 30
                            }
                        ],
                        "text": "A video available on YouTube (Ross, 2010a) shows a qualitative comparison of the behavior obtained with each method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of imitation learning approaches on"
            },
            "venue": {
                "fragments": [],
                "text": "Super Tux Kart, 2010a. URL http://www. youtube.com/watch?v=V00npNnWzSU"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of imitation learning approaches on Super Tux Kart"
            },
            "venue": {
                "fragments": [],
                "text": "Comparison of imitation learning approaches on Super Tux Kart"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Andrew Bagnell References"
            },
            "venue": {
                "fragments": [],
                "text": "Andrew Bagnell References"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lower bounds for reductions , 2006 . Atomic Learning workshop . S . Kakade and J . Langford . Approximately optimal approximate reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems ( NIPS ) Trends in Cognitive Sciences"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "K\u00e4\u00e4ri\u00e4inen (2006) demonstrated this in a sequence prediction setting1 and\n1In their example, an error rate of > 0 when trained to predict the next output in sequence with the previous correct output as input can lead to an expected number of mistakes of T 2 \u2212 1\u2212(1\u22122 ) T+1 4 + 1 2 over sequences of\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lower bounds for reductions Atomic Learning workshop"
            },
            "venue": {
                "fragments": [],
                "text": "Lower bounds for reductions Atomic Learning workshop"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of imitation learning approaches on Super Mario Bros"
            },
            "venue": {
                "fragments": [],
                "text": "Comparison of imitation learning approaches on Super Mario Bros"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 30
                            }
                        ],
                        "text": "A video available on YouTube (Ross, 2010b) also shows a qualitative comparison of the behavior obtained with each method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 30
                            }
                        ],
                        "text": "A video available on YouTube (Ross, 2010a) shows a qualitative comparison of the behavior obtained with each method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of imitation learning approaches on"
            },
            "venue": {
                "fragments": [],
                "text": "Super Mario Bros, 2010b. URL http://www. youtube.com/watch?v=anOI0xZ3kGM"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 15,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Reduction-of-Imitation-Learning-and-Structured-to-Ross-Gordon/79ab3c49903ec8cb339437ccf5cf998607fc313e?sort=total-citations"
}