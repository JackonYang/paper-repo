{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3243917"
                        ],
                        "name": "P. Shoemaker",
                        "slug": "P.-Shoemaker",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Shoemaker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shoemaker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19419511,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "385f68e2d50236024a349aa0584282ec53a43112",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network models are considered as mathematical classifiers whose inputs comprise random variables generated according to arbitrary stationary class distributions, and the implication of learning based on minimization of sum-square classification error over a training set of these observations for which class assignments are absolutely determined is addressed. Expectations for network outputs in such cases are weighted least-squares approximations to a posteriori probabilities for the classes, which justifies interpretation of network outputs as indicating degree of confidence in class membership. The author demonstrates this with a straightforward proof in which class probability densities are regarded as primitives and which for simplicity does not rely on probability theory or statistics. The author cites more detailed results giving conditions for consistency of the estimators and discusses some issues relating to the suitability of neural network models and back-propagation training for approximation of conditional probabilities in classification tasks."
            },
            "slug": "A-note-on-least-squares-learning-procedures-and-by-Shoemaker",
            "title": {
                "fragments": [],
                "text": "A note on least-squares learning procedures and classification by neural network models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The author demonstrates this with a straightforward proof in which class probability densities are regarded as primitives and which for simplicity does not rely on probability theory or statistics."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191677"
                        ],
                        "name": "J. Hampshire",
                        "slug": "J.-Hampshire",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hampshire",
                            "middleNames": [
                                "B."
                            ],
                            "suffix": "II"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hampshire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14038203,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "85241210389fbce403f5d12597b9bf32a5633dc2",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Equivalence-Proofs-for-Multi-Layer-Perceptron-and-Hampshire-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Equivalence Proofs for Multi-Layer Perceptron Classifiers and the Bayesian Discriminant Function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793645"
                        ],
                        "name": "H. Gish",
                        "slug": "H.-Gish",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Gish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Gish"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123202935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that training a neural network using a mean-square-error criterion gives network outputs that approximate posterior class probabilities. Based on this probabilistic interpretation of the network operation, information-theoretic training criteria such as maximum mutual information and the Kullback-Liebler measure are investigated. It is shown that both of these criteria are equivalent to the maximum-likelihood estimation (MLE) of the network parameters. MLE of a network allows for the comparison of network models using the Akaike information criterion and the minimum-description length criterion.<<ETX>>"
            },
            "slug": "A-probabilistic-approach-to-the-understanding-and-Gish",
            "title": {
                "fragments": [],
                "text": "A probabilistic approach to the understanding and training of neural network classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "It is shown that training a neural network using a mean-square-error criterion gives network outputs that approximate posterior class probabilities and information-theoretic training criteria such as maximum mutual information and the Kullback-Liebler measure are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 169
                            }
                        ],
                        "text": "It can also be interpreted as minimizing the Kullback-Liebler probability distance measure, maximizing mutual information, or as maximum likelihood parameter estimation (Baum and Wilczek 1988; Bridle 1990; Gish 1990; Hinton 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 135
                            }
                        ],
                        "text": "[With certain assumptions, the squared-error and cross-entropy cost functions have impIicit maximum likelihood interpretations as well (Baum and Wilczek 1988; Bridle 1990; Gish 1990; Hinton 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18865663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "830ccb44084d9d6cdcb70d623df5012ae4835142",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mutual Information training, which has been used successfully to improve the performance of hidden Markov models for speech recognition. If the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'."
            },
            "slug": "Training-Stochastic-Model-Recognition-Algorithms-as-Bridle",
            "title": {
                "fragments": [],
                "text": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that once the output layer of a multilayer perceptron is modified to provide mathematically correct probability distributions, and the usual squared error criterion is replaced with a probability-based score, the result is equivalent to Maximum Mutual Information training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402969335"
                        ],
                        "name": "A. El-Jaroudi",
                        "slug": "A.-El-Jaroudi",
                        "structuredName": {
                            "firstName": "Amro",
                            "lastName": "El-Jaroudi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. El-Jaroudi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40598043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adf81acbfb348c7ebacb97858beb3d193766bb2a",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an error criterion for training which improves the performance of neural nets as posterior probability estimators, as compared to using least squares. The proposed criterion is similar to the Kullback-Leibler information measure and is simple to use. A straightforward iterative algorithm for the minimization of the error criterion which has been shown to have good convergence properties is described. The authors applied the proposed technique to some classification examples and showed it to produce better posterior probability estimates than least squares, especially for low probabilities"
            },
            "slug": "A-new-error-criterion-for-posterior-probability-El-Jaroudi-Makhoul",
            "title": {
                "fragments": [],
                "text": "A new error criterion for posterior probability estimation with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "An error criterion for training is introduced which improves the performance of neural nets as posterior probability estimators, as compared to using least squares, and is similar to the Kullback-Leibler information measure."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065228513"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213649"
                        ],
                        "name": "C. Wellekens",
                        "slug": "C.-Wellekens",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wellekens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wellekens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 87
                            }
                        ],
                        "text": "Many recent papers have provided new derivations for the two-class and multiclass case (Bourlard and Wellekens 1989; Gish 1990; Hampshire and Perlmutter 1990; Ruck et al. 1990; Shoemaker 1991; Wan 1990; White 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14700006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee50abb5aff3e5c43a38f24396b9552d593a9ae0",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The statistical use of a particular classic form of a connectionist system, the multilayer perceptron (MLP), is described in the context of the recognition of continuous speech. A discriminant hidden Markov model (HMM) is defined, and it is shown how a particular MLP with contextual and extra feedback input units can be considered as a general form of such a Markov model. A link between these discriminant HMMs, trained along the Viterbi algorithm, and any other approach based on least mean square minimization of an error function (LMSE) is established. It is shown theoretically and experimentally that the outputs of the MLP (when trained along the LMSE or the entropy criterion) approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities. Results of a series of speech recognition experiments are reported. The possibility of embedding MLP into HMM is described. Relations with other recurrent networks are also explained. >"
            },
            "slug": "Links-Between-Markov-Models-and-Multilayer-Bourlard-Wellekens",
            "title": {
                "fragments": [],
                "text": "Links Between Markov Models and Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown theoretically and experimentally that the outputs of the MLP approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19920501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c8293e7054230cc6cc6e3172f761d89d267f7a7",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning algorithms have been used both on feed-forward deterministic networks and on feed-back statistical networks to capture input-output relations and do pattern classification. These learning algorithms are examined for a class of problems characterized by noisy or statistical data, in which the networks learn the relation between input data and probability distributions of answers. In simple but nontrivial networks the two learning rules are closely related. Under some circumstances the learning problem for the statistical networks can be solved without Monte Carlo procedures. The usual arbitrary learning goals of feed-forward networks can be given useful probabilistic meaning."
            },
            "slug": "Learning-algorithms-and-probability-distributions-Hopfield",
            "title": {
                "fragments": [],
                "text": "Learning algorithms and probability distributions in feed-forward and feed-back networks."
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "These learning algorithms are examined for a class of problems characterized by noisy or statistical data, in which the networks learn the relation between input data and probability distributions of answers, in simple but nontrivial networks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 87
                            }
                        ],
                        "text": "Many recent papers have provided new derivations for the two-class and multiclass case (Bourlard and Wellekens 1989; Gish 1990; Hampshire and Perlmutter 1990; Ruck et al. 1990; Shoemaker 1991; Wan 1990; White 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 314,
                                "start": 302
                            }
                        ],
                        "text": "Although the above proofs demonstrate that this cost function is minimized when network outputs estimate true Bayesian probabilities, estimation accuracy may be poor with limited training data, incorrect network size, and the nonoptimal heuristic search procedures typically used for training networks (White 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40446176"
                        ],
                        "name": "Kenney Ng",
                        "slug": "Kenney-Ng",
                        "structuredName": {
                            "firstName": "Kenney",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenney Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14874298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "218f32b107e3d461781cad50888845a08486bb24",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Seven different pattern classifiers were implemented on a serial computer and compared using artificial and speech recognition tasks. Two neural network (radial basis function and high order polynomial GMDH network) and five conventional classifiers (Gaussian mixture, linear tree, K nearest neighbor, KD-tree, and condensed K nearest neighbor) were evaluated. Classifiers were chosen to be representative of different approaches to pattern classification and to complement and extend those evaluated in a previous study (Lee and Lippmann, 1989). This and the previous study both demonstrate that classification error rates can be equivalent across different classifiers when they are powerful enough to form minimum error decision regions, when they are properly tuned, and when sufficient training data is available. Practical characteristics such as training time, classification time, and memory requirements, however, can differ by orders of magnitude. These results suggest that the selection of a classifier for a particular task should be guided not so much by small differences in error rate, but by practical considerations concerning memory usage, computational resources, ease of implementation, and restrictions on training and classification times."
            },
            "slug": "A-Comparative-Study-of-the-Practical-of-Neural-and-Ng-Lippmann",
            "title": {
                "fragments": [],
                "text": "A Comparative Study of the Practical Characteristics of Neural Network and Conventional Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results suggest that the selection of a classifier for a particular task should be guided not so much by small differences in error rate, but by practical considerations concerning memory usage, computational resources, ease of implementation, and restrictions on training and classification times."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1933935"
                        ],
                        "name": "D. Ruck",
                        "slug": "D.-Ruck",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Ruck",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30603673"
                        ],
                        "name": "S. Rogers",
                        "slug": "S.-Rogers",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Rogers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786615"
                        ],
                        "name": "M. Kabrisky",
                        "slug": "M.-Kabrisky",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kabrisky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kabrisky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693682"
                        ],
                        "name": "M. Oxley",
                        "slug": "M.-Oxley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Oxley",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oxley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832848"
                        ],
                        "name": "B. Suter",
                        "slug": "B.-Suter",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Suter",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Suter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13199036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b217788dd6d274ad391ee950e6f6a34033bd2fc7",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The multilayer perceptron, when trained as a classifier using backpropagation, is shown to approximate the Bayes optimal discriminant function. The result is demonstrated for both the two-class problem and multiple classes. It is shown that the outputs of the multilayer perceptron approximate the a posteriori probability functions of the classes being trained. The proof applies to any number of layers and any type of unit activation function, linear or nonlinear."
            },
            "slug": "The-multilayer-perceptron-as-an-approximation-to-a-Ruck-Rogers",
            "title": {
                "fragments": [],
                "text": "The multilayer perceptron as an approximation to a Bayes optimal discriminant function"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The multilayer perceptron, when trained as a classifier using backpropagation, is shown to approximate the Bayes optimal discriminant function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 137
                            }
                        ],
                        "text": "Normalizing outputs by training-data subword-unit class probabilities in our experiments and in speech recognition experiments by others (Bourlard and Morgan 1990) has resulted in a large reduction in word error rate over unnormalized outputs."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 17831368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5001470e8808afe9887afbe48e2eaaf1a0395d10",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We are developing a phoneme based, speaker-dependent continuous speech recognition system embedding a Multilayer Perceptron (MLP) (i.e., a feedforward Artificial Neural Network), into a Hidden Markov Model (HMM) approach. In [Bourlard & Wellekens], it was shown that MLPs were approximating Maximum a Posteriori (MAP) probabilities and could thus be embedded as an emission probability estimator in HMMs. By using contextual information from a sliding window on the input frames, we have been able to improve frame or phoneme classification performance over the corresponding performance for Simple Maximum Likelihood (ML) or even MAP probabilities that are estimated without the benefit of context. However, recognition of words in continuous speech was not so simply improved by the use of an MLP, and several modifications of the original scheme were necessary for getting acceptable performance. It is shown here that word recognition performance for a simple discrete density HMM system appears to be somewhat better when MLP methods are used to estimate the emission probabilities."
            },
            "slug": "A-Continuous-Speech-Recognition-System-Embedding-Bourlard-Morgan",
            "title": {
                "fragments": [],
                "text": "A Continuous Speech Recognition System Embedding MLP into HMM"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown here that word recognition performance for a simple discrete density HMM system appears to be somewhat better when MLP methods are used to estimate the emission probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 82
                            }
                        ],
                        "text": "Many neural network and conventional classifiers use squared-error cost functions (Lippmann 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15355298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "333756f18144c75c61e6e4cda8821861353ead19",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "The author extends a previous review and focuses on feed-forward neural-net classifiers for static patterns with continuous-valued inputs. He provides a taxonomy of neural-net classifiers, examining probabilistic, hyperplane, kernel, and exemplar classifiers. He then discusses back-propagation and decision-tree classifiers; matching classifier complexity to training data; GMDH (generalized method of data handling) networks and high-order nets; K nearest-neighbor classifiers; the feature-map classifier; the learning vector quantizer; hypersphere classifiers; and radial-basis function classifiers.<<ETX>>"
            },
            "slug": "Pattern-classification-using-neural-networks-Lippmann",
            "title": {
                "fragments": [],
                "text": "Pattern classification using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The author extends a previous review and focuses on feed-forward neural-net classifiers for static patterns with continuous-valued inputs, examining probabilistic, hyperplane, kernel, and exemplar classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Communications Magazine"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48385057"
                        ],
                        "name": "E. Wan",
                        "slug": "E.-Wan",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35552556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82faa6788ed8ec6fc2222b83bffccf2bb203e307",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The relationship between minimizing a mean squared error and finding the optimal Bayesian classifier is reviewed. This provides a theoretical interpretation for the process by which neural networks are used in classification. A number of confidence measures are proposed to evaluate the performance of the neural network classifier within a statistical framework."
            },
            "slug": "Neural-network-classification:-a-Bayesian-Wan",
            "title": {
                "fragments": [],
                "text": "Neural network classification: a Bayesian interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The relationship between minimizing a mean squared error and finding the optimal Bayesian classifier is reviewed and a number of confidence measures are proposed to evaluate the performance of the neural network classifier within a statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144158372"
                        ],
                        "name": "F. Wilczek",
                        "slug": "F.-Wilczek",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Wilczek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wilczek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 169
                            }
                        ],
                        "text": "It can also be interpreted as minimizing the Kullback-Liebler probability distance measure, maximizing mutual information, or as maximum likelihood parameter estimation (Baum and Wilczek 1988; Bridle 1990; Gish 1990; Hinton 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 135
                            }
                        ],
                        "text": "[With certain assumptions, the squared-error and cross-entropy cost functions have impIicit maximum likelihood interpretations as well (Baum and Wilczek 1988; Bridle 1990; Gish 1990; Hinton 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 44
                            }
                        ],
                        "text": "These have been derived using cross-entropy (Baum and Wilczek 1988; Hinton 1990; Solla et al. 1988), Kullback-Liebler information (El-Jaroudi and Makhoul 1990; Gish 19901, maximum mutual information (Bridle 1990; Gish 19901, and Minkowski-r (Hanson and Burr 1988) criteria."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 149
                            }
                        ],
                        "text": "The most popular alternative cost function measures the cross-entropy between actual outputs and desired outputs, which are treated as probabilities (Baum and Wilczek 1988; Hinton 1990; Hopfield 1987; Solla et al. 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10578219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d9ed799fcc2ba2f929532a4f403091198bcfd83",
            "isKey": true,
            "numCitedBy": 187,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose that the back propagation algorithm for supervised learning can be generalized, put on a satisfactory conceptual footing, and very likely made more efficient by defining the values of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood, rather than the 'error'."
            },
            "slug": "Supervised-Learning-of-Probability-Distributions-by-Baum-Wilczek",
            "title": {
                "fragments": [],
                "text": "Supervised Learning of Probability Distributions by Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The back propagation algorithm for supervised learning can be generalized, put on a satisfactory conceptual footing, and very likely made more efficient by defining the values of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood, rather than the 'error'."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47963084"
                        ],
                        "name": "E. Singer",
                        "slug": "E.-Singer",
                        "structuredName": {
                            "firstName": "Elliot",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 205
                            }
                        ],
                        "text": "We, for example, have used this approach to obtain scaled word likelihoods by multiplying scaled likelihoods (normalized network outputs from RBF networks) from classifiers that model subword speech units (Singer and Lippmann 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11929799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd813d23e78488c50d83bfe00c15d2c665c9d5b4",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A high performance speaker-independent isolated-word hybrid speech recognizer was developed which combines Hidden Markov Models (HMMs) and Radial Basis Function (RBF) neural networks. In recognition experiments using a speaker-independent E-set database, the hybrid recognizer had an error rate of 11.5% compared to 15.7% for the robust unimodal Gaussian HMM recognizer upon which the hybrid system was based. These results and additional experiments demonstrate that RBF networks can be successfully incorporated in hybrid recognizers and suggest that they may be capable of good performance with fewer parameters than required by Gaussian mixture classifiers. A global parameter optimization method designed to minimize the overall word error rather than the frame recognition error failed to reduce the error rate."
            },
            "slug": "Improved-Hidden-Markov-Models-Speech-Recognition-Singer-Lippmann",
            "title": {
                "fragments": [],
                "text": "Improved Hidden Markov Models Speech Recognition Using Radial Basis Function Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that RBF networks can be successfully incorporated in hybrid recognizers and suggested that they may be capable of good performance with fewer parameters than required by Gaussian mixture classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jos\u00e9"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251336"
                        ],
                        "name": "D. Burr",
                        "slug": "D.-Burr",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Burr",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burr"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37457881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4789ed5137d5384b33fc184b5a91151b5f6816e6",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Many connectionist learning models are implemented using a gradient descent in a least squares error function of the output and teacher signal. The present model generalizes, in particular, back-propagation [1] by using Minkowski-r power metrics. For small r's a \"city-block\" error metric is approximated and for large r's the \"maximum\" or \"supremum\" metric is approached. while for r=2 the standard back-propagation model results. An implementation of Minkowski-r back-propagation is described, and several experiments are done which show that different values of r may be desirable for various purposes. Different r values may be appropriate for the reduction of the effects of outliers (noise), modeling the input space with more compact clusters, or modeling the statistics of a particular domain more naturally or in a way that may be more perceptually or psychologically meaningful (e.g. speech or vision)."
            },
            "slug": "Minkowski-r-Back-Propagation:-Learning-in-Models-Hanson-Burr",
            "title": {
                "fragments": [],
                "text": "Minkowski-r Back-Propagation: Learning in Connectionist Models with Non-Euclidian Error Signals"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The present model generalizes back-propagation by using Minkowski-r power metrics, and several experiments are done which show that different values of r may be desirable for various purposes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3018380"
                        ],
                        "name": "M. Holt",
                        "slug": "M.-Holt",
                        "structuredName": {
                            "firstName": "Murray",
                            "lastName": "Holt",
                            "middleNames": [
                                "J.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Holt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32268793"
                        ],
                        "name": "S. Semnani",
                        "slug": "S.-Semnani",
                        "structuredName": {
                            "firstName": "Shahram",
                            "lastName": "Semnani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Semnani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62166615,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8a00f5ced4339b47d97c8d9117d2d352869678b3",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A variation of the back-propagation algorithm is described, using a log-likelihood cost function. Appropriate choices of learning parameters are discussed. An example is given where the range of initial weights leading to proper convergence is increased, and the number of iterations required is significantly reduced."
            },
            "slug": "Convergence-of-back-propagation-in-neural-networks-Holt-Semnani",
            "title": {
                "fragments": [],
                "text": "Convergence of back-propagation in neural networks using a log-likelihood cost function"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A variation of the back-propagation algorithm is described, using a log-likelihood cost function, where the range of initial weights leading to proper convergence is increased, and the number of iterations required is significantly reduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7840452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a57c6d627ffc667ae3547073876c35d6420accff",
            "isKey": false,
            "numCitedBy": 1575,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-Procedures-Hinton",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning Procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57909018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aaa051450b3db30f95918e7901db6f1aba62d41",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "20-\u2013-CONNECTIONIST-LEARNING-PROCEDURES1-Hinton",
            "title": {
                "fragments": [],
                "text": "20 \u2013 CONNECTIONIST LEARNING PROCEDURES1"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145881625"
                        ],
                        "name": "M. Fleisher",
                        "slug": "M.-Fleisher",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fleisher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fleisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2024543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "247698d0a716f0d99c0645050d049525e0b08ec2",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abst ract . Learning in layered neu ral networks is posed as the mini\u00ad miz at ion of an error function defined over t he training set. A proba\u00ad bilistic interpretation of the target act ivities sugges ts th e use of rela\u00ad t ive entro py as an error measure. We investigate t he merits of using this error function over t he traditional quad ratic function for gradient descent learni ng. Com parative numerical sim ulations for the conrf\u00ad guity problem show marked redu ct ion s in learn ing t imes. This im \u00ad provement is explained in terms of the characteristic steepness of the landscape defined by the error function in configuration space."
            },
            "slug": "Accelerated-Learning-in-Layered-Neural-Networks-Solla-Levin",
            "title": {
                "fragments": [],
                "text": "Accelerated Learning in Layered Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work investigates the merits of using this error function over t he traditional quad ratic function for gradient descent for conrf\u00ad guity problem and explains the characteristic steepness of the landscape defined by the error function in configuration space."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191677"
                        ],
                        "name": "J. Hampshire",
                        "slug": "J.-Hampshire",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hampshire",
                            "middleNames": [
                                "B."
                            ],
                            "suffix": "II"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hampshire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1975998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b4081a0b5d775172269854c07dadb0c07977806",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present single- and multispeaker recognition results for the voiced stop consonants /b, d, g/ using time-delay neural networks (TDNN), a new objective function for training these networks, and a simple arbitration scheme for improved classification accuracy. With these enhancements a median 24% reduction in the number of misclassifications made by TDNNs trained with the traditional backpropagation objective function is achieved. This redundant results in /b, d, g/ recognition rates that consistently exceed 98% for TDNNs trained with individual speakers; it yields a 98.1% recognition rate for a TDNN trained with three male speakers.<<ETX>>"
            },
            "slug": "A-novel-objective-function-for-improved-phoneme-Hampshire-Waibel",
            "title": {
                "fragments": [],
                "text": "A novel objective function for improved phoneme recognition using time delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present single- and multispeaker recognition results for the voiced stop consonants /b, d, g/ using time-delay neural networks (TDNN), a new objective function for training these networks, and a simple arbitration scheme for improved classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 150
                            }
                        ],
                        "text": "false positives and false negatives on a medical screening test) and require class likelihoods and likelihood ratios to make classification decisions (Duda and Hart 1973; Fukunaga 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 138
                            }
                        ],
                        "text": "In these situations statistical theory suggests rejecting an input if all Bayesian probabilities for that input are less than a threshold (Fukunaga 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62359231,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f1277592f221ea26fa1d2321a38b64c58b33d75b",
            "isKey": false,
            "numCitedBy": 8009,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises."
            },
            "slug": "Introduction-to-Statistical-Pattern-Recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition, which is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 155
                            }
                        ],
                        "text": "A popular approach to parameter estimation with desirable asymptotic properties finds network parameters that maximize the likelihood of the training data (Duda and Hart 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 150
                            }
                        ],
                        "text": "false positives and false negatives on a medical screening test) and require class likelihoods and likelihood ratios to make classification decisions (Duda and Hart 1973; Fukunaga 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61724906,
            "fieldsOfStudy": [],
            "id": "73aa78f62329c01e60af4cf438645845aa9a803a",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A novel objective function for improved phoneme recognition using time delay neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 124
                            }
                        ],
                        "text": "High-order polynomial networks, hereafter referred to as GMDH networks, were created with the Group Method of Data Handling (Barron 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive learning networks: Development and application in the United States of algorithms related to GMDH"
            },
            "venue": {
                "fragments": [],
                "text": "Self-organizing Methods in Modeling, S. J. Farlow, ed., pp. 25-65. Marcel Dekker, New York."
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Competing experts: A n experimental investigation of associative mixture models"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. CRG-TR-90-5, University of Toronto, September. Ruck, D. W., Rogers, S. K., Kabrisky, M., Oxley, M. E., and Suter, B. W. 1990. The multilayer perceptron as an approximation to a Bayes optimal discriminant function."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Merging rnultilayer perceptrons and hidden Markov models: Some experiments in continuous speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. 89-033, International Computer Science Institute, Berkeley, CA, July."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A novel objective function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Covergence of back propagation in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 3,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Neural-Network-Classifiers-Estimate-Bayesian-a-Richard-Lippmann/8a6d820385527df2183a36ae1615f426ba894c5d?sort=total-citations"
}