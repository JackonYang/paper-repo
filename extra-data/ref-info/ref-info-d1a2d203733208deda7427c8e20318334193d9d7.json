{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6541629"
                        ],
                        "name": "K. Wagstaff",
                        "slug": "K.-Wagstaff",
                        "structuredName": {
                            "firstName": "Kiri",
                            "lastName": "Wagstaff",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wagstaff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144476811"
                        ],
                        "name": "Seth Rogers",
                        "slug": "Seth-Rogers",
                        "structuredName": {
                            "firstName": "Seth",
                            "lastName": "Rogers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seth Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830204"
                        ],
                        "name": "Stefan Schr\u00f6dl",
                        "slug": "Stefan-Schr\u00f6dl",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schr\u00f6dl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Schr\u00f6dl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Constrained K-means: K-means but subject to points (xi; xj) 2 S always being assigned to the same cluster [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] for clustering with similarity information."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13491515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bacca0993a3f51649a6bb8dbb093fc8d8481ad4",
            "isKey": false,
            "numCitedBy": 2612,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering is traditionally viewed as an unsupervised method for data analysis. However, in some cases information about the problem domain is available in addition to the data instances themselves. In this paper, we demonstrate how the popular k-means clustering algorithm can be protably modied to make use of this information. In experiments with articial constraints on six data sets, we observe improvements in clustering accuracy. We also apply this method to the real-world problem of automatically detecting road lanes from GPS data and observe dramatic increases in performance."
            },
            "slug": "Constrained-K-means-Clustering-with-Background-Wagstaff-Cardie",
            "title": {
                "fragments": [],
                "text": "Constrained K-means Clustering with Background Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper demonstrates how the popular k-means clustering algorithm can be protably modied to make use of information about the problem domain that is available in addition to the data instances themselves."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6541629"
                        ],
                        "name": "K. Wagstaff",
                        "slug": "K.-Wagstaff",
                        "structuredName": {
                            "firstName": "Kiri",
                            "lastName": "Wagstaff",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wagstaff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Following [12], in the case of 2-cluster data, we will measure how well the \u0109i\u2019s match the ci\u2019s according to"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[13, 12] for clustering with similarity information."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15761113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7736b3cba7b2a30e5e817f8cbb6fa02c4e7c7cdf",
            "isKey": false,
            "numCitedBy": 633,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering algorithms conduct a search through the space of possible organizations of a data set. In this paper, we propose two types of instance-level clustering constraints { must-link and cannot-link constraints { and show how they can be incorporated into a clustering algorithm to aid that search. For three of the four data sets tested, our results indicate that the incorporation of surprisingly few such constraints can increase clustering accuracy while decreasing runtime. We also investigate the relative eects of each type of constraint and nd that the type that contributes most to accuracy improvements depends on the behavior of the clustering algorithm without constraints."
            },
            "slug": "Clustering-with-Instance-Level-Constraints-Wagstaff-Cardie",
            "title": {
                "fragments": [],
                "text": "Clustering with Instance-Level Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper proposes two types of instance-level clustering constraints { must-link and cannot-link constraints} and shows how they can be incorporated into a clustering algorithm to aid that search."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741392"
                        ],
                        "name": "C. Domeniconi",
                        "slug": "C.-Domeniconi",
                        "structuredName": {
                            "firstName": "Carlotta",
                            "lastName": "Domeniconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Domeniconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736832"
                        ],
                        "name": "D. Gunopulos",
                        "slug": "D.-Gunopulos",
                        "structuredName": {
                            "firstName": "Dimitrios",
                            "lastName": "Gunopulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gunopulos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant examples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7839315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ad1950b11acd319949ce8523e899b31cad4d82",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets."
            },
            "slug": "Adaptive-Nearest-Neighbor-Classification-Using-Domeniconi-Gunopulos",
            "title": {
                "fragments": [],
                "text": "Adaptive Nearest Neighbor Classification Using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A technique that computes a locally flexible metric by means of Support Vector Machines by using the maximum margin boundary found by the SVM to determine the most discriminant direction over the query's neighborhood."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762240"
                        ],
                        "name": "W. Bialek",
                        "slug": "W.-Bialek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bialek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "(See also [11], for a different way of supervising clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8936496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c76c62c5ab6c076a80f925d277ef04dd36f6bf9c",
            "isKey": false,
            "numCitedBy": 2410,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We define the relevant information in a signal $x\\in X$ as being the information that this signal provides about another signal $y\\in \\Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\\X$ that preserves the maximum information about $\\Y$. That is, we squeeze the information that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a limited set of codewords $\\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \\to \\tX$ and $\\tX \\to \\Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere."
            },
            "slug": "The-information-bottleneck-method-Tishby-Pereira",
            "title": {
                "fragments": [],
                "text": "The information bottleneck method"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant examples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2354883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09370d132a1e238a778f5e39a7a096994dc25ec1",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Nearest neighbor classification expects the class conditional probabilities to be locally constant, and suffers from bias in high dimensions We propose a locally adaptive form of nearest neighbor classification to try to finesse this curse of dimensionality. We use a local linear discriminant analysis to estimate an effective metric for computing neighborhoods. We determine the local decision boundaries from centroid information, and then shrink neighborhoods in directions orthogonal to these local decision boundaries, and elongate them parallel to the boundaries. Thereafter, any neighborhood-based classifier can be employed, using the modified neighborhoods. The posterior probabilities tend to be more homogeneous in the modified neighborhoods. We also propose a method for global dimension reduction, that combines local dimension information. In a number of examples, the methods demonstrate the potential for substantial improvements over nearest neighbour classification."
            },
            "slug": "Discriminant-Adaptive-Nearest-Neighbor-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Discriminant Adaptive Nearest Neighbor Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A locally adaptive form of nearest neighbor classification is proposed to try to finesse this curse of dimensionality, and a method for global dimension reduction is proposed, that combines local dimension information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(See also [11], for a different way of supervising clustering.) This literature is too wide to survey here, but some relevant examples include [10, 5, 3,  6 ], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14336127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e45c2420e6dc59ba6d357fb0c996ebf43c861560",
            "isKey": false,
            "numCitedBy": 1619,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis."
            },
            "slug": "Exploiting-Generative-Models-in-Discriminative-Jaakkola-Haussler",
            "title": {
                "fragments": [],
                "text": "Exploiting Generative Models in Discriminative Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This includes algorithms such as Multidimensional Scaling (MDS) [2], and Locally Linear Embedding (LLE) [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 72532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da6495102377f8332b21cef50c35d27fbff627df",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Many problems in information processing involve some form of dimensionality reduction. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. LLE attempts to discover nonlinear structure in high dimensional data by exploiting the local symmetries of linear reconstructions. Notably, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations\u2014 though capable of generating highly nonlinear embeddings\u2014do not involve local minima. We illustrate the method on images of lips used in audiovisual speech synthesis."
            },
            "slug": "An-Introduction-to-Locally-Linear-Embedding-Saul-Roweis",
            "title": {
                "fragments": [],
                "text": "An Introduction to Locally Linear Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Locally linear embedding is described, an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data that attempts to discover nonlinear structure in highdimensional data by exploiting the local symmetries of linear reconstructions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This includes algorithms such as Multidimensional Scaling (MDS) [2], and Locally Linear Embedding (LLE) [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13979,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant examples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9219592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b502694a8e23d3b9ab08fa7a0c6d5e82bd1f066",
            "isKey": false,
            "numCitedBy": 1689,
            "numCiting": 403,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper surveys locally weighted learning, a form of lazy learning and memory-based learning, and focuses on locally weighted linear regression. The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, interference between old and new data, implementing locally weighted learning efficiently, and applications of locally weighted learning. A companion paper surveys how locally weighted learning can be used in robot learning and control."
            },
            "slug": "Locally-Weighted-Learning-Atkeson-Moore",
            "title": {
                "fragments": [],
                "text": "Locally Weighted Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, and applications of locally weighted learning."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence Review"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant examples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": false,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1588609548"
                        ],
                        "name": "Stephen Morrisson",
                        "slug": "Stephen-Morrisson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Morrisson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Morrisson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976991"
                        ],
                        "name": "Marion Kee",
                        "slug": "Marion-Kee",
                        "structuredName": {
                            "firstName": "Marion",
                            "lastName": "Kee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marion Kee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145800808"
                        ],
                        "name": "Kenneth Goodman",
                        "slug": "Kenneth-Goodman",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": ", [8]) we derive a different alg orithm for this setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 214625972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f00def7c0205e2896b1287cc01081b1ca6407da1",
            "isKey": false,
            "numCitedBy": 4644,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the parser, especially its mapping rule interpreter, used in KBMT-89. The interpreter is characterized by its ability to produce semantic and syntactic structures of a parse simultaneously and therefore more efficiently than other kinds of analyzers. Applicable forms of parser mapping rules, which map syntactic structures to semantic structures, are introduced. The parser, a modified version of Tomita's universal parser, is briefly described. Sample traces illustrate the functioning of the parser and mapping rule interpreter."
            },
            "slug": "Analysis-Morrisson-Kee",
            "title": {
                "fragments": [],
                "text": "Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The parser, especially its mapping rule interpreter, used in KBMT-89 is described, characterized by its ability to produce semantic and syntactic structures of a parse simultaneously and therefore more efficiently than other kinds of analyzers."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Translation"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "More importantly, methods such as LLE and MDS also suffer from the \u201cno right answer\u201d problem: For example, if MDS finds an embdding that fails to capture the structure important to a user, it is unclear what systematic corrective actions would be available."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "This includes algorithms such as Multidimensional Scaling (MDS) [2], andLocally Linear Embedding (LLE) [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "But similar to MDS and LLE, the (\u201cinstance-level\u201d) constraints that they use do not generalize to previously unseen data whose similarity/dissimilarity to the training set isnot known."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "This includes algorithms such as Multidimensional Scaling (MDS) [2], and Locally Linear Embedding (LLE) [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to locally line ar embedding"
            },
            "venue": {
                "fragments": [],
                "text": "(draft version)"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "More importantly, methods such as LLE and MDS also suffer from the \u201cno right answer\u201d problem: For example, if MDS finds an embdding that fails to capture the structure important to a user, it is unclear what systematic corrective actions would be available."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "This includes algorithms such as Multidimensional Scaling (MDS) [2], andLocally Linear Embedding (LLE) [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "But similar to MDS and LLE, the (\u201cinstance-level\u201d) constraints that they use do not generalize to previously unseen data whose similarity/dissimilarity to the training set isnot known."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "This includes algorithms such as Multidimensional Scaling (MDS) [2], and Locally Linear Embedding (LLE) [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cox.Multidimensional Scaling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66418228"
                        ],
                        "name": "\u4e38\u5c71 \u5fb9",
                        "slug": "\u4e38\u5c71-\u5fb9",
                        "structuredName": {
                            "firstName": "\u4e38\u5c71",
                            "lastName": "\u5fb9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u4e38\u5c71 \u5fb9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": ", [8]) we derive a different algorithm for this setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117573922,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b272701e77ddb860741a193ac1701ca382853680",
            "isKey": false,
            "numCitedBy": 7926,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convex-Analysis\u306e\u4e8c,\u4e09\u306e\u9032\u5c55\u306b\u3064\u3044\u3066-\u4e38\u5c71",
            "title": {
                "fragments": [],
                "text": "Convex Analysis\u306e\u4e8c,\u4e09\u306e\u9032\u5c55\u306b\u3064\u3044\u3066"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[13, 12] for clustering with similarity information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Followi ng [12], in the case of 2-cluster data, we will measure how well the \u0109i\u2019s match theci\u2019s according to Accuracy=Xi>j 1f1fci = cjg = 1f\u0109i = \u0109jgg 0:5m(m 1) ; where1f g is the indicator function ( 1fTrueg = 1, 1fFalseg = 0)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Clustering with instance-le vel constraints"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the Seventeenth International Conference on Machine Learning "
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant exam ples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminant adaptive near  est neighbor classification"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Learning  ,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant exam ples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting generative mo  dels in discriminaive classifier"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of Tenth Conference on Advances in Neural Information Proce  ssing Systems"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant exam ples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locally weighted lea  rning"
            },
            "venue": {
                "fragments": [],
                "text": "AI Review,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "(See also [11], for a different way of su pervising clustering."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The information bo  ttleneck method"
            },
            "venue": {
                "fragments": [],
                "text": "InProc. of the 37th Allerton Conference on Communication, Control and Computi  ng,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": ") This literature is too wide to survey here, but some relevant exam ples include [10, 5, 3, 6], and [1] also gives a good overview of some of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive nearest neighb or classification using support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "InAdvances in Neural Information Processing Systems"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Distance-Metric-Learning-with-Application-to-with-Xing-Ng/d1a2d203733208deda7427c8e20318334193d9d7?sort=total-citations"
}