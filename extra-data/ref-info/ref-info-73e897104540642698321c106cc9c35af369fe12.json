{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 11
                            }
                        ],
                        "text": "1The terms Computational Linguistics and Natural Language Processing will be used interchangeably in this paper.\nimportance in NLP of both these approaches to natural language semantics, a fundamental question is whether they can be integrated in a useful and meaningful fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 141
                            }
                        ],
                        "text": "Representing the meanings of words and sentences in a form suitable for use by a computer is a central problem in Computational Linguistics (Jurafsky & Martin 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60691216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54bcfca3fddc26b8889739a247a25e445818149",
            "isKey": false,
            "numCitedBy": 3827,
            "numCiting": 263,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing."
            },
            "slug": "Speech-and-language-processing-an-introduction-to-Jurafsky-Martin",
            "title": {
                "fragments": [],
                "text": "Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora, to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in artificial intelligence"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 67
                            }
                        ],
                        "text": "However, the recent improvements in wide-coverage parsing (Clark & Curran 2004) have led to attempts to scale up symbolic approaches, with some success (Bos 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 1
                            }
                        ],
                        "text": "(Curran 2004) applies these methods to a large corpus (two billion words) to create a noun thesaurus: an entry is created for each noun headword consisting of the top-N most similar headwords, for some suitable value of N ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 227290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 235,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical-semantic resources, including thesauri and WORDNET, have been successfully incorporated into a wide range of applications in Natural Language Processing. However they are very difficult and expensive to create and maintain, and their usefulness has been severely hampered by their limited coverage, bias and inconsistency. Automated and semi-automated methods for developing such resources are therefore crucial for further resource development and improved application performance. Systems that extract thesauri often identify similar words using the distributional hypothesis that similar words appear in similar contexts. This approach involves using corpora to examine the contexts each word appears in and then calculating the similarity between context distributions. Different definitions of context can be used, and I begin by examining how different types of extracted context influence similarity. To be of most benefit these systems must be capable of finding synonyms for rare words. Reliable context counts for rare events can only be extracted from vast collections of text. In this dissertation I describe how to extract contexts from a corpus of over 2 billion words. I describe techniques for processing text on this scale and examine the trade-off between context accuracy, information content and quantity of text analysed. Distributional similarity is at best an approximation to semantic similarity. I develop improved approximations motivated by the intuition that some events in the context distribution are more indicative of meaning than others. For instance, the object-of-verb context wear is far more indicative of a clothing noun than get. However, existing distributional techniques do not effectively utilise this information. The new context-weighted similarity metric I propose in this dissertation significantly outperforms every distributional similarity metric described in the literature. Nearest-neighbour similarity algorithms scale poorly with vocabulary and context vector size. To overcome this problem I introduce a new context-weighted approximation algorithm with bounded complexity in context vector size that significantly reduces the system runtime with only a minor performance penalty. I also describe a parallelized version of the system that runs on a Beowulf cluster for the 2 billion word experiments. To evaluate the context-weighted similarity measure I compare ranked similarity lists against gold-standard resources using precision and recall-based measures from Information Retrieval,"
            },
            "slug": "From-distributional-to-semantic-similarity-Curran",
            "title": {
                "fragments": [],
                "text": "From distributional to semantic similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This dissertation describes how to extract contexts from a corpus of over 2 billion words and introduces a new context-weighted approximation algorithm with bounded complexity in context vector size that significantly reduces the system runtime with only a minor performance penalty."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "A possible solution to this problem is to extend the tree kernel of (Collins & Duffy 2002), which is a dynamic programming method for calculating the number of common subtrees between two trees."
                    },
                    "intents": []
                }
            ],
            "corpusId": 396794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c7adc28e20d361d5c35aa9808094b10f6a34d1",
            "isKey": false,
            "numCitedBy": 932,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees."
            },
            "slug": "Convolution-Kernels-for-Natural-Language-Collins-Duffy",
            "title": {
                "fragments": [],
                "text": "Convolution Kernels for Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and experimental results on the ATIS corpus of parse trees are given."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767844"
                        ],
                        "name": "Diederik Aerts",
                        "slug": "Diederik-Aerts",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Aerts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik Aerts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747306"
                        ],
                        "name": "L. Gabora",
                        "slug": "L.-Gabora",
                        "structuredName": {
                            "firstName": "Liane",
                            "lastName": "Gabora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gabora"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 1
                            }
                        ],
                        "text": "(Aerts & Gabora 2005) also propose use of the tensor product, in order to combine vector-based representations of concepts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13988880,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9932e26c541e74dc6b454dd11bcab5f870fdb3a6",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "The sets of contexts and properties of a concept are embedded in the complex Hilbert space of quantum mechanics. States are unit vectors or density operators, and contexts and properties are orthogonal projections. The way calculations are done in Hilbert space makes it possible to model how context influences the state of a concept. Moreover, a solution to the combination of concepts is proposed. Using the tensor product, a procedure for describing combined concepts is elaborated, providing a natural solution to the pet fish problem. This procedure allows the modeling of an arbitrary number of combined concepts. By way of example, a model for a simple sentence containing a subject, a predicate and an object, is presented."
            },
            "slug": "A-Theory-of-Concepts-and-Their-Combinations-II:-A-Aerts-Gabora",
            "title": {
                "fragments": [],
                "text": "A Theory of Concepts and Their Combinations II: A Hilbert Space Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using the tensor product, a procedure for describing combined concepts is elaborated, providing a natural solution to the pet fish problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8754851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words."
            },
            "slug": "Automatic-Word-Sense-Discrimination-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Automatic Word Sense Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering that demonstrates good performance of context- group discrimination for a sample of natural and artificial ambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 141
                            }
                        ],
                        "text": "The context can be a simple window surrounding the headword, or a more linguistically-motivated context consisting of grammatical relations (Grefenstette 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16105777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36c4c51917b1f53ee85c459f2597e115df53eb05",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "One aspect of world knowledge essential to information retrieval is knowing when two words are related. Knowing word relatedness allows a system given a user's query terms to retrieve relevant documents not containing those exact terms. Two words can be said to be related if they appear in the same contexts Document co-occurrence gives a measure of word relatedness that has proved to be too rough to be useful. The relatively recent apparition of on-line dictionaries and robust and rapid parsers permits the extraction of finer word contexts from large corpora. In this paper, we will describe such an extraction technique that uses only coarse syntactic analysis and no domain knowledge. This technique produces lists of words related to any work appearing in a corpus. When the closest related terms were used in query expansion of a standard information retrieval testbed, the results were much better than that given by document co-occurence techniques, and slightly better than using unexpanded queries, supporting the contention that semantically similar words were indeed extracted by this technique."
            },
            "slug": "Use-of-syntactic-context-to-produce-term-lists-for-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Use of syntactic context to produce term association lists for text retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "When the closest related terms were used in query expansion of a standard information retrieval testbed, the results were much better than that given by document co-occurence techniques, and slightly better than using unexpanded queries, supporting the contention that semantically similar words were indeed extracted by this technique."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3461596"
                        ],
                        "name": "Johan Bos",
                        "slug": "Johan-Bos",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Bos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johan Bos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "In some implemented NLP systems (Bos 2005) these axioms are derived automatically from community resources like WordNet."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 153
                            }
                        ],
                        "text": "However, the recent improvements in wide-coverage parsing (Clark & Curran 2004) have led to attempts to scale up symbolic approaches, with some success (Bos 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18586841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "913c0fe31e99379dcc3e9e8c584c596bf6113941",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Wide-coverage and robust NLP techniques always seemed to go hand in hand with shallow analyses. This was certainly true a couple of years ago, but the state-of-the-art in stochastic approaches has advanced considerably and nowadays there are sophisticated parsers available achieving high coverage and producing accurate syntactic analyses. It seems we have finally reached a stage in NLP where we can apply well known techniques of formal and computational semantics to a larger scale, and get a detailed semantic analysis from a wide-coverage parser. A proof of concept of this idea was demonstrated in [BCS04], with a coverage of over 95% on newspaper texts. In this paper we discuss the further developments in this work, generating semantic representations for sentences or small texts, showing how we can calculate background knowledge required for reasoning, and performing inferences using state-of-the-art theorem provers and model builders. The semantic representation language that we will use is a first-order language, arguing that given the current state of automated deduction, any language with more expressive power (such as second or higher-order logic) cannot be used efficiently to perform inference tasks. There are however highly sophisticated inference tools for first-order logic available which we will use in our work. Despite the tradition in formal semantics to use higher-order logics, firstorder logic is able to cover a (perhaps surprisingly) large variety of interesting natural language phenomena. The language we are going to adopt is developed in Discourse Representation Theory (DRT), closed under a translation to first-order logic, and is described in Section 2. The choice for DRT is motivated by its impressive theoretical coverage of linguistic phenomena [KR93, VdS92]. Next, of course, we need a grammar formalism suitable for computational semantics (i.e. one that is able to produce fine-grained syntactic analyses)."
            },
            "slug": "Towards-Wide-Coverage-Semantic-Interpretation-Bos",
            "title": {
                "fragments": [],
                "text": "Towards Wide-Coverage Semantic Interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Further developments in this work are discussed, generating semantic representations for sentences or small texts, showing how to calculate background knowledge required for reasoning, and performing inferences using state-of-the-art theorem provers and model builders."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32372837"
                        ],
                        "name": "Joe Pater",
                        "slug": "Joe-Pater",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Pater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joe Pater"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 1
                            }
                        ],
                        "text": "(Smolensky & Legendre 2006) integrate the connectionist and symbolic models in the following way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 174
                            }
                        ],
                        "text": "First,\na symbolic structure s is defined by a collection of structural roles {ri} each of which may be occupied 2The following description of the two approaches is based on\n(Smolensky & Legendre 2006).\nby a filler fi; s is a set of constituents, each a filler/role binding fi/ri."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 1
                            }
                        ],
                        "text": "(Smolensky & Legendre 2006) argue at length for why the tensor product is appropriate for combining connectionist and symbolic structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 197465099,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3233c665304a393ae6dc7a098005404d23327492",
            "isKey": true,
            "numCitedBy": 308,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The harmonic mind is a two-volume collection of 23 chapters authored by Smolensky, Legendre and their collaborators.* Some of these are reprints or slight updates of previously published material, but approximately threequarters of it has never appeared before. Every phonologist who takes at all seriously the notion that phonology is a branch of cognitive science should own a copy of this book. Smolensky & Legendre (vol. 1, p. 1) present a view of cognition that incorporates two distinct, but related levels of formal description: \u2018the continuous, numerical lower-level description of brain\u2019, characterised in terms of a connectionist network, and \u2018the discrete, structural, higher-level description of mind\u2019, characterised in terms of Optimality Theory (OT). The depth and breadth of the presentation are awe-inspiring, and quite likely intimidating in some places for any single reader. In their preface, Smolensky & Legendre characterise the intended audience as interdisciplinary groups of researchers in cognitive science, and a joint reading of this book by such a group would undoubtedly be extremely educational for all. Bridges between the disciplines are built by careful exposition of the fundamental ideas, and the experts in each area will be satisfied by the wealth of insights, and the formal precision of their presentation. Since phonology is a central focus, this book provides phonologists with a unique opportunity to engage with both the research and the researchers in other realms of cognitive science. There is furthermore much that phonologists (and other linguists) working on their own can gain from this book. The presentation of the overall cognitive framework in the first few chapters of Volume 1, Cognitive architecture, should be accessible to readers with little or no background in mathematics or computational modelling of cognition. This section should particularly appeal to phonologists curious about connectionism, but who are put off by reductionist applications that seek to eliminate the types of representations used in generative"
            },
            "slug": "The-harmonic-mind-:-from-neural-computation-to-Pater",
            "title": {
                "fragments": [],
                "text": "The harmonic mind : from neural computation to optimality-theoretic grammar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "However, the recent improvements in wide-coverage parsing (Clark & Curran 2004) have led to attempts to scale up symbolic approaches, with some success (Bos 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6802974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9eea85e590f6e522e3681b8e45012684c60b0fd",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG). A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including non-standard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers."
            },
            "slug": "Parsing-the-WSJ-Using-CCG-and-Log-Linear-Models-Clark-Curran",
            "title": {
                "fragments": [],
                "text": "Parsing the WSJ Using CCG and Log-Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation and a new efficient parsing algorithm for CCG which maximises expected recall of dependencies is developed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 72
                            }
                        ],
                        "text": "Some limited kind of compositionality has perhaps been demonstrated by (Widdows 2004) who showed that a kind of negation on word senses could be defined in vector space\nmodels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 1
                            }
                        ],
                        "text": "(Widdows 2004) suggests a link between quantum logic and the lattices he uses to model negation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": "Some limited kind of compositionality has perhaps been demonstrated by (Widdows 2004) who showed that a kind of negation on word senses could be defined in vector space drinks"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17581,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "36eff99a7f23cec395e4efc80ff7f937934c7be6",
            "isKey": true,
            "numCitedBy": 306,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "From Pythagoras's harmonic sequence to Einstein's theory of relativity, geometric models of position, proximity, ratio, and the underlying properties of physical space have provided us with powerful ideas and accurate scientific tools. Currently, similar geometric models are being applied to another type of space the conceptual space of information and meaning, where the contributions of Pythagoras and Einstein are a part of the landscape itself. The rich geometry of conceptual space can be glimpsed, for instance, in internet documents: while the documents themselves define a structure of visual layouts and point-to-point links, search engines create an additional structure by matching keywords to nearby documents in a spatial arrangement of content. What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded.\""
            },
            "slug": "Geometry-and-Meaning-Widdows",
            "title": {
                "fragments": [],
                "text": "Geometry and Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709413"
                        ],
                        "name": "C. J. V. Rijsbergen",
                        "slug": "C.-J.-V.-Rijsbergen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Rijsbergen",
                            "middleNames": [
                                "J.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. V. Rijsbergen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 5
                            }
                        ],
                        "text": "(van Rijsbergen 2004) has proposed a quantum mechanical theory for modelling relevance in IR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "( van Rijsbergen 2004 ) has proposed a quantum mechanical theory for modelling relevance in IR. Our proposal is that the interaction of QM and language is a fruitful area of research for AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5312750,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "99614334a3e9b809e43384777409af7eccde3db6",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Prologue 1. Introduction 2. On sets and kinds in IR 3. Vector and Hilbert spaces 4. Linear transformations, operators and matrices 5. Conditional logic in IR 6. The geometry of IR Appendix I. Linear algebra Appendix II. Quantum mechanics Appendix III. Probability Bibliography Index."
            },
            "slug": "The-geometry-of-information-retrieval-Rijsbergen",
            "title": {
                "fragments": [],
                "text": "The geometry of information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The geometry of IR is studied through the lens of linear algebra, quantum mechanics, and vector and Hilbert spaces, with a focus on linear transformations, operators and matrices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 68
                            }
                        ],
                        "text": "A possible solution to this problem is to use a convolution kernel (Haussler 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17702358,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac",
            "isKey": false,
            "numCitedBy": 1371,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs. The method can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set. The family of kernels generated generalizes the family of radial basis kernels. It can also be used to deene kernels in the form of joint Gibbs probability distributions. Kernels can be built from hidden Markov random elds, generalized regular expressions, pair-HMMs, or ANOVA de-compositions. Uses of the method lead to open problems involving the theory of innnitely divisible positive deenite functions. Fundamentals of this theory and the theory of reproducing kernel Hilbert spaces are reviewed and applied in establishing the validity of the method."
            },
            "slug": "Convolution-kernels-on-discrete-structures-Haussler",
            "title": {
                "fragments": [],
                "text": "Convolution kernels on discrete structures"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs is introduced, which can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52287177"
                        ],
                        "name": "David R. Dowty",
                        "slug": "David-R.-Dowty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Dowty",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Dowty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40383053"
                        ],
                        "name": "R. Wall",
                        "slug": "R.-Wall",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wall",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840346"
                        ],
                        "name": "S. Peters",
                        "slug": "S.-Peters",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Peters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Peters"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been two distinct approaches to the representation of meaning in NLP.The first, the symbolic approach, follows the tradition of Montague in using a logic to express the meanings of sentences ( Dowty, Wall, & Peters 1981 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61127848,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "03380e7083807d3264472871dc0582036cf79479",
            "isKey": false,
            "numCitedBy": 884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction.- 2. The Syntax and Semantics of Two Simple Languages.- I. The Language L0.- 1. Syntax of L0.- 2. Semantics of L0.- II. The Language L0E.- 1. Syntax of L0E.- 2. Semantics of L0E.- 3. Alternative Formulations of L0E and L0.- III. A Synopsis of Truth-Conditional Semantics.- IV. The Notion of Truth Relative to a Model.- V. Validity and Entailment Defined in Terms of Possible Models.- VI. Model Theory and Deductive Systems.- Exercises.- Note.- 3. First-Order Predicate Logic.- I. The Language L1.- 1. Syntax of L1.- 2. Semantics of L1.- II. The Language L1E.- 1. Syntax of L1E.- 2. Semantics of L1E.- Exercises.- Notes.- 4. A Higher-Order Type-Theoretic Language.- I. A Notational Variant of L1.- II. The Language Ltype.- 1. Syntax of Ltype.- 2. Semantics of Ltype.- III. Lambda Abstraction and the Language L?.- Exercises.- Notes.- 5. Tense and Modal Operators.- I. Tense Operators and Their Interpretation.- II. The Other Varieties of Modal Logic the Operators ? and ?.- III. Languages Containing Both Tense and Modal Operators: Coordinate Semantics.- Exercises.- Notes.- 6. Montague's Intensional Logic.- I. Compositionality and the Intension-Extension Distinction.- II. The Intensional Logic of PTQ.- 1. Syntax of IL.- 2. Semantics of IL.- III. Examples of 'Oblique Contexts' as Represented in IL.- IV. Some Unresolved Issues with Possible Worlds Semantics and Propositional Attitudes.- Notes.- 7. The Grammar of PTQ.- I. The Overall Organization of the PTQ Grammar.- 1. The Syntactic Categories of English in the PTQ Grammar.- 2. The Correspondence Between Categories of English and Types of IL.- II. Subject-Predicate and Determiner-Noun Rules.- III. Conjoined Sentences, Verb Phrases, and Term Phrases.- IV. Anaphoric Pronouns as Bound Variables Scope Ambiguities and Relative Clauses.- V. Be, Transitive Verbs, Meaning Postulates, and Non-Specific Readings.- VI. Adverbs and Infinitive Complement Verbs.- VII. De dicto Pronouns and Some Pronoun Problems.- VIII. Prepositions, Tenses, and Negation.- Exercises.- Notes.- 8. Montague's General Semiotic Program.- 9. An Annotated Bibliography of Further Work in Montague Semantics.- Appendix I: Index of Symbols.- Appendix II: Variable Type Conventions for Chapter 7.- Notes.- References.- Answers to Selected Problems and Exercises."
            },
            "slug": "Introduction-to-Montague-semantics-Dowty-Wall",
            "title": {
                "fragments": [],
                "text": "Introduction to Montague semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book discusses Montague's Intensional Logic, a Higher-Order Type-Theoretic Language, and some Unresolved Issues with Possible Worlds Semantics and Propositional Attitudes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5737624"
                        ],
                        "name": "E. Rosch",
                        "slug": "E.-Rosch",
                        "structuredName": {
                            "firstName": "Eleanor",
                            "lastName": "Rosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050353"
                        ],
                        "name": "C. Mervis",
                        "slug": "C.-Mervis",
                        "structuredName": {
                            "firstName": "Carolyn",
                            "lastName": "Mervis",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mervis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "This observation has been used to attack the claim that prototypes (Rosch & Mervis 1975) can serve as word senses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17258322,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "93aecb20183cd5e561bb0b6b961702a0798cbee2",
            "isKey": false,
            "numCitedBy": 5119,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Family-resemblances:-Studies-in-the-internal-of-Rosch-Mervis",
            "title": {
                "fragments": [],
                "text": "Family resemblances: Studies in the internal structure of categories"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38663378"
                        ],
                        "name": "J. Fodor",
                        "slug": "J.-Fodor",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Fodor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fodor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113061150"
                        ],
                        "name": "E. Lepore",
                        "slug": "E.-Lepore",
                        "structuredName": {
                            "firstName": "Ernie",
                            "lastName": "Lepore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lepore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14948287,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "b71ab3386f6eb9fbf5e855fa4cf4b80d9dc3b268",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Critique de la notion de similarite du contenu qui caracterise les theories semantiques contemporaines. Mettant en evidence la non-pertinence de l'idee de similarite du point de vue des theories de la signification, l'A. refute l'entreprise de reconstruction conceptuelle developpee par P. Churchland et montre que celle-ci se reduit a une sorte d'ignoratio elenchi (argument par ignorance). Examinant les conditions de satisfaction, ainsi que les criteres de la compositionnalite, de la traduction et de l'explication intentionnelle, l'A. montre que la position de Churchland ne rend compte ni de l'individuation et de l'identite du contenu, d'une part, ni de la nature transdimensionnelle des etats cerebraux, d'autre part"
            },
            "slug": "All-at-sea-in-semantic-space-:-Churchland-on-Fodor-Lepore",
            "title": {
                "fragments": [],
                "text": "All at sea in semantic space : Churchland on meaning similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145073045"
                        ],
                        "name": "R. Hughes",
                        "slug": "R.-Hughes",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Hughes",
                            "middleNames": [
                                "I.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10160281"
                        ],
                        "name": "L. Ballentine",
                        "slug": "L.-Ballentine",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Ballentine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ballentine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 195
                            }
                        ],
                        "text": "The operator we have proposed for combining word meanings is the tensor product; composite systems in QM, formed by interacting quantum-mechanical systems, are represented using tensor products (Hughes 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122191332,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "046050dfd401b0bd89cf2b3803e71362878957a0",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Introduction. The Stern-Gerlach Experiment PART I THE STRUCTURE OF QUANTUM THEORY 1. Vector Spaces Vectors Operators Eigenvectors and Eigenvalues Inner Products of Vectors in R2 Complex Numbers The Space C2 The Pauli Spin Matrices Mathematical Generalization Vector Spaces Linear Operators Inner Products on V Subspaces and Projection Operators Orthonormal Bases Operators with a Discrete Spectrum Operators with a Continuous Spectrum Hilbert Spaces 2. States and Observables in Quantum Mechanics Classical Mechanics: Systems and Their States Observables and Experimental Questions States and Observables in Quantum Theory Probabilities and Expectation Values The Evolution of States in Classical Mechanics Determinism The Evolution of States in Quantum Mechanics Theories and Models 3. Physical Theory and Hilbert Spaces Minimal Assumptions for Physical Theory The Representation of Outcomes and Events The Representation of States Determinism, Indeterminism, and the Principle of Superposition Mixed States Observables and Operators Relations between Observables: Functional Dependence and Compatibility Incompatible Observables The Representational Capacity of Hilbert Spaces The Schrodinger Equation 4. Spin and Its Representation Symmetry Conditions and Spin States A Partial Representation of Spin in R2 The Representation of (Sa) in C2 Conclusion 5. Density Operators and Tensor-Product Spaces Operators of the Trace Class Density Operators Density Operators on C2 Pure and Mixed States The Dynamical Evolution of States Gleason's Theorem Composite Systems and Tensor-Product Spaces The Reduction of States of Composite Systems Part II The Interpretation of Quantum Theory 6. The Problem of Properties Properties, Experimental Questions, and the Dispersion Principle The EPR Argument Bohm's Version of the EPR Experiment The Statistical Interpretation Kochen and Specker's Example Generalizing the Problem The Bell-Wigner Inequality Hidden Variables Interpreting Quantum Theory: Statistical States and Value States 7. Quantum Logic The Algebra of Properties of a Simple Classical System Boolean Algebras Posets and Lattices The Structure of S(H) The Algebra of Events A Formal Approach to Quantum Logic An Unexceptionable Interpretation of Quantum Logic Putnam on Quantum Logic Properties and Deviant Logic 8. Probability, Causality, and Explanation Probability Generalized Two Uniqueness Results The Two-Slit Experiment: Waves and Particles The Two-Slit Experiment: Conditional Probabilities The Bell-Wigner Inequality and Classical Probability Bell Inequalities and Einstein-Locality Bell Inequalities and Causality Coupled Systems and Conditional Probabilities Probability, Causality, and Explanation 9. Measurement Three Principles of Limitation Indeterminacy and Measurement Projection Postulates Measurement and Conditionalization The Measurement Problem and Schrodinger's Cat Jauch's Model of the Measurement Process A Problem for Internal Accounts of Measurement Three Accounts of Measurement 10. An Interpretation of Quantum Theory Abstraction and Interpretation Properties and Latencies: The Quantum Event Interpretation The Copenhagen Interpretation The Priority of the Classical World Quantum Theory and the Classical Horizon Appendix A. Gleason's Theorem Appendix B. The Lyders Rule Appendix C. Coupled Systems and Conditionalization References Index"
            },
            "slug": "The-Structure-and-Interpretation-of-Quantum-Hughes-Ballentine",
            "title": {
                "fragments": [],
                "text": "The Structure and Interpretation of Quantum Mechanics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82323309"
                        ],
                        "name": "Karen Sparck Jones",
                        "slug": "Karen-Sparck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sparck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sparck Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144610645"
                        ],
                        "name": "P. Willett",
                        "slug": "P.-Willett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Willett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Willett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 84
                            }
                        ],
                        "text": "Vector space models of document content originated in information retrieval (Sparck Jones & Willett 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60563699,
            "fieldsOfStudy": [
                "Biology",
                "Economics",
                "Sociology"
            ],
            "id": "80688d9152645cb59cdd8c685ee2301f5245c9a4",
            "isKey": false,
            "numCitedBy": 697,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapter 1 Overall Introduction Chapter 2 History Chapter 3 Key Concepts Chapter 4 Evaluation Chapter 5 Models Chapter 6 Techniques Chapter 7 Systems Chapter 8 Extensions Chapter 9 Envoi"
            },
            "slug": "Readings-in-information-retrieval-Jones-Willett",
            "title": {
                "fragments": [],
                "text": "Readings in information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 1
                            }
                        ],
                        "text": "(Aerts & Gabora 2005) also propose use of the tensor product, in order to combine vector-based representations of concepts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A state-context-property model of concepts and their combinations II: A Hilbert space representation"
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetes 34(1&2):192\u2013221."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "A primary example of this combinatorial strategy comes from linguistics, in which combinatorial theories of syntax, based on the seminal work of (Chomsky 1957), explain how humans can produce and comprehend an unbounded number of sentences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures. The Hague: Mouton"
            },
            "venue": {
                "fragments": [],
                "text": "Syntactic Structures. The Hague: Mouton"
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "A primary example of this combinatorial strategy comes from linguistics, in which combinatorial theories of syntax, based on the seminal work of (Chomsky 1957), explain how humans can produce and comprehend an unbounded number of sentences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "The Hague: Mouton."
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "In some implemented NLP systems (Bos 2005) these axioms are derived automatically from community resources like WordNet."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 153
                            }
                        ],
                        "text": "However, the recent improvements in wide-coverage parsing (Clark & Curran 2004) have led to attempts to scale up symbolic approaches, with some success (Bos 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards widecoverage semantic interpretation"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Sixth International Workshop on Computational Semantics ( IWCS6 )"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Readings in Information Retrieval The Geometry of Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Readings in Information Retrieval The Geometry of Information Retrieval"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Combining-Symbolic-and-Distributional-Models-of-Clark-Pulman/73e897104540642698321c106cc9c35af369fe12?sort=total-citations"
}