{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Moody (1989) has also mentioned this type of dimensionality reduction as a factor in the success of some of the models he has worked with."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5309076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf895330739ec25aa4077ca375daa2cf3d265215",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A class of fast, supervised learning algorithms is presented. They use local representations, hashing, and multiple scales of resolution to approximate functions which are piece-wise continuous. Inspired by Albus's CMAC model, the algorithms learn orders of magnitude more rapidly than typical implementations of back propagation, while often achieving comparable qualities of generalization. Furthermore, unlike most traditional function approximation methods, the algorithms are well suited for use in real time adaptive signal processing. Unlike simpler adaptive systems, such as linear predictive coding, the adaptive linear combiner, and the Kalman filter, the new algorithms are capable of efficiently capturing the structure of complicated non-linear systems. As an illustration, the algorithm is applied to the prediction of a chaotic timeseries."
            },
            "slug": "Fast-Learning-in-Multi-Resolution-Hierarchies-Moody",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Multi-Resolution Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A class of fast, supervised learning algorithms inspired by Albus's CMAC model that use local representations, hashing, and multiple scales of resolution to approximate functions which are piece-wise continuous are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18100780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb4bad84a2fd896edfa4f5c22061b2913fec500d",
            "isKey": false,
            "numCitedBy": 1446,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-discovery-by-competitive-learning-Rumelhart-Zipser",
            "title": {
                "fragments": [],
                "text": "Feature discovery by competitive learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112105858"
                        ],
                        "name": "William Y. Huang",
                        "slug": "William-Y.-Huang",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Huang",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Y. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 130
                            }
                        ],
                        "text": "Moody and Darken (1988) have previously applied to this data an architecture which is very similar to the one suggested here, and Huang and Lippmann (1988) have compared the performance of a number of different classifiers on this same data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11607279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1382a29539b3de419d567f679b5f28cee459a49",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on nets with continuous-valued inputs led to generative procedures to construct convex decision regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions with three-layer perceptrons (two hidden layers). Here we demonstrate that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions. Such classifiers are robust, train rapidly, and provide good performance with simple decision regions. When complex decision regions are required, however, convergence time can be excessively long and performance is often no better than that of k-nearest neighbor classifiers. Three neural net classifiers are presented that provide more rapid training under such situations. Two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms. A third \"feature map classifier\" uses both unsupervised and supervised training. It provides good performance with little supervised training in situations such as speech recognition where much unlabeled training data is available. The architecture of this classifier can be used to implement a neural net k-nearest neighbor classifier."
            },
            "slug": "Neural-Net-and-Traditional-Classifiers-Huang-Lippmann",
            "title": {
                "fragments": [],
                "text": "Neural Net and Traditional Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Bridle (1989) has suggested a different type of output unit for supervised networks, which incorporates the idea of a \"soft max\" type of competition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 16
                            }
                        ],
                        "text": "More recently , Bridle (1989) has applied a supervised algorithm which uses a \"softmax\" output function to this data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59636530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f462943c8d0af69c12a09058251848324135e5a",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct."
            },
            "slug": "Probabilistic-Interpretation-of-Feedforward-Network-Bridle",
            "title": {
                "fragments": [],
                "text": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two modifications are explained: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non- linearity of feed-forward non-linear networks with multiple outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 153
                            }
                        ],
                        "text": "A network of this type can form a smooth approximation to an arbitrary function, with the RBF centers serving as control points for fitting the function (Keeler and Kowalski, 1989; Poggio and Girosi, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807117"
                        ],
                        "name": "T. Sanger",
                        "slug": "T.-Sanger",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sanger",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sanger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 121
                            }
                        ],
                        "text": "Interest in unsupervised learning has increased recently due to the application of more sophisticated mathematical tools (Linsker, 1988; Plumbley and Fallside, 1988; Sanger, 1989) and the success of several elegant simulations of large scale selforganization (Linsker, 1986; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 707975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3975117d907c0e582a35c34137231c87956aa93b",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an optimality principle for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs. We describe an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity. Examples of applications to the problems of image coding, feature detection, and analysis of random-dot stereograms are presented."
            },
            "slug": "An-Optimality-Principle-for-Unsupervised-Learning-Sanger",
            "title": {
                "fragments": [],
                "text": "An Optimality Principle for Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An optimality principle is proposed for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs and an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 200
                            }
                        ],
                        "text": "The classification network consists of a layer of Radial Basis Functions (RBF's) followed by a layer of linear units which attempt to find a least mean square (LMS) fit to the desired output function (Broomhead and Lowe, 1988; Lee and Kill, 1988; Niranjan and Fallside, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 131
                            }
                        ],
                        "text": "One method for using RBF's places the centers of the RBF's at the interstices of some coarse lattice defined over the input space (Broomhead and Lowe, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117200472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5558a34dfd1dbb572895664d38fca04029a99cb",
            "isKey": false,
            "numCitedBy": 2933,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain."
            },
            "slug": "Radial-Basis-Functions,-Multi-Variable-Functional-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed, leading naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108429088"
                        ],
                        "name": "Sukhan Lee",
                        "slug": "Sukhan-Lee",
                        "structuredName": {
                            "firstName": "Sukhan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sukhan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779619"
                        ],
                        "name": "R. Kil",
                        "slug": "R.-Kil",
                        "structuredName": {
                            "firstName": "Rhee",
                            "lastName": "Kil",
                            "middleNames": [
                                "Man"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17239389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c5605cd4430c5c1ce68b9b6e5713ed85dae42a2",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a multilayer feedforward network, called the Gaussian potential function network (GPFN), performing association or classification based on a set of potentially fields synthesized over the domain of input space by a number of Gaussian potential function units (GPFUs). A GPFU as a basic component of the GPFN is designed to generate a Gaussian form of a potential field. A weighted summation of Gaussian potential fields generated by a suitable number of GPFUs provides an arbitrary shape of a potential field over the domain of input space. The authors also present a detailed learning algorithm for the GPFN. Learning consists of the determination of the minimally necessary number of GPFUs and the adjustment of the locations and shapes of the individual Gaussian potential fields defined by GPFUs as well as the summation weights. The learning of the minimally necessary number of GPFUs is based on the control of the effective radius of GPFUs, while the parameter learning is based on the gradient descent procedure.<<ETX>>"
            },
            "slug": "Multilayer-feedforward-potential-function-network-Lee-Kil",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward potential function network"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a multilayer feedforward network, called the Gaussian potential function network (GPFN), performing association or classification based on a set of potentially fields synthesized over the domain of input space by a number of GaRussian potential function units (GPFUs)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 121
                            }
                        ],
                        "text": "Interest in unsupervised learning has increased recently due to the application of more sophisticated mathematical tools (Linsker, 1988; Plumbley and Fallside, 1988; Sanger, 1989) and the success of several elegant simulations of large scale selforganization (Linsker, 1986; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2410937"
                        ],
                        "name": "K. Basford",
                        "slug": "K.-Basford",
                        "structuredName": {
                            "firstName": "Kaye",
                            "lastName": "Basford",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Basford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 187
                            }
                        ],
                        "text": "Variants on this theme are the schemes in which, in addition to the winner, all of the losers are updated in some uniform fashion2 \u2022 Within the statistical pattern recognition literature (Duda and Hart, 1973; McLachlan and Basford, 1988) a rather different form of competition is frequently encountered."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119405289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "034a70c9fbe8e0075f5a5b3a7b06bdf7d3cab4a1",
            "isKey": false,
            "numCitedBy": 2073,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "General Introduction Introduction History of Mixture Models Background to the General Classification Problem Mixture Likelihood Approach to Clustering Identifiability Likelihood Estimation for Mixture Models via EM Algorithm Start Values for EMm Algorithm Properties of Likelihood Estimators for Mixture Models Information Matrix for Mixture Models Tests for the Number of Components in a Mixture Partial Classification of the Data Classification Likelihood Approach to Clustering Mixture Models with Normal Components Likelihood Estimation for a Mixture of Normal Distribution Normal Homoscedastic Components Asymptotic Relative Efficiency of the Mixture Likelihood Approach Expected and Observed Information Matrices Assessment of Normality for Component Distributions: Partially Classified Data Assessment of Typicality: Partially Classified Data Assessment of Normality and Typicality: Unclassified Data Robust Estimation for Mixture Models Applications of Mixture Models to Two-Way Data Sets Introduction Clustering of Hemophilia Data Outliers in Darwin's Data Clustering of Rare Events Latent Classes of Teaching Styles Estimation of Mixing Proportions Introduction Likelihood Estimation Discriminant Analysis Estimator Asymptotic Relative Efficiency of Discriminant Analysis Estimator Moment Estimators Minimum Distance Estimators Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture Likelihood Approach to Clustering Introduction Estimators of the Allocation Rates Bias Correction of the Estimated Allocation Rates Estimated Allocation Rates of Hemophilia Data Estimated Allocation Rates for Simulated Data Other Methods of Bias Corrections Bias Correction for Estimated Posterior Probabilities Partitioning of Treatment Means in ANOVA Introduction Clustering of Treatment Means by the Mixture Likelihood Approach Fitting of a Normal Mixture Model to a RCBD with Random Block Effects Some Other Methods of Partitioning Treatment Means Example 1 Example 2 Example 3 Example 4 Mixture Likelihood Approach to the Clustering of Three-Way Data Introduction Fitting a Normal Mixture Model to Three-Way Data Clustering of Soybean Data Multidimensional Scaling Approach to the Analysis of Soybean Data References Appendix"
            },
            "slug": "Mixture-models-:-inference-and-applications-to-McLachlan-Basford",
            "title": {
                "fragments": [],
                "text": "Mixture models : inference and applications to clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The Mixture Likelihood Approach to Clustering and the Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture likelihood approach toClustering."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7708166,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f8fba8a2a6f1209f4e09cede01c8b2c64a656aea",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The functional architecture of mammalian visual cortex has been elucidated in impressive detail by experimental work of the past 20-25 years. The origin of many of the salient features of this architecture, however, has remained unexplained. This paper is the first of three (the others will appear in subsequent issues of these Proceedings) that address the origin and organization of feature-analyzing (spatial-opponent and orientation-selective) cells in simple systems governed by biologically plausible development rules. I analyze the progressive maturation of a system composed of a few layers of cells, with connections that develop according to a simple set of rules (including Hebb-type modification). To understand the prenatal origin of orientation-selective cells in certain primates, I consider the case in which there is no external input, with the first layer exhibiting random spontaneous electrical activity. No orientation preference is specified to the system at any stage, and none of the basic developmental rules is specific to visual processing. Here I introduce the theory of \"modular self-adaptive networks,\" of which this system is an example, and explicitly demonstrate the emergence of a layer of spatial-opponent cells. This sets the stage for the emergence, in succeeding layers, of an orientation-selective cell population."
            },
            "slug": "From-basic-network-principles-to-neural-emergence-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture: emergence of spatial-opponent cells."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper is the first of three that address the origin and organization of feature-analyzing cells in simple systems governed by biologically plausible development rules, and introduces the theory of \"modular self-adaptive networks,\" of which this system is an example, and explicitly demonstrates the emergence of a layer of spatial-opponent cells."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 32
                            }
                        ],
                        "text": "In the basic k-means algorithm (Duda and Hart, 1973) the training samples are first assigned to the class of the closest mean."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16928,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145387873"
                        ],
                        "name": "M. Niranjan",
                        "slug": "M.-Niranjan",
                        "structuredName": {
                            "firstName": "Mahesan",
                            "lastName": "Niranjan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Niranjan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 200
                            }
                        ],
                        "text": "The classification network consists of a layer of Radial Basis Functions (RBF's) followed by a layer of linear units which attempt to find a least mean square (LMS) fit to the desired output function (Broomhead and Lowe, 1988; Lee and Kill, 1988; Niranjan and Fallside, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62772106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ae0232bb68e0e25392751991bde4f6d51b9f32b",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-and-radial-basis-functions-in-Niranjan-Fallside",
            "title": {
                "fragments": [],
                "text": "Neural networks and radial basis functions in classifying static speech patterns"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143736719"
                        ],
                        "name": "E. Hartman",
                        "slug": "E.-Hartman",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Hartman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hartman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1952587"
                        ],
                        "name": "J. Keeler",
                        "slug": "J.-Keeler",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Keeler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Keeler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39221643"
                        ],
                        "name": "J. Kowalski",
                        "slug": "J.-Kowalski",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Kowalski",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kowalski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 153
                            }
                        ],
                        "text": "A network of this type can form a smooth approximation to an arbitrary function, with the RBF centers serving as control points for fitting the function (Keeler and Kowalski, 1989; Poggio and Girosi, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44931577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68ec8a1e9aea1916e2280489729bab74d5bf6631",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network with a single layer of hidden units of gaussian type is proved to be a universal approximator for real-valued maps defined on convex, compact sets of Rn."
            },
            "slug": "Layered-Neural-Networks-with-Gaussian-Hidden-Units-Hartman-Keeler",
            "title": {
                "fragments": [],
                "text": "Layered Neural Networks with Gaussian Hidden Units as Universal Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A neural network with a single layer of hidden units of gaussian type is proved to be a universal approximator for real-valued maps defined on convex, compact sets of Rn."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144472878"
                        ],
                        "name": "G. E. Peterson",
                        "slug": "G.-E.-Peterson",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Peterson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. E. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32757098"
                        ],
                        "name": "H. L. Barney",
                        "slug": "H.-L.-Barney",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Barney",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. L. Barney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 138
                            }
                        ],
                        "text": "The data consisted of a digitized version of the first and second formant frequencies of 10 vowels for multiple male and female speakers (Peterson and Barney, 1952)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33784988,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "970d43f4f3bb68bd11cd3e16bba758287a3ee9e1",
            "isKey": false,
            "numCitedBy": 3013,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Relationships between a listener's identification of a spoken vowel and its properties as revealed from acoustic measurement of its sound wave have been a subject of study by many investigators. Both the utterance and the identification of a vowel depend upon the language and dialectal backgrounds and the vocal and auditory characteristics of the individuals concerned. The purpose of this paper is to discuss some of the control methods that have been used in the evaluation of these effects in a vowel study program at Bell Telephone Laboratories. The plan of the study, calibration of recording and measuring equipment, and methods for checking the performance of both speakers and listeners are described. The methods are illustrated from results of tests involving some 76 speakers and 70 listeners."
            },
            "slug": "Control-Methods-Used-in-a-Study-of-the-Vowels-Peterson-Barney",
            "title": {
                "fragments": [],
                "text": "Control Methods Used in a Study of the Vowels"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Control methods used in the evaluation of effects of language and dialectal backgrounds and vocal and auditory characteristics of the individuals concerned in a vowel study program at Bell Telephone Laboratories are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060224464"
                        ],
                        "name": "Y. L. Cun",
                        "slug": "Y.-L.-Cun",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Cun",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. L. Cun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403028455"
                        ],
                        "name": "Fran\u00e7oise Fogelman-Souli\u00e9",
                        "slug": "Fran\u00e7oise-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7oise Fogelman-Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195643413,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "47c7e23da6bdd9ac91dae8751af25696fea2c332",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Etude des modeles d'apprentissage simples et application aux memoires associatives. Methodes d'apprentissage pour reseaux a cellules cachees. Algorithme de retro propagation et ses variantes. Applications diverses (associative, reconnaissance de caracteres, diagnostic medical) et logiciel de simulation"
            },
            "slug": "Mod\u00e8les-connexionnistes-de-l'apprentissage-Cun-Fogelman-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Mod\u00e8les connexionnistes de l'apprentissage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47804608"
                        ],
                        "name": "R. Bek",
                        "slug": "R.-Bek",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Bek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20520403,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a09d930fea5161406f735acf02e703e7824811dc",
            "isKey": false,
            "numCitedBy": 721,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Discourse-on-one-way-in-which-a-quantum-mechanics-Bek",
            "title": {
                "fragments": [],
                "text": "Discourse on one way in which a quantum-mechanics language on the classical logical base can be built up"
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetika"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Clauijication And Scene Analy&i"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1913
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 173
                            }
                        ],
                        "text": "Some simulations have also been performed with a network in which each RBF had a diagonal covariance matrix, and each of the d variance components was estimated separately (Nowlan, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood competition in RBF networks"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CRGT~90-2,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature discovery by competitive learning. In Parallel di6trib.ted proceuing: Exploration. in the micro!tructure of cognition, volume I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of networks for approximation and learning. A.I. Memo 1140"
            },
            "venue": {
                "fragments": [],
                "text": "A theory of networks for approximation and learning. A.I. Memo 1140"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sin the digit task, there are over 25 times as many parameters in the unsupervised part of the network as there are in the supervised part"
            },
            "venue": {
                "fragments": [],
                "text": "Sin the digit task, there are over 25 times as many parameters in the unsupervised part of the network as there are in the supervised part"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Moody and Darken (1988) have suggested a method in which a much smaller number of RBF's are used, however the centers of these RBF's are allowed to adapt to the input samples, so they learn to represent only the part of the input space actually represented by the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Moody and Darken (1988) have previously applied to this data an architecture which is very similar to the one suggested here, and Huang and Lippmann (1988) have compared the performance of a number of different classifiers on this same data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with localized receptive fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding. oj the 1988 Connectioni!t Model! Summer School,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 74
                            }
                        ],
                        "text": "The back propagation network was developed specifically for this task (Ie Cun, 1987), and used a specialized architecture with three layers of hidden units, localized receptive fields, and weight sharing to reduce the number of free parameters in the system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modele! Connexionni!te$ de l'Apprentiuage"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Universit~ Pierre et Marie Curie,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 200
                            }
                        ],
                        "text": "The classification network consists of a layer of Radial Basis Functions (RBF's) followed by a layer of linear units which attempt to find a least mean square (LMS) fit to the desired output function (Broomhead and Lowe, 1988; Lee and Kill, 1988; Niranjan and Fallside, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multilayer feedfo.,ward potential function networks"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceeding! IEEE Second International ConJerence on Ne.ral Network!,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 160
                            }
                        ],
                        "text": "These algorithms can be regarded as hard competitive processes, and new algorithms which use the soft assumption may be developed from the bootstrap procedure (Nowlan and Hinton, 1989)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood decision-directed adaptive equalization"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CRG-TR-89-8,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 121
                            }
                        ],
                        "text": "Interest in unsupervised learning has increased recently due to the application of more sophisticated mathematical tools (Linsker, 1988; Plumbley and Fallside, 1988; Sanger, 1989) and the success of several elegant simulations of large scale selforganization (Linsker, 1986; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information theoretic approach to unsupervised connectionist models"
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding! oj the 1988 Connec. tioni$t Model! Summer School,"
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Maximum-Likelihood-Competitive-Learning-Nowlan/57c4baa5528ba805fc27eee86613c99503978fed?sort=total-citations"
}