{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24265259"
                        ],
                        "name": "Yuichi Nakamura",
                        "slug": "Yuichi-Nakamura",
                        "structuredName": {
                            "firstName": "Yuichi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuichi Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 284
                            }
                        ],
                        "text": "The dictionary consists of two different sources: the Oxford Advanced Learner's Dictionary [13] for general words (69,517 words) and noun word collections which are compiled by analyzing closed caption information of videos to obtain people, organization or place names (3,035 words) [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7581740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8527aa77e561497773d990cd72d6c16228d72632",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Spotting by Association method for video analysis is a novel metliod to detect video segments with typical semantics. Video data contains various kinds of information through continuous images, natural language, and sound. For videos to be stored and retrieved in a Digital Library, it is essential to segment the video data into meaningful pieces. To detect meaningful segments, we need to identify the segment in each modality (video, language, and sound) that corresponds to the same story. For this purpose, we propose a new method for making correspondences between image clues detected by image analysis and Iangriage clries detected by natural language analysis. As a result, relevant video segments with sufficient informat ion froni every modality are obtained. We applied OUT nietliod to closed-captioned C N N Headline News. Video segments with important events, such as a public speech, meeting, or visit. are detc-cted fairly well."
            },
            "slug": "Semantic-analysis-for-video-contents-by-association-Nakamura-Kanade",
            "title": {
                "fragments": [],
                "text": "Semantic analysis for video contents extraction\u2014spotting by association in news video"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new method for making correspondences between image clues detected by image analysis and Iangriage clries detected by natural language analysis is proposed and applied to closed-captioned C N N Headline News."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [3], there is an assumption that characters are drawn in high contrast against the background."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "Automatic character segmentation was performed for titles and credits in motion picture videos in [2] and [3]; however, both papers have insu cient consideration of character recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "These frame numbers are determined by text region detection [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "Smith and Kanade describe text region detection using the properties in [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "Automatic character segmentation was performed for titles and credits in motion picture videos in [2] and [3]; however, both papers have insu cient consideration of character recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14666057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d98302953046ebf4d574d05db26a90db95789cc",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Digital video is rapidly becoming important for education, entertainment, and a host of multimedia applications. With the size of the video collections growing to thousands of hours, technology is needed to effectively browse segments in a short time without losing the content of the video. We propose a method to extract the significant audio and video information and create a \"skim\" video which represents a very short synopsis of the original. The goal of this work is to show the utility of integrating language and image understanding techniques for video skimming by extraction of significant information, such as specific objects, audio keywords and relevant video structure. The resulting skim video is much shorter, where compaction is as high as 20:1, and yet retains the essential content of the original segment."
            },
            "slug": "Video-skimming-and-characterization-through-the-of-Smith",
            "title": {
                "fragments": [],
                "text": "Video skimming and characterization through the combination of image and language understanding techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The goal of this work is to show the utility of integrating language and image understanding techniques for video skimming by extraction of significant information, such as specific objects, audio keywords and relevant video structure."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679880"
                        ],
                        "name": "H. Wactlar",
                        "slug": "H.-Wactlar",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Wactlar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wactlar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48817314"
                        ],
                        "name": "S. Stevens",
                        "slug": "S.-Stevens",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Stevens",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8345108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b4d170b81ffc895e5b7960040a3f44a044d6bb8",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval. Intelligent, automatic mechanisms will be developed to populate the library. Search and retrieval from digital video, audio, and text libraries will take place via desktop computer over local, metropolitan, and wide-area networks. The project's approach applies several techniques for content-based searching and video-sequence retrieval. Content is conveyed in both the narrative (speech and language) and the image. Only by the collaborative interaction of image, speech, and natural language understanding technology is it possible to successfully populate, segment, index, and search diverse video collections with satisfactory recall and precision. This collaborative interaction approach uniquely compensates for problems of interpretation and search in error-ridden and ambiguous data sets. The authors have focused the work on two corpuses. One is science documentaries and lectures, the other is broadcast news content with partial closed-captions. Further work will continue to improve the accuracy and performance of the underlying processing as well as explore performance issues related to Web-based access and interoperability with other digital video resources."
            },
            "slug": "Intelligent-Access-to-Digital-Video:-Informedia-Wactlar-Kanade",
            "title": {
                "fragments": [],
                "text": "Intelligent Access to Digital Video: Informedia Project"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval, and focused the work on two corpuses."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113455824"
                        ],
                        "name": "Zhibin Lei",
                        "slug": "Zhibin-Lei",
                        "structuredName": {
                            "firstName": "Zhibin",
                            "lastName": "Lei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhibin Lei"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "The problem of resolution is discussed in OCR of World Wide Web images [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29236195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f719c98abe9e3da61cb39ca1b836aeb09a4624",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A significant amount of text now present in World Wide Web documents is embedded in image data, and a large portion of it does not appear elsewhere at all. To make this information available, we need to develop techniques for recovering textual information from in-line Web images. In this paper, we describe two methods for Web image OCR. Recognizing text extracted from in-line Web images is difficult because characters in these images are often rendered at a low spatial resolution. Such images are typically considered to be 'low quality' by traditional OCR technologies. Our proposed methods utilize the information contained in the color bits to compensate for the loss of information due to low sampling resolution. The first method uses a polynomial surface fitting technique for object recognition. The second method is based on the traditional n-tuple technique. We collected a small set of character samples from Web documents and tested the two algorithms. Preliminary experimental results show that our n-tuple method works quite well. However, the surface fitting method performs rather poorly due to the coarseness and small number of color shades used in the text."
            },
            "slug": "OCR-for-World-Wide-Web-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "OCR for World Wide Web images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two methods for Web image OCR based on the traditional n-tuple technique, which utilize the information contained in the color bits to compensate for the loss of information due to low sampling resolution."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "For the cases of the license plate research [4], the scene image research [5] and the document image analysis [7], the di culty of complex backgrounds is rarely a problem although examples with plain backgrounds are illustrated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "For the cases of the license plate research [4], the scene image research [5] and the document image analysis [7], the di culty of complex backgrounds is rarely a problem although examples with plain backgrounds are illustrated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [5], characters in scene images are segmented and recognized based on adaptive thresholding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "There are similar research elds which concern character recognition of videos [4] [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700567"
                        ],
                        "name": "S. Satoh",
                        "slug": "S.-Satoh",
                        "structuredName": {
                            "firstName": "Shin\u2019ichi",
                            "lastName": "Satoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "For instance, OCR results can improve a system that associates names and faces in news videos [15] to enhance query functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2304541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58942abb806c435d880cc6717eccd4963528dc53",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel approach to extract meaningful content information from video by collaborative integration of image understanding and natural language processing. As an actual example, we developed a system that associates faces and names in videos, called Name-It, which is given news videos as a knowledge source, then automatically extracts face and name association as content information. The system can infer the name of a given unknown face image, or guess faces which are likely to have the name given to the system. This paper explains the method with several successful matching results which reveal effectiveness in integrating heterogeneous techniques as well as the importance of real content information extraction from video, especially face-name association."
            },
            "slug": "Name-It:-association-of-face-and-name-in-video-Satoh-Kanade",
            "title": {
                "fragments": [],
                "text": "Name-It: association of face and name in video"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A system that associates faces and names in videos, called Name-It, is developed, which is given news videos as a knowledge source, then automatically extracts face and name association as content information."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2600277"
                        ],
                        "name": "Dong-June Lee",
                        "slug": "Dong-June-Lee",
                        "structuredName": {
                            "firstName": "Dong-June",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong-June Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974518"
                        ],
                        "name": "Hee-Seon Park",
                        "slug": "Hee-Seon-Park",
                        "structuredName": {
                            "firstName": "Hee-Seon",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hee-Seon Park"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "Character segmentation based on recognition results [11] offers the possibility to improve the accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 891630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbad29bda5e5955e46d3d1b2889a37fee1a8da02",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Generally speaking, through the binarization of gray-scale images, useful information for the segmentation of touching or overlapping characters may be lost. If we analyze gray-scale images, however, specific topographic features and the variation of intensity can be observed in the character boundaries. We believe that such kinds of clues obtained from gray-scale images should be useful for efficient character segmentation. In this paper, we propose a new methodology for character segmentation and recognition which makes the best use of the characteristics of gray-scale images. In the proposed methodology, the character segmentation regions are determined by using projection profiles and topographic features extracted form gray-scale images. Then the nonlinear character segmentation path in each character segmentation region is found by using multistage graph search algorithm. Finally, in order to confirm the character segmentation paths and recognition results, recognition based segmentation method is adopted."
            },
            "slug": "A-new-methodology-for-gray-scale-character-and-Lee-Lee",
            "title": {
                "fragments": [],
                "text": "A new methodology for gray-scale character segmentation and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In this paper, a new methodology for character segmentation and recognition which makes the best use of the characteristics of gray-scale images is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143773571"
                        ],
                        "name": "Yi Lu",
                        "slug": "Yi-Lu",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Lu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Although there are many approaches for character segmentation [10], errors still remain because most methods analyze only a vertical projection pro le."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205014256,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0341a881c439aa8070c29fd8d55df8ee52567bec",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-printed-character-segmentation-;-An-Lu",
            "title": {
                "fragments": [],
                "text": "Machine printed character segmentation --; An overview"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115439468"
                        ],
                        "name": "Yuntao Cui",
                        "slug": "Yuntao-Cui",
                        "structuredName": {
                            "firstName": "Yuntao",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntao Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [4], character extraction from the car license plate using video images is presented."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "For the cases of the license plate research [4], the scene image research [5] and the document image analysis [7], the di culty of complex backgrounds is rarely a problem although examples with plain backgrounds are illustrated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "There are similar research elds which concern character recognition of videos [4] [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2459300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e75b09dff3656243fde5cacf9baf1c42a4ea972",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images. We model the extraction of characters as a Markov random field (MRF). With the MRF modeling, the extraction of characters is formulated as the problem of maximizing the a posteriori probability based on given prior and observations. A genetic algorithm with local greedy mutation operator is employed to optimize the objective function. Experiments and comparison study were conducted. It is shown that our approach provides better performance than other single frame methods."
            },
            "slug": "Character-extraction-of-license-plates-from-video-Cui-Huang",
            "title": {
                "fragments": [],
                "text": "Character extraction of license plates from video"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown that this approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images provides better performance than other single frame methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47246616"
                        ],
                        "name": "R. Brunelli",
                        "slug": "R.-Brunelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Brunelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brunelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [8], several template matching methods are compared to determine which is the best for detecting an eye in a face image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 381214,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "ebb3de0cede2df3868cbd075aabf695a585a029b",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Template-matching:-matched-spatial-filters-and-Brunelli-Poggio",
            "title": {
                "fragments": [],
                "text": "Template matching: matched spatial filters and beyond"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "It is assumed that, for characters composed of line elements, a line detection method [9] is more applicable."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14638828,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "3f19b1da7aa9f02164e8ad3a20fe40fa2e7650ee",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Angiography is a method used by radiologists to examine the structure and health of blood vessels. The method consists of injecting an x-ray opaque contrast agent into the bloodstream, and taking one or more x-ray images of the vessel. Although these images are clear and have high resolution, they individually show structures in only two-dimensions. The goal of our work is to automatically reconstruct the threedimensional structure of a blood vessel using a small number of angiogram images taken from different angles. Among the types of information about blood vessels that are useful to physicians, the most important for diagnostic purposes is a measure of the cross-sectional area of the vessel at every location along its length. This information reveals constrictions caused by blood clots, cholesterol build-ups, or injuries. Treatment of these conditions may involve inserting a catheter, with a small video camera and scraping tool mounted at its tip, into the vessel to remove the obstruction. A computer can aid in planning this surgery by simulating what the tip of the catheter would encounter in the vessel, and by computing how far the catheter must be inserted to begin repair. This paper presents an algorithm which performs the three-dimensional reconstruction task. Each section of the paper will examine one of the main steps of the algorithm: detecting vessels in single images, finding the positions of the vessels in three dimensions, and finally performing diameter measurements. The paper concludes with some reconstruction results."
            },
            "slug": "Reconstructing-3-D-Blood-Vessel-Shapes-from-X-Ray-Rowley-Kanade",
            "title": {
                "fragments": [],
                "text": "Reconstructing 3-D Blood Vessel Shapes from Multiple X-Ray Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents an algorithm which performs the three-dimensional reconstruction task and examines one of the main steps of the algorithm: detecting vessels in single images, finding the positions of the vessels in three dimensions, and finally performing diameter measurements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [4], character extraction from the car license plate using video images is presented."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "For the cases of the license plate research [4], the scene image research [5] and the document image analysis [7], the di culty of complex backgrounds is rarely a problem although examples with plain backgrounds are illustrated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "There are similar research elds which concern character recognition of videos [4] [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Character extraction of license plates"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1994 annual research report"
            },
            "venue": {
                "fragments": [],
                "text": "1994 annual research report"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [8], several template matching methods are compared to determine which is the best for detecting an eye in a face image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Template match-  ing: Matched spatial lters and beyond,"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern  Recognition,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "The problem of resolution is discussed in OCR of World Wide Web images [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Z"
            },
            "venue": {
                "fragments": [],
                "text": "Lei, \\OCR for World  Wide Web images,\" In Proceedings of SPIE Doc-  ument Recognition IV, vol. 3027, pp. 58-66"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toshio Sato is currently working for Toshiba Corporation , Japan. E-mail: toshio4.sato@toshiba.co"
            },
            "venue": {
                "fragments": [],
                "text": "Toshio Sato is currently working for Toshiba Corporation , Japan. E-mail: toshio4.sato@toshiba.co"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "For the cases of the license plate research [4], the scene image research [5] and the document image analysis [7], the di culty of complex backgrounds is rarely a problem although examples with plain backgrounds are illustrated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and E"
            },
            "venue": {
                "fragments": [],
                "text": "M. Riseman,  \\Finding text in images,\" In 20th International  ACM SIGIR Conference on Research and De-  velopment in Information Retrieval, pp. 3-12,  Philadelphia"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Template matching: Matched spatial lters and beyond Pattern Recognition, v ol Reconstructing 3-D blood vessel shapes from multiple X-ray images"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Workshop on Computer Vision for Medical Image Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic analysis  for video contents extraction - spotting by asso-  ciation in news"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stevens, \\Intelligent access to digital video: The Informedia project"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Computer"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Video-OCR-for-Digital-News-Archives-Sato-Kanade/71d684a6ddbdc3f816e678e4f2ca9ec0a58f3387?sort=total-citations"
}