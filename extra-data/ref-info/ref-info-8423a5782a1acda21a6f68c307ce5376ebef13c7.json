{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734327"
                        ],
                        "name": "N. Alon",
                        "slug": "N.-Alon",
                        "structuredName": {
                            "firstName": "Noga",
                            "lastName": "Alon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "1, repeating a previous analysis [2]), trace-norm or max-norm (Sections 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "Generalization error bounds for prediction with low-rank matrices can be obtained by considering the number of sign configurations of low-rank matrices [2] (following techniques introduced in [8]):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 520370,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2ece0331780d3377071134263355c3e63f04eb05",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove generalization error bounds for predicting entries in a partially observed matrix by fitting the observed entries with a low-rank matrix. In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of finite pseudodimension such that the sums of functions from this class have unbounded pseudodimension."
            },
            "slug": "Generalization-Error-Bounds-for-Collaborative-with-Srebro-Alon",
            "title": {
                "fragments": [],
                "text": "Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "It is proved that generalization error bounds for predicting entries in a partially observed matrix are generalized by fitting the observed entries with a low-rank matrix."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "R\u0302S(X1[1]) = E\u03c3 \u23a1 \u23a3 sup |u|=|v|=1 \u2223\u2223\u2223\u2223\u2223 2 |S| |S| \u2211"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "We use the fact that the Rademacher complexity does not change when taking convex combinations, and calculate the Rademacher complexity of X1[1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "R\u0302S(X1[1]) = E\u03c3 \u23a1 \u23a3 sup |u|,|v|=1 \u2223\u2223\u2223\u2223\u2223 2 |S| \u2211"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Therefore, X [1] = convX1[1], where X1[1] ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "Recently, the trace-norm and max-norm were suggested as alternative measures of complexity with strong connections to maximum-margin linear classification [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "3, elaborating on and proving previously quoted bounds [1])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "R\u0302S(X1[1]) \u2264 2 |S|E\u03c3[\u2016\u03c3\u2016Fro] \u2264 2 |S| \u221a |S| = 2 \u221a|S| (4)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "1 of [10] to \u03c3\u0303ij , we obtain: R\u0302S(X1[1]) \u2264 2 |S| [\u2016\u03c3\u0303\u20162] 2 |S| \u2264 K(ln m) 1 4 ( max i |si\u00b7| + max j |s\u00b7j | ) (5)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5048382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf154c28178370d95510112413dc8cb48120a8",
            "isKey": false,
            "numCitedBy": 1105,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them."
            },
            "slug": "Maximum-Margin-Matrix-Factorization-Srebro-Rennie",
            "title": {
                "fragments": [],
                "text": "Maximum-Margin Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel approach to collaborative prediction is presented, using low-norm instead of low-rank factorizations, inspired by, and has strong connections to, large-margin linear discrimination."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3207496"
                        ],
                        "name": "Y. Seginer",
                        "slug": "Y.-Seginer",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Seginer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Seginer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "1 of [10], which bounds the expected spectral norm of matrices with entries of fixed magnitudes but random signs in terms of the maximum row and column magnitude norms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "1 of [10] to \u03c3\u0303ij , we obtain: R\u0302S(X1[1]) \u2264 2 |S| [\u2016\u03c3\u0303\u20162] 2 |S| \u2264 K(ln m) 1 4 ( max i |si\u00b7| + max j |s\u00b7j | ) (5)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 21647971,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3a49c23e1ff954d3d002e9c85f4005279a14222b",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare the Euclidean operator norm of a random matrix with the Euclidean norm of its rows and columns. In the first part of this paper, we show that if A is a random matrix with i.i.d. zero mean entries, then E\u2225A\u2225h [les ] Kh (E maxi \u2225ai[bull ] \u2225h + E maxj \u2225aj[bull ] \u2225h), where K is a constant which does not depend on the dimensions or distribution of A (h, however, does depend on the dimensions). In the second part we drop the assumption that the entries of A are i.i.d. We therefore consider the Euclidean operator norm of a random matrix, A, obtained from a (non-random) matrix by randomizing the signs of the matrix's entries. We show that in this case, the best inequality possible (up to a multiplicative constant) is E\u2225A\u2225h [les ] (c log1/4 min {m, n})h (E maxi \u2225ai[bull ] \u2225h + E maxj \u2225aj[bull ] \u2225h) (m, n the dimensions of the matrix and c a constant independent of m, n)."
            },
            "slug": "The-Expected-Norm-of-Random-Matrices-Seginer",
            "title": {
                "fragments": [],
                "text": "The Expected Norm of Random Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The Euclidean operator norm of a random matrix, A, obtained from a (non-random) matrix by randomizing the signs of the matrix's entries is considered, which is the best inequality possible (up to a multiplicative constant)."
            },
            "venue": {
                "fragments": [],
                "text": "Combinatorics, Probability and Computing"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2266267"
                        ],
                        "name": "S. Mendelson",
                        "slug": "S.-Mendelson",
                        "structuredName": {
                            "firstName": "Shahar",
                            "lastName": "Mendelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mendelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093789"
                        ],
                        "name": "G. Schechtman",
                        "slug": "G.-Schechtman",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Schechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2994279"
                        ],
                        "name": "A. Shraibman",
                        "slug": "A.-Shraibman",
                        "structuredName": {
                            "firstName": "Adi",
                            "lastName": "Shraibman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shraibman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "= min{rank X|X \u2208 SP(Y )} = min{rank X|X \u2208 SP(1)(Y )} (1) The max-norm (also known as the \u03b32-norm [7]) of a matrix X is given by: \u2016X\u2016max ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13109934,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d849c7c58892e9db6524738e41023d6abfdae694",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider four previously known parameters of sign matrices from a complexity-theoretic perspective. The main technical contributions are tight (or nearly tight) inequalities that we establish among these parameters. Several new open problems are raised as well."
            },
            "slug": "Complexity-measures-of-sign-matrices-Linial-Mendelson",
            "title": {
                "fragments": [],
                "text": "Complexity measures of sign matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Four previously known parameters of sign matrices from a complexity-theoretic perspective are considered and tight (or nearly tight) inequalities that are established among these parameters are established."
            },
            "venue": {
                "fragments": [],
                "text": "Comb."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 188
                            }
                        ],
                        "text": "The class of rank-one sign matrices is a finite class of size |X\u00b1| = 2n+m\u22121, and so its empirical Rademacher complexity (for any sample) can be bounded by R\u0302S(X\u00b1) < \u221a 7 2(n+m)+log |S| |S| [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1841097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e39054d8b3c10aaa50e716230ef27db40f2f0c5c",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "Matrices that can be factored into a product of two simpler matrices can serve as a useful and often natural model in the analysis of tabulated or high-dimensional data. Models based on matrix factorization (Factor Analysis, PCA) have been extensively used in statistical analysis and machine learning for over a century, with many new formulations and models suggested in recent years (Latent Semantic Indexing, Aspect Models, Probabilistic PCA, Exponential PCA, Non-Negative Matrix Factorization and others). In this thesis we address several issues related to learning with matrix factorizations: we study the asymptotic behavior and generalization ability of existing methods, suggest new optimization methods, and present a novel maximum-margin high-dimensional matrix factorization formulation. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Learning-with-matrix-factorizations-Srebro",
            "title": {
                "fragments": [],
                "text": "Learning with matrix factorizations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis addresses several issues related to learning with matrix factorizations, study the asymptotic behavior and generalization ability of existing methods, suggest new optimization methods, and present a novel maximum-margin high-dimensional matrix factorization formulation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098146"
                        ],
                        "name": "Nadav Eiron",
                        "slug": "Nadav-Eiron",
                        "structuredName": {
                            "firstName": "Nadav",
                            "lastName": "Eiron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadav Eiron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Ben David et al [4] used this to show that dc(Y ) = O(mc(2)(Y ) log n)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 214
                            }
                        ],
                        "text": "Previously, examples in which the max-complexity is a polynomial function of the dimensional-complexity [5], or where the dimensional-complexity is constant but the max-complexity is logarithmic in the matrix size [4] have been shown."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Ben David et al [4] used this to show that dc(Y ) = O(mc2(Y ) log n)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 249
                            }
                        ],
                        "text": "kernel methods) as a generic approach to classification leads one to ask what concept classes can or cannot be embedded as low-dimensional or large-margin linear separators; that is, what matrices have high dimensional-complexity and max-complexity [4, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "In Section 4 we show this for general measures of discrepancy, generalizing previous results [3, 4] for binary target matrices."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14274886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13ea05e6c5af98e109b66e656e95677476bf3880",
            "isKey": true,
            "numCitedBy": 100,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The notion of embedding a class of dichotomies in a class of linear half spaces is central to the support vector machines paradigm. We examine the question of determining the minimal Euclidean dimension and the maximal margin that can be obtained when the embedded class has a finite VC dimension. We show that an overwhelming majority of the family of finite concept classes of any constant VC dimension cannot be embedded in low-dimensional half spaces. (In fact, we show that the Euclidean dimension must be almost as high as the size of the instance space.) We strengthen this result even further by showing that an overwhelming majority of the family of finite concept classes of any constant VC dimension cannot be embedded in half spaces (of arbitrarily high Euclidean dimension) with a large margin. (In fact, the margin cannot be substantially larger than the margin achieved by the trivial embedding.) Furthermore, these bounds are robust in the sense that allowing each image half space to err on a small fraction of the instances does not imply a significant weakening of these dimension and margin bounds. Our results indicate that any universal learning machine, which transforms data into the Euclidean space and then applies linear (or large margin) classification, cannot enjoy any meaningful generalization guarantees that are based on either VC dimension or margins considerations. This failure of generalization bounds applies even to classes for which \"straight forward\" empirical risk minimization does enjoy meaningful generalization guarantees."
            },
            "slug": "Limitations-of-Learning-Via-Embeddings-in-Euclidean-Ben-David-Eiron",
            "title": {
                "fragments": [],
                "text": "Limitations of Learning Via Embeddings in Euclidean Half Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results indicate that any universal learning machine, which transforms data into the Euclidean space and then applies linear (or large margin) classification, cannot enjoy any meaningful generalization guarantees that are based on either VC dimension or margins considerations."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144817431"
                        ],
                        "name": "J. Forster",
                        "slug": "J.-Forster",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Forster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Forster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 249
                            }
                        ],
                        "text": "Although counting arguments prove that for any n, there exist n\u00d7n sign matrices for which dc(Y ) > n/11 (Lemma 3 below, following Alon et al [8] who give a slightly weaker bound), the Hadamard matrix, for which it is known that \u221a n \u2264 dc(Hlog n) \u2264 n [6], is the most extreme known concrete example."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 249
                            }
                        ],
                        "text": "kernel methods) as a generic approach to classification leads one to ask what concept classes can or cannot be embedded as low-dimensional or large-margin linear separators; that is, what matrices have high dimensional-complexity and max-complexity [4, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26876616,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9ccd63501fcb6780b202afa2bdb829dc4985609d",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses theoretical limitations of classification systems that are based on feature maps and use a separating hyperplane in the feature space. In particular, we study the embeddability of a given concept class into a class of Euclidean half spaces of low dimension, or of arbitrarily large dimension but realizing a large margin. New bounds on the smallest possible dimension or on the largest possible margin are presented. In addition, we present new results on the rigidity of matrices and briefly mention applications in complexity and learning theory."
            },
            "slug": "On-the-Smallest-Possible-Dimension-and-the-Largest-Forster-Simon",
            "title": {
                "fragments": [],
                "text": "On the Smallest Possible Dimension and the Largest Possible Margin of Linear Arrangements Representing Given Concept Classes Uniform Distribution"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The embeddability of a given concept class into a class of Euclidean half spaces of low dimension, or of arbitrarily large dimension but realizing a large margin, is studied."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320565"
                        ],
                        "name": "G. Pisier",
                        "slug": "G.-Pisier",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Pisier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pisier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61076044,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0b40ccdb233cd700dad2cdf446ca278ecc1fc083",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Absolutely summing operators and basic applications Factorization through a Hilbert space Type and cotype. Kwapien's theorem The \"\"abstract\"\" version of Grothendieck's theorem Grothendieck's theorem Banach spaces satisfying Grothendieck's theorem Applications of the volume ratio method Banach lattices $C^*$-algebras Counterexamples to Grothendieck's conjecture."
            },
            "slug": "Factorization-of-Linear-Operators-and-Geometry-of-Pisier",
            "title": {
                "fragments": [],
                "text": "Factorization of Linear Operators and Geometry of Banach Spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752048"
                        ],
                        "name": "R. Arriaga",
                        "slug": "R.-Arriaga",
                        "structuredName": {
                            "firstName": "Rosa",
                            "lastName": "Arriaga",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arriaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": "the identity matrix has max-norm one but rank n), using random projections, a low max-norm matrix can be approximated by a low rank matrix [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Let A \u2208 Rk\u00d7d be a random matrix with independent normally distributed entries, then for any u, v with u\u2032 = Auandv\u2032 = Av we have [3]: Pr (1 \u2212 \u03b5) |u \u2212 v|(2) \u2264 |u\u2032 \u2212 v\u2032|(2) \u2264 (1 + \u03b5) |u \u2212 v|(2) \u2265 1 \u2212 2e\u2212k(\u03b52\u2212\u03b53)/4 (9)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "In Section 4 we show this for general measures of discrepancy, generalizing previous results [3, 4] for binary target matrices."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 17741789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05fb162abd00afc4cb453cad8685191fd199928c",
            "isKey": true,
            "numCitedBy": 335,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the phenomenon of cognitive learning from an algorithmic standpoint. How does the brain effectively learn concepts from a small number of examples despite the fact that each example contains a huge amount of information? We provide a novel algorithmic analysis via a model of robust concept learning (closely related to \u201cmargin classifiers\u201d), and show that a relatively small number of examples are sufficient to learn rich concept classes. The new algorithms have several advantages\u2014they are faster, conceptually simpler, and resistant to low levels of noise. For example, a robust half-space can be learned in linear time using only a constant number of training examples, regardless of the number of attributes. A general (algorithmic) consequence of the model, that \u201cmore robust concepts are easier to learn\u201d, is supported by a multitude of psychological studies."
            },
            "slug": "An-algorithmic-theory-of-learning:-Robust-concepts-Arriaga-Vempala",
            "title": {
                "fragments": [],
                "text": "An algorithmic theory of learning: Robust concepts and random projection"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work provides a novel algorithmic analysis via a model of robust concept learning (closely related to \u201cmargin classifiers\u201d), and shows that a relatively small number of examples are sufficient to learn rich concept classes."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784986"
                        ],
                        "name": "V. Koltchinskii",
                        "slug": "V.-Koltchinskii",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Koltchinskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltchinskii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611153"
                        ],
                        "name": "D. Panchenko",
                        "slug": "D.-Panchenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Panchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Panchenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2307733,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6388150296152c8173fae995e30c80a86d7cf1f7",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers. Such combinations could be implemented by neural networks or by voting methods of combining the classifiers, such as boosting and bagging. The bounds are in terms of the empirical distribution of the margin of the combined classifier. They are based on the methods of the theory of Gaussian and empirical processes (comparison inequalities, symmetrization method, concentration inequalities) and they improve previous results of Bartlett (1998) on bounding the generalization error of neural networks in terms of 1 -norms of the weights of neurons and of Schapire, Freund, Bartlett and Lee (1998) on bounding the generalization error of boosting. We also obtain rates of convergence in Levy distance of empirical margin distribution to the true margin distribution uniformly over the classes of classifiers and prove the optimality of these rates."
            },
            "slug": "Empirical-margin-distributions-and-bounding-the-of-Koltchinskii-Panchenko",
            "title": {
                "fragments": [],
                "text": "Empirical margin distributions and bounding the generalization error of combined classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "New probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifier combinations, based on the methods of the theory of Gaussian and empirical processes are proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734327"
                        ],
                        "name": "N. Alon",
                        "slug": "N.-Alon",
                        "structuredName": {
                            "firstName": "Noga",
                            "lastName": "Alon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678770"
                        ],
                        "name": "P. Frankl",
                        "slug": "P.-Frankl",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Frankl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frankl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733389"
                        ],
                        "name": "V. R\u00f6dl",
                        "slug": "V.-R\u00f6dl",
                        "structuredName": {
                            "firstName": "Vojtech",
                            "lastName": "R\u00f6dl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. R\u00f6dl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 129
                            }
                        ],
                        "text": "Although counting arguments prove that for any n, there exist n\u00d7n sign matrices for which dc(Y ) > n/11 (Lemma 3 below, following Alon et al [8] who give a slightly weaker bound), the Hadamard matrix, for which it is known that \u221a n \u2264 dc(Hlog n) \u2264 n0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "Although counting arguments prove that for any n, there exist n\u00d7n sign matrices for which dc(Y ) > n/11 (Lemma 3 below, following Alon et al [8] who give a slightly weaker bound), the Hadamard matrix, for which it is known that \u221a n \u2264 dc(Hlog n) \u2264 n [6], is the most extreme known concrete example."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 192
                            }
                        ],
                        "text": "Generalization error bounds for prediction with low-rank matrices can be obtained by considering the number of sign configurations of low-rank matrices [2] (following techniques introduced in [8]):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8416636,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "16e91a2b92a5b83944956d5c8bf1bc2cdae7906e",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Let d = d(n) be the minimum d such that for every sequence of n subsets F1, F2, . . . , Fn of {1, 2, . . . , n} there exist n points P1, P2, . . . , Pn and n hyperplanes H1, H2 .... , Hn in Rd such that Pj lies in the positive side of Hi iff j \u2208 Fi. Then n/32 \u2264 d(n) \u2264 (1/2 + 0(1)) \u00bf n. This implies that the probabilistic unbounded-error 2-way complexity of almost all the Boolean functions of 2p variables is between p-5 and p, thus solving a problem of Yao and another problem of Paturi and Simon. The proof of (1) combines some known geometric facts with certain probabilistic arguments and a theorem of Milnor from real algebraic geometry."
            },
            "slug": "Geometrical-realization-of-set-systems-and-Alon-Frankl",
            "title": {
                "fragments": [],
                "text": "Geometrical realization of set systems and probabilistic communication complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "It is shown that the probabilistic unbounded-error 2-way complexity of almost all the Boolean functions of 2p variables is between p-5 and p, thus solving a problem of Yao and another problem of Paturi and Simon."
            },
            "venue": {
                "fragments": [],
                "text": "26th Annual Symposium on Foundations of Computer Science (sfcs 1985)"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722702"
                        ],
                        "name": "B. Awerbuch",
                        "slug": "B.-Awerbuch",
                        "structuredName": {
                            "firstName": "Baruch",
                            "lastName": "Awerbuch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Awerbuch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633757"
                        ],
                        "name": "Robert D. Kleinberg",
                        "slug": "Robert-D.-Kleinberg",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert D. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 297
                            }
                        ],
                        "text": "We note that a large gap between the max-complexity and the dimensional-complexity is possible only when the lowrank matrix realizing the dimensional-complexity has entries of vastly varying magnitudes: For a rank-k matrix X with entries bounded by R, Awerbuch and Kleinberg\u2019s Barycentric spanner [13] construction can be used to obtain a factorization X = UV \u2032, U \u2208 Rn\u00d7k, V \u2208 Rm\u00d7k, such that the entries of U and V are bounded by \u221a R."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2490893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ab9dfad3ddd859e9548a5b8db896c6641f4e654",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Minimal delay routing is a fundamental task in networks. Since delays depend on the (potentially unpredictable) traffic distribution, online delay optimization can be quite challenging. While uncertainty about the current network delays may make the current routing choices sub-optimal, the algorithm can nevertheless try to learn the traffic patterns and keep adapting its choice of routing paths so as to perform nearly as well as the best static path. This online shortest path problem is a special case of online linear optimization, a problem in which an online algorithm must choose, in each round, a strategy from some compact set S \u2286 Rd so as to try to minimize a linear cost function which is only revealed at the end of the round. Kalai and Vempala[4] gave an algorithm for such problems in the transparent feedback model, where the entire cost function is revealed at the end of the round. Here we present an algorithm for online linear optimization in the more challenging opaque feedback model, in which only the cost of the chosen strategy is revealed at the end of the round. In the special case of shortest paths, opaque feedback corresponds to the notion that in each round the algorithm learns only the end-to-end cost of the chosen path, not the cost of every edge in the network.We also present a second algorithm for online shortest paths, which solves the shortest-path problem using a chain of online decision oracles, one at each node of the graph. This has several advantages over the online linear optimization approach. First, it is effective against an adaptive adversary, whereas our linear optimization algorithm assumes an oblivious adversary. Second, even in the case of an oblivious adversary, the second algorithm performs better than the first, as measured by their additive regret."
            },
            "slug": "Adaptive-routing-with-end-to-end-feedback:-learning-Awerbuch-Kleinberg",
            "title": {
                "fragments": [],
                "text": "Adaptive routing with end-to-end feedback: distributed learning and geometric approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A second algorithm for online shortest paths is presented, which solves the shortest-path problem using a chain of online decision oracles, one at each node of the graph, which has several advantages over the online linear optimization approach."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '04"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "Previously, examples in which the max-complexity is a polynomial function of the dimensional-complexity [5], or where the dimensional-complexity is constant but the max-complexity is logarithmic in the matrix size [4] have been shown."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Using \u2016Hp\u20162 = 2 we get mc(Hlog n) = tc(Hlog n) = \u221a n [5]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 23264507,
            "fieldsOfStudy": [],
            "id": "59922a45dc7b6943a2ce2f864c752897d1a3ab8c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating the Optimal Margins of Embeddings in Euclidean Half Spaces"
            },
            "venue": {
                "fragments": [],
                "text": "COLT/EuroCOLT"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 2,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 13,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Rank,-Trace-Norm-and-Max-Norm-Srebro-Shraibman/8423a5782a1acda21a6f68c307ce5376ebef13c7?sort=total-citations"
}