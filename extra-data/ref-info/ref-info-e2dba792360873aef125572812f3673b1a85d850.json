{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821711"
                        ],
                        "name": "Thang Luong",
                        "slug": "Thang-Luong",
                        "structuredName": {
                            "firstName": "Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "We start by briefly reviewing the continuous skipgram model introduced by Mikolov et al. (2013b), from which our model is derived."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 54
                            }
                        ],
                        "text": "The methods used are: the recursive neural network of Luong et al. (2013), the morpheme CBOW of Qiu et al. (2014) and the morphological transformations of Soricut and Och (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 123
                            }
                        ],
                        "text": "For English, we use theWS353 dataset introduced by Finkelstein et al. (2001) and the rare word dataset (RW), introduced by Luong et al. (2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14276764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ab89807caead278d3deb7b6a4180b277d3cb77",
            "isKey": true,
            "numCitedBy": 794,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way."
            },
            "slug": "Better-Word-Representations-with-Recursive-Neural-Luong-Socher",
            "title": {
                "fragments": [],
                "text": "Better Word Representations with Recursive Neural Networks for Morphology"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper combines recursive neural networks, where each morpheme is a basic unit, with neural language models to consider contextual information in learning morphologicallyaware word representations and proposes a novel model capable of building representations for morphologically complex words from their morphemes."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34421144"
                        ],
                        "name": "Siyu Qiu",
                        "slug": "Siyu-Qiu",
                        "structuredName": {
                            "firstName": "Siyu",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siyu Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145742001"
                        ],
                        "name": "Qing Cui",
                        "slug": "Qing-Cui",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qing Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152441498"
                        ],
                        "name": "Jiang Bian",
                        "slug": "Jiang-Bian",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Bian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Bian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701488"
                        ],
                        "name": "Bin Gao",
                        "slug": "Bin-Gao",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264337"
                        ],
                        "name": "Tie-Yan Liu",
                        "slug": "Tie-Yan-Liu",
                        "structuredName": {
                            "firstName": "Tie-Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie-Yan Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14219425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "750f26d613d3bda4ce043944aa3ef358b0c5de68",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The techniques of using neural networks to learn distributed word representations (i.e., word embeddings) have been used to solve a variety of natural language processing tasks. The recently proposed methods, such as CBOW and Skip-gram, have demonstrated their effectiveness in learning word embeddings based on context information such that the obtained word embeddings can capture both semantic and syntactic relationships between words. However, it is quite challenging to produce high-quality word representations for rare or unknown words due to their insufficient context information. In this paper, we propose to leverage morphological knowledge to address this problem. Particularly, we introduce the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. As a result, beyond word representations, the proposed neural network model will produce morpheme representations, which can be further employed to infer the representations of rare or unknown words based on their morphological structure. Experiments on an analogical reasoning task and several word similarity tasks have demonstrated the effectiveness of our method in producing high-quality words embeddings compared with the state-of-the-art methods."
            },
            "slug": "Co-learning-of-Word-Representations-and-Morpheme-Qiu-Cui",
            "title": {
                "fragments": [],
                "text": "Co-learning of Word Representations and Morpheme Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework and will produce morpheme representations, which can be further employed to infer the representations of rare or unknown words based on their morphological structure."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790831"
                        ],
                        "name": "C. D. Santos",
                        "slug": "C.-D.-Santos",
                        "structuredName": {
                            "firstName": "C\u00edcero",
                            "lastName": "Santos",
                            "middleNames": [
                                "Nogueira",
                                "dos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Santos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2834402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6",
            "isKey": false,
            "numCitedBy": 528,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result."
            },
            "slug": "Learning-Character-level-Representations-for-Santos-Zadrozny",
            "title": {
                "fragments": [],
                "text": "Learning Character-level Representations for Part-of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A deep neural network is proposed that learns character-level representation of words and associate them with usual word representations to perform POS tagging and produces state-of-the-art POS taggers for two languages."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3275163"
                        ],
                        "name": "Xinxiong Chen",
                        "slug": "Xinxiong-Chen",
                        "structuredName": {
                            "firstName": "Xinxiong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinxiong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112281287"
                        ],
                        "name": "Lei Xu",
                        "slug": "Lei-Xu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49293587"
                        ],
                        "name": "Zhiyuan Liu",
                        "slug": "Zhiyuan-Liu",
                        "structuredName": {
                            "firstName": "Zhiyuan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiyuan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753344"
                        ],
                        "name": "Maosong Sun",
                        "slug": "Maosong-Sun",
                        "structuredName": {
                            "firstName": "Maosong",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maosong Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371599"
                        ],
                        "name": "Huanbo Luan",
                        "slug": "Huanbo-Luan",
                        "structuredName": {
                            "firstName": "Huanbo",
                            "lastName": "Luan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huanbo Luan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations. Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 511,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations. Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules. Word representations trained on morphologically annotated data were introduced by Cotterell and Sch\u00fctze (2015). Closest to our approach, Sch\u00fctze (1993) learned representations of character fourgrams through singular value decomposition, and derived representations for words by summing the fourgrams representations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7874533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e662c87a36779dbe9f56f7ffd3ade756059d094",
            "isKey": true,
            "numCitedBy": 234,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Most word embedding methods take a word as a basic unit and learn embeddings according to words' external contexts, ignoring the internal structures of words. However, in some languages such as Chinese, a word is usually composed of several characters and contains rich internal information. The semantic meaning of a word is also related to the meanings of its composing characters. Hence, we take Chinese for example, and present a character-enhanced word embedding model (CWE). In order to address the issues of character ambiguity and non-compositional words, we propose multiple prototype character embeddings and an effective word selection method. We evaluate the effectiveness of CWE on word relatedness computation and analogical reasoning. The results show that CWE outperforms other baseline methods which ignore internal character information. The codes and data can be accessed from https://github.com/Leonard-Xu/CWE."
            },
            "slug": "Joint-Learning-of-Character-and-Word-Embeddings-Chen-Xu",
            "title": {
                "fragments": [],
                "text": "Joint Learning of Character and Word Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A character-enhanced word embedding model (CWE) is presented to address the issues of character ambiguity and non-compositional words, and the effectiveness of CWE on word relatedness computation and analogical reasoning is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 76
                            }
                        ],
                        "text": "We use the syntactic (en-syn) and semantic (en-sem) questions introduced by Mikolov et al. (2013a) for En-\nsumming the vectors of charactern-grams."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "We start by briefly reviewing the continuous skip-gram model (Mikolov et al., 2013b) from which our model is derived."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 86
                            }
                        ],
                        "text": "Our main contribution is to introduce an extension of the continuous skip-gram model (Mikolov et al., 2013b), which takes into account subword information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 48
                            }
                        ],
                        "text": "WhenP = W , our model is the skip-gram model of Mikolov et al. (2013b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 65
                            }
                        ],
                        "text": "This is the skipgram model with negative sampling, introduced by Mikolov et al. (2013b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Mikolov et al. (2013b) proposed simple log-bilinear models to learn continuous representations of words on very large corpora efficiently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5959482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "330da625c15427c6e42ccfa3b747fb29e5835bf0",
            "isKey": false,
            "numCitedBy": 21886,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            },
            "slug": "Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen",
            "title": {
                "fragments": [],
                "text": "Efficient Estimation of Word Representations in Vector Space"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Two novel model architectures for computing continuous vector representations of words from very large data sets are proposed and it is shown that these vectors provide state-of-the-art performance on the authors' test set for measuring syntactic and semantic word similarities."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16447573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "isKey": false,
            "numCitedBy": 26054,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
            },
            "slug": "Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "Distributed Representations of Words and Phrases and their Compositionality"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145742001"
                        ],
                        "name": "Qing Cui",
                        "slug": "Qing-Cui",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qing Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701488"
                        ],
                        "name": "Bin Gao",
                        "slug": "Bin-Gao",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152441498"
                        ],
                        "name": "Jiang Bian",
                        "slug": "Jiang-Bian",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Bian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Bian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34421144"
                        ],
                        "name": "Siyu Qiu",
                        "slug": "Siyu-Qiu",
                        "structuredName": {
                            "firstName": "Siyu",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siyu Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2791430"
                        ],
                        "name": "H. Dai",
                        "slug": "H.-Dai",
                        "structuredName": {
                            "firstName": "Hanjun",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264337"
                        ],
                        "name": "Tie-Yan Liu",
                        "slug": "Tie-Yan-Liu",
                        "structuredName": {
                            "firstName": "Tie-Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie-Yan Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12643315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "976ac6f1d38e6c45d203ba421a1ded8d38477252",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network techniques are widely applied to obtain high-quality distributed representations of words (i.e., word embeddings) to address text mining, information retrieval, and natural language processing tasks. Most recent efforts have proposed several efficient methods to learn word embeddings from context such that they can encode both semantic and syntactic relationships between words. However, it is quite challenging to handle unseen or rare words with insufficient context. Inspired by the study on the word recognition process in cognitive psychology, in this article, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both words\u2019 contextual information and morphological knowledge to learn word embeddings. Meanwhile, this new learning architecture is also able to benefit from noisy knowledge and balance between contextual information and morphological knowledge. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings."
            },
            "slug": "KNET:-A-General-Framework-for-Learning-Word-Using-Cui-Gao",
            "title": {
                "fragments": [],
                "text": "KNET: A General Framework for Learning Word Embedding Using Morphological Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This article introduces a novel neural network architecture called KNET that leverages both words\u2019 contextual information and morphological knowledge to learn word embeddings and demonstrates that the proposed KNET framework can greatly enhance the effectiveness of wordembeddings."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379953252"
                        ],
                        "name": "Wang Ling",
                        "slug": "Wang-Ling",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691021"
                        ],
                        "name": "I. Trancoso",
                        "slug": "I.-Trancoso",
                        "structuredName": {
                            "firstName": "Isabel",
                            "lastName": "Trancoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Trancoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394760"
                        ],
                        "name": "Ram\u00f3n Fern\u00e1ndez Astudillo",
                        "slug": "Ram\u00f3n-Fern\u00e1ndez-Astudillo",
                        "structuredName": {
                            "firstName": "Ram\u00f3n",
                            "lastName": "Astudillo",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840645"
                        ],
                        "name": "Silvio Amir",
                        "slug": "Silvio-Amir",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Amir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Silvio Amir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728026"
                        ],
                        "name": "Lu\u00eds Marujo",
                        "slug": "Lu\u00eds-Marujo",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Marujo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lu\u00eds Marujo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979183"
                        ],
                        "name": "T. Lu\u00eds",
                        "slug": "T.-Lu\u00eds",
                        "structuredName": {
                            "firstName": "Tiago",
                            "lastName": "Lu\u00eds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lu\u00eds"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1689426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dab1c6491929d396e9e5463bc2e87af88602aa2",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form\u2010function relationship in language, our \u201ccomposed\u201d word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish)."
            },
            "slug": "Finding-Function-in-Form:-Compositional-Character-Ling-Dyer",
            "title": {
                "fragments": [],
                "text": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A model for constructing vector representations of words by composing characters using bidirectional LSTMs that requires only a single vector per character type and a fixed set of parameters for the compositional model, which yields state- of-the-art results in language modeling and part-of-speech tagging."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692656"
                        ],
                        "name": "Giacomo Berardi",
                        "slug": "Giacomo-Berardi",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Berardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Giacomo Berardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699411"
                        ],
                        "name": "Andrea Esuli",
                        "slug": "Andrea-Esuli",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Esuli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Esuli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022957"
                        ],
                        "name": "Diego Marcheggiani",
                        "slug": "Diego-Marcheggiani",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Marcheggiani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diego Marcheggiani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 25
                            }
                        ],
                        "text": "(2015) for German and by Berardi et al. (2015) for Italian."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3066021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b04a6db56cb66429e5216cb505f041f05146b4a5",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present some preliminary results on the generation of word embeddings for the Italian language. We compare two popular word representation models, word2vec and GloVe, and train them on two datasets with different stylistic properties. We test the generated word embeddings on a word analogy test derived from the one originally proposed for word2vec, adapted to capture some of the linguistic aspects that are specific of Italian. Results show that the tested models are able to create syntactically and semantically meaningful word embeddings despite the higher morphological complexity of Italian with respect to English. Moreover, we have found that the stylistic properties of the training dataset plays a relevant role in the type of information captured by the produced vectors."
            },
            "slug": "Word-Embeddings-Go-to-Italy:-A-Comparison-of-Models-Berardi-Esuli",
            "title": {
                "fragments": [],
                "text": "Word Embeddings Go to Italy: A Comparison of Models and Training Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Preliminary results on the generation of word embeddings for the Italian language show that the tested models are able to create syntactically and semantically meaningful word embeddeddings despite the higher morphological complexity of Italian with respect to English."
            },
            "venue": {
                "fragments": [],
                "text": "IIR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707242"
                        ],
                        "name": "Minh-Thang Luong",
                        "slug": "Minh-Thang-Luong",
                        "structuredName": {
                            "firstName": "Minh-Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minh-Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 54
                            }
                        ],
                        "text": "The methods used are: the recursive neural network of Luong et al. (2013), the morpheme cbow of Qiu et al. (2014) and the morphological transformations of Soricut and Och (2015). In order to make the results comparable, we trained our model on the same datasets as the methods we are comparing to: the English Wikipedia data released by Shaoul and Westbury (2010), and the news crawl data from the 2013 WMT shared task for German, Spanish and French."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "We use the datasets introduced by Mikolov et al. (2013a) for English, by Svoboda and Brychcin (2016) for Czech, by K\u00f6per et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 54
                            }
                        ],
                        "text": "The methods used are: the recursive neural network of Luong et al. (2013), the morpheme cbow of Qiu et al. (2014) and the morphological transformations of Soricut and Och (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 119
                            }
                        ],
                        "text": "Finally, recent works in machine translation have proposed using subword units to obtain representations of rare words (Sennrich et al., 2016; Luong and Manning, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 54
                            }
                        ],
                        "text": "The methods used are: the recursive neural network of Luong et al. (2013), the morpheme cbow of Qiu et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13972671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "733b821faeebe49b6efcf5369e3b9902b476529e",
            "isKey": true,
            "numCitedBy": 336,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words."
            },
            "slug": "Achieving-Open-Vocabulary-Neural-Machine-with-Luong-Manning",
            "title": {
                "fragments": [],
                "text": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel word-character solution to achieving open vocabulary NMT that can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082372"
                        ],
                        "name": "Rico Sennrich",
                        "slug": "Rico-Sennrich",
                        "structuredName": {
                            "firstName": "Rico",
                            "lastName": "Sennrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rico Sennrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259100"
                        ],
                        "name": "B. Haddow",
                        "slug": "B.-Haddow",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Haddow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Haddow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539211"
                        ],
                        "name": "Alexandra Birch",
                        "slug": "Alexandra-Birch",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Birch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Birch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 119
                            }
                        ],
                        "text": "Finally, recent works in machine translation have proposed using subword units to obtain representations of rare words (Sennrich et al., 2016; Luong and Manning, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 121
                            }
                        ],
                        "text": "Finally, recent works in machine translation have proposed to use subword units to obtain representations of rare words (Sennrich et al., 2016; Luong and Manning, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1114678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1af68821518f03568f913ab03fc02080247a27ff",
            "isKey": false,
            "numCitedBy": 4795,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively."
            },
            "slug": "Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation of Rare Words with Subword Units"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper introduces a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units, and empirically shows that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.3 BLEU."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5181932"
                        ],
                        "name": "Henning Sperr",
                        "slug": "Henning-Sperr",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "Sperr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henning Sperr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2920247"
                        ],
                        "name": "J. Niehues",
                        "slug": "J.-Niehues",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Niehues",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Niehues"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14300428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d5236e301aaccf2906aabc85a4d2be9dfc0b815",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a letter-based encoding for words in continuous space language models. We represent the words completely by letter n-grams instead of using the word index. This way, similar words will automatically have a similar representation. With this we hope to better generalize to unknown or rare words and to also capture morphological information. We show their influence in the task of machine translation using continuous space language models based on restricted Boltzmann machines. We evaluate the translation quality as well as the training time on a German-to-English translation task of TED and university lectures as well as on the news translation task translating from English to German. Using our new approach a gain in BLEU score by up to 0.4 points can be achieved."
            },
            "slug": "Letter-N-Gram-based-Input-Encoding-for-Continuous-Sperr-Niehues",
            "title": {
                "fragments": [],
                "text": "Letter N-Gram-based Input Encoding for Continuous Space Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A letter-based encoding for words in continuous space language models is presented, which represents the words completely by letter n-grams instead of using the word index to better generalize to unknown or rare words and to also capture morphological information."
            },
            "venue": {
                "fragments": [],
                "text": "CVSM@ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38367242"
                        ],
                        "name": "Yoon Kim",
                        "slug": "Yoon-Kim",
                        "structuredName": {
                            "firstName": "Yoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262249"
                        ],
                        "name": "Yacine Jernite",
                        "slug": "Yacine-Jernite",
                        "structuredName": {
                            "firstName": "Yacine",
                            "lastName": "Jernite",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yacine Jernite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746662"
                        ],
                        "name": "D. Sontag",
                        "slug": "D.-Sontag",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sontag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 686481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "isKey": false,
            "numCitedBy": 1428,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway net work over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.\n \n"
            },
            "slug": "Character-Aware-Neural-Language-Models-Kim-Jernite",
            "title": {
                "fragments": [],
                "text": "Character-Aware Neural Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A simple neural language model that relies only on character-level inputs that is able to encode, from characters only, both semantic and orthographic information and suggests that on many languages, character inputs are sufficient for language modeling."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35025872"
                        ],
                        "name": "Jan A. Botha",
                        "slug": "Jan-A.-Botha",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Botha",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan A. Botha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2838374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46f418bf6fab132f193661226c5c27d67f870ea5",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models."
            },
            "slug": "Compositional-Morphology-for-Word-Representations-Botha-Blunsom",
            "title": {
                "fragments": [],
                "text": "Compositional Morphology for Word Representations and Language Modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model, and performs both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that the model learns morphological representation that both perform well on word similarity tasks and lead to substantial reductions in perplexity."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797344"
                        ],
                        "name": "Maximilian K\u00f6per",
                        "slug": "Maximilian-K\u00f6per",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "K\u00f6per",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maximilian K\u00f6per"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775432"
                        ],
                        "name": "Christian Scheible",
                        "slug": "Christian-Scheible",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Scheible",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Scheible"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7965906"
                        ],
                        "name": "Sabine Schulte im Walde",
                        "slug": "Sabine-Schulte-im-Walde",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Schulte im Walde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sabine Schulte im Walde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 477536,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "db0b31ea7a04315ed9d751ff9c9185decb9827dc",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "While continuous word vector representations enjoy increasing popularity, it is still poorly understood (i) how reliable they are for other languages than English, and (ii) to what extent they encode deep semantic relatedness such as paradigmatic relations. This study presents experiments with continuous word vectors for English and German, a morphologically rich language. For evaluation, we use both published and newly created datasets of morpho-syntactic and semantic relations. Our results show that (i) morphological complexity causes a drop in accuracy, and (ii) continuous representations lack the ability to solve analogies of paradigmatic relations."
            },
            "slug": "Multilingual-Reliability-and-\u201cSemantic\u201d-Structure-K\u00f6per-Scheible",
            "title": {
                "fragments": [],
                "text": "Multilingual Reliability and \u201cSemantic\u201d Structure of Continuous Word Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show that (i) morphological complexity causes a drop in accuracy, and (ii) continuous representations lack the ability to solve analogies of paradigmatic relations."
            },
            "venue": {
                "fragments": [],
                "text": "IWCS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038285"
                        ],
                        "name": "Alessandro Lenci",
                        "slug": "Alessandro-Lenci",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Lenci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Lenci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5584134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "553fb89d5858826c02f26e94262e8958debc777e",
            "isKey": false,
            "numCitedBy": 618,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this \u201cone task, one model\u201d approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
            },
            "slug": "Distributional-Memory:-A-General-Framework-for-Baroni-Lenci",
            "title": {
                "fragments": [],
                "text": "Distributional Memory: A General Framework for Corpus-Based Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Distributional Memory approach is shown to be tenable despite the constraints imposed by its multi-purpose nature, and performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against several state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40591835"
                        ],
                        "name": "K. Lund",
                        "slug": "K.-Lund",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61090106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4",
            "isKey": false,
            "numCitedBy": 1721,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "slug": "Producing-high-dimensional-semantic-spaces-from-Lund-Burgess",
            "title": {
                "fragments": [],
                "text": "Producing high-dimensional semantic spaces from lexical co-occurrence"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word, which provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771118"
                        ],
                        "name": "J. Wieting",
                        "slug": "J.-Wieting",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wieting",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wieting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3202289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12e9d005c77f76e344361f79c4b008034ae547eb",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks."
            },
            "slug": "Charagram:-Embedding-Words-and-Sentences-via-Wieting-Bansal",
            "title": {
                "fragments": [],
                "text": "Charagram: Embedding Words and Sentences via Character n-grams"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is demonstrated that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329288"
                        ],
                        "name": "Piotr Bojanowski",
                        "slug": "Piotr-Bojanowski",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Bojanowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Bojanowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319608"
                        ],
                        "name": "Armand Joulin",
                        "slug": "Armand-Joulin",
                        "structuredName": {
                            "firstName": "Armand",
                            "lastName": "Joulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Armand Joulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 150
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-ofspeech tagging (Ling et al., 2015) and parsing (Ballesteros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 89
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-of-speech tagging (Ling et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2415419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3da0d432d73d19c71fb72e8ff0cb431d1b3c080a",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data."
            },
            "slug": "Alternative-structures-for-character-level-RNNs-Bojanowski-Joulin",
            "title": {
                "fragments": [],
                "text": "Alternative structures for character-level RNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes two alternative structural modifications to the classical RNN model, one of which consists on conditioning the character level representation on the previous word representation, and the other uses the character history to condition the output probability."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750769"
                        ],
                        "name": "Ryan Cotterell",
                        "slug": "Ryan-Cotterell",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Cotterell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Cotterell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13398754,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "ec27e759576f14addeace610304365d96737f644",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word's morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study."
            },
            "slug": "Morphological-Word-Embeddings-Cotterell-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Morphological Word-Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word's morphology, i.e., words close in the embedded space share morphological features."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5024,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544109"
                        ],
                        "name": "Colette Joubarne",
                        "slug": "Colette-Joubarne",
                        "structuredName": {
                            "firstName": "Colette",
                            "lastName": "Joubarne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colette Joubarne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697366"
                        ],
                        "name": "D. Inkpen",
                        "slug": "D.-Inkpen",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Inkpen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Inkpen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Finally, we evaluate the French word vectors on the translated dataset RG65 (Joubarne and Inkpen, 2011) and the Spanish word vectors on the dataset WS353 (Hassan and Mihalcea, 2009).3\nSome words from these datasets do not appear in our training data, and thus, we cannot obtain word representation for these words using the CBOW and skip-gram baselines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 67
                            }
                        ],
                        "text": "We evaluate the French word vectors on the translated dataset RG65 (Joubarne and Inkpen, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6425853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d8b687058a0519a47f673dd88d1dc2856571286",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the growth in digitization of data, there are still many languages without sufficient corpora to achieve valid measures of semantic similarity. If it could be shown that manually-assigned similarity scores from one language can be transferred to another language, then semantic similarity values could be used for languages with fewer resources. We test an automatic word similarity measure based on second-order co-occurrences in the Google ngram corpus, for English, German, and French. We show that the scores manually-assigned in the experiments of Rubenstein and Goodenough's for 65 English word pairs can be transferred directly into German and French. We do this by conducting human evaluation experiments for French word pairs (and by using similarly produced scores for German). We show that the correlation between the automatically-assigned semantic similarity scores and the scores assigned by human evaluators is not very different when using the Rubenstein and Goodenough's scores across language, compared to the language-specific scores."
            },
            "slug": "Comparison-of-Semantic-Similarity-for-Different-the-Joubarne-Inkpen",
            "title": {
                "fragments": [],
                "text": "Comparison of Semantic Similarity for Different Languages Using the Google n-gram Corpus and Second-Order Co-occurrence Measures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An automatic word similarity measure based on second-order co-occurrences in the Google ngram corpus is test, showing that the scores manually-assigned in the experiments of Rubenstein and Goodenough's for 65 English word pairs can be transferred directly into French."
            },
            "venue": {
                "fragments": [],
                "text": "Canadian Conference on AI"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2756960"
                        ],
                        "name": "Grzegorz Chrupa\u0142a",
                        "slug": "Grzegorz-Chrupa\u0142a",
                        "structuredName": {
                            "firstName": "Grzegorz",
                            "lastName": "Chrupa\u0142a",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grzegorz Chrupa\u0142a"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9026691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3779a1a74e9901db6d99c62fa36ef8475f4a4b35",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Tweets often contain a large proportion of abbreviations, alternative spellings, novel words and other non-canonical language. These features are problematic for standard language analysis tools and it can be desirable to convert them to canonical form. We propose a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings. The text embeddings are generated using an Simple Recurrent Network. We find that enriching the feature set with text embeddings substantially lowers word error rates on an English tweet normalization dataset. Our model improves on stateof-the-art with little training data and without any lexical resources."
            },
            "slug": "Normalizing-tweets-with-edit-scripts-and-recurrent-Chrupa\u0142a",
            "title": {
                "fragments": [],
                "text": "Normalizing tweets with edit scripts and recurrent neural embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings that substantially lowers word error rates on an English tweet normalization dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713801"
                        ],
                        "name": "Anoop Deoras",
                        "slug": "Anoop-Deoras",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Deoras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop Deoras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2675251"
                        ],
                        "name": "Stefan Kombrink",
                        "slug": "Stefan-Kombrink",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kombrink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Kombrink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899242"
                        ],
                        "name": "J. Cernock\u00fd",
                        "slug": "J.-Cernock\u00fd",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cernock\u00fd",
                            "middleNames": [
                                "Honza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cernock\u00fd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 90
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-ofspeech tagging (Ling et al., 2015) and parsing (Ballesteros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 89
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-of-speech tagging (Ling et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 96
                            }
                        ],
                        "text": "The model described in this section is the skipgram model with negative sampling, introduced by Mikolov et al. (2013b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46542477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the performance of several types of language mode ls on the word-level and the character-level language modelin g tasks. This includes two recently proposed recurrent neural netwo rk architectures, a feedforward neural network model, a maximum ent ropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from th e data, and show that it combines advantages of both character and wo rdlevel models. Finally, we show that neural network based lan gu ge models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task. By using sub-word un its, the size can be reduced even more."
            },
            "slug": "SUBWORD-LANGUAGE-MODELING-WITH-NEURAL-NETWORKS-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple technique for learning sub-word level units from data is proposed, and it is shown that neural network based models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 104
                            }
                        ],
                        "text": "These representations are typically derived from large unlabeled corpora using co-occurrence statistics (Deerwester et al., 1990; Sch\u00fctze, 1992; Lund and Burgess, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 121
                            }
                        ],
                        "text": "Our approach, which incorporates character n-grams into the skipgram model, is related to an idea that was introduced by Sch\u00fctze (1993). Because of its simplicity, our model trains fast and does not require any preprocessing or supervision."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15829786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e569d99f3a0fcfa038631dda2b44c73a6e8e97b8",
            "isKey": false,
            "numCitedBy": 454,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The representation of documents and queries as vectors in a high-dimensional space is well-established in information retrieval. The author proposes that the semantics of words and contexts in a text be represented as vectors. The dimensions of the space are words and the initial vectors are determined by the words occurring close to the entity to be represented, which implies that the space has several thousand dimensions (words). This makes the vector representations (which are dense) too cumbersome to use directly. Therefore, dimensionality reduction by means of a singular value decomposition is employed. The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction. >"
            },
            "slug": "Dimensions-of-meaning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Dimensions of meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction and finds that dimensionality reduction by means of a singular value decomposition is employed."
            },
            "venue": {
                "fragments": [],
                "text": "Supercomputing '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2670103"
                        ],
                        "name": "H. Sak",
                        "slug": "H.-Sak",
                        "structuredName": {
                            "firstName": "Hasim",
                            "lastName": "Sak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2535627"
                        ],
                        "name": "M. Sara\u00e7lar",
                        "slug": "M.-Sara\u00e7lar",
                        "structuredName": {
                            "firstName": "Murat",
                            "lastName": "Sara\u00e7lar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sara\u00e7lar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765713"
                        ],
                        "name": "Tunga G\u00fcng\u00f6r",
                        "slug": "Tunga-G\u00fcng\u00f6r",
                        "structuredName": {
                            "firstName": "Tunga",
                            "lastName": "G\u00fcng\u00f6r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tunga G\u00fcng\u00f6r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11201363,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "805ccb88e942346413b8f951177fbe3715866eb3",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore morphology-based and sub-word language modeling approaches proposed for morphologically rich languages, and evaluate and contrast them for Turkish broadcast news transcription task. In addition, as a morphology-based model, we improve our previously proposed morphology-integrated model for automatic speech recognition. This model is built by composing the finite-state transducer of the morphological parser with a language model over lexical morphemes. This approach provides a morphology-integrated search network with an unlimited vocabulary, generating only valid word forms while reducing the out-of-vocabulary rate and hence improving the word error rate. We also analyze the effect of morpho-tactics and morphological disambiguation on the speech recognition accuracy for the morphology-integrated model. The improved morphology-integrated model performs better than statistically derived sub-word models with added benefit of generating morpho-syntactic and semantic features."
            },
            "slug": "Morphology-based-and-sub-word-language-modeling-for-Sak-Sara\u00e7lar",
            "title": {
                "fragments": [],
                "text": "Morphology-based and sub-word language modeling for Turkish speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The improved morphology-integrated model performs better than statistically derived sub-word models with added benefit of generating morpho-syntactic and semantic features."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 267
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-ofspeech tagging (Ling et al., 2015) and parsing (Ballesteros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 360,
                                "start": 21
                            }
                        ],
                        "text": ", 2015) and parsing (Ballesteros et al., 2015). Another family of models are convolutional neural networks trained on characters, which were applied to part-of-speech tagging (dos Santos and Zadrozny, 2014), sentiment analysis (dos Santos and Gatti, 2014), text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016). Sperr et al. (2013) introduced a language model based on restricted Boltzmann machines, in which words are encoded as a set of character ngrams."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 256149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96526726f87233fb017f6ea9483090f04e0f0530",
            "isKey": false,
            "numCitedBy": 285,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words."
            },
            "slug": "Improved-Transition-based-Parsing-by-Modeling-of-Ballesteros-Dyer",
            "title": {
                "fragments": [],
                "text": "Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672644"
                        ],
                        "name": "Angeliki Lazaridou",
                        "slug": "Angeliki-Lazaridou",
                        "structuredName": {
                            "firstName": "Angeliki",
                            "lastName": "Lazaridou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angeliki Lazaridou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48188880"
                        ],
                        "name": "M. Marelli",
                        "slug": "M.-Marelli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Marelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713535"
                        ],
                        "name": "Roberto Zamparelli",
                        "slug": "Roberto-Zamparelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Zamparelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Zamparelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7371294,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "b4fcb3921b42d156a812484dcabf60c3f4410d42",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics."
            },
            "slug": "Compositional-ly-Derived-Representations-of-Complex-Lazaridou-Marelli",
            "title": {
                "fragments": [],
                "text": "Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work adapts compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts, and demonstrates the usefulness of a compositional morphology component in distributional semantics."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49610089"
                        ],
                        "name": "Andrei Alexandrescu",
                        "slug": "Andrei-Alexandrescu",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Alexandrescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Alexandrescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783839"
                        ],
                        "name": "Katrin Kirchhoff",
                        "slug": "Katrin-Kirchhoff",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Kirchhoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Kirchhoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 28
                            }
                        ],
                        "text": "To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 760,
                                "start": 28
                            }
                        ],
                        "text": "To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features. These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turkish (Sak et al., 2010). Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014). These different approaches rely on a morphological decomposition of words, while ours does not. Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1386,
                                "start": 28
                            }
                        ],
                        "text": "To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features. These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turkish (Sak et al., 2010). Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014). These different approaches rely on a morphological decomposition of words, while ours does not. Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations. Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules. Word representations trained on morphologically annotated data were introduced by Cotterell and Sch\u00fctze (2015). Closest to our approach, Sch\u00fctze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing the four-grams representations. Very recently, Wieting et al. (2016) also proposed to represent words using character n-gram count vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 660,
                                "start": 28
                            }
                        ],
                        "text": "To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features. These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turkish (Sak et al., 2010). Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014). These different approaches rely on a morphological decomposition of words, while ours does not. Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1141,
                                "start": 28
                            }
                        ],
                        "text": "To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features. These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turkish (Sak et al., 2010). Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014). These different approaches rely on a morphological decomposition of words, while ours does not. Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations. Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules. Word representations trained on morphologically annotated data were introduced by Cotterell and Sch\u00fctze (2015). Closest to our approach, Sch\u00fctze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing the four-grams representations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 868,
                                "start": 28
                            }
                        ],
                        "text": "To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features. These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turkish (Sak et al., 2010). Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014). These different approaches rely on a morphological decomposition of words, while ours does not. Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters. Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations. Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 937826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a73ae2ce1cfafae61238b3fdb1bbb61093962b4d",
            "isKey": true,
            "numCitedBy": 98,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models."
            },
            "slug": "Factored-Neural-Language-Models-Alexandrescu-Kirchhoff",
            "title": {
                "fragments": [],
                "text": "Factored Neural Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new type of neural probabilistic language model is presented that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction and significantly reduces perplexity on sparse-data tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3211177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d24afe3a62331ebfad400c3fec77c836d2b99db",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Representations for semantic information about words are necessary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large number of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are successfully applied to word sense disambiguation using a nearest neighbor method."
            },
            "slug": "Word-Space-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Word Space"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An efficient, corpus-based method for inducing distributed semantic representations for a large number of words from lexical coccurrence statistics by means of a large-scale linear regression is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144033941"
                        ],
                        "name": "Alexander Panchenko",
                        "slug": "Alexander-Panchenko",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Panchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Panchenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065795"
                        ],
                        "name": "Dmitry Ustalov",
                        "slug": "Dmitry-Ustalov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Ustalov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitry Ustalov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6153848"
                        ],
                        "name": "N. Arefyev",
                        "slug": "N.-Arefyev",
                        "structuredName": {
                            "firstName": "Nikolay",
                            "lastName": "Arefyev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Arefyev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129425"
                        ],
                        "name": "Denis Paperno",
                        "slug": "Denis-Paperno",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Paperno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Paperno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005322183"
                        ],
                        "name": "N. Konstantinova",
                        "slug": "N.-Konstantinova",
                        "structuredName": {
                            "firstName": "Natalia",
                            "lastName": "Konstantinova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Konstantinova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724001"
                        ],
                        "name": "Natalia V. Loukachevitch",
                        "slug": "Natalia-V.-Loukachevitch",
                        "structuredName": {
                            "firstName": "Natalia",
                            "lastName": "Loukachevitch",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Natalia V. Loukachevitch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31565315"
                        ],
                        "name": "Chris Biemann",
                        "slug": "Chris-Biemann",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Biemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Biemann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2576137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40947612162cc4644f9489721ec1ca94fe7e765c",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic relatedness of terms represents similarity of meaning by a numerical score. On the one hand, humans easily make judgements about semantic relatedness. On the other hand, this kind of information is useful in language processing systems. While semantic relatedness has been extensively studied for English using numerous language resources, such as associative norms, human judgements and datasets generated from lexical databases, no evaluation resources of this kind have been available for Russian to date. Our contribution addresses this problem. We present five language resources of different scale and purpose for Russian semantic relatedness, each being a list of triples \\(({word}_{i}, {word}_{j}, {similarity}_{ij}\\)). Four of them are designed for evaluation of systems for computing semantic relatedness, complementing each other in terms of the semantic relation type they represent. These benchmarks were used to organise a shared task on Russian semantic relatedness, which attracted 19 teams. We use one of the best approaches identified in this competition to generate the fifth high-coverage resource, the first open distributional thesaurus of Russian. Multiple evaluations of this thesaurus, including a large-scale crowdsourcing study involving native speakers, indicate its high accuracy."
            },
            "slug": "Human-and-Machine-Judgements-for-Russian-Semantic-Panchenko-Ustalov",
            "title": {
                "fragments": [],
                "text": "Human and Machine Judgements for Russian Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work uses one of the best approaches identified in this competition to generate the fifth high-coverage resource, the first open distributional thesaurus of Russian, which multiple evaluations indicate its high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "AIST"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 103
                            }
                        ],
                        "text": "This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1500900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
            "isKey": false,
            "numCitedBy": 2722,
            "numCiting": 208,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "slug": "From-Frequency-to-Meaning:-Vector-Space-Models-of-Turney-Pantel",
            "title": {
                "fragments": [],
                "text": "From Frequency to Meaning: Vector Space Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065725861"
                        ],
                        "name": "Luk\u00e1s Svoboda",
                        "slug": "Luk\u00e1s-Svoboda",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Svoboda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Svoboda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917913"
                        ],
                        "name": "Tomas Brychcin",
                        "slug": "Tomas-Brychcin",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Brychcin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Brychcin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4630787,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "aaf402a21587e242446606d58cf6193f91e344b0",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The word embedding methods have been proven to be very useful in many tasks of NLP (Natural Language Processing). Much has been investigated about word embeddings of English words and phrases, but only little attention has been dedicated to other languages."
            },
            "slug": "New-word-analogy-corpus-for-exploring-embeddings-of-Svoboda-Brychcin",
            "title": {
                "fragments": [],
                "text": "New word analogy corpus for exploring embeddings of Czech words"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The word embedding methods have been proven to be very useful in many tasks of NLP (Natural Language Processing), but only little attention has been dedicated to other languages."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790831"
                        ],
                        "name": "C. D. Santos",
                        "slug": "C.-D.-Santos",
                        "structuredName": {
                            "firstName": "C\u00edcero",
                            "lastName": "Santos",
                            "middleNames": [
                                "Nogueira",
                                "dos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Santos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771662"
                        ],
                        "name": "M. Gatti",
                        "slug": "M.-Gatti",
                        "structuredName": {
                            "firstName": "Ma\u00edra",
                            "lastName": "Gatti",
                            "middleNames": [
                                "A.",
                                "de",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gatti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 187
                            }
                        ],
                        "text": "Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15874232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0aca3e7877c3c20958b0fae5cbf2dd602104859",
            "isKey": false,
            "numCitedBy": 1107,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that exploits from characterto sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: the Stanford Sentiment Treebank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7% accuracy, and fine-grained classification, with 48.3% accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4%."
            },
            "slug": "Deep-Convolutional-Neural-Networks-for-Sentiment-of-Santos-Gatti",
            "title": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new deep convolutional neural network is proposed that exploits from characterto sentence-level information to perform sentiment analysis of short texts and achieves state-of-the-art results for single sentence sentiment prediction."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737285"
                        ],
                        "name": "Radu Soricut",
                        "slug": "Radu-Soricut",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Soricut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Soricut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16326127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39ef3906b13ac2758ebc0d8f75e738d4f6314b39",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a language agnostic, unsupervised method for inducing morphological transformations between words. The method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages."
            },
            "slug": "Unsupervised-Morphology-Induction-Using-Word-Soricut-Och",
            "title": {
                "fragments": [],
                "text": "Unsupervised Morphology Induction Using Word Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A language agnostic, unsupervised method for inducing morphological transformations between words that relies on certain regularities manifest in highdimensional vector spaces and is capable of discovering a wide range of morphological rules."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 112
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-ofspeech tagging (Ling et al., 2015) and parsing (Ballesteros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 89
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-of-speech tagging (Ling et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8843166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de",
            "isKey": false,
            "numCitedBy": 1254,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date."
            },
            "slug": "Generating-Text-with-Recurrent-Neural-Networks-Sutskever-Martens",
            "title": {
                "fragments": [],
                "text": "Generating Text with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The power of RNNs trained with the new Hessian-Free optimizer by applying them to character-level language modeling tasks is demonstrated, and a new RNN variant that uses multiplicative connections which allow the current input character to determine the transition matrix from one hidden state vector to the next is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730400"
                        ],
                        "name": "Iryna Gurevych",
                        "slug": "Iryna-Gurevych",
                        "structuredName": {
                            "firstName": "Iryna",
                            "lastName": "Gurevych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iryna Gurevych"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16584933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ebe88fab46e53e980b5aef28c414c04d133fc6f",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences."
            },
            "slug": "Using-the-Structure-of-a-Conceptual-Network-in-Gurevych",
            "title": {
                "fragments": [],
                "text": "Using the Structure of a Conceptual Network in Computing Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis and can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "These representations are typically derived from large unlabeled corpora using co-occurrence statistics (Deerwester et al., 1990; Sch\u00fctze, 1992; Lund and Burgess, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 42
                            }
                        ],
                        "text": "Inspired by the distributional hypothesis (Harris, 1954), word representations are trained to predict well words that appear in its context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 86680084,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "decd9bc0385612bdf936928206d83730718e737e",
            "isKey": false,
            "numCitedBy": 2643,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "For the purposes of the present discussion, the term structure will be used in the following non-rigorous sense: A set of phonemes or a set of data is structured in respect to some feature, to the extent that we can form in terms of that feature some organized system of statements which describes the members of the set and their interrelations (at least up to some limit of complexity). In this sense, language can be structured in respect to various independent features. And whether it is structured (to more than a trivial extent) in respect to, say, regular historical change, social intercourse, meaning, or distribution - or to what extent it is structured in any of these respects - is a matter decidable by investigation. Here we will discuss how each language can be described in terms of a distributional structure, i.e. in terms of the occurrence of parts (ultimately sounds) relative to other parts, and how this description is complete without intrusion of other features such as history or meaning. It goes without saying that other studies of language - historical, psychological, etc.-are also possible, both in relation to distributional structure and independently of it."
            },
            "slug": "Distributional-Structure-Harris",
            "title": {
                "fragments": [],
                "text": "Distributional Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This discussion will discuss how each language can be described in terms of a distributional structure, i.e. in Terms of the occurrence of parts relative to other parts, and how this description is complete without intrusion of other features such as history or meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7818229"
                        ],
                        "name": "J. Zhao",
                        "slug": "J.-Zhao",
                        "structuredName": {
                            "firstName": "Junbo",
                            "lastName": "Zhao",
                            "middleNames": [
                                "Jake"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 13
                            }
                        ],
                        "text": "These different approaches rely on a morphological decomposition of words, while our does not."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 368182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51a55df1f023571a7e07e338ee45a3e3d66ef73e",
            "isKey": false,
            "numCitedBy": 3477,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks."
            },
            "slug": "Character-level-Convolutional-Networks-for-Text-Zhang-Zhao",
            "title": {
                "fragments": [],
                "text": "Character-level Convolutional Networks for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This article constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results in text classification."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920653"
                        ],
                        "name": "Samer Hassan",
                        "slug": "Samer-Hassan",
                        "structuredName": {
                            "firstName": "Samer",
                            "lastName": "Hassan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samer Hassan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557251"
                        ],
                        "name": "Rada Mihalcea",
                        "slug": "Rada-Mihalcea",
                        "structuredName": {
                            "firstName": "Rada",
                            "lastName": "Mihalcea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rada Mihalcea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 88
                            }
                        ],
                        "text": "Spanish, Arabic and Romanian word vectors are evaluated using the datasets described in (Hassan and Mihalcea, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1856431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4443628698e77b7c25f61410cc45384dee80a99",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness."
            },
            "slug": "Cross-lingual-Semantic-Relatedness-Using-Knowledge-Hassan-Mihalcea",
            "title": {
                "fragments": [],
                "text": "Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper introduces a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages, that performs well, with a performance comparable to monolingual measures of relatedness."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780779"
                        ],
                        "name": "Torsten Zesch",
                        "slug": "Torsten-Zesch",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Zesch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Torsten Zesch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730400"
                        ],
                        "name": "Iryna Gurevych",
                        "slug": "Iryna-Gurevych",
                        "structuredName": {
                            "firstName": "Iryna",
                            "lastName": "Gurevych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iryna Gurevych"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7898806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e69b28380b4efc695a60054f85e8c46a9731c35b",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic relatedness is a special form of linguistic distance between words. Evaluating semantic relatedness measures is usually performed by comparison with human judgments. Previous test datasets had been created analytically and were limited in size. We propose a corpus-based system for automatically creating test datasets. Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts."
            },
            "slug": "Automatically-Creating-Datasets-for-Measures-of-Zesch-Gurevych",
            "title": {
                "fragments": [],
                "text": "Automatically Creating Datasets for Measures of Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A corpus-based system for automatically creating test datasets that cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2006"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 89
                            }
                        ],
                        "text": "A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupa\u0142a, 2014), part-of-speech tagging (Ling et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1697424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b1f4740ae37fd04f6ac007577bdd34621f0861",
            "isKey": false,
            "numCitedBy": 3153,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
            },
            "slug": "Generating-Sequences-With-Recurrent-Neural-Networks-Graves",
            "title": {
                "fragments": [],
                "text": "Generating Sequences With Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 95
                            }
                        ],
                        "text": "Learning continuous representations of words has a long history in natural language processing (Rumelhart et al., 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20332,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50064811"
                        ],
                        "name": "L. Finkelstein",
                        "slug": "L.-Finkelstein",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745572"
                        ],
                        "name": "Y. Matias",
                        "slug": "Y.-Matias",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Matias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Matias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316511"
                        ],
                        "name": "Zach Solan",
                        "slug": "Zach-Solan",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "Solan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zach Solan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073936"
                        ],
                        "name": "G. Wolfman",
                        "slug": "G.-Wolfman",
                        "structuredName": {
                            "firstName": "Gadi",
                            "lastName": "Wolfman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wolfman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12956853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c01df98a6b633b25c96c1a99b713ac96f1c5be",
            "isKey": false,
            "numCitedBy": 1725,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (\"the context\"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."
            },
            "slug": "Placing-search-in-context:-the-concept-revisited-Finkelstein-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "Placing search in context: the concept revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new conceptual paradigm for performing search in context is presented, that largely automates the search process, providing even non-professional users with highly relevant results."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061358113"
                        ],
                        "name": "Edward Rosenfeld",
                        "slug": "Edward-Rosenfeld",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8160958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d0e5a21512d2aea34026f83b1ff86ea30b8c0d6",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "ion for Knowledge Acquisition\u201d by T. Bylander and B. Chadrasekaran. Chandrasekaran\u2019s papers are usually illuminating, and this one does not fail: He and Bylander re-examine such traditional beliefs as knowledge should be uniformly represented and controlled and the knowledge base should be separated from the inference engine. The final 10 papers in volume 1 discuss generalized learning and ruleinduction techniques. They are interesting and informative, particularly \u201cGeneralization and Noise\u201d by Y. Kodratoff and M. Manango, which discusses symbolic and numeric rule induction. Most rule-induction techniques focus on the use of examples and numeric analysis such as repertory grids. Kodratoff\u2019s and Manango\u2019s exploration of how the two complement each other is refreshing. Because of their technical nature and the amount of work it would take to put their content to use, most of the papers in this section of the volume are more appropriate for a specialized or research-oriented group. For those just getting involved in knowledge-based\u2013systems development, Knowledge Acquisition Tools for Expert Systems is the more useful volume. In addition to discussing the tools themselves, most of the papers contain details of the knowledgeacquisition techniques that are automated, thus providing much of the same information which is available in the first volume. As an added benefit, they also often discuss the underlying architectures for solving domain-specific problems. For instance, the details of the medical diagnostic architecture laid out in \u201cDesign for Acquisition: Principles of Knowledge System Design to Facilitate Knowledge Acquisition\u201d by T. R. Gruber and P. R. Cohen are almost as useful as the discussion of how to build a knowledge-acquisition system. Volume 2 is particularly germane given the rise in commercial interest about automated knowledge acquisition following this year\u2019s introduction of Neuron Data\u2019s NEXTRATM product and last year\u2019s introduction of Test Bench by Texas Instruments. Test Bench is actually discussed in \u201cA Mixed-Initiative Workbench for Knowledge Acquisition\u201d by G. S. Kahn, E. H. Breaux, P. De Klerk, and R. L. Joseph. This volume provides the background necessary to evaluate knowledge-acquisition tools such as NEXTRA, Test Bench, and AutoIntelligence (IntelligenceWare). The vendors of knowledge-based\u2013systems development tools, for example, Inference, IntelliCorp, Aion, AI Corp., and IBM, would do well to pay heed to these books because they point the way to removing the knowledge bottleneck from knowledge-based\u2013systems development. Overall, the papers in both volumes are comprehensive and well integrated, a sometimes difficult state to achieve when compiling a collection of papers resulting from a small conference. The collection is comparable to Anna Hart\u2019s Knowledge Acquisition for Expert Systems (McGraw-Hill, 1986), but it is broader in scope and not as structured. The arrangement of the papers is marred only by an overly brief index. Few readers can be expected to read a collection from beginning to end, and a better index would facilitate more enlightened use. Less important\u2014but nevertheless distracting\u2014is the large number of typographical errors in both volumes. In conclusion, the set is recommended for both the commercial and research knowledge-based\u2013systems practitioner. Reading the volumes in reverse order might be more useful to the commercial developer given the extra information available in volume 2. Neurocomputing: Foundations of Research"
            },
            "slug": "Neurocomputing:-Foundations-of-Research-Anderson-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Neurocomputing: Foundations of Research"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The set is recommended for both the commercial and research knowledge-based\u2013systems practitioner and provides the background necessary to evaluate knowledge-acquisition tools such as NEXTRA, Test Bench, and AutoIntelligence (IntelligenceWare)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9229182"
                        ],
                        "name": "B. Recht",
                        "slug": "B.-Recht",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Recht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Recht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803218"
                        ],
                        "name": "Christopher R\u00e9",
                        "slug": "Christopher-R\u00e9",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "R\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher R\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47657030"
                        ],
                        "name": "Feng Niu",
                        "slug": "Feng-Niu",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Niu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Niu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6108215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36f49b05d764bf5c10428b082c2d96c13c4203b9",
            "isKey": false,
            "numCitedBy": 2018,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude."
            },
            "slug": "Hogwild:-A-Lock-Free-Approach-to-Parallelizing-Recht-R\u00e9",
            "title": {
                "fragments": [],
                "text": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking, and presents an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31723440"
                        ],
                        "name": "Julia M. Mayer",
                        "slug": "Julia-M.-Mayer",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Mayer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julia M. Mayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144812959"
                        ],
                        "name": "Quentin Jones",
                        "slug": "Quentin-Jones",
                        "structuredName": {
                            "firstName": "Quentin",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quentin Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749200"
                        ],
                        "name": "S. R. Hiltz",
                        "slug": "S.-R.-Hiltz",
                        "structuredName": {
                            "firstName": "Starr",
                            "lastName": "Hiltz",
                            "middleNames": [
                                "Roxanne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. R. Hiltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9178878,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "885446efab3abded405264f01c3ff731490746c9",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "Mobile social matching systems have the potential to transform the way we make new social ties, but only if we are able to overcome the many challenges that exist as to how systems can utilize contextual data to recommend interesting and relevant people to users and facilitate valuable encounters between strangers. This article outlines how context and mobility influence people's motivations to meet new people and presents innovative design concepts for mediating mobile encounters through context-aware social matching systems. Findings from two studies are presented. The first, a survey study (n = 117) explored the concept of contextual rarity of shared user attributes as a measure to improve desirability in mobile social matches. The second, an interview study (n = 58) explored people's motivations to meet others in various contexts. From these studies we derived a set of novel context-aware social matching concepts, including contextual sociability and familiarity as an indicator of opportune social context; contextual engagement as an indicator of opportune personal context; and contextual rarity, oddity, and activity partnering as an indicator of opportune relational context. The findings of these studies establish the importance of different contextual factors and frame the design space of context-aware social matching systems."
            },
            "slug": "Identifying-Opportunities-for-Valuable-Encounters:-Mayer-Jones",
            "title": {
                "fragments": [],
                "text": "Identifying Opportunities for Valuable Encounters: Toward Context-Aware Social Matching Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "How context and mobility influence people's motivations to meet new people is outlined and innovative design concepts for mediating mobile encounters through context-aware social matching systems are presented."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31037184"
                        ],
                        "name": "M. Meyer",
                        "slug": "M.-Meyer",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Meyer",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145150984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bec4d526a2490df741011b598326727fcdf1004",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Proof-and-Measurement-of-Association-between-Meyer",
            "title": {
                "fragments": [],
                "text": "The Proof and Measurement of Association between Two Things."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1904
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 151
                            }
                        ],
                        "text": "These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turk-\nish (Sak et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 137
                            }
                        ],
                        "text": "In particular, they ignore the internal structure of words, which is an important limitation for morphologically rich languages, such as Turkish or Finnish."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Morphology-based and sub-word"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hogwild : A lockfree approach to parallelizing stochastic gradient descent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "We carry out the optimization in parallel, by resorting to Hogwild (Recht et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hogwild: A lock-free approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sch\u00fctze 1992 ] Hinrich Sch\u00fctze . 1992 . Dimensions of meaning . In Proc . Supercomputing . [ Sch\u00fctze 1993 ] Hinrich Sch\u00fctze . 1993 . Word space"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 380,
                                "start": 105
                            }
                        ],
                        "text": "These representations are typically derived from large unlabeled corpora using co-occurrence statistics (Deerwester et al., 1990; Sch\u00fctze, 1992; Lund and Burgess, 1996). A large body of work, known as distributional semantics, have studied the properties of these methods (Turney et al., 2010; Baroni and Lenci, 2010). In the neural network community, Collobert and Weston (2008) proposed to learn word embeddings using a feedforward"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 104
                            }
                        ],
                        "text": "These representations are typically derived from large unlabeled corpora using co-occurrence statistics (Deerwester et al., 1990; Sch\u00fctze, 1992; Lund and Burgess, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Indexing by latent semantic analysis.Journal of the American society for information science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "guage modeling for turkish speech recognition Dimensions of meaning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICASSP. [Sch\u00fctze1992] Hinrich Sch\u00fctze Proc. Supercomputing. [Sch\u00fctze1993] Hinrich Sch\u00fctze. 1993. Word space. In Adv. NIPS"
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 15
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 55,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Enriching-Word-Vectors-with-Subword-Information-Bojanowski-Grave/e2dba792360873aef125572812f3673b1a85d850?sort=total-citations"
}