{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144964966"
                        ],
                        "name": "D. Dori",
                        "slug": "D.-Dori",
                        "structuredName": {
                            "firstName": "Dov",
                            "lastName": "Dori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11000656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9adca5cac518016f32f59e4fb2ac20d130f34ad",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an objective, comprehensive, and complexity independent metric for performance evaluation of graphics/text separation (text segmentation) algorithms. The metric includes a positive set and a negative set of indices, at both the character and the character string (text) levels, _and it evaluates the detection accuracy of the location, width, height, orientation, skew, string length, and the fragmentation of both characters and strings. Assigning a Segmentation Difficulty (SD) value to the ground truth characters, the performance indices are normalized with respect to the character SD and are therefore independent of the ground truth complexity. The evaluation provides an overall, objective, and comprehensive metric of the text segmentation capability of various algorithms aimed at performing this task."
            },
            "slug": "A-Proposed-Scheme-for-Performance-Evaluation-of-Liu-Dori",
            "title": {
                "fragments": [],
                "text": "A Proposed Scheme for Performance Evaluation of Graphics/Text Separation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The evaluation provides an overall, objective, and comprehensive metric of the text segmentation capability of various algorithms aimed at performing this task."
            },
            "venue": {
                "fragments": [],
                "text": "GREC"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Many efforts have been done for text detection in videos and images [2][3][ 4 ][5][6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898481"
                        ],
                        "name": "Wei-Song Qi",
                        "slug": "Wei-Song-Qi",
                        "structuredName": {
                            "firstName": "Wei-Song",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Song Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3062639"
                        ],
                        "name": "L. Gu",
                        "slug": "L.-Gu",
                        "structuredName": {
                            "firstName": "Lie",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152631223"
                        ],
                        "name": "Hao Jiang",
                        "slug": "Hao-Jiang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15472062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9bab6c1c2270e91e94d393e82d86fa1e05d2b61",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system developed for content-based broadcast news video browsing for home users. There are three main factors that distinguish our work from other similar ones. First, we have integrated the image and audio analysis results in identifying news segments. Second, we use the video OCR technology to detect text from frames, which provides a good source of textual information for story classification when transcripts and close captions are not available. Finally, natural language processing (NLP) technologies are used to perform automated categorization of news stories based on the texts obtained from close caption or video OCR process. Based on these video structure and content analysis technologies, we have developed two advanced video browsers for home users: intelligent highlight player and HTML-based video browser."
            },
            "slug": "Integrating-visual,-audio-and-text-analysis-for-Qi-Gu",
            "title": {
                "fragments": [],
                "text": "Integrating visual, audio and text analysis for news video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two advanced video browsers for home users are developed: intelligent highlight player and HTML-based video browser that perform automated categorization of news stories based on the texts obtained from close caption or video OCR process."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13609029,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "641483b9dcaa2bac13f95a3f2f6738140c170184",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed. The approach has been tested extensively on a large variety of video frame sizes such 352/spl times/240 up to 1920/spl times/1280 and a large representative set of video sequences such as home videos, newscasts, title sequences and commercials. 95% of the text bounding boxes in videos were localized correctly. 80% of all characters were segmented correctly, while 7.8% characters were damaged. 90% of the correctly segmented characters were recognized correctly by standard OCR software."
            },
            "slug": "On-the-segmentation-of-text-in-videos-Wernicke-Lienhart",
            "title": {
                "fragments": [],
                "text": "On the segmentation of text in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed that has been tested extensively on a large variety of video frame sizes and a large representative set of video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6781817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7721138e41d82fedabca59c9a66e67d9b7053f3",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically locate captions in MPEG video. Caption text regions are segmented from the background using their distinguishing texture characteristics. This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain. Therefore, only a small amount of decoding is required. The proposed algorithm achieves about 4.0% false reject rate and less than 5.7% false positive rate on a variety of MPEG compressed video containing more than 42,000 frames."
            },
            "slug": "Automatic-caption-localization-in-compressed-video-Zhong-Zhang",
            "title": {
                "fragments": [],
                "text": "Automatic caption localization in compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain, so that only a small amount of decoding is required."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780258"
                        ],
                        "name": "Jisheng Liang",
                        "slug": "Jisheng-Liang",
                        "structuredName": {
                            "firstName": "Jisheng",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jisheng Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34817063"
                        ],
                        "name": "A. Chhabra",
                        "slug": "A.-Chhabra",
                        "structuredName": {
                            "firstName": "Atul",
                            "lastName": "Chhabra",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chhabra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5083726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "335e19d744b147a597a6102463bee969d474c296",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper defines a computational protocol for evaluating the performance of raster to vector conversion systems. The graphical entities handled by this protocol are continuous and dashed lines, arcs, and circles, and text regions. The protocol allows matches of the type one-to-one, one-to-many, and many-to-one between the ground truth and the recognition results."
            },
            "slug": "A-Performance-Evaluation-Protocol-for-Graphics-Phillips-Liang",
            "title": {
                "fragments": [],
                "text": "A Performance Evaluation Protocol for Graphics Recognition Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A computational protocol for evaluating the performance of raster to vector conversion systems and graphical entities handled are continuous and dashed lines, arcs, and circles, and text regions."
            },
            "venue": {
                "fragments": [],
                "text": "GREC"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144905110"
                        ],
                        "name": "L. Lam",
                        "slug": "L.-Lam",
                        "structuredName": {
                            "firstName": "Louisa",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37451448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33df6dc0b06fb30c76b4ea1ad92bb60eed5ffb1f",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors report on the performance of 10 parallel thinning algorithms from the perspective of character recognition. The algorithms evaluated include the complete range of four-subcycle, two-subcycle, and the most recent fully parallel methods. The authors consider different aspects of the performance of each algorithm. Statistics are gathered such as computing time, deviation from perfect 8-connectedness, and number of possible noise spurs present in the skeletons. In addition, the effects of each algorithm on an OCR system are examined through training and testing the system on the skeletons obtained from each thinning process.<<ETX>>"
            },
            "slug": "Evaluation-of-thinning-algorithms-from-an-OCR-Lam-Suen",
            "title": {
                "fragments": [],
                "text": "Evaluation of thinning algorithms from an OCR viewpoint"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The authors report on the performance of 10 parallel thinning algorithms from the perspective of character recognition, including the complete range of four-sub cycle, two-subcycle, and the most recent fully parallel methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 612034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb086853b759bbad022a345dfe63cd68794a1c39",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014F\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 omputer vision emerged as a subfield in computer science and in electrical engineering in the 1960s. Two main motivations for research in computer vision are to develop algorithms to solve vision problems and to understand and model the human visual system. It turns out that finding satisfactory answers to either motivation is significantly harder than common wisdom initially assumed. Research in computer vision has actively continued to the current time. Most of the research in the computer vision and pattern recognition community is focused on developing solutions to vision problems. With three decades of research behind current efforts and with the availability of powerful, inexpensive computers, there is a common belief that computer vision is poised to deliver reliable solutions. The area of empirical evaluation of computer vision algorithms is developing the methods and tools for measuring the ability of algorithms to meet requirements to be fielded, for determining the state-of-the-art, and for pointing out future research directions. The goal of this special theme section of IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) is to highlight progress in empirical evaluation and identify it as a maturing area of computer vision. Out of 18 submissions, three were accepted for this special section. In addition, one submission was accepted to appear in a regular issue, and two others are being revised for consideration as regular papers. \u201cFiltering for Supervised Texture Segmentation: A Comparative Study\u201d by T. Randen and J.H. Husoy presents a comparative study of methods for texture classification. The emphasis of the study is filtering methods from signal processing. Most major filtering approaches are evaluated. For reference, a statistical algorithm and a model-based algorithm are also evaluated. The paper presents performance results on a number of mosaic texture images. In a first for PAMI, the raw image files for these images are being made available as part of the electronic version of the paper. (The electronic version of the paper is part of the Computer Society\u2019s digital library, accessible online at www.computer.org.) It is hoped that future papers on texture segmentation will take advantage of this in order to present directly comparable experimental results. \u201cPerformance Evaluation and Analysis of Monocular Building Extraction From Aerial Imagery\u201d by J.A. Shufelt evaluates end-to-end performance of four systems on their ability to extract buildings from 83 aerial images of 18 sites. The methodology allows for an examination of traditional assumptions made in designing algorithms that extract buildings from monocular imagery. \u201cEvaluation of Methods for Ridge and Valley Detection\u201d by A.M. Lopez, F. Lumbreras, and J. Serrat evaluates ridge and valley detectors. The authors discuss what are desirable properties of ridge and valley detectors and the methods for measuring desirable properties. Then they present an evaluation using these methods. We hope the papers in this special section are interesting and present challenges for future researchers."
            },
            "slug": "Introduction-to-the-Special-Section-on-Empirical-of-Phillips-Bowyer",
            "title": {
                "fragments": [],
                "text": "Introduction to the Special Section on Empirical Evaluation of Computer Vision Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The goal of this special theme section of IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) is to highlight progress in empirical evaluation and identify it as a maturing area of computer vision."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Many efforts have been done for text detection in videos and images [2][3][4][5][6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In response to such needs, various video content analysis schemes using one or a combination of image, audio, and textual information in the videos have been proposed to parse, index, or abstract massive amount of data [1][2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207871522,
            "fieldsOfStudy": [],
            "id": "291d916a267c640f30d9b891441433ee3be9ffc6",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Caption Localization in Compressed Video"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "An overview of PE can be found at [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Benchmarking and Performance Evaluation"
            },
            "venue": {
                "fragments": [],
                "text": "Benchmarking and Performance Evaluation"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 12,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Automatic-performance-evaluation-for-video-text-Hua-Liu/844ceedec94d46b15198de140646e0f2ae953eb0?sort=total-citations"
}