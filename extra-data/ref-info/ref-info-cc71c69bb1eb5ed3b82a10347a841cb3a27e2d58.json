{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35126865"
                        ],
                        "name": "Yasuto Ishitani",
                        "slug": "Yasuto-Ishitani",
                        "structuredName": {
                            "firstName": "Yasuto",
                            "lastName": "Ishitani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuto Ishitani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "magazines Ishitani [19] b/w 300 dpi 30 0:12 complex documents with few text lines Bagdanov Kanai [20] b/w, jbig 300 dpi 3 documents with no or a few non textual parts Srihari Govindaraju [21] b/w 128 dpi 90 1 text only documents Hinds et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "3 Skew estimation Most of the skew estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection pro les [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Ishitani [19] uses a profile which is defined in a quite different way."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Ishitani [19] uses a pro le which is de ned in a quite di erent way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10578766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0884a248d8b0f7eaa080e164fb84f703f248e4d",
            "isKey": true,
            "numCitedBy": 97,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method is proposed for detecting skew in document images which contain a mixture of text areas, photographs, figures, charts, and tables. Two basic ideas are introduced in the method. One idea is that a new parameter is used for skew detection to discern the orientation of text lines in document images. This parameter is based on the document image complexity and is obtained from the number of transitions from white to black pixels or vice versa. The other idea is that skew is detected in local regions in which only text lines are expected. Such local regions are extracted from a document image automatically and the obtained skew angle is defined as the overall document skew. Document skew has been measured in experiments with an error of 0.12 degrees on the average for all test documents.<<ETX>>"
            },
            "slug": "Document-skew-detection-based-on-local-region-Ishitani",
            "title": {
                "fragments": [],
                "text": "Document skew detection based on local region complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new method is proposed for detecting skew in document images which contain a mixture of text areas, photographs, figures, charts, and tables and it is proposed that skew is detected in local regions in which only text lines are expected."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704694"
                        ],
                        "name": "J. Sauvola",
                        "slug": "J.-Sauvola",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "Sauvola",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sauvola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48819369"
                        ],
                        "name": "T. Sepp\u00e4nen",
                        "slug": "T.-Sepp\u00e4nen",
                        "structuredName": {
                            "firstName": "Tapio",
                            "lastName": "Sepp\u00e4nen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sepp\u00e4nen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141938"
                        ],
                        "name": "S. Haapakoski",
                        "slug": "S.-Haapakoski",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Haapakoski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haapakoski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "These elements have to be isolated and processed by appropriate modules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206775223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ccc9d0c186afb682b852d342207f3a273ceb867",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type related degradations are addressed. The algorithm uses document characteristics to determine (surface) attributes, often used in document segmentation. Using characteristic analysis, two new algorithms are applied to determine a local threshold for each pixel. An algorithm based on soft decision control is used for thresholding the background and picture regions. An approach utilizing local mean and variance of gray values is applied to textual regions. Tests were performed with images including different types of document components and degradations. The results show that the method adapts and performs well in each case."
            },
            "slug": "Adaptive-document-binarization-Sauvola-Sepp\u00e4nen",
            "title": {
                "fragments": [],
                "text": "Adaptive document binarization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture, using document characteristics to determine (surface) attributes, often used in document segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1939973"
                        ],
                        "name": "D. Le",
                        "slug": "D.-Le",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Le",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145116486"
                        ],
                        "name": "G. Thoma",
                        "slug": "G.-Thoma",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Thoma",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Thoma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 217
                            }
                        ],
                        "text": "3 Skew estimation Most of the skew estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection pro les [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24] describe an algorithm for the identi cation of page orientation (portrait or landscape) and of the document skew."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24] b/w 200 dpi 15 0:5 complex documents, e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2510232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f2aec50e45cbd544df66254b430d814643b0ed0",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automated-page-orientation-and-skew-angle-detection-Le-Thoma",
            "title": {
                "fragments": [],
                "text": "Automated page orientation and skew angle detection for binary document images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[44, 45] b/w right polygons text blocks text only, cleaned docs; one text direction; spacing constraints skew up to 5 is dealt; no input parameters"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Baird [45] describes a segmentation technique based on a previous work [44], in which the structure of the document background is analyzed in order to determine the geometric page layout."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26229616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da44af7e967c0c847e63b9600811a58ee7716f45",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for analyzing the structure of the white background in document images is described, along with applications to the problem of isolating blocks of machine-printed text. The approach is based on computational-geometry algorithms for off-line enumeration of maximal white rectangles and on-line rectangle unification. These support a fast, simple, and general heuristic for geometric layout segmentation, in which white space is covered greedily by rectangles until all text blocks are isolated. Design of the heuristic can be substantially automated by an analysis of the empirical statistical distribution of properties of covering rectangles: for example, the stopping rule can be chosen by Rosenblatt\u2019s perceptron training algorithm. Experimental trials show good behavior on the large and useful class of textual Manhattan layouts. On complex layouts from English-language technical journals of many publishers, the method finds good segmentations in a uniform and nearly parameter-free manner. On a variety of non-Latin texts, some with vertical text lines, the method finds good segmentations without prior knowledge of page and text-line orientation."
            },
            "slug": "Background-Structure-in-Document-Images-Baird",
            "title": {
                "fragments": [],
                "text": "Background Structure in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A method for analyzing the structure of the white background in document images is described, along with applications to the problem of isolating blocks of machine-printed text."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066251026"
                        ],
                        "name": "T. Saitoh",
                        "slug": "T.-Saitoh",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Saitoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61250424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "504d556660ffa093d2a32519f61ec3b28dd929fc",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A new technique for page segmentation without skew normalization is described and applied to both English and Japanese complex printed-page layouts. There is no need to make any assumption about the shape of blocks, hence the technique can handle not only skewed pages but it can also be extended to handle documents where columns are not rectangles. In this technique, based on the bottom-up strategy, the connected components are extracted on the reduced image and are classified with their local information. Since the skew angle is also estimated with the local information of blocks, the computational time is very short. Merging text blocks into string lines and into columns is performed with the skew information.<<ETX>>"
            },
            "slug": "Page-segmentation-without-rectangle-assumption-Saitoh-Pavlidis",
            "title": {
                "fragments": [],
                "text": "Page segmentation without rectangle assumption"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new technique for page segmentation without skew normalization is described and applied to both English and Japanese complex printed-page layouts that can handle not only skewed pages but it can also be extended to handle documents where columns are not rectangles."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768752"
                        ],
                        "name": "H. Aghajan",
                        "slug": "H.-Aghajan",
                        "structuredName": {
                            "firstName": "Hamid",
                            "lastName": "Aghajan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aghajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1813617"
                        ],
                        "name": "B. Khalaj",
                        "slug": "B.-Khalaj",
                        "structuredName": {
                            "firstName": "Babak",
                            "lastName": "Khalaj",
                            "middleNames": [
                                "Hossein"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Khalaj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720911"
                        ],
                        "name": "T. Kailath",
                        "slug": "T.-Kailath",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kailath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kailath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24590090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cba05a020f6a58f629abf70ce9462d49af7925f",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A new signal processing method is developed for estimating the skew angle in text document images. Detection of the skew angle is an important step in text processing tasks such as optical character recognition (OCR) and computerized filing. Based on a recently introduced multiline-fitting algorithm, the proposed method reformulates the skew detection problem into a special parameter-estimation framework such that a signal structure similar to the one in the field of sensor array processing is obtained. In this framework, straight lines in an image are modeled as wavefronts of propagating planar waves. Certain measurements are defined in this virtual propagation environment such that the large amount of coherency that exists between the locations of the pixels on parallel lines is exploited to enhance a subspace in the space spanned by the measurements. The well-studied techniques of sensor array processing (e.g., the ESPRIT algorithm) are then exploited to produce a closed form and high-resolution estimate for the skew angle."
            },
            "slug": "Estimation-of-skew-angle-in-text-image-analysis-Aghajan-Khalaj",
            "title": {
                "fragments": [],
                "text": "Estimation of skew angle in text-image analysis bySLIDE: Subspace-based line detection"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new signal processing method is developed for estimating the skew angle in text document images based on a recently introduced multiline-fitting algorithm that reformulates the skew detection problem into a special parameter-estimation framework such that a signal structure similar to the one in the field of sensor array processing is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146529005"
                        ],
                        "name": "D. Olivier",
                        "slug": "D.-Olivier",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Olivier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Olivier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66807260"
                        ],
                        "name": "B. Dominique",
                        "slug": "B.-Dominique",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dominique",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dominique"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195869176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf3364809beb238d33e25f19b8fc0d92152e2606",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for segmenting multilevels images of documents. The documents are considered difficult ones in the sense they may contain text paragraphs with different orientations and shapes, mixed with graphics and photographs. The proposed method extracts and separates blocks of text lines (printed or handwritten characters) and headers as well as stroke structures. The generic approach is first based on a multiscale analysis with the use of a pyramid representation of the image. At each level, text location is performed by a line borders detection scheme. Then, an efficient bottom-up procedure generates bodies (text paragraphs) as the output of algebric transformations upon a set of four directed graphs associated with the topological relationships of physical components."
            },
            "slug": "Segmentation-of-complex-documents-multilevel-a-and-Olivier-Dominique",
            "title": {
                "fragments": [],
                "text": "Segmentation of complex documents multilevel images: a robust and fast text bodies-headers detection and extraction scheme"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A method for segmenting multilevels images of documents that extracts and separates blocks of text lines and headers as well as stroke structures based on a multiscale analysis with the use of a pyramid representation of the image."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122930"
                        ],
                        "name": "J. Ha",
                        "slug": "J.-Ha",
                        "structuredName": {
                            "firstName": "Jaekyu",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[46] b/w 300dpi single column words, text lines, paragraphs text only, cleaned docs; no skew; hierarchic spacing protocol determination of text orientation"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[46] present a simple method for page segmentation and classification into words, text lines, paragraphs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3361432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e393d7d2a5a1f9bfd59db0c14a5a1e8c972f8f1d",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for extracting words, textlines and text blocks by analyzing the spatial configuration of bounding boxes of connected component on a given document image. The basic idea is that connected components of black pixels can be used as computational units in document image analysis. In this paper, the problem of extracting words, textlines and text blocks is viewed as a clustering problem in the 2-dimensional discrete domain. Our main strategy is that profiling analysis is utilized to measure horizontal or vertical gaps of (groups of) components during the process of image segmentation. For this purpose, we compute the smallest rectangular box, called the bounding box, which circumscribes a connected component. Those boxes are projected horizontally and/or vertically, and local and global projection profiles are analyzed for word, textline and text-block segmentation. In the last step of segmentation, the document decomposition hierarchy is produced from these segmented objects."
            },
            "slug": "Document-page-decomposition-by-the-bounding-box-Ha-Haralick",
            "title": {
                "fragments": [],
                "text": "Document page decomposition by the bounding-box project"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The problem of extracting words, textlines and text blocks is viewed as a clustering problem in the 2-dimensional discrete domain and profiling analysis is utilized to measure horizontal or vertical gaps of (groups of) components during the process of image segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "As reported in Haralick's review [2], Derrien-Peden [108] denes a frame based system to determine the logical structure of scienti c and technical documents."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "Although some review papers have been recently published, (see the reviews by Haralick [2], Tang et al. [3] and the survey of methods by Jain and Yu in [4]), along with the tutorial text by O\u2019Gorman and Kasturi [5], we believe that an attempt to provide a reasoned systematization of the field can be of great interest."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 269
                            }
                        ],
                        "text": "] they can be partitioned into isolated, convex blocks (columns) of text by horizontal and vertical line segments cutting through white space\", O'Gorman [29] says \\Manhattan layouts, that is layouts whose blocks are separable by vertical and horizontal cuts\", Haralick [2] says: \\A Manhattan page layout is one where the regions of the page layout are all rectangular and the rectangles are in the same orientation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 9
                            }
                        ],
                        "text": "Chen and Haralick [36] present a text skew estimation algorithm based on opening and closing morphological transforms [41]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "Although some review papers have been recently published, (see the reviews by Haralick [2], Tang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13615381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d91e0d202fa23b7a2e81c5b3b04eb4cc5327b0f9",
            "isKey": true,
            "numCitedBy": 144,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs. The physical reader of the paper document is the scanner just like the physical reader of the floppy is the floppy drive and the physical reader of the tape cartridge is the tape cartridge drive, and the physical reader of the CDROM is the CDROM drive. In the survey presented, we restrict ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences. Understanding such documents involves estimating the rotation skew of each document page, determining the geometric page layout, labeling blocks as text or non-text, determining the read order for text blocks, recognizing the text of text blocks through an OCR system, determining the logical page layout, and formatting the data and information of the document in a suitable way for use by a word processing system or by an information retrieval system.<<ETX>>"
            },
            "slug": "Document-image-understanding:-geometric-and-logical-Haralick",
            "title": {
                "fragments": [],
                "text": "Document image understanding: geometric and logical layout"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs and restricts ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144598687"
                        ],
                        "name": "O. D\u00e9forges",
                        "slug": "O.-D\u00e9forges",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "D\u00e9forges",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. D\u00e9forges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144812989"
                        ],
                        "name": "D. Barba",
                        "slug": "D.-Barba",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Barba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46585502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0e2e83578c9b94980ae49d5dbd5be7b3b49a62e",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This work is devoted to the description and the evaluation of a generic method for locating text and lines in document images. One of the possible applications of this scheme is to detect and carefully extract the destination address block (DAB) on flat mail pieces. Concerning the low level design stages, the methodology is based on a multiresolution description of the document image and a joint extraction of both text lines and non text lines using their global aspect. Intermediate and high level stages consist of grouping text words into block structures, and this is performed in a efficient and robust way relying not only on some lines properties, but also on the adjacent lines.<<ETX>>"
            },
            "slug": "A-fast-multiresolution-text-line-and-non-text-line-D\u00e9forges-Barba",
            "title": {
                "fragments": [],
                "text": "A fast multiresolution text line and non text-line structures extraction and discrimination scheme for document image analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work is devoted to the description and the evaluation of a generic method for locating text and lines in document images and one of the possible applications is to detect and carefully extract the destination address block (DAB) on flat mail pieces."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1st International Conference on Image Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: document image processing, document image understanding, automatic zoning, geometric and logical layout, page decomposition, document segmentation and classification, text segmentation, skew angle detection, binarization, document model, performance evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46138594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b39beea0f761152e65fac0e498af387821d887f1",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Transforming a paper document to its electronic version in a form suitable for efficient storage, retrieval, and interpretation continues to be a challenging problem. An efficient representation scheme for document images is necessary to solve this problem. Document representation involves techniques of thresholding, skew detection, geometric layout analysis, and logical layout analysis. The derived representation can then be used in document storage and retrieval. Page segmentation is an important stage in representing document images obtained by scanning journal pages. The performance of a document understanding system greatly depends on the correctness of page segmentation and labeling of different regions such as text, tables, images, drawings, and rulers. We use the traditional bottom-up approach based on the connected component extraction to efficiently implement page segmentation and region identification. A new document model which preserves top-down generation information is proposed based on which a document is logically represented for interactive editing, storage, retrieval, transfer, and logical analysis. Our algorithm has a high accuracy and takes approximately 1.4 seconds on a SGI Indy workstation for model creation, including orientation estimation, segmentation, and labeling (text, table, image, drawing, and ruler) for a 2550/spl times/3300 image of a typical journal page scanned at 300 dpi. This method is applicable to documents from various technical journals and can accommodate moderate amounts of skew and noise."
            },
            "slug": "Document-Representation-and-Its-Application-to-Page-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Document Representation and Its Application to Page Decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new document model which preserves top-down generation information is proposed based on which a document is logically represented for interactive editing, storage, retrieval, transfer, and logical analysis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117208225"
                        ],
                        "name": "Venu Govindaraju",
                        "slug": "Venu-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Venu Govindaraju"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Srihari and Govindaraju [21] apply this technique to binary document images, or a subregion thereof, that is known to contain only text and where the entire text block has a single orientation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6598794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02d93ad345f7e98bbb58b6112196a22b09744918",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The analysis of images of printed pages of text is considered. Since printed text can be viewed as textured line, the use of the Hough transform for detecting straight lines is proposed as an analysis tool. Methods for handling several discretization problems that arise in mapping the rectangular image space to the (\u03c1, \u0398) accumulator array are described. Several applications of analyzing the accumulator array are proposed. They include detecting the text skew angle, determining the signature of a text line so as to accept or reject a block as containing only text, using profile analysis to segment text into lines, and determining whether a textual block is rightside-up or otherwise."
            },
            "slug": "Analysis-of-textual-images-using-the-Hough-Srihari-Govindaraju",
            "title": {
                "fragments": [],
                "text": "Analysis of textual images using the Hough transform"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Methods for handling several discretization problems that arise in mapping the rectangular image space to the (\u03c1, \u0398) accumulator array are described."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34286355"
                        ],
                        "name": "Shin-Ywan Wang",
                        "slug": "Shin-Ywan-Wang",
                        "structuredName": {
                            "firstName": "Shin-Ywan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shin-Ywan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128300"
                        ],
                        "name": "T. Yagasaki",
                        "slug": "T.-Yagasaki",
                        "structuredName": {
                            "firstName": "Toshiaki",
                            "lastName": "Yagasaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yagasaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Wang and Yagasaki [67] present a method which is based on a hierarchical selection and classification of the connected components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Wang Yagasaki [67] b/w 5075dpi freely shaped text blocks, text lines, pictures, tables, separators text size in (6,72) points range dynamic parameters estimation; multiple skews are dealt Simon et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5434785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1997866ee54ab0b22c90d3d6086932e40a5118e",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a page segmentation method called block selection which not only segments the page image into categorized blocks but also provides a novel tree structure to represent the page blocks for selection. Block selection, more than classifying the text and nontext areas only, can identify the major document elements, such as text, picture, table, frame and line. This ability fits block selection into a wider range of document processing applications. In order to make the usage of block selection more practical to various document styles, many restrictions set on the document by some existing technologies are freed. The language on the document could be English-like, Kanji-like or both. The direction of text could be horizontal, vertical, slanted, or mixed. The editing style of the document is unconstrained. No skew correction is involved regardless of the document style. The formed blocks are described by a hierarchical tree to reflect the page arrangement in the \"object\" sense. This structural result can be efficiently used for further storage, retrieval or other manipulation purposes. The possible applications using this proposed method are discussed."
            },
            "slug": "Block-selection:-a-method-for-segmenting-a-page-of-Wang-Yagasaki",
            "title": {
                "fragments": [],
                "text": "Block selection: a method for segmenting a page image of various editing styles"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A page segmentation method called block selection is presented which not only segments the page image into categorized blocks but also provides a novel tree structure to represent the page blocks for selection that can be efficiently used for further storage, retrieval or other manipulation purposes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058879081"
                        ],
                        "name": "J. Sch\u00fcrmann",
                        "slug": "J.-Sch\u00fcrmann",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Sch\u00fcrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sch\u00fcrmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574837"
                        ],
                        "name": "N. Bartneck",
                        "slug": "N.-Bartneck",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Bartneck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bartneck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47530341"
                        ],
                        "name": "T. Bayer",
                        "slug": "T.-Bayer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053121569"
                        ],
                        "name": "J. Franke",
                        "slug": "J.-Franke",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Franke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934407"
                        ],
                        "name": "E. Mandler",
                        "slug": "E.-Mandler",
                        "structuredName": {
                            "firstName": "Eberhard",
                            "lastName": "Mandler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mandler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2481184"
                        ],
                        "name": "Matthias F. Oberl\u00e4nder",
                        "slug": "Matthias-F.-Oberl\u00e4nder",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Oberl\u00e4nder",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias F. Oberl\u00e4nder"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61736757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df266ec246f8586e7882dcb6a6797772245cdadf",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a conceptual framework for solving the task of document analysis, which, in essence, consists in the conversion of the document's pixel representation into an equivalent knowledge network representation holding the document's content and layout. Starting on the pixel level, the formation of elementary geometric objects on which layout analysis as well as the definition of character objects is based is described. Character recognition accomplishes the mapping from geometric object to character meaning in ASCII representation. On the next level of abstraction words are formed and verified by contextual processing. Modeled knowledge about complete documents and about how their constituents are related to the application forms the highest level of abstraction. The various problems arising at each stage are discussed. The dependencies between the different levels are exemplified and technical solutions put forward. >"
            },
            "slug": "Document-analysis-from-pixels-to-contents-Sch\u00fcrmann-Bartneck",
            "title": {
                "fragments": [],
                "text": "Document analysis-from pixels to contents"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a conceptual framework for solving the task of document analysis, which, in essence, consists in the conversion of the document's pixel representation into an equivalent knowledge network representation holding the document\"s content and layout."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34741785"
                        ],
                        "name": "P. Parodi",
                        "slug": "P.-Parodi",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Parodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Parodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329435"
                        ],
                        "name": "G. Piccioli",
                        "slug": "G.-Piccioli",
                        "structuredName": {
                            "firstName": "Giulia",
                            "lastName": "Piccioli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Piccioli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39112292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b58aa8006910a9b7760095c1e9c4babc3a4b7a6",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient, novel technique for segmenting document pages of mixed content into text and non-text regions is presented. The aim of the technique is to provide a pre-processing for an OCR system, so that large amounts of documents of unknown layout can be examined and the written content of such documents can be put into digital format without human intervention. The user fixes two parameters, the minimum width w of the text to be detected, and the precision /spl epsiv/ needed (both expressed as a percentage of the image width), according to the implementation needs (default values that yield good results for widely varying kinds of documents are /spl epsiv/=2%, w=4/spl epsiv/). The method works by detecting pieces of text lines in small overlapping columns of width w-2/spl epsiv/, shifted with respect to each other by /spl epsiv/ and by merging such pieces in a bottom-up fashion to form complete text lines and blocks of text lines. The algorithm is very fast and flexible and is able to work on low-resolution document pages. Experimental results are given which demonstrate the effectiveness of the method on several different kinds of documents."
            },
            "slug": "An-efficient-pre-processing-of-mixed-content-images-Parodi-Piccioli",
            "title": {
                "fragments": [],
                "text": "An efficient pre-processing of mixed-content document images for OCR systems"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An efficient, novel technique for segmenting document pages of mixed content into text and non-text regions is presented to provide a pre-processing for an OCR system, so that large amounts of documents of unknown layout can be examined and the written content of such documents can be put into digital format without human intervention."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780258"
                        ],
                        "name": "Jisheng Liang",
                        "slug": "Jisheng-Liang",
                        "structuredName": {
                            "firstName": "Jisheng",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jisheng Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053206762"
                        ],
                        "name": "R. Rogers",
                        "slug": "R.-Rogers",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Rogers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 86
                            }
                        ],
                        "text": "To overcome this limitation a region-based approach has been more recently introduced [91, 92, 47, 93, 94, 95]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7869564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38f43b60ae9307c3aba4755ced2f14a595e95dbe",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A document image analysis toolbox including a collection of data structures and algorithms to support a variety of applications, is described in this paper. An experimental environment is built to allow developers to develop, test and optimize their algorithms and systems. Appropriate and quantitative performance metrics for each kind of information a document analysis technique infers have been developed. The performance of each algorithm has been evaluated based on these metrics and the UW-III document image database which contains a total of 1600 English document images randomly selected from scientific and technical journals."
            },
            "slug": "UW-ISL-document-image-analysis-toolbox:-an-Liang-Rogers",
            "title": {
                "fragments": [],
                "text": "UW-ISL document image analysis toolbox: an experimental environment"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A document image analysis toolbox including a collection of data structures and algorithms to support a variety of applications, is described in this paper."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113154928"
                        ],
                        "name": "O. Yanagida",
                        "slug": "O.-Yanagida",
                        "structuredName": {
                            "firstName": "Osamu",
                            "lastName": "Yanagida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Yanagida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143882780"
                        ],
                        "name": "S. Takamatsu",
                        "slug": "S.-Takamatsu",
                        "structuredName": {
                            "firstName": "Shinobu",
                            "lastName": "Takamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Takamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 253
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection profile analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[63] b/w 90dpi freely shaped interblock gaps wider that interline gaps; fixed parameters; no wide word gaps multiple skews are dealt; segmentation of text blocks, figures, separators"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[63] present a method which is based on the thinning of the document background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41049994,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f5b0110c64d0bf696dc12bee3d3f2b97e87c7521",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new method of page segmentation based on the analysis of background (white areas). The proposed method is capable of segmenting pages with non-rectangular layout as well as with various angles of skew. The characteristics of the method are as follows: (1) thinning of the background enables us to represent white areas of any shape as connected thin lines or chains and the robustness for tilted page images is also achieved by the representation; and (2) based on this representation, the task of page segmentation is defined as to find the loops enclosing printed areas. The task is achieved by eliminating unnecessary chains using not only a feature of white areas, but also a feature of black areas divided by a chain. Based on the experimental results and the comparison with previous methods, we discuss the advantages and limitations of the proposed method."
            },
            "slug": "Page-segmentation-based-on-thinning-of-background-Kise-Yanagida",
            "title": {
                "fragments": [],
                "text": "Page segmentation based on thinning of background"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The proposed method is capable of segmenting pages with non-rectangular layout as well as with various angles of skew, and the advantages and limitations of the proposed method are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144167309"
                        ],
                        "name": "U. Pal",
                        "slug": "U.-Pal",
                        "structuredName": {
                            "firstName": "Umapada",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759420"
                        ],
                        "name": "B. Chaudhuri",
                        "slug": "B.-Chaudhuri",
                        "structuredName": {
                            "firstName": "Bidyut.",
                            "lastName": "Chaudhuri",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chaudhuri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Pal and Chaudhuri [26] present a second approach based on a clustering of two sets of points, L1 and L2, extracted from the image (see the first approach described in Section 3.2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 248
                            }
                        ],
                        "text": "envelopes) with line gaps wider than character gaps Nearest neighbor clustering O'Gorman [29] b/w 300 dpi 90 text only documents with few touching characters; multiple text directions Smith [30] b/w 300 dpi 15 0:05 one text direction Pal Chaudhuri [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Pal and Chaudhuri [26] propose two skew estimation techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 217
                            }
                        ],
                        "text": "3 Skew estimation Most of the skew estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection pro les [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "IRST 10 Pal and Chaudhuri [26] present a second approach based on a clustering of two sets of points, L1 and L2, extracted from the image (see the rst approach described in Section 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "[25] b/w 300 dpi 20 0:5 noisy structured documents with tables; an estimate of interline gaps is needed Pal Chaudhuri [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34632184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e2aed9f334846d14d941aa3e2d2028967b8d733",
            "isKey": true,
            "numCitedBy": 91,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-improved-document-skew-angle-estimation-Pal-Chaudhuri",
            "title": {
                "fragments": [],
                "text": "An improved document skew angle estimation technique"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73282614"
                        ],
                        "name": "Y. Hirayama",
                        "slug": "Y.-Hirayama",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Hirayama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hirayama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45360202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cac66edcb6a8d8ee34293183af5f0b2bfc6d0879",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a novel block segmentation method for document images which can be applied to various document formats. Some documents have complicated column structures, in which some figure areas have no surrounding rectangles and others cut across text areas. In the approach presented, in order to segment documents into text and figure areas, the text areas are analyzed first, and the figure areas are then detected by analyzing information on the text areas. The overall process is as follows. First, character strings are merged into text groups by analyzing regularity in the text areas. Next, border lines of columns are detected by linking the edges of the text groups. After that, the whole page is segmented into small blocks according to the border lines. The blocks are then unified by using the column information, and some unified blocks are detected. Finally, a projection profile method is applied to the unified blocks in order to detect text areas and figure areas. This method was applied to 61 pages of Japanese technical papers and magazines, and 93.3% of the text areas and 93.2% of the figure areas were detected correctly.<<ETX>>"
            },
            "slug": "A-block-segmentation-method-for-document-images-Hirayama",
            "title": {
                "fragments": [],
                "text": "A block segmentation method for document images with complicated column structures"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Presents a novel block segmentation method for document images which can be applied to various document formats and was applied to 61 pages of Japanese technical papers and magazines correctly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266621"
                        ],
                        "name": "M. Viswanathan",
                        "slug": "M.-Viswanathan",
                        "structuredName": {
                            "firstName": "Mahesh",
                            "lastName": "Viswanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Viswanathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 144
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection profile analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[57, 56] have developed a syntactic based system working with technical journals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2530196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62a53caea5213ea177298d7b2aff292b1386c37a",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Gobbledoc, a system providing remote access to stored documents, which is based on syntactic document analysis and optical character recognition (OCR), is discussed. In Gobbledoc, image processing, document analysis, and OCR operations take place in batch mode when the documents are acquired. The document image acquisition process and the knowledge base that must be entered into the system to process a family of page images are described. The process by which the X-Y tree data structure converts a 2-D page-segmentation problem into a series of 1-D string-parsing problems that can be tackled using conventional compiler tools is also described. Syntactic analysis is used in Gobbledoc to divide each page into labeled rectangular blocks. Blocks labeled text are converted by OCR to obtain a secondary (ASCII) document representation. Since such symbolic files are better suited for computerized search than for human access to the document content and because too many visual layout clues are lost in the OCR process (including some special characters), Gobbledoc preserves the original block images for human browsing. Storage, networking, and display issues specific to document images are also discussed.<<ETX>>"
            },
            "slug": "A-prototype-document-image-analysis-system-for-Nagy-Seth",
            "title": {
                "fragments": [],
                "text": "A prototype document image analysis system for technical journals"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The document image acquisition process and the knowledge base that must be entered into the system to process a family of page images are described, and the process by which the X-Y tree data structure converts a 2-D page-segmentation problem into a series of 1-D string-parsing problems that can be tackled using conventional compiler tools."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3700278"
                        ],
                        "name": "S. Stoddard",
                        "slug": "S.-Stoddard",
                        "structuredName": {
                            "firstName": "Spotswood",
                            "lastName": "Stoddard",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stoddard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[54, 84] b/w 300 dpi xy-cut cleaned docs; no skew; segmentation into columns, blocks, text lines Projection Pro les Pavlidis Zhou [59] b/w 300dpi right polygons blocks surrounded by straight white streams; xed parameters skew up to 15 is dealt; segmentation into rectangular blocks Texture or Local Analysis Jain Bhattacharjee [60] g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 54
                            }
                        ],
                        "text": "A popular approach to page segmentation, described in [54, 84], is the Recursive X-Y Cuts, RXYC, algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59896225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e33ac20d85869b44815de3b87a4d6d4e841b76d",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "DOCUMENT-ANALYSIS-WITH-AN-EXPERT-SYSTEM-Nagy-Seth",
            "title": {
                "fragments": [],
                "text": "DOCUMENT ANALYSIS WITH AN EXPERT SYSTEM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "The first step of the algorithm aims at efficiently computing the connected components and their centroides by means of a structure called block adjacency graph."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41609314,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca920e10343757ba0d4151bc8ce152cbd5191bee",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-robust-and-fast-skew-detection-algorithm-for-Yu-Jain",
            "title": {
                "fragments": [],
                "text": "A robust and fast skew detection algorithm for generic documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 86
                            }
                        ],
                        "text": "To overcome this limitation a region-based approach has been more recently introduced [91, 92, 47, 93, 94, 95]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[47] b/w 150dpi freely shaped words text only (synthetic) docs skew up to 0:5 is tolerated; trainable Background Analysis Baird at al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[47] describe a segmentation algorithm for the detection of words in textual documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7129006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "736b90c9e23a6ca700341e0a5b99cd0ed5c15288",
            "isKey": true,
            "numCitedBy": 6,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The textual structures like the characters, words, text lines, paragraphs on a document image are usually laid out in a very structured manner -- having preferred spatial relations. These spatial relations are rarely deterministic; instead, they describe correlations and likelihoods. Therefore, any realistic document layout analysis algorithm should utilize this type of knowledge in order to optimize its performances. In this paper, we first describe a method for automatically generating a large amount of almost 100% correct ground truth data for the document layout analysis. The bounding boxes for the characters, words, text lines, paragraphs are expressed in a hierarchy. Then based on these layout ground-truth, we build statistical models to model the layout structures for the words, text lines, paragraphs on document images. Finally, we described an algorithm that utilizes these statistical models to extract the text words on document images. The performance of the algorithm is evaluated and reported."
            },
            "slug": "Extraction-of-text-layout-structures-on-document-on-Chen-Haralick",
            "title": {
                "fragments": [],
                "text": "Extraction of text layout structures on document images based on statistical characterization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper described a method for automatically generating a large amount of almost 100% correct ground truth data for the document layout analysis and built statistical models to model the layout structures for the words, text lines, paragraphs on document images."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398550688"
                        ],
                        "name": "L. O'Gorman",
                        "slug": "L.-O'Gorman",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "O'Gorman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. O'Gorman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another method for skew detection based on clustering by nearness is presented by O\u2019Gorman [ 29 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Most of the skew estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection proflles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28,  29 , 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22995244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d85097da36118fbccfeb7802abf89bf4b4c63a3e",
            "isKey": false,
            "numCitedBy": 728,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Page layout analysis is a document processing technique used to determine the format of a page. This paper describes the document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components. The method yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks. It is advantageous over many other methods in three main ways: independence from skew angle, independence from different text spacings, and the ability to process local regions of different text orientations within the same image. Results of the method shown for several different page formats and for randomly oriented subpages on the same image illustrate the versatility of the method. We also discuss the differences, advantages, and disadvantages of the docstrum with respect to other lay-out methods. >"
            },
            "slug": "The-Document-Spectrum-for-Page-Layout-Analysis-O'Gorman",
            "title": {
                "fragments": [],
                "text": "The Document Spectrum for Page Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components, yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757030"
                        ],
                        "name": "Berrin A. Yanikoglu",
                        "slug": "Berrin-A.-Yanikoglu",
                        "structuredName": {
                            "firstName": "Berrin",
                            "lastName": "Yanikoglu",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berrin A. Yanikoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820595"
                        ],
                        "name": "L. Vincent",
                        "slug": "L.-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 123
                            }
                        ],
                        "text": "2 The region-based approach Region-based page segmentation benchmarking environments are proposed by Yanikoglu and Vincent [92, 93], and Haralick et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 86
                            }
                        ],
                        "text": "To overcome this limitation a region-based approach has been more recently introduced [91, 92, 47, 93, 94, 95]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8690249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3125b0dd9affad0ddcaa6dc2e26b0b45f5d304f",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pink-Panther:-A-Complete-Environment-For-And-Page-Yanikoglu-Vincent",
            "title": {
                "fragments": [],
                "text": "Pink Panther: A Complete Environment For Ground-Truthing And Benchmarking Document Page Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713698"
                        ],
                        "name": "N. Normand",
                        "slug": "N.-Normand",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Normand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Normand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398288037"
                        ],
                        "name": "C. Viard-Gaudin",
                        "slug": "C.-Viard-Gaudin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Viard-Gaudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Viard-Gaudin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Background Analysis Normand ViardGaudin [62] b/w 150dpi freely shaped no automatic way for relevant nodes selection is provided multiple skews are dealt; segmentation of text, large text, graphics areas Kise et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 253
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection profile analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Normand and Viard-Gaudin [62] present a 2D smoothing algorithm for the analysis of the document background, which is basically an extension of the RLSA to two dimensions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62474643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2dc6cdb40cdae24103da91773c7b9c07a617467",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel page segmentation algorithm is provided in this paper. Based on the extraction of the background, it offers the benefit of being adaptive to the context of the document and to be insensitive to the orientation of the text blocks. It involves a two-dimensional isotropic structuring element used to characterized the white streams. This element is a disk approximated by a regular octagon which can be recursively generated. Another advantage of the proposed method is that a hierarchical segmentation can be derived from the image built upon the octagonal pattern. This tree allows to perform an isotropic multi-scale smearing, which leads to a physical segmentation. The algorithms are based on an input-time tracing principle and use a single scan of the image, they are very well suited to a real-time implementation."
            },
            "slug": "A-background-based-adaptive-page-segmentation-Normand-Viard-Gaudin",
            "title": {
                "fragments": [],
                "text": "A background based adaptive page segmentation algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A novel page segmentation algorithm is provided, based on the extraction of the background, which offers the benefit of being adaptive to the context of the document and to be insensitive to the orientation of the text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152996923"
                        ],
                        "name": "Hong Yan",
                        "slug": "Hong-Yan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Yan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Most of the skew estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection proflles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31,  32 , 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The method described by Yan [ 32 ] has the interesting advantage that it can be applied directly to gray level or color images as well as binary images and does not require components extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43702917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc0f2aa66992b4c376a994382479df2f73d8f876",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An efficient algorithm is presented in this paper for correcting skew of text lines in scanned document images. In this method, the cross-correlation between two lines in the image with a fixed distance is calculated. The correlation functions for all pairs of lines in the image are accumulated. The shift for which the accumulated cross-correlation function takes the maximum is then used for determining the skew angle. The image is rotated in the opposite direction for skew correction. The correlation function can be calculated without multiplications for binary images, thus the algorithm can be very efficiently implemented. The method can be used directly for gray-scale and color images as well as binary images. It has been tested on scanned document images with good results."
            },
            "slug": "Skew-Correction-of-Document-Images-Using-Interline-Yan",
            "title": {
                "fragments": [],
                "text": "Skew Correction of Document Images Using Interline Cross-Correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The correlation function can be calculated without multiplications for binary images, thus the algorithm can be very efficiently implemented and has been tested on scanned document images with good results."
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111216375"
                        ],
                        "name": "Dacheng Wang",
                        "slug": "Dacheng-Wang",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dacheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Srihari and Govindaraju [21] apply this technique to binary document images, or a subregion thereof, that is known to contain only text and where the entire text block has a single orientation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Wang and Srihari [55] point out some limitations of the approach proposed by Wong et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 83
                            }
                        ],
                        "text": "The major part are based on feature extraction and linear discriminant classifiers [53, 55, 59, 77], but other techniques are presented [78, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Wang and Srihari [55] compare the RLSA and RXYC approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 134
                            }
                        ],
                        "text": "Block classification techniques described in this section can be grouped into features extraction and linear discriminant classifiers [53, 77, 55, 59], binary classification tree [78], and neural networks [79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 144
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection profile analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "This method is used, for example, by Srihari and Govindaraju [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Wang Srihari [55] b/w 100200dpi small, medium, large text, graphics, halftones no skew trainable; independence of blocks size"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26685555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "873fa0ca7454bd1ca4de25128c522e088b635ddc",
            "isKey": true,
            "numCitedBy": 250,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Classification-of-newspaper-image-blocks-using-Wang-Srihari",
            "title": {
                "fragments": [],
                "text": "Classification of newspaper image blocks using texture analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704694"
                        ],
                        "name": "J. Sauvola",
                        "slug": "J.-Sauvola",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "Sauvola",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sauvola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Sauvola and Pietika\u0308inen [34] propose an approach for skew detection based on gradient direction analysis, that may be applied to binary or gray-level images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Other techniques have been proposed which are based on gradient analysis [34, 35], on the analysis of the Fourier spectrum [16], on the use of morphological transforms [36], and on subspace line detection [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16121760,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a4efc90e9d2e8efeee08bf90b81bc889e48e59f",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In a document analysis system the skew error detection is one of the most crucial parts. This paper describes a method that takes advantage of texture orientation analysis in order to find the skew error from the source document. The source image is first shrunk to melt the characters in together, reduce noise and decrease the amount of computing. After pre-processing a texture direction analysis method is applied. This procedure produces an unambiguous angle in degrees as the skew angle."
            },
            "slug": "Skew-Angle-Detection-Using-Texture-Direction-Sauvola-Pietik\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Skew Angle Detection Using Texture Direction Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method that takes advantage of texture orientation analysis in order to find the skew error from the source document using an unambiguous angle in degrees as the skew angle is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23410608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "999e76f9115af2741b0cd973d875163ae714d5da",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Page-segmentation-and-classification-Pavlidis-Zhou",
            "title": {
                "fragments": [],
                "text": "Page segmentation and classification"
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3141168"
                        ],
                        "name": "P. W. Palumbo",
                        "slug": "P.-W.-Palumbo",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Palumbo",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. W. Palumbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144452040"
                        ],
                        "name": "P. Swaminathan",
                        "slug": "P.-Swaminathan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Swaminathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Swaminathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Nowadays the zoning process is a mix of the so-called geometric and logical layout analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62135560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ca2225b504ae9766797d3d44bd97ffda9628f8",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The extraction of a binary image from a gray level image is a common image processing operation particularly for document image analysis and optical character recognition. Various methods for this task are described in the literature including global and adaptive binarization. This paper evaluates three adaptive binarization techniques viz., a contrast measure approach, a weighted running average approach and a second derivative approach, and compares them to global binarization methods. Experiments with noisy document (postal letter mail) images lead to the following conclusions. Image contrast binarization often yields nearly the same results as the edge operator, with considerably less computation and is less sensitive to parameter settings. In addition, the edge operator is more sensitive to image resolution than the contrast operator. The weighted running-average approach is highly sensitive to the parameters involved in the calculation of the average but produces a quick binarization."
            },
            "slug": "Document-Image-Binarization:-Evaluation-Of-Palumbo-Swaminathan",
            "title": {
                "fragments": [],
                "text": "Document Image Binarization: Evaluation Of Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper evaluates three adaptive binarization techniques viz., a contrast measure approach, a weighted running average approach and a second derivative approach, and compares them to global Binarization methods."
            },
            "venue": {
                "fragments": [],
                "text": "Optics & Photonics"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797498"
                        ],
                        "name": "F. Shih",
                        "slug": "F.-Shih",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Shih",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2566475"
                        ],
                        "name": "Shy-Shyan Chen",
                        "slug": "Shy-Shyan-Chen",
                        "structuredName": {
                            "firstName": "Shy-Shyan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shy-Shyan Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29853030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a82c347486d34c53f2f5fcd4fd156ef318ba5720",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an adaptive block segmentation and classification technique for daily-received office documents having complex layout structures such as multiple columns and mixed-mode contents of text, graphics, and pictures. First, an improved two-step block segmentation algorithm is performed based on run-length smoothing for decomposing any document into single-mode blocks. Then, a rule-based block classification is used for classifying each block into the text, horizontal/vertical line, graphics, or-picture type. The document features and rules used are independent of character font and size and the scanning resolution. Experimental results show that our algorithms are capable of correctly segmenting and classifying different types of mixed-mode printed documents."
            },
            "slug": "Adaptive-document-block-segmentation-and-Shih-Chen",
            "title": {
                "fragments": [],
                "text": "Adaptive document block segmentation and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental results show that the algorithms are capable of correctly segmenting and classifying different types of mixed-mode printed documents, independent of character font and size and the scanning resolution."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39702442"
                        ],
                        "name": "Y. Tang",
                        "slug": "Y.-Tang",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Tang",
                            "middleNames": [
                                "Yan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: document image processing, document image understanding, automatic zoning, geometric and logical layout, page decomposition, document segmentation and classification, text segmentation, skew angle detection, binarization, document model, performance evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29737518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92df66c37211e55faafdf879f95185fa86f196a6",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-document-processing:-A-survey-Tang-Lee",
            "title": {
                "fragments": [],
                "text": "Automatic document processing: A survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145470231"
                        ],
                        "name": "Kwan Y. Wong",
                        "slug": "Kwan-Y.-Wong",
                        "structuredName": {
                            "firstName": "Kwan",
                            "lastName": "Wong",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kwan Y. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34896449"
                        ],
                        "name": "R. Casey",
                        "slug": "R.-Casey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Casey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880661"
                        ],
                        "name": "F. Wahl",
                        "slug": "F.-Wahl",
                        "structuredName": {
                            "firstName": "Friedrich",
                            "lastName": "Wahl",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wahl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] related to the needed information about the geometric characteristics of the text lines."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 83
                            }
                        ],
                        "text": "The major part are based on feature extraction and linear discriminant classifiers [53, 55, 59, 77], but other techniques are presented [78, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 134
                            }
                        ],
                        "text": "Block classification techniques described in this section can be grouped into features extraction and linear discriminant classifiers [53, 77, 55, 59], binary classification tree [78], and neural networks [79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection profile analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] b/w 240dpi text lines, graphics and halftones, horizontal lines, vertical lines no skew; known chars size; problems with close or high text lines"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "in [53] computes some basic features from the blocks produced in the segmentation step (see Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] b/w 240dpi rectangular blocks clean docs; no skew; known chars size segmentation of graphics or picture blocks and text lines Nagy et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15921038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abca302c74d2f5adfd323a28e26d40b019df2b5",
            "isKey": true,
            "numCitedBy": 594,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper outlines the requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing. Several critical functions have been investigated and the technical approaches are discussed. The first is the segmentation and classification of digitized printed documents into regions of text and images. A nonlinear, run-length smoothing algorithm has been used for this purpose. By using the regular features of text lines, a linear adaptive classification scheme discriminates text regions from others. The second technique studied is an adaptive approach to the recognition of the hundreds of font styles and sizes that can occur on printed documents. A preclassifier is constructed during the input process and used to speed up a well-known pattern-matching method for clustering characters from an arbitrary print source into a small sample of prototypes. Experimental results are included."
            },
            "slug": "Document-Analysis-System-Wong-Casey",
            "title": {
                "fragments": [],
                "text": "Document Analysis System"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing, are outlined and several critical functions have been investigated and the technical approaches are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409052480"
                        ],
                        "name": "S. C. Hinds",
                        "slug": "S.-C.-Hinds",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Hinds",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Hinds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168553590"
                        ],
                        "name": "James L. Fisher",
                        "slug": "James-L.-Fisher",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Fisher",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402307078"
                        ],
                        "name": "D. D'Amato",
                        "slug": "D.-D'Amato",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "D'Amato",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. D'Amato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Hinds et al. [22] develop a skew estimation algorithm which reduces the amount of pixels to be processed by the Hough transform."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61919721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b27f45bebdec8e8d8503f7314533674c51de0e84",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "As part of the development of a document image analysis system, a method, based on the Hough transform, was devised for the detection of document skew and interline spacing-necessary parameters for the automatic segmentation of text from graphics. Because the Hough transform is computationally expensive, the amount of data within a document image is reduced through the computation of its horizontal and vertical black runlengths. Histograms of these runlengths are used to determine whether the document is in portrait or landscape orientation. A gray scale burst image is created from the black runlengths that are perpendicular to the text lines by placing the length of the run in the run's bottom-most pixel. By creating a burst image from the original document image, the processing time of the Hough transform can be reduced by a factor of as much as 7.4 for documents with gray-scale images. Because only small runlengths are input to the Hough transform and because the accumulator array is incremented by the runlength associated with a pixel rather than by a factor of 1, the negative effects of noise, black margins, and figures are avoided. Consequently, interline spacing can be determined more accurately.<<ETX>>"
            },
            "slug": "A-document-skew-detection-method-using-run-length-Hinds-Fisher",
            "title": {
                "fragments": [],
                "text": "A document skew detection method using run-length encoding and the Hough transform"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "By creating a burst image from the original document image, the processing time of the Hough transform can be reduced by a factor of as much as 7.4 for documents with gray-scale images and interline spacing can be determined more accurately."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings. 10th International Conference on Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778494"
                        ],
                        "name": "H. Don",
                        "slug": "H.-Don",
                        "structuredName": {
                            "firstName": "Hon-Son",
                            "lastName": "Don",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Don"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 90
                            }
                        ],
                        "text": "Several algorithms have been speci cally designed for the binarization of document images [7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2734435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "968a55e72fa04b74d71a14990d6137eee2f1e512",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. A new thresholding method, called the noise attribute thresholding method (NAT), for document image binarization is presented in this paper. This method utilizes the noise attribute features extracted from the images to make the selection of threshold values for image thresholding. These features are based on the properties of noise in the images and are independent of the strength of the signals (objects and background) in the image. A simple noise model is given to explain these noise properties. The NAT method has been applied to the problem of removing text and figures printed on the back of the paper. Conventional global thresholding methods cannot solve this kind of problem satisfactorily. Experimental results show that the NAT method is very effective."
            },
            "slug": "A-noise-attribute-thresholding-method-for-document-Don",
            "title": {
                "fragments": [],
                "text": "A noise attribute thresholding method for document image binarization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The NAT method has been applied to the problem of removing text and figures printed on the back of the paper because conventional global thresholding methods cannot solve this kind of problem satisfactorily."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083596961"
                        ],
                        "name": "Frank H\u00f6nes",
                        "slug": "Frank-H\u00f6nes",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "H\u00f6nes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank H\u00f6nes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101641932"
                        ],
                        "name": "J. Lichter",
                        "slug": "J.-Lichter",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Lichter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lichter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8845604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b7359c77967810bd683541cd908a7276298c0f3",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Proper processing and efficient representation of the digitized images of printed documents require the separation of the various information types: text, graphics, and image elements. For most applications it is sufficient to separate text and nontext, because text contains the most information. This paper describes the implementation and performance of a robust algorithm for text extraction and segmentation that is completely independent of text orientation and can deal with text in various font styles and sizes. Text objects can be nested in nontext areas, and inverse printing can also be analyzed. It should be mentioned that the classification is based only on rough image features, and individual characters are not recognized. The three main processing steps of the system are the generation of connected components, neighborhood analysis, and generation of text lines and blocks. As output, connected components are classified as text or nontext. Text components are grouped as characters, words, lines, and blocks. Nontext objects are accumulated as a separate nontext block."
            },
            "slug": "Layout-extraction-of-mixed-mode-documents-H\u00f6nes-Lichter",
            "title": {
                "fragments": [],
                "text": "Layout extraction of mixed mode documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes the implementation and performance of a robust algorithm for text extraction and segmentation that is completely independent of text orientation and can deal with text in various font styles and sizes."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554154"
                        ],
                        "name": "Younki Min",
                        "slug": "Younki-Min",
                        "structuredName": {
                            "firstName": "Younki",
                            "lastName": "Min",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Younki Min"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723666"
                        ],
                        "name": "Sung-Bae Cho",
                        "slug": "Sung-Bae-Cho",
                        "structuredName": {
                            "firstName": "Sung-Bae",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sung-Bae Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710749"
                        ],
                        "name": "Yillbyung Lee",
                        "slug": "Yillbyung-Lee",
                        "structuredName": {
                            "firstName": "Yillbyung",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yillbyung Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30892773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dc6a7d6e0f57afb81babbdbb4f8f7e061e9b182",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Document recognition usually requires several preprocessing steps in which skew estimation and correction are critical to get a useful system. This paper proposes an efficient data reduction method to enhance the performance of document skew estimation by using a Hough transformation. The time complexity of the Hough transformation is O(/spl Theta/N), where N is the number of black pixels in a document and /spl Theta/ is the skew estimation range divided by /spl Delta//spl theta/. We might enhance the performance by reducing N or /spl Theta/. The proposed method uses an efficient data reduction method called the modified version of divided horizontal histograms, which reduces the number of black pixels N, while retaining the skewness of document. In order to show the superiority of the proposed method, we have also performed experiments with scanned documents, comparing the result with those of the usual data reduction methods: vertical run-length and connected component methods."
            },
            "slug": "A-data-reduction-method-for-efficient-document-skew-Min-Cho",
            "title": {
                "fragments": [],
                "text": "A data reduction method for efficient document skew estimation based on Hough transformation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An efficient data reduction method to enhance the performance of document skew estimation by using a Hough transformation, called the modified version of divided horizontal histograms, which reduces the number of black pixels N, while retaining the skewness of document."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40562121"
                        ],
                        "name": "N. Amamoto",
                        "slug": "N.-Amamoto",
                        "structuredName": {
                            "firstName": "Naohiro",
                            "lastName": "Amamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Amamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71197433"
                        ],
                        "name": "Shin Torigoe",
                        "slug": "Shin-Torigoe",
                        "structuredName": {
                            "firstName": "Shin",
                            "lastName": "Torigoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shin Torigoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49788722"
                        ],
                        "name": "Yoshitaka Hirogaki",
                        "slug": "Yoshitaka-Hirogaki",
                        "structuredName": {
                            "firstName": "Yoshitaka",
                            "lastName": "Hirogaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshitaka Hirogaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "RLSA received great interest, principally because of its easy implementation, and has been employed, with some modifications, in several systems to perform the segmentation phase [18, 82, 83, 79, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27805018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6aced08ac04f06e282c1558921d0f3a087037a14",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of a computational method for extracting character portions from a complicated document is described. The goal of the proposed method is to realize a practical document structure analysis for advanced optical character recognition systems with document databases. The authors mainly discuss a structural extraction technique called \"text area extraction\" which locates all text areas in 255 of the 309 (83%) evaluation documents.<<ETX>>"
            },
            "slug": "Block-segmentation-and-text-area-extraction-of-Amamoto-Torigoe",
            "title": {
                "fragments": [],
                "text": "Block segmentation and text area extraction of vertically/horizontally written document"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors mainly discuss a structural extraction technique called \"text area extraction\" which locates all text areas in 255 of the 309 (83%) evaluation documents."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122930"
                        ],
                        "name": "J. Ha",
                        "slug": "J.-Ha",
                        "structuredName": {
                            "firstName": "Jaekyu",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 86
                            }
                        ],
                        "text": "To overcome this limitation a region-based approach has been more recently introduced [91, 92, 47, 93, 94, 95]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6826352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebbbb7c50c5a5bc6f402230c8c77b83c83db4b48",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Segmentation of document images can be performed by projecting image pixels. This pixel projection approach is one of widely used top-down segmentation methods and is based on the assumption that the document image has been correctly deskewed. Unfortunately, the pixel projection approach is computationally inefficient. It is because each symbol is not treated as a computational unit. In this paper, we explain a new technique which is highly tactical in the profiling analysis. Instead of projecting image pixels, we first compute the bounding box of each connected component in a document image and then we project those bounding boxes. Using the new technique, this paper describes how to extract words, text lines, and text blocks (e.g., paragraphs). This bounding box projection approach has many advantages over the pixel projection approach. It is less computationally involved. When applied to text zones, it is also possible to infer from the projection profiles how bounding boxes (and, therefore, primitive symbols) are aligned and/or where significant horizontal and vertical gaps are present. Since the new technique manipulates only bounding boxes, it can be applied to any noncursive language documents."
            },
            "slug": "Document-page-decomposition-using-bounding-boxes-of-Ha-Phillips",
            "title": {
                "fragments": [],
                "text": "Document page decomposition using bounding boxes of connected components of black pixels"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper describes how to extract words, text lines, and text blocks (e.g., paragraphs) using a new technique which is highly tactical in the profiling analysis and has many advantages over the pixel projection approach."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1945685"
                        ],
                        "name": "Don Sylwester",
                        "slug": "Don-Sylwester",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Sylwester",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don Sylwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection profile analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A trainable algorithm for page segmentation based on RXYC procedure is presented by Sylwester and Seth [58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 690733,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "0f1f02ed8cb0a5145cfa99df6f5be35e2c0cff7d",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Column segmentation logically precedes OCR in the document analysis process. The trainable algorithm XYCUT relies on horizontal and vertical binary profiles to produce an XY-tree representing the column structure of a page of a technical document in a single pass through the bit image. Training against ground truth adjusts a single, resolution independent, parameter using only local information and guided by an edit distance function. The algorithm correctly segments the page image for a (fairly) wide range of parameter values, although small, local and repairable errors may be made, an effect measured by a repair cost function."
            },
            "slug": "A-trainable,-single-pass-algorithm-for-column-Sylwester-Seth",
            "title": {
                "fragments": [],
                "text": "A trainable, single-pass algorithm for column segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Column segmentation logically precedes OCR in the document analysis process and correctly segments the page image for a (fairly) wide range of parameter values, although small, local and repairable errors may be made, an effect measured by a repair cost function."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757655"
                        ],
                        "name": "S. Bhattacharjee",
                        "slug": "S.-Bhattacharjee",
                        "structuredName": {
                            "firstName": "Sushil",
                            "lastName": "Bhattacharjee",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhattacharjee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "The authors aim at overcoming the limitations of the approach presented in [60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "In the work of Jain and Bhattacharjee [60] text/non-text segmentation is viewed as a texture segmentation problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 201
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection pro le analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 331,
                                "start": 327
                            }
                        ],
                        "text": "[54, 84] b/w 300 dpi xy-cut cleaned docs; no skew; segmentation into columns, blocks, text lines Projection Pro les Pavlidis Zhou [59] b/w 300dpi right polygons blocks surrounded by straight white streams; xed parameters skew up to 15 is dealt; segmentation into rectangular blocks Texture or Local Analysis Jain Bhattacharjee [60] g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13639250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "808f205f834cfdfa823b97fbf1f20bf82ddaa8d7",
            "isKey": true,
            "numCitedBy": 226,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a considerable interest in designing automatic systems that will scan a given paper document and store it on electronic media for easier storage, manipulation, and access. Most documents contain graphics and images in addition to text. Thus, the document image has to be segmented to identify the text regions, so that OCR techniques may be applied only to those regions. In this paper, we present a simple method for document image segmentation in which text regions in a given document image are automatically identified. The proposed segmentation method for document images is based on a multichannel filtering approach to texture segmentation. The text in the document is considered as a textured region. Nontext contents in the document, such as blank spaces, graphics, and pictures, are considered as regions with different textures. Thus, the problem of segmenting document images into text and nontext regions can be posed as a texture segmentation problem. Two-dimensional Gabor filters are used to extract texture features for each of these regions. These filters have been extensively used earlier for a variety of texture segmentation tasks. Here we apply the same filters to the document image segmentation problem. Our segmentation method does not assume any a priori knowledge about the content or font styles of the document, and is shown to work even for skewed images and handwritten text. Results of the proposed segmentation method are presented for several test images which demonstrate the robustness of this technique."
            },
            "slug": "Text-segmentation-using-gabor-filters-for-automatic-Jain-Bhattacharjee",
            "title": {
                "fragments": [],
                "text": "Text segmentation using gabor filters for automatic document processing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a simple method for document image segmentation in which text regions in a given document image are automatically identified and is shown to work even for skewed images and handwritten text."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48717516"
                        ],
                        "name": "T. Taxt",
                        "slug": "T.-Taxt",
                        "structuredName": {
                            "firstName": "Torfinn",
                            "lastName": "Taxt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Taxt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704876"
                        ],
                        "name": "P. Flynn",
                        "slug": "P.-Flynn",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Flynn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Flynn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "These elements have to be isolated and processed by appropriate modules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11165034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9b1e7801c61164e559bccd225ae4866a8740ec1",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Several methods for segmentation of document images (maps, drawings, etc.) are explored. The segmentation operation is posed as a statistical classification task with two pattern classes: print and background. A number of classification strategies are available. All require some prior information about the distribution of gray levels for the two classes. Training (either supervised or unsupervised) is employed to form these initial density estimates. Automatic updating of the class-conditional densities is performed within subregions in the image to adapt these global density estimates to the local image area. After local class-conditional densities have been obtained, each pixel is classified within the window using several techniques: a noncontextual Bayes classifier, Besag's classifier, relaxation, Owen and Switzer's classifier, and Haslett's classifier. Four test images were processed. In two of these, the relaxation method performed best, and in the other two, the noncontextual method performed best. Automatic updating improved the results for both classifiers. >"
            },
            "slug": "Segmentation-of-Document-Images-Taxt-Flynn",
            "title": {
                "fragments": [],
                "text": "Segmentation of Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Several methods for segmentation of document images (maps, drawings, etc.) are explored and a noncontextual Bayes classifier performed best, and automatic updating improved the results for both classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116628331"
                        ],
                        "name": "Ching-Huei Wang",
                        "slug": "Ching-Huei-Wang",
                        "structuredName": {
                            "firstName": "Ching-Huei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ching-Huei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3141168"
                        ],
                        "name": "P. W. Palumbo",
                        "slug": "P.-W.-Palumbo",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Palumbo",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. W. Palumbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5406950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe842041ab0e1f77b1481a8c25732276b7c31a88",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "An important task in postal automation technology is determining the position and orientation of the destination address block in the image of a mail piece such as a letter, magazine, or parcel. The corresponding subimage is then presented to a human operator or a machine reader (optical character reader) that can read the zip code and, if necessary, other address information and direct the mail piece to the appropriate sorting bin. Analysis of physical characteristics of mail pieces indicates that in order to automate the address finding task, several different image analysis operations are necessary. Some examples are locating a rectangular white address label on a multicolor background, progressively grouping characters into text lines and text lines into text blocks, eliminating candidate regions by specialized detectors (for example, detecting regions such as postage stamps), and identifying handwritten regions. Described here are several operations, their utility as predicted by statistics of mail piece characteristics, and the results of applying the operations to a task set of mail piece images. A problem-solving architecture based on the blackboard model of problem solving for appropriately invoking the tools and combining their results is described."
            },
            "slug": "Recognizing-Address-Blocks-on-Mail-Pieces:-Tools-Srihari-Wang",
            "title": {
                "fragments": [],
                "text": "Recognizing Address Blocks on Mail Pieces: Specialized Tools and Problem-Solving Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A problem-solving architecture based on the blackboard model of problem solving for appropriately invoking the tools and combining their results is described and the results of applying the operations to a task set of mail piece images are described."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232446"
                        ],
                        "name": "B. Gatos",
                        "slug": "B.-Gatos",
                        "structuredName": {
                            "firstName": "Basilios",
                            "lastName": "Gatos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368634"
                        ],
                        "name": "N. Papamarkos",
                        "slug": "N.-Papamarkos",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Papamarkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Papamarkos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728570"
                        ],
                        "name": "C. Chamzas",
                        "slug": "C.-Chamzas",
                        "structuredName": {
                            "firstName": "Christodoulos",
                            "lastName": "Chamzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chamzas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Other techniques have been proposed which are based on gradient analysis [34, 35], on the analysis of the Fourier spectrum [16], on the use of morphological transforms [36], and on subspace line detection [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "The estimated skew is the inverse tangent of the ratio between the value of s that maximizes R(s), and D.\nGatos et al. [33] propose a skew estimation technique based on a correlation measure between vertical stripes of the image preprocessed by an horizontal run smoothing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205905844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c664274b7cce4997a11f5db2d2eddeff08979c5a",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Skew-detection-and-text-line-position-determination-Gatos-Papamarkos",
            "title": {
                "fragments": [],
                "text": "Skew detection and text line position determination in digitized documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49983785"
                        ],
                        "name": "J. L. Fisher",
                        "slug": "J.-L.-Fisher",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Fisher",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409052480"
                        ],
                        "name": "S. C. Hinds",
                        "slug": "S.-C.-Hinds",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Hinds",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Hinds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402307078"
                        ],
                        "name": "D. D'Amato",
                        "slug": "D.-D'Amato",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "D'Amato",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. D'Amato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "RLSA received great interest, principally because of its easy implementation, and has been employed, with some modifications, in several systems to perform the segmentation phase [18, 82, 83, 79, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61327298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23fdda45268bf3898a95bad889e7a169081c4a77",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A rule-based system for automatically segmenting a document image into regions of text and nontext is presented. The initial stages of the system perform image enhancement functions such as adaptive thresholding, morphological processing, and skew detection and correction. The image segmentation process consists of smearing the original image via the run length smoothing algorithm, calculating the connected components locations and statistics, and filtering (segmenting) the image based on these statistics. The text regions can be converted (via an optical character reader) to a computer-searchable form, and the nontext regions can be extracted and preserved. The rule-based structure allows easy fine tuning of the algorithmic steps to produce robust rules, to incorporate additional tools (as they become available), and to handle special segmentation needs.<<ETX>>"
            },
            "slug": "A-rule-based-system-for-document-image-segmentation-Fisher-Hinds",
            "title": {
                "fragments": [],
                "text": "A rule-based system for document image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A rule-based system for automatically segmenting a document image into regions of text and nontext is presented and allows easy fine tuning of the algorithmic steps to produce robust rules, to incorporate additional tools (as they become available), and to handle special segmentation needs."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings. 10th International Conference on Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2270291"
                        ],
                        "name": "S. Tsujimoto",
                        "slug": "S.-Tsujimoto",
                        "structuredName": {
                            "firstName": "Shuichi",
                            "lastName": "Tsujimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tsujimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3358140"
                        ],
                        "name": "H. Asada",
                        "slug": "H.-Asada",
                        "structuredName": {
                            "firstName": "Haruo",
                            "lastName": "Asada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Asada"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Following the approach described in [71], at the end of each stage the nodes are classified using some geometric features, like the height of the group of components associated to the node, the aspect ratio of its bounding box, and the density of black pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Tsujimoto and Asada [71] describe an analysis and interpretation system for different types of documents: magazines, journals, newspapers, manuals, letters, scientific papers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Smearing Tsujimoto Asada [71] b/w 300dpi rectangular blocks text lines, figures, graphics, tables, separators, noise no skew frequent touching chars are admitted"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "In Tsujimoto and Asada\u2019s system [71], tree transformation is performed with a set of deterministic rules, possibly repeated, which label the blocks and define their reading"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62000868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1248ffebf372e65e2ced57f014d8dd933050ab30",
            "isKey": true,
            "numCitedBy": 153,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The document image processes used in a recently developed text reading system are described. The system consists of three major components: document analysis, document understanding, and character segmentation/recognition. The document analysis component extracts lines of text from a page for recognition. The document understanding component extracts logical relationships between the document constituents. The character segmentation/recognition component extracts characters from a text line and recognizes them. Experiments on more than a hundred documents have proved that the proposed approaches to document analysis and document understanding are robust even for multicolumned and multiarticle documents containing graphics and photographs, and that the proposed character segmentation/recognition method is robust enough to cope with omnifont characters which frequently touch each other. >"
            },
            "slug": "Major-components-of-a-complete-text-reading-system-Tsujimoto-Asada",
            "title": {
                "fragments": [],
                "text": "Major components of a complete text reading system"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experiments have proved that the proposed approaches to document analysis and document understanding are robust even for multicolumned and multiarticle documents containing graphics and photographs, and thatThe proposed character segmentation/recognition method is robust enough to cope with omnifont characters which frequently touch each other."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757030"
                        ],
                        "name": "Berrin A. Yanikoglu",
                        "slug": "Berrin-A.-Yanikoglu",
                        "structuredName": {
                            "firstName": "Berrin",
                            "lastName": "Yanikoglu",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berrin A. Yanikoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820595"
                        ],
                        "name": "L. Vincent",
                        "slug": "L.-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 123
                            }
                        ],
                        "text": "2 The region-based approach Region-based page segmentation benchmarking environments are proposed by Yanikoglu and Vincent [92, 93], and Haralick et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 86
                            }
                        ],
                        "text": "To overcome this limitation a region-based approach has been more recently introduced [91, 92, 47, 93, 94, 95]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16994816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23b63ccfe44badde2b162d26cd836839f2133a6f",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new approach for evaluating page segmentation algorithms. Unlike techniques that rely on OCR output, our method is region-based: the segmentation output, described as a set of regions together with their types, output order etc., is matched against the pre-stored set of ground-truth regions. Misclassifications, splitting, and merging of regions are among the errors that are detected by the system. Each error is weighted individually for a particular application and a global estimate of segmentation quality is derived. The system can be customized to benchmark specific aspects of segmentation (e.g., headline detection) and according to the type of error correction that might follow (e.g., re-typing). Segmentation ground-truth files are quickly and easily generated and edited using GroundsKeeper, an X-Window based tool that allows one to view a document, manually draw regions (arbitrary polygons) on it, and specify information about each region (e.g., type, parent)."
            },
            "slug": "Ground-truthing-and-benchmarking-document-page-Yanikoglu-Vincent",
            "title": {
                "fragments": [],
                "text": "Ground-truthing and benchmarking document page segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This work describes a new approach for evaluating page segmentation algorithms that is region-based: the segmentation output, described as a set of regions together with their types, output order etc., is matched against the pre-stored set of ground-truth regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749498"
                        ],
                        "name": "Andrew D. Bagdanov",
                        "slug": "Andrew-D.-Bagdanov",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Bagdanov",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew D. Bagdanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499975"
                        ],
                        "name": "J. Kanai",
                        "slug": "J.-Kanai",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Kanai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kanai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18056036,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "8b7e1d30c87335e823f17e04f91b5c18b5bcb0b9",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. A new projection profile based skew estimation algorithm is presented. It extracts fiducial points corresponding to objects on a page by decoding a JBIG compressed image. These points are projected along parallel lines into an accumulator array. The angle of projection within a search interval that maximizes alignment of the fiducial points is the skew angle. This algorithm and three other algorithms were tested. Results showed that the new algorithm performed comparably to the other algorithms. The JBIG progressive coding scheme reduces the effects of noise and graphics, and the accuracy of the new algorithm on 75 dpi unfiltered images and 300 dpi filtered images was similar."
            },
            "slug": "Projection-profile-based-skew-estimation-algorithm-Bagdanov-Kanai",
            "title": {
                "fragments": [],
                "text": "Projection profile based skew estimation algorithm for JBIG compressed images"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A new projection profile based algorithm that extracts fiducial points needed to estimate a skew angle by decoding a JBIG compressed image is presented and performed competitively with the other three algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144706917"
                        ],
                        "name": "M. Ali",
                        "slug": "M.-Ali",
                        "structuredName": {
                            "firstName": "Majdi",
                            "lastName": "Ali",
                            "middleNames": [
                                "Ben",
                                "Hadj"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20501775"
                        ],
                        "name": "Frank Fein",
                        "slug": "Frank-Fein",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Fein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083596961"
                        ],
                        "name": "Frank H\u00f6nes",
                        "slug": "Frank-H\u00f6nes",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "H\u00f6nes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank H\u00f6nes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145601766"
                        ],
                        "name": "T. J\u00e4ger",
                        "slug": "T.-J\u00e4ger",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "J\u00e4ger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J\u00e4ger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2881875"
                        ],
                        "name": "A. Weigel",
                        "slug": "A.-Weigel",
                        "structuredName": {
                            "firstName": "Achim",
                            "lastName": "Weigel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54707927,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bebf2b474b6f8b48febe7545107f2239863afbb7",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Document analysis is responsible for an essential progress in office automation. This paper is part of an overview about the combined research efforts in document analysis at the DFKI. Common to all document analysis projects is the global goal of providing a high level electronic representation of documents in terms of iconic, structural, textual, and semantic information. These symbolic document descriptions enable an \"intelligent'; access to a document database. Currently there are three ongoing document analysis projects at DFKI: INCA, OMEGA, and PASCAL2000/PASCAL+. Though the projects pursue different goals in different application domains, they all share the same problems which have to be resolved with similar techniques. For that reason the activities in these projects are bundled to avoid redundant work. At DFKI we have divided the problem of document analysis into two main tasks, text recognition and text analysis, which themselves are divided into a set of subtasks. In a series of three research reports the work of the document analysis and office automation department at DFKI is presented. The first report discusses the problem of text recognition, the second that of text analysis. In a third report we describe our concept for a specialized document analysis knowledge representation language. The report in hand describes the activities dealing with the text recognition task. Text recognition covers the phase starting with capturing a document image up to identifying the written words. This comprises the following subtasks: preprocessing the pictorial information, segmenting into blocks, lines, words, and characters, classifying characters, and identifying the input words. For each subtask several competing solution algorithms, called specialists or knowledge sources, may exist. To efficiently control and organize these specialists an intelligent situation-based planning component is necessary, which is also described in this report. It should be mentioned that the planning component is also responsible to control the overall document analysis system instead of the text recognition phase only"
            },
            "slug": "Document-analysis-at-DFKI.-Part-1:-Image-analysis-Ali-Fein",
            "title": {
                "fragments": [],
                "text": "Document analysis at DFKI. - Part 1: Image analysis and text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "In a series of three research reports the work of the document analysis and office automation department at DFKI is presented, the concept for a specialized document analysis knowledge representation language is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398550688"
                        ],
                        "name": "L. O'Gorman",
                        "slug": "L.-O'Gorman",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "O'Gorman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. O'Gorman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several algorithms have been speciflcally designed for the binarization of document images [7, 8,  9 , 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44674932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2a5b702aec0aa01946f02d190bf9418000559bd",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Thresholding is a common image processing operation applied to gray-scale images to obtain binary or multilevel images. Traditionally, one of two approaches is used: global or locally adaptive processing. However, each of these approaches has a disadvantage: the global approach neglects local information, and the locally adaptive approach neglects global information. A thresholding method is described here that is global in approach, but uses a measure of local information, namely connectivity. Thresholds are found at the intensity levels that best preserve the connectivity of regions within the image. Thus, this method has advantages of both global and locally adaptive approaches. This method is applied here to document images. Experimental comparisons against other thresholding methods show that the connectivity-preserving method yields much improved results. On binary images, this method has been shown to improve subsequent OCR recognition rates from about 95% to 97,5%. More importantly, the new method has been shown to reduce the number of binarization failures (where text is so poorly binarized as to be totally unrecognizable by a commercial OCR system) from 33% to 6% on difficult images. For multilevel document images, as well, the results show similar improvement."
            },
            "slug": "Binarization-and-Multithresholding-of-Document-O'Gorman",
            "title": {
                "fragments": [],
                "text": "Binarization and Multithresholding of Document Images Using Connectivity"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This method has been shown to reduce the number of binarization failures from 33% to 6% on difficult images and to improve subsequent OCR recognition rates from about 95% to 97,5% on binary images."
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755278"
                        ],
                        "name": "Changming Sun",
                        "slug": "Changming-Sun",
                        "structuredName": {
                            "firstName": "Changming",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changming Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31506411"
                        ],
                        "name": "Deyi Si",
                        "slug": "Deyi-Si",
                        "structuredName": {
                            "firstName": "Deyi",
                            "lastName": "Si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deyi Si"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "A similar technique is used by Sun and Si [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 73
                            }
                        ],
                        "text": "Other techniques have been proposed which are based on gradient analysis [34, 35], on the analysis of the Fourier spectrum [16], on the use of morphological transforms [36], and on subspace line detection [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "20 1 complex documents also with few text lines, one text direction Sun Si [35] g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13602082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9065b4a6ae8cf8bd0c878ffcc7af6679c75bb0a",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A fast algorithm is presented for skew and slant correction in printed document images. The algorithm employs only the gradient information. The skew angle is obtained by searching for a peak in the histogram of the gradient orientation of the input grey-level image. The skewness of the document is corrected by a rotation at such an angle. The slant of characters can also be detected using the same technique, and can be corrected by a shear operation. A second method for character slant correction by fitting parallelograms to the connected components is also described. Document images with different contents (tables, figures, and photos) have been tested for skew correction and the algorithm gives accurate results on all the test images, and the algorithm is very easy to implement."
            },
            "slug": "Skew-and-slant-correction-for-document-images-using-Sun-Si",
            "title": {
                "fragments": [],
                "text": "Skew and slant correction for document images using gradient direction"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A fast algorithm is presented for skew and slant correction in printed document images by searching for a peak in the histogram of the gradient orientation of the input grey-level image."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153691390"
                        ],
                        "name": "R. Smith",
                        "slug": "R.-Smith",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19998207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25ad5e46506b054e588cf63c6b327cfd7cae7b65",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An important part of any document recognition system is detection of skew in the image of a page. This paper presents a new, accurate and robust skew detection algorithm based on a method for finding rows of text in page images. Results of a comparison of the new algorithm against Baird's well-known algorithm on 400 pages show the new algorithm to be more accurate, robust and somewhat faster. In particular, the new algorithm only breaks down at skew angles in excess of 15 degrees, compared to the almost uniform distribution of breakdowns of Baird's algorithm."
            },
            "slug": "A-simple-and-efficient-skew-detection-algorithm-via-Smith",
            "title": {
                "fragments": [],
                "text": "A simple and efficient skew detection algorithm via text row accumulation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A new, accurate and robust skew detection algorithm based on a method for finding rows of text in page images is presented, which is more accurate, robust and somewhat faster than Baird's algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Chen and Haralick [36] present a text skew estimation algorithm based on opening and closing morphological transforms [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "160 dpi 45 a dominant text direction Morphology Chen Haralick [36] b/w 300 dpi 5 0:5 complex documents with a dominant text direction, line gaps wider than character gaps SLIDE algorithm Aghajan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "Other techniques have been proposed which are based on gradient analysis [34, 35], on the analysis of the Fourier spectrum [16], on the use of morphological transforms [36], and on subspace line detection [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "Although some review papers have been recently published, (see the reviews by Haralick [2], Tang et al. [3] and the survey of methods by Jain and Yu in [4]), along with the tutorial text by O\u2019Gorman and Kasturi [5], we believe that an attempt to provide a reasoned systematization of the field can be of great interest."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1755672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad02a1ffba1e5e9fe9dc90aec81e1f04011bd92a",
            "isKey": true,
            "numCitedBy": 73,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The text skew estimation algorithm utilizes recursive morphological transforms. With hand tuned parameters the algorithm produces estimated text skew angles which are within 0.1/spl deg/ of the true text skew angles 99% of the time. We also developed methodology to allow the algorithm to determine the optimal algorithm parameter settings on the fly without any human interaction. Under this automatic mode, our experimental results indicate that the algorithm generates estimated text skew angles which are within 0.5/spl deg/ of the true text skew angles 99% of the time. To process a 3300/spl times/2550 document image, the algorithm takes about 10 seconds on SUN Sparc 10 machines if discounting the document image file reading time.<<ETX>>"
            },
            "slug": "An-automatic-algorithm-for-text-skew-estimation-in-Chen-Haralick",
            "title": {
                "fragments": [],
                "text": "An automatic algorithm for text skew estimation in document images using recursive morphological transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Methodology is developed to allow the algorithm to determine the optimal algorithm parameter settings on the fly without any human interaction and results indicate that the algorithm generates estimated text skew angles which are within 0.5/spl deg/ of the true text skew angle 99% of the time."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1st International Conference on Image Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Nagy et al. [56, 57] present a top down approach that\ncombines structural segmentation and functional labeling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "As an example, Nagy [87] reports a set of generic"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61651523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19b2308e9bbe77d2059706891a757bb90cb73049",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "It is argued that it is time for a major change of approach to optical character recognition (OCR) research. The traditional approach, focusing on the correct classification of isolated characters, has been exhausted. The demonstration of the superiority of a new classification method under operational conditions requires large experimental facilities and databases beyond the resources of most researchers. In any case, even perfect classification of individual characters is insufficient for the conversion of complex archival documents to a useful computer-readable form. Many practical OCR tasks require integrated treatment of entire documents and well-organized typographic and domain-specific knowledge. New OCR systems should take advantage of the typographic uniformity of paragraphs or other layout components. They should also exploit the unavoidable interaction with human operators to improve themselves without explicit 'training'. >"
            },
            "slug": "At-the-frontiers-of-OCR-Nagy",
            "title": {
                "fragments": [],
                "text": "At the frontiers of OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "It is argued that it is time for a major change of approach to optical character recognition (OCR) research, and new OCR systems should take advantage of the typographic uniformity of paragraphs or other layout components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148700"
                        ],
                        "name": "O. Akindele",
                        "slug": "O.-Akindele",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Akindele",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Akindele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128453"
                        ],
                        "name": "A. Bela\u00efd",
                        "slug": "A.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Abdel",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bela\u00efd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41120654,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "34f76569e4e33d2572ece2ae268960423b89f29f",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A page segmentation method that allows one to cut a document page image into polygonal blocks as well as into classical rectangular blocks is described. The intercolumn and interparagraph gaps are extracted as horizontal and vertical lines. The points of intersection between these lines are treated as vertices of polygonal blocks. With the aid of the 4-connected chain codes and an intersection table, simple isothetic polygonal blocks are constructed from these points of intersection.<<ETX>>"
            },
            "slug": "Page-segmentation-by-segment-tracing-Akindele-Bela\u00efd",
            "title": {
                "fragments": [],
                "text": "Page segmentation by segment tracing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A page segmentation method that allows one to cut a document page image into polygonal blocks as well as into classical rectangular blocks is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49420283"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 90
                            }
                        ],
                        "text": "Several algorithms have been speci cally designed for the binarization of document images [7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22489408,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ce3051ddbba33f77e109841de10a7e699f78607",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Binarization has been difficult for document images with poor contrast, strong noise, complex patterns, and/or variable modalities in gray-scale histograms. We developed a texture feature based thresholding algorithm to address this problem. Our algorithm consists of three steps: 1) candidate thresholds are produced through iterative use of Otsu's algorithm (1978); 2) texture features associated with each candidate threshold are extracted from the run-length histogram of the accordingly binarized image; 3) the optimal threshold is selected so that desirable document texture features are preserved. Experiments with 9,000 machine printed address blocks from an unconstrained US mail stream demonstrated that over 99.6 percent of the images were successfully binarized by the new thresholding method, appreciably better than those obtained by typical existing thresholding techniques. Also, a system run with 500 troublesome mail address blocks showed that an 8.1 percent higher character recognition rate was achieved with our algorithm as compared with Otsu's algorithm."
            },
            "slug": "Document-Image-Binarization-Based-on-Texture-Liu-Srihari",
            "title": {
                "fragments": [],
                "text": "Document Image Binarization Based on Texture Features"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A texture feature based thresholding algorithm that is appreciably better than those obtained by typical existing thresholding techniques for document images with poor contrast, strong noise, complex patterns, and/or variable modalities in gray-scale histograms is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863322"
                        ],
                        "name": "Anik\u00f3 Simon",
                        "slug": "Anik\u00f3-Simon",
                        "structuredName": {
                            "firstName": "Anik\u00f3",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anik\u00f3 Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1907819"
                        ],
                        "name": "Jean-Christophe Pret",
                        "slug": "Jean-Christophe-Pret",
                        "structuredName": {
                            "firstName": "Jean-Christophe",
                            "lastName": "Pret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Christophe Pret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406054409"
                        ],
                        "name": "A. Johnson",
                        "slug": "A.-Johnson",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Johnson",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Johnson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[68] b/w freely shaped words, text lines, text and graphic blocks cleaned docs; one text direction skew up to 5 is tolerated; chemical docs"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 19
                            }
                        ],
                        "text": "component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[68] present a page decomposition approach which is applied"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29276706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7094063edf765c44dcce4aada3ed0ca725b74d96",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new bottom-up method for document layout analysis. The algorithm was implemented in the CLIDE (Chemical Literature Data Extraction) system, but the method described here is suitable for a broader range of documents. It is based on Kruskal's algorithm and uses a special distance-metric between the components to construct the physical page structure. The method has all the major advantages of bottom-up systems: independence from different text spacing and independence from different block alignments. The algorithms computational complexity is reduced to linear by using heuristics and path-compression."
            },
            "slug": "A-Fast-Algorithm-for-Bottom-Up-Document-Layout-Simon-Pret",
            "title": {
                "fragments": [],
                "text": "A Fast Algorithm for Bottom-Up Document Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new bottom-up method for document layout analysis based on Kruskal's algorithm and uses a special distance-metric between the components to construct the physical page structure."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260288"
                        ],
                        "name": "L. Fletcher",
                        "slug": "L.-Fletcher",
                        "structuredName": {
                            "firstName": "Lloyd",
                            "lastName": "Fletcher",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fletcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2685456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b08e547ba4edb60902d1708a5593d71f075aa7f1",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The development and implementation of an algorithm for automated text string separation that is relatively independent of changes in text font style and size and of string orientation are described. It is intended for use in an automated system for document analysis. The principal parts of the algorithm are the generation of connected components and the application of the Hough transform in order to group components into logical character strings that can then be separated from the graphics. The algorithm outputs two images, one containing text strings and the other graphics. These images can then be processed by suitable character recognition and graphics recognition systems. The performance of the algorithm, both in terms of its effectiveness and computational efficiency, was evaluated using several test images and showed superior performance compared to other techniques. >"
            },
            "slug": "A-Robust-Algorithm-for-Text-String-Separation-from-Fletcher-Kasturi",
            "title": {
                "fragments": [],
                "text": "A Robust Algorithm for Text String Separation from Mixed Text/Graphics Images"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The development and implementation of an algorithm for automated text string separation that is relatively independent of changes in text font style and size and of string orientation are described and showed superior performance compared to other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258400"
                        ],
                        "name": "\u00d8. Trier",
                        "slug": "\u00d8.-Trier",
                        "structuredName": {
                            "firstName": "\u00d8ivind",
                            "lastName": "Trier",
                            "middleNames": [
                                "Due"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d8. Trier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48717516"
                        ],
                        "name": "T. Taxt",
                        "slug": "T.-Taxt",
                        "structuredName": {
                            "firstName": "Torfinn",
                            "lastName": "Taxt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Taxt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 96
                            }
                        ],
                        "text": "Other papers provide a comparison of several binarization techniques applied to document images [13, 14, 15]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17374833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66126ec1fe61b833ae695db9c5bac54641fab482",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an evaluation of eleven locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. Niblack's method (1986) with the addition of the postprocessing step of Yanowitz and Bruckstein's method (1989) added performed the best and was also one of the fastest binarization methods. >"
            },
            "slug": "Evaluation-of-Binarization-Methods-for-Document-Trier-Taxt",
            "title": {
                "fragments": [],
                "text": "Evaluation of Binarization Methods for Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper presents an evaluation of eleven locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise and Niblack's method with the addition of the postprocessing step of Yanowitz and Bruckstein's method (1989) performed the best and was also one of the fastest binarized methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166512"
                        ],
                        "name": "Fr\u00e9d\u00e9ric Bapst",
                        "slug": "Fr\u00e9d\u00e9ric-Bapst",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bapst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fr\u00e9d\u00e9ric Bapst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497000"
                        ],
                        "name": "R. Brugger",
                        "slug": "R.-Brugger",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Brugger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brugger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680326"
                        ],
                        "name": "R. Ingold",
                        "slug": "R.-Ingold",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Ingold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingold"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 247
                            }
                        ],
                        "text": "For example the CIDRE (Cooperative and Interactive Document Reverse Engineering) project at the Fribourg University (Switzerland) is aimed at developing an interactive document understanding system running in three different and complex scenarios [115]: (1) construction of a structured bibliography (2) building an on-line help facility and (3) mail distribution in office automation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59840974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7f9a1ad726b19846a3f06cf8d2a587675dc23ff",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new research theme at our institute in the eld of document engineering; it describes a new general approach for document structure recognition and transformation. Several kinds of applications are presented, showing the great interest of such tools. The project that is described deals with three major research topics: cooperative recognition strategies in a multi-agents environment, automatic inference of a well-deened knowledge base, and high interactivity. The paper shows the problems detected so far and gives some main ideas currently discussed in our research group."
            },
            "slug": "Towards-an-Interactive-Document-Structure-System-Bapst-Brugger",
            "title": {
                "fragments": [],
                "text": "Towards an Interactive Document Structure Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This paper presents a new research theme at the institute in the eld of document engineering; it describes a new general approach for document structure recognition and transformation, and shows the problems detected so far."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499975"
                        ],
                        "name": "J. Kanai",
                        "slug": "J.-Kanai",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Kanai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kanai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "The text-based approach was the first proposed in the literature [88, 89, 90]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30733052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bd99ebf0bbe9a8513350d9eae4f3570c99d559b",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Many current optical character recognition (OCR) systems attempt to decompose printed pages into a set of zones, each containing a single column of text, before converting the characters into coded form. The authors present a methodology for automatically assessing the accuracy of such decompositions, and demonstrate its use in evaluating six OCR systems. >"
            },
            "slug": "Automated-Evaluation-of-OCR-Zoning-Kanai-Rice",
            "title": {
                "fragments": [],
                "text": "Automated Evaluation of OCR Zoning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A methodology for automatically assessing the accuracy of optical character recognition decompositions is presented, and its use in evaluating six OCR systems is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2201799"
                        ],
                        "name": "A. Kam",
                        "slug": "A.-Kam",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Kam",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3333419"
                        ],
                        "name": "G. Kopec",
                        "slug": "G.-Kopec",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kopec",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kopec"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This solution allows a very high recognition rate but the required time is huge. In a second paper [ 113 ] Kam IRST 55"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15860728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87c9557ef6567e9801b1ef4c54f651a6f12b5640",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This correspondence describes an approach to reducing the computational cost of document image decoding by viewing it as a heuristic search problem. The kernel of the approach is a modified dynamic programming (DP) algorithm, called the iterated complete path (ICP) algorithm, that is intended for use with separable source models. A set of heuristic functions are presented for decoding formatted text with ICP. Speedups of 3-25 over DP have been observed when decoding text columns and telephone yellow pages using ICP and the proposed heuristics."
            },
            "slug": "Document-Image-Decoding-by-Heuristic-Search-Kam-Kopec",
            "title": {
                "fragments": [],
                "text": "Document Image Decoding by Heuristic Search"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An approach to reducing the computational cost of document image decoding by viewing it as a heuristic search problem by using a modified dynamic programming algorithm, called the iterated complete path (ICP) algorithm, intended for use with separable source models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700821"
                        ],
                        "name": "F. Esposito",
                        "slug": "F.-Esposito",
                        "structuredName": {
                            "firstName": "Floriana",
                            "lastName": "Esposito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Esposito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738657"
                        ],
                        "name": "D. Malerba",
                        "slug": "D.-Malerba",
                        "structuredName": {
                            "firstName": "Donato",
                            "lastName": "Malerba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malerba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467353"
                        ],
                        "name": "G. Semeraro",
                        "slug": "G.-Semeraro",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Semeraro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Semeraro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[114]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31908206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc3a2e6c8657abe7c6869564a5f510eac7cf2462",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a methodology for document classification and understanding is proposed. It is based on a multistrategy approach to learning from examples. By document classification, we mean the process of identification of the particular class to which a document belongs. Document understanding is defined as the process of detecting the logical structure of a document. The multistrategy approach for document classification and understanding has been implemented in a system called PLRS, which embeds two empirical learning systems: RES and INDUBIIH. Given a set of documents whose layout structure has already been detected and such that the membership class has been defined by the user, RES generates the knowledge base of an expert system devoted to the classification of a document. The language used to describe both the layout of the training documents and the learned rules is a first-order language. The learning methodology adopted for the problem of learning classification rules integrates both a paramet..."
            },
            "slug": "Multistrategy-Learning-for-Document-Recognition-Esposito-Malerba",
            "title": {
                "fragments": [],
                "text": "Multistrategy Learning for Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A methodology for document classification and understanding is proposed, based on a multistrategy approach to learning from examples, which embeds two empirical learning systems: RES and INDUBIIH."
            },
            "venue": {
                "fragments": [],
                "text": "Appl. Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1896610"
                        ],
                        "name": "S. Randriamasy",
                        "slug": "S.-Randriamasy",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Randriamasy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Randriamasy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820595"
                        ],
                        "name": "L. Vincent",
                        "slug": "L.-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726209"
                        ],
                        "name": "B. Wittner",
                        "slug": "B.-Wittner",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Wittner",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wittner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 86
                            }
                        ],
                        "text": "To overcome this limitation a region-based approach has been more recently introduced [91, 92, 47, 93, 94, 95]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7668022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42dbc78252120d60c5671463d7384c3a44c281d0",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic bitmap-level, set-based benchmarking scheme for page segmentation, comparing results with predefined `ground truth files' containing all the possible correct solutions, is presented. A successful page segmentation is a necessary precondition for a document recognition process to be successful. The problems addressed here are: design methods to describe all possible correct segmentations for a given page and design methods to compare two segmentations. The proposed segmentation ground truth representation scheme defines ground truth text regions as non-mergeable maximal sets of text lines, merged in a language- dependent direction. It includes the other possible correct segmentations in that authorized cuts in the region are explicitly specified. At this low-level stage, quality criteria for a page segmentation are mainly defined as providing correct input for region ordering and classification. The qualitative and quantitative evaluation method tests the overlap between the two sets of regions. In fact, the regions are defined as being the black pixels contained in the derived polygons."
            },
            "slug": "Automatic-benchmarking-scheme-for-page-segmentation-Randriamasy-Vincent",
            "title": {
                "fragments": [],
                "text": "Automatic benchmarking scheme for page segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An automatic bitmap-level, set-based benchmarking scheme for page segmentation, comparing results with predefined `ground truth files' containing all the possible correct solutions, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751773"
                        ],
                        "name": "M. Krishnamoorthy",
                        "slug": "M.-Krishnamoorthy",
                        "structuredName": {
                            "firstName": "Mukkai",
                            "lastName": "Krishnamoorthy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Krishnamoorthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266621"
                        ],
                        "name": "M. Viswanathan",
                        "slug": "M.-Viswanathan",
                        "structuredName": {
                            "firstName": "Mahesh",
                            "lastName": "Viswanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Viswanathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[57, 56] have developed a syntactic based system working with technical journals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[57] have proposed an approach in which segmentation and classi cation are claimed to be performed simultaneously."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 143
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection pro le analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16107554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "717a4ae91ad20667f7ac03ce5538eff36313c299",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for extracting alternating horizontal and vertical projection profiles are from nested sub-blocks of scanned page images of technical documents is discussed. The thresholded profile strings are parsed using the compiler utilities Lex and Yacc. The significant document components are demarcated and identified by the recursive application of block grammars. Backtracking for error recovery and branch and bound for maximum-area labeling are implemented with Unix Shell programs. Results of the segmentation and labeling process are stored in a labeled x-y tree. It is shown that families of technical documents that share the same layout conventions can be readily analyzed. Results from experiments in which more than 20 types of document entities were identified in sample pages from two journals are presented. >"
            },
            "slug": "Syntactic-Segmentation-and-Labeling-of-Digitized-Krishnamoorthy-Nagy",
            "title": {
                "fragments": [],
                "text": "Syntactic Segmentation and Labeling of Digitized Pages from Technical Journals"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that families of technical documents that share the same layout conventions can be readily analyzed and backtracking for error recovery and branch and bound for maximum-area labeling are implemented with Unix Shell programs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116628331"
                        ],
                        "name": "Ching-Huei Wang",
                        "slug": "Ching-Huei-Wang",
                        "structuredName": {
                            "firstName": "Ching-Huei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ching-Huei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22756168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "699507eae525ee8b7ab7d4a366534180e7f29751",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A computational framework for recognizing an object of interest in a complex visual environment is described. Arising from the problem of finding the destination address block on a mail piece, a general framework for coordinating a collection of specialized image-analysis tools is described. The resulting system is capable of dealing with a wide range of environments\u2014from those having a high degree of global spatial structure (e.g., letter mail envelopes that conform to specifications) to those with no structure (e.g., magazines with randomly pasted address labels). The problem-solving architecture accounts for uncertainty in the imaging environment by using the blackboard model. This paper discusses systematic derivation of a set of object recognition heuristics (knowledge base), specialized image analysis tools for extracting those features that are called for by the heuristics, and a control structure for integrating evidence and managing tools. Experimental results with a database of difficult cases demonstrating the promise of the methodology are presented."
            },
            "slug": "A-framework-for-object-recognition-in-a-visually-to-Wang-Srihari",
            "title": {
                "fragments": [],
                "text": "A framework for object recognition in a visually complex environment and its application to locating address blocks on mail pieces"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper discusses systematic derivation of a set of object recognition heuristics (knowledge base), specialized image analysis tools for extracting those features that are called for by theHeuristics, and a control structure for integrating evidence and managing tools."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107838290"
                        ],
                        "name": "J. White",
                        "slug": "J.-White",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "White",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. White"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143647718"
                        ],
                        "name": "G. Rohrer",
                        "slug": "G.-Rohrer",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Rohrer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rohrer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several algorithms have been speciflcally designed for the binarization of document images [ 7 , 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7337721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "756d29ac3cc9b56054f08501cc4814183dc948d7",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Two new, cost-effective thresholding algorithms for use in extracting binary images of characters from machine- or hand-printed documents are described. The creation of a binary representation from an analog image requires such algorithms to determine whether a point is converted into a binary one because it falls within a character stroke or a binary zero because it does not. This thresholding is a critical step in Optical Character Recognition (OCR). It is also essential for other Character Image Extraction (CIE) applications, such as the processing of machine-printed or handwritten characters from carbon copy forms or bank checks, where smudges and scenic backgrounds, for example, may have to be suppressed. The first algorithm, a nonlinear, adaptive procedure, is implemented with a minimum of hardware and is intended for many CIE applications. The second is a more aggressive approach directed toward specialized, high-volume applications which justify extra complexity."
            },
            "slug": "Image-Thresholding-for-Optical-Character-and-Other-White-Rohrer",
            "title": {
                "fragments": [],
                "text": "Image Thresholding for Optical Character Recognition and Other Applications Requiring Character Image Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Two new, cost-effective thresholding algorithms for use in extracting binary images of characters from machine- or hand-printed documents are described, with a more aggressive approach directed toward specialized, high-volume applications which justify extra complexity."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32244730"
                        ],
                        "name": "R. Sivaramakrishnan",
                        "slug": "R.-Sivaramakrishnan",
                        "structuredName": {
                            "firstName": "Ramaswamy",
                            "lastName": "Sivaramakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sivaramakrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122930"
                        ],
                        "name": "J. Ha",
                        "slug": "J.-Ha",
                        "structuredName": {
                            "firstName": "Jaekyu",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3954428"
                        ],
                        "name": "S. Subramanium",
                        "slug": "S.-Subramanium",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Subramanium",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Subramanium"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 964787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bf02c6fbed2b2ef308f0b0d2c3eb733467a7d8e",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A document can be divided into zones on the basis of its content. For example, a zone can be either text or non-text. This paper describes an algorithm to classify each given document zone into one of nine different classes. Features for each zone such as run length mean and variance, spatial mean and variance, fraction of the total number of black pixels in the zone, and the zone width ratio for each zone are extracted. Run length related features are computed along four different canonical directions. A decision tree classifier is used to assign a zone class on the basis of its feature vector. The performance on an independent test set was 97%."
            },
            "slug": "Zone-classification-in-a-document-using-the-method-Sivaramakrishnan-Phillips",
            "title": {
                "fragments": [],
                "text": "Zone classification in a document using the method of feature vector generation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An algorithm to classify each given document zone into one of nine different classes by using a decision tree classifier to assign a zone class on the basis of its feature vector."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971263"
                        ],
                        "name": "K. Etemad",
                        "slug": "K.-Etemad",
                        "structuredName": {
                            "firstName": "Kamran",
                            "lastName": "Etemad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Etemad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 82
                            }
                        ],
                        "text": "component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27678281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95dffcc92bda88d9f4f5b112d100f43951745b8c",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information. Multiscale feature vectors are classified locally using a neural network to allow soft/fuzzy multi-class membership assignments. Segmentation is performed by integrating soft local decision vectors to reduce their \"ambiguities\"."
            },
            "slug": "Multiscale-Segmentation-of-Unstructured-Document-Etemad-Doermann",
            "title": {
                "fragments": [],
                "text": "Multiscale Segmentation of Unstructured Document Pages Using Soft Decision Integration"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15144597"
                        ],
                        "name": "Franck Lebourgeois",
                        "slug": "Franck-Lebourgeois",
                        "structuredName": {
                            "firstName": "Franck",
                            "lastName": "Lebourgeois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Franck Lebourgeois"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372426"
                        ],
                        "name": "Z. Bublinski",
                        "slug": "Z.-Bublinski",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Bublinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Bublinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739381"
                        ],
                        "name": "H. Emptoz",
                        "slug": "H.-Emptoz",
                        "structuredName": {
                            "firstName": "Hubert",
                            "lastName": "Emptoz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Emptoz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62602509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2c3f55e7a84ad5fa5eaa577d8d58ee73e14e2b",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Outlines a fast and efficient method for extracting graphics and text paragraphs from printed documents. The method presented is based on bottom-up approach to document analysis and it achieves very good performance in most cases. During the preprocessing characters are linked together to form blocks. Created blocks are segmented, labelled and merged into paragraphs. Simultaneously, graphics are extracted from the image. Algorithms for each step of processing are presented. Also, the obtained experimental results are included.<<ETX>>"
            },
            "slug": "A-fast-and-efficient-method-for-extracting-text-and-Lebourgeois-Bublinski",
            "title": {
                "fragments": [],
                "text": "A fast and efficient method for extracting text paragraphs and graphics from unconstrained documents"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "Outlines a fast and efficient method for extracting graphics and text paragraphs from printed documents based on bottom-up approach to document analysis and achieves very good performance in most cases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704694"
                        ],
                        "name": "J. Sauvola",
                        "slug": "J.-Sauvola",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "Sauvola",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sauvola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6709739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe98e6ee18dfa1d37c74736e9e88995a267b9bdf",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Page segmentation and classification are important parts of the document analysis process. The aim is to extract and classify different parts of the page. This paper proposes an approach in which these two phases are combined. The integration process includes fast feature extraction with rule-based classification and label propagation using connectivity analysis providing classified areas in three categories: background, text and picture."
            },
            "slug": "Page-segmentation-and-classification-using-fast-and-Sauvola-Pietik\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Page segmentation and classification using fast feature extraction and connectivity analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes an approach in which fast feature extraction with rule-based classification and label propagation using connectivity analysis providing classified areas in three categories: background, text and picture."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50738156"
                        ],
                        "name": "Teruo Akiyama",
                        "slug": "Teruo-Akiyama",
                        "structuredName": {
                            "firstName": "Teruo",
                            "lastName": "Akiyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Teruo Akiyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781078"
                        ],
                        "name": "N. Hagita",
                        "slug": "N.-Hagita",
                        "structuredName": {
                            "firstName": "Norihiro",
                            "lastName": "Hagita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hagita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Akiyama and Hagita [31] present an approach for the page decomposition of deskewed documents (see Section 3.4) containing text and graphic zones."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Akiyama and Hagita [31] describe a fast approach for skew detection: the document is divided into several vertical strips with the same width."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 611741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8061d131bbaed4369e5879e4fedbe0ea03c17ed",
            "isKey": true,
            "numCitedBy": 257,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automated-entry-system-for-printed-documents-Akiyama-Hagita",
            "title": {
                "fragments": [],
                "text": "Automated entry system for printed documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144081526"
                        ],
                        "name": "A. A. Zlatopolsky",
                        "slug": "A.-A.-Zlatopolsky",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Zlatopolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. A. Zlatopolsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 19
                            }
                        ],
                        "text": "component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 22
                            }
                        ],
                        "text": "The technique used by Zlatopolsky [66] is based on a progressive growing process that starts from the connected components."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 259
                            }
                        ],
                        "text": "Akiyama Hagita [31] b/w 200dpi rectangular blocks headline text lines graphics separators text majority; one text direction; hierarchic spacing protocol skew up to 10 is dealt; English and Japanese documents are dealt Connected Component Analysis Zlatopolsky [66] b/w 300dpi rectangular blocks text lines, text blocks, separators, graphics blocks interblock gaps wider that interline gaps; no textured pictures multiple skews are dealt"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "The technique used by Zlatopolsky [66] is based on a progressive growing"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36823878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47d84273587aa07a40c2b5171aa5b2a617e69663",
            "isKey": true,
            "numCitedBy": 18,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automated-document-segmentation-Zlatopolsky",
            "title": {
                "fragments": [],
                "text": "Automated document segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1939973"
                        ],
                        "name": "D. Le",
                        "slug": "D.-Le",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Le",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145116486"
                        ],
                        "name": "G. Thoma",
                        "slug": "G.-Thoma",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Thoma",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Thoma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Block classification techniques described in this section can be grouped into features extraction and linear discriminant classifiers [53, 77, 55, 59], binary classification tree [78], and neural networks [79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "RLSA received great interest, principally because of its easy implementation, and has been employed, with some modifications, in several systems to perform the segmentation phase [18, 82, 83, 79, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[79] present a new method for the classification of blocks extracted from binary document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The major part are based on feature extraction and linear discriminant classifiers [53, 55, 59, 77], but other techniques are presented [78, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62726885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dedcd047c188f26dd5ff6c5de24e9f5d5a191abc",
            "isKey": true,
            "numCitedBy": 27,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new method for the classification of binary document images as textual or nontextual data blocks using neural network models. Binary document images are first segmented into blocks by the constrained run-length algorithm (CRLA). The component-labeling procedure is used to label the resulting blocks. The features for each block, calculated from the coordinates of its extremities, are then fed into the input layer of a neural network for classification. Four neural networks were considered, and they include back propagation (BP), radial basis functions (RBF), probabilistic neural network (PNN), and Kohonen's self-organizing feature maps (SOFMs). The performance and behavior of these neural network models are analyzed and compared in terms of training times, memory requirements, and classification accuracy. The experiments carried out on a variety of medical journals show the feasibility of using the neural network approach for textual block classification and indicate that in terms of both accuracy and training time RBF should be preferred."
            },
            "slug": "Classification-of-binary-document-images-into-or-Le-Thoma",
            "title": {
                "fragments": [],
                "text": "Classification of binary document images into textual or nontextual data blocks using network models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experiments carried out show the feasibility of using the neural network approach for textual block classification and indicate that in terms of both accuracy and training time RBF should be preferred."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830195"
                        ],
                        "name": "R. Bleisinger",
                        "slug": "R.-Bleisinger",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Bleisinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bleisinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36422303"
                        ],
                        "name": "R. Hoch",
                        "slug": "R.-Hoch",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Hoch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hoch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20501775"
                        ],
                        "name": "Frank Fein",
                        "slug": "Frank-Fein",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Fein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083596961"
                        ],
                        "name": "Frank H\u00f6nes",
                        "slug": "Frank-H\u00f6nes",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "H\u00f6nes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank H\u00f6nes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1584642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a263aa246f730ea1c28817eafd0d8e5f899e61c",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of the model-based document analysis system called Pi ODA (paper interface to office document architecture), which was developed as a prototype for the analysis of single-sided business letters in German, are presented. Initially, Pi ODA extracts a part-of hierarchy of nested layout objects such as text-blocks, lines, and words based on their presentation on the page. Subsequently, in a step called logical labeling, the layout objects and their compositions are geometrically analyzed to identify corresponding logical objects that can be related to a human perceptible meaning, such as sender, recipient, and date in a letter. A context-sensitive text recognition for logical objects is then applied using logical vocabularies and syntactic knowledge. As a result, Pi ODA produces a document representation that conforms to the ODA international standard.<<ETX>>"
            },
            "slug": "From-paper-to-office-document-standard-Dengel-Bleisinger",
            "title": {
                "fragments": [],
                "text": "From paper to office document standard representation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The principles of the model-based document analysis system called Pi ODA (paper interface to office document architecture), which was developed as a prototype for the analysis of single-sided business letters in German, are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1939973"
                        ],
                        "name": "D. Le",
                        "slug": "D.-Le",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Le",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145116486"
                        ],
                        "name": "G. Thoma",
                        "slug": "G.-Thoma",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Thoma",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Thoma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 136
                            }
                        ],
                        "text": "The major part are based on feature extraction and linear discriminant classifiers [53, 55, 59, 77], but other techniques are presented [78, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "Block classification techniques described in this section can be grouped into features extraction and linear discriminant classifiers [53, 77, 55, 59], binary classification tree [78], and neural networks [79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[79] present a new method for the classification of blocks extracted from binary document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "RLSA received great interest, principally because of its easy implementation, and has been employed, with some modifications, in several systems to perform the segmentation phase [18, 82, 83, 79, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[79] b/w 200dpi text, non text blocks no skew; only text/nontext block classification trainable"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11512168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bee6288205fdc56f0da663b29c0394ffad7289b",
            "isKey": true,
            "numCitedBy": 19,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new method for the classification of binary document images as textual or nontextual data blocks using neural network models. Binary document images are first segmented into blocks by the constrained run-length algorithm (CRLA). The component-labeling procedure is used to label the resulting blocks. The features for each block, calculated from the coordinates of its extremities, are then fed into the input layer of a neural network for classification. Four neural networks were considered, and they include back propagation (BP), radial basis functions (RBF), probabilistic neural network (PNN), and Kohonen's self-organizing feature maps (SOFMs). The performance and behavior of these neural network models are analyzed and compared in terms of training times, memory requirements, and classification accuracy. The experiments carried out on a variety of medical journals show the feasibility of using the neural network approach for textual block classification and indicate that in terms of both accuracy and training time RBF should be preferred."
            },
            "slug": "Classification-of-binary-document-images-into-or-Le-Thoma",
            "title": {
                "fragments": [],
                "text": "Classification of binary document images into textual or nontextual data blocks using neural network models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experiments carried out show the feasibility of using the neural network approach for textual block classification and indicate that in terms of both accuracy and training time RBF should be preferred."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "The author claims [38] the approach to be fast and accurate and to work without modifications on a variety of layouts, including multiple blocks, sparse table and mixed size and typefaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61890978,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9b0ac5a41c2742de9695810dadab2c156028f730",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "An experimental printed-page reader that is easy to adapt to various languages is described. Changing the target language may involve simultaneous changes in symbol sets, typefaces, sizes of text, page layouts, linguistic contexts, and imaging defects. The strategy has been to isolate the effects of these sources of variation within separate, independent engineering subsystems. In this way, it has been possible to construct, with a minimum of manual effort, classifiers for arbitrary combinations of symbols, typefaces, sizes, and imaging defects. An attempt has been made to rid the algorithms of all language-specific rules, relying instead on automatic learning from examples and generalized table-driven methods. For some tasks it has been feasible to avoid language dependency altogether. Linguistic context can be exploited through data-directed filtering algorithms in a uniform and modular manner, so that preexisting tools developed by computational linguistics can readily be applied. These principles are illustrated by trials on English, Swedish, Tibetan, and special technical texts. >"
            },
            "slug": "Anatomy-of-a-versatile-page-reader-Baird",
            "title": {
                "fragments": [],
                "text": "Anatomy of a versatile page reader"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An experimental printed-page reader that is easy to adapt to various languages is described, and an attempt has been made to rid the algorithms of all language-specific rules, relying instead on automatic learning from examples and generalized table-driven methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15307178"
                        ],
                        "name": "F. Jenkins",
                        "slug": "F.-Jenkins",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Jenkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jenkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although actual Optical Character Recognition (OCR) systems proved to be powerful enough to meet the requirements of many users [1], room for improvement still exists and further research efforts are required."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "a score in [0, 1] for each class."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "in order to improve their recognition accuracy [1], most OCR commercial systems work, at present, on bilevel inputs [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17017303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fae039cc89b2cd453acb85d208e021907528b062",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "For four years, ISRI has conducted an annual test of optical character recognition (OCR) systems known as \u201cpage readers.\u201d These systems accept as input a bitmapped image of any document page, and attempt to identify the machine-printed characters on the page. In the annual test, we measure the accuracy of this process by comparing the text that is produced as output with the correct text. The goals of the test include:"
            },
            "slug": "The-Fourth-Annual-Test-of-OCR-Accuracy-Rice-Jenkins",
            "title": {
                "fragments": [],
                "text": "The Fourth Annual Test of OCR Accuracy"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The annual test of optical character recognition systems known as \u201cpage readers\u201d accepts as input a bitmapped image of any document page, and attempts to identify the machine-printed characters on the page."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150628"
                        ],
                        "name": "A. T. Abak",
                        "slug": "A.-T.-Abak",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Abak",
                            "middleNames": [
                                "Toygar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. T. Abak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288054"
                        ],
                        "name": "U. Baris",
                        "slug": "U.-Baris",
                        "structuredName": {
                            "firstName": "U.",
                            "lastName": "Baris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Baris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145940271"
                        ],
                        "name": "B. Sankur",
                        "slug": "B.-Sankur",
                        "structuredName": {
                            "firstName": "B\u00fclent",
                            "lastName": "Sankur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sankur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Nowadays the zoning process is a mix of the so-called geometric and logical layout analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40492652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8316d7d1e2f5bf0b4cdafc71290658aa67fd5f94",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents performance evaluation of thresholding algorithms in the context of document analysis and character recognition systems. Several thresholding algorithms are comparatively evaluated on the basis of the original bitmaps of characters. Different distance measures such as Hausdorff, Jaccard, and Yule are used to measure the similarity between thresholded bitmaps and original bitmaps of characters."
            },
            "slug": "The-performance-evaluation-of-thresholding-for-Abak-Baris",
            "title": {
                "fragments": [],
                "text": "The performance evaluation of thresholding algorithms for optical character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The paper presents performance evaluation of thresholding algorithms in the context of document analysis and character recognition systems using Hausdorff, Jaccard, and Yule measures to measure the similarity between thresholded bitmaps and original bitmaps of characters."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35162579"
                        ],
                        "name": "E. R. Davies",
                        "slug": "E.-R.-Davies",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Davies",
                            "middleNames": [
                                "Roy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. R. Davies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "Techniques using the well-known Hough transform [39, 40] have been explored by several authors and are based on the observations that a distinguishing feature for text is the alignment of characters and that text lines of a document are usually parallel each other."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5061394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4e30bcea2b040c1ab035257c97adba1c9562fd5",
            "isKey": false,
            "numCitedBy": 1089,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-vision-theory,-algorithms,-practicalities-Davies",
            "title": {
                "fragments": [],
                "text": "Machine vision - theory, algorithms, practicalities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3466226"
                        ],
                        "name": "P. Yeh",
                        "slug": "P.-Yeh",
                        "structuredName": {
                            "firstName": "Pen-Shu",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720626"
                        ],
                        "name": "S. Antoy",
                        "slug": "S.-Antoy",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Antoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Antoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3260285"
                        ],
                        "name": "Anne Litcher",
                        "slug": "Anne-Litcher",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Litcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anne Litcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[111] for the same task (address location on envelopes)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37131698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca58d313ed704627b86f01379dc31b6ac15ed0d3",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Address-location-on-envelopes-Yeh-Antoy",
            "title": {
                "fragments": [],
                "text": "Address location on envelopes"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3333419"
                        ],
                        "name": "G. Kopec",
                        "slug": "G.-Kopec",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kopec",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kopec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720816"
                        ],
                        "name": "P. Chou",
                        "slug": "P.-Chou",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Chou",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Chou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 122
                            }
                        ],
                        "text": "A completely different technique based on an extension of HMMs (Hidden Markov Models) has been proposed by Kopec and Chou [112] for telephone yellow page text extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17899690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944db1fc7f65d13a77cf9c70679ee2ff4ef5fba8",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a communication theory approach to document image reconstruction, patterned after the use of hidden Markov models in speech recognition. A document recognition problem is viewed as consisting of three elements-an image generator, a noisy channel, and an image decoder. A document image generator is a Markov source which combines a message source with an imager. The message source produces a string of symbols which contains the information to be transmitted. The imager is modeled as a finite-state transducer, which converts the message into an ideal bitmap. The channel transforms the ideal image into a noisy observed image. The decoder estimates the message from the observed image by finding the a posteriori most probable path through the combined source and channel models using a Viterbi-like algorithm. Application of the proposed method to decoding telephone yellow pages is described.<<ETX>>"
            },
            "slug": "Document-image-decoding-using-Markov-source-models-Kopec-Chou",
            "title": {
                "fragments": [],
                "text": "Document Image Decoding Using Markov Source Models"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A communication theory approach to document image reconstruction, patterned after the use of hidden Markov models in speech recognition, is described, and application of the proposed method to decoding telephone yellow pages is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39702442"
                        ],
                        "name": "Y. Tang",
                        "slug": "Y.-Tang",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Tang",
                            "middleNames": [
                                "Yan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143607634"
                        ],
                        "name": "Hong Ma",
                        "slug": "Hong-Ma",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46701493"
                        ],
                        "name": "Xiaogang Mao",
                        "slug": "Xiaogang-Mao",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118822004"
                        ],
                        "name": "Dan Liu",
                        "slug": "Dan-Liu",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34262339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c31fa4fbc530c7ec28c5561cc09104cbba5da3d0",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach to document analysis. The proposed approach is based on modified fractal signature. Instead of the time-consuming traditional approaches (top-down and bottom-up approaches) where iterative operations are necessary to break a document into blocks to extract its geometric (layout) structure, this new approach can divide a document into blocks in only one step. This approach can be used to process documents with high geometrical complexity. Experiments have been conducted to prove the proposed new approach for document processing."
            },
            "slug": "A-new-approach-to-document-analysis-based-on-Tang-Ma",
            "title": {
                "fragments": [],
                "text": "A new approach to document analysis based on modified fractal signature"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper presents a new approach to document analysis based on modified fractal signature that can divide a document into blocks in only one step and be used to process documents with high geometrical complexity."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143935167"
                        ],
                        "name": "S. Latifi",
                        "slug": "S.-Latifi",
                        "structuredName": {
                            "firstName": "Shahram",
                            "lastName": "Latifi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Latifi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26111857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e1361ece16d38f2915198934c2a0e913d444a41",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In processing a page image by a given zoning algorithm (automatic or manual), a certain text string is generated which may not be the same as the correct string. The difference may be due to the incorrect reading order selected by the employed zoning algorithm or poor recognition of characters. A difference algorithm is commonly used to find the best match between the generated string and the correct string. The output of such an algorithm will then be a sequence of matched substrings which are not in the correct order. To determine the performance of a given zoning algorithm, it is of interest to find the minimum number of moves needed to obtain the correct string from the string generated by that algorithm. The problem can be modeled as a sorting problem where a string of n integers ordered in a random manner, must be sorted in ascending (or descending) order. In this paper, we derive bounds on the time complexity of sorting a given string and present a near-optimal algorithm for that."
            },
            "slug": "How-Can-Permutations-Be-Used-in-The-Evaluation-of-Latifi",
            "title": {
                "fragments": [],
                "text": "How Can Permutations Be Used in The Evaluation of Zoning Algorithms?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Borders on the time complexity of sorting a given string are derived and a near-optimal algorithm for that is presented to determine the performance of a given zoning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499975"
                        ],
                        "name": "J. Kanai",
                        "slug": "J.-Kanai",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Kanai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kanai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "The text-based approach was the rst proposed in the literature [88, 89, 90]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28850566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef93cb20d9efbe6974eea654d780626b09777b5a",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Requirements for the objective evaluation of automated data-entry systems are presented. Because the cost of correcting errors dominates the document conversion process, the most important characteristic of an OCR device is accuracy. However, different measures of accuracy (error metrics) are appropriate for different applications, and at the character, word, text-line, text-block, and document levels. For wholly objective assessment, OCR devices must be tested under programmed, rather than interactive, control.<<ETX>>"
            },
            "slug": "Performance-metrics-for-document-understanding-Kanai-Nartker",
            "title": {
                "fragments": [],
                "text": "Performance metrics for document understanding systems"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Requirements for the objective evaluation of automated data-entry systems are presented and different measures of accuracy (error metrics) are appropriate for different applications, and at the character, word, text-line,Text-block, and document levels."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071912005"
                        ],
                        "name": "W. Horak",
                        "slug": "W.-Horak",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Horak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Horak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8305166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfbe48c2406a0277df1ea65dba1343cbef8bd0c2",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "D1 ta,uUAi ctura HI, dec eni Multimedia (Communications riT cture and rchange tatus of lardization imens AG om language, the docu-plays a central role in the f information. Increasing-e tools at personal worksta-cilitate the handling of elec-documents in the office. Office usually consists of a sequence of esses involving a number of tools tributed over several workstations. he interchange of documents between cooperating tools in office systems necessitates a fundamental, common understanding of the structure of documents. For this reason, international standards committees are currently making great efforts to draw up standards that will enable the interchange of documents among open systems. Specifically, the CCITT, or the International Telegraph and Telephone Consultative Committee, the ISO, or the International Organization for Standardization, and the ECMA, or the European Computer Manufacturers Association, have been, or still are, working on the standards shown in the sidebar. The results obtained by ISO and ECMA with regard to the topics of office document architecture, or ODA, and office document interchange formats , or ODIF, are essentially the same in both organizations. In this article, the architectural model , the underlying processing model, and the principles of the interchange formats of the ECMA 101 and ISO drafts are introduced, and possibilities of further development indicated. In its details, the discussion that follows is based on ECMA 101. Document architecture model Document, text, and content. Within the scope of the ODA/ODIF standards , a document is a structured amount of text that can be interchanged as a unit between an originator and a recipient. A document can be interchanged either in image form, to permit its being printed and displayed as intended by the originator, or in processibleform, to permit document editing and layout revision by the recipient. Text is a representation of information for human perception that can be reproduced in two-dimensional form. Text consists of graphic elements such as graphic characters, geometric ele"
            },
            "slug": "Office-Document-Architecture-and-Office-Document-of-Horak",
            "title": {
                "fragments": [],
                "text": "Office Document Architecture and Office Document Interchange Formats: Current Status of International Standardization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The architectural model, the underlying processing model, and the principles of the interchange formats of the ECMA 101 and ISO drafts are introduced, and possibilities of further development indicated."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The recursive closing transform is computed with a structuring element 2\u00d7 2 or 2\u00d7 3, depending on the expected range of the skew angle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22044280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcd501b0776ce21e6284d4a2749d54b5ef6fb856",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A new group of recursive morphological transforms on the discrete space Z(2) are discussed. The set of transforms include the recursive erosion transform (RET), the recursive dilation transform (RDT), the recursive opening transform (ROT), and the recursive closing transform (RCT), The transforms are able to compute in constant time per pixel erosions, dilations, openings, and closings with all sized structuring elements simultaneously. They offer a solution to some vision tasks that need to perform a morphological operation but where the size of the structuring element has to be determined after a morphological examination of the content of the image. The computational complexities of the transforms show that the recursive erosion and dilation transform can be done in N+2 operations per pixel, where N is the number of pixels in the base structuring element. The recursive opening and closing transform can be done in 14N operations per pixel based on experimental results."
            },
            "slug": "Recursive-erosion,-dilation,-opening,-and-closing-Chen-Haralick",
            "title": {
                "fragments": [],
                "text": "Recursive erosion, dilation, opening, and closing transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new group of recursive morphological transforms on the discrete space Z(2) are discussed, which offer a solution to some vision tasks that need to perform a morphological operation but where the size of the structuring element has to be determined after a Morphological examination of the content of the image."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33380113"
                        ],
                        "name": "A. Hashizume",
                        "slug": "A.-Hashizume",
                        "structuredName": {
                            "firstName": "Akihide",
                            "lastName": "Hashizume",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hashizume"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3466226"
                        ],
                        "name": "P. Yeh",
                        "slug": "P.-Yeh",
                        "structuredName": {
                            "firstName": "Pen-Shu",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 245
                            }
                        ],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Hashizume et al. [28] present a bottom up technique based on nearest neighbor clustering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6261643,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9f3f5ed92b3221c777ac250b5fa36a0b4ce4adab",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-method-of-detecting-the-orientation-of-aligned-Hashizume-Yeh",
            "title": {
                "fragments": [],
                "text": "A method of detecting the orientation of aligned components"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80601195"
                        ],
                        "name": "Joan M. Smith",
                        "slug": "Joan-M.-Smith",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Smith",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan M. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52131653"
                        ],
                        "name": "Robert Stutely",
                        "slug": "Robert-Stutely",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Stutely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Stutely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "In order to overcome the limitations of the private formats, the international community has defined official standards such as SGML (Standard Generalized Markup Language, ISO 8879:1986, [99]) and ODA (Open Document Architecture, ISO 8613:1989, [100, 101])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59662231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "167dad02c84e200507da0db2ad51c36cda645ece",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Anyone who has attempted to read the SGML Standard (ISO 8879: Standard Generalized Markup Language) will welcome the idea of a guide or companion to assist in comprehending its often tortuous prose. Be warned, the title of this book is misleading; it is not a guide to the standard for the untutored reader. Rather, it is addressed to the knowledgeable reader, providing useful aids to assist such a reader in navigating the text of the standard. For example, the actual standard does not have an index; this guide provides several. The navigational assistance is provided under five headings. First come tables of all the syntax productions in SGML ordered by numerical sequence and in alphabetic sequence, together with an index of syntax productions. Next is a list of all the abbreviations provided in the standard. This is followed by a listing of all the character entities defined in the standard grouped under several headings \u2014 alphabetic characters, general use characters, technical use characters and mathematical symbols \u2014 together with indices of character entities ordered by entity reference name and by description. The book is rounded off with a general index to ISO 8879 and a list of the SGML keywords and reserved names. If you need to find your way around the text of the SGML standard, the information in this book will be invaluable. If, however, you want a book that helps you to understand the standard you should go for Charles Goldfarb\u2019s SGML Companion (Oxford University Press, 1990)."
            },
            "slug": "SGML:-The-User's-Guide-to-ISO-8879-Smith-Stutely",
            "title": {
                "fragments": [],
                "text": "SGML: The User's Guide to ISO 8879"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This book is addressed to the knowledgeable reader, providing useful aids to assist such a reader in navigating the text of the SGML standard."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144555188"
                        ],
                        "name": "D. Massart",
                        "slug": "D.-Massart",
                        "structuredName": {
                            "firstName": "Desire",
                            "lastName": "Massart",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Massart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934445"
                        ],
                        "name": "L. Buydens",
                        "slug": "L.-Buydens",
                        "structuredName": {
                            "firstName": "Lutgarde",
                            "lastName": "Buydens",
                            "middleNames": [
                                "M.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Buydens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11501787"
                        ],
                        "name": "C. Armanino",
                        "slug": "C.-Armanino",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Armanino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Armanino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13660445"
                        ],
                        "name": "I. Broeckaert",
                        "slug": "I.-Broeckaert",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Broeckaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Broeckaert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2973096"
                        ],
                        "name": "D. Coomans",
                        "slug": "D.-Coomans",
                        "structuredName": {
                            "firstName": "Danny",
                            "lastName": "Coomans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Coomans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92124490"
                        ],
                        "name": "W. H. Dekker",
                        "slug": "W.-H.-Dekker",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Dekker",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. H. Dekker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3570202"
                        ],
                        "name": "M. Derde",
                        "slug": "M.-Derde",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Derde",
                            "middleNames": [
                                "Paule"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Derde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13435040"
                        ],
                        "name": "M. Detaevernier",
                        "slug": "M.-Detaevernier",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Detaevernier",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Detaevernier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48684809"
                        ],
                        "name": "K. Esbensen",
                        "slug": "K.-Esbensen",
                        "structuredName": {
                            "firstName": "Kim",
                            "lastName": "Esbensen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Esbensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3868998"
                        ],
                        "name": "M. Forina",
                        "slug": "M.-Forina",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Forina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Forina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8390907"
                        ],
                        "name": "M. Jonckheer",
                        "slug": "M.-Jonckheer",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jonckheer",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jonckheer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32189053"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401595554"
                        ],
                        "name": "A. Massart-Le\u00ebn",
                        "slug": "A.-Massart-Le\u00ebn",
                        "structuredName": {
                            "firstName": "Anne-Marie",
                            "lastName": "Massart-Le\u00ebn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Massart-Le\u00ebn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398864274"
                        ],
                        "name": "J. Smeyers-Verbeke",
                        "slug": "J.-Smeyers-Verbeke",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Smeyers-Verbeke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Smeyers-Verbeke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153198539"
                        ],
                        "name": "J. Hartog",
                        "slug": "J.-Hartog",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hartog",
                            "middleNames": [
                                "C.",
                                "Den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hartog"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "The separation surfaces in the space (F1, F2, F3) are experimentally determined by using a fixed error correction procedure (see [85]) on a preclassified training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 93669581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b26a1b1faa377f7915ea7f8d1ee8c99d7a2de5b",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This poster illustrates the lecture on Pattern Recognition and gives recently published and unpublished examples, mainly from the laboratory from the first author. The applications concern:- the determination of metabolic pathways of branched chain fatty acids (by clustering),- the development of a genetic classification of meteorites (by clustering),- the classification of cholinergic agents according to their interaction with different receptors (by clustering),- the structure of a data set consisting of gaschromatographic profiles in samples collected in pollution monitoring stations (by factor analysis and pattern recognition),- factors determining GLC behaviour of solutes (by factor analysis and multiple regression),- the classification of olive oils according to geographic origin (by principal components and pattern recognition),- the diagnosis of thyroid status (by pattern recognition)."
            },
            "slug": "Applications-of-pattern-recognition-Massart-Buydens",
            "title": {
                "fragments": [],
                "text": "Applications of pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This poster illustrates the lecture on Pattern Recognition and gives recently published and unpublished examples, mainly from the laboratory from the first author."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722211"
                        ],
                        "name": "E. Ukkonen",
                        "slug": "E.-Ukkonen",
                        "structuredName": {
                            "firstName": "Esko",
                            "lastName": "Ukkonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ukkonen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The zoning score is based on the number of edit operations required to transform an OCR output to the correct text, using string matching algorithms [96, 97]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205886218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "530f4487992599b3598bd4bb45d74de8436fc3fc",
            "isKey": false,
            "numCitedBy": 647,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-for-Approximate-String-Matching-Ukkonen",
            "title": {
                "fragments": [],
                "text": "Algorithms for Approximate String Matching"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "The second method presented by Postl in [16] computes the Fourier transform of the document page and makes use of the power spectrum of the Fourier space to associate a score to each selected angle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "\u2026estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "In the method proposed by Postl [16] only points on a coarse grid are used to compute the projection profile and the premium to be maximized corresponds to the sum of squared differences between successive bins in the projection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The optimization function is the same proposed by Postl [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection of linear oblique structures and skew scan in digitized documents"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 8th International Conference on Pattern Recognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "RLSA received great interest, principally because of its easy implementation, and has been employed, with some modi cations, in several systems to perform the segmentation phase [18, 82, 83, 79, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 135
                            }
                        ],
                        "text": "The major part are based on feature extraction and linear discriminant classi ers [53, 55, 59, 77], but other techniques are presented [78, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 202
                            }
                        ],
                        "text": "Block classi cation techniques described in this section can be grouped into features extraction and linear discriminant classi ers [53, 77, 55, 59], binary classi cation tree [78], and neural networks [79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[79] present a new method for the classi cation of blocks extracted from binary document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[79] b/w 200dpi text, non text blocks no skew; only text/nontext block classi cation trainable Table 7: Features of the algorithms for block classi cation."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classi cation of Binary Doc-  ument Images into Textual or Nontextual Data Blocks using Neural  Network Models"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications, 8:289{304"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Dias\u2019 algorithm makes the assumption that intercharacter spacing is smaller than interline spacing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "noisy background and multiple skews are dealt; non text may be present Dias [42] b/w 300dpi freely shaped headers, footers, text lines, text blocks text only; no skew; line gaps wider than char gaps independence of text orientation"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "The Minimum Spanning Tree (MST) method described by Dias [42], is based on a work of Ittner [81], which employs the MST to detect the text lines orientation (horizontal or vertical) of a document."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minimum Spanning Trees for Text Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of Fifth Annual Symposium on Document Analysis and Information Retrieval"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "RLSA received great interest, principally because of its easy implementation, and has been employed, with some modi cations, in several systems to perform the segmentation phase [18, 82, 83, 79, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "3 Skew estimation Most of the skew estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection pro les [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "work [18], only a selected subregion (one with high density of black pixels per row) of the document image is projected; the function to be maximized is the mean square deviation of the pro le."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[18] b/w 300 dpi 45 0:7 complex documents, e."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Roc-  cotelli. An experimental system for o ce document handling and text  recognition"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of the 9th International Conference on Pattern  Recognition,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Srihari and Govindaraju [21] apply this technique to binary document images, or a subregion thereof, that is known to contain only text and where the entire text block has a single orientation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Wang and Srihari [55] compare the RLSA and RXYC approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 143
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection pro le analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 82
                            }
                        ],
                        "text": "The major part are based on feature extraction and linear discriminant classi ers [53, 55, 59, 77], but other techniques are presented [78, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 340,
                                "start": 336
                            }
                        ],
                        "text": "[53] b/w 240dpi text lines, graphics and halftones, horizontal lines, vertical lines no skew; known chars size; problems with close or high text lines Linear Discriminant Classi ers Shih Chen [77] b/w 300dpi text lines, horizontal lines, vertical lines, graphics, pictures no skew independence of char sizes and resolution Wang Srihari [55] b/w 100200dpi small, medium, large text, graphics, halftones no skew trainable; independence of blocks size Pavlidis Zhou [59] b/w 300dpi text blocks, diagrams, halftones xed parameters Binary Classi cation Tree Sivaramaakrishnan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "This method is used, for example, by Srihari and Govindaraju [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "IRST 34 Wang and Srihari [55] point out some limitations of the approach proposed by Wong et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 132
                            }
                        ],
                        "text": "Block classi cation techniques described in this section can be grouped into features extraction and linear discriminant classi ers [53, 77, 55, 59], binary classi cation tree [78], and neural networks [79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classi cation of Newspaper Image Blocks  Using Texture Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision, Graphics and Image Pro-  cessing, 47:327{352"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634477"
                        ],
                        "name": "J. Perkins",
                        "slug": "J.-Perkins",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Perkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Perkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109711752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e54af6e97f124d6e18b81b521bf7b7711f6aeca8",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-recognition-in-practice-Perkins",
            "title": {
                "fragments": [],
                "text": "Pattern recognition in practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "In Baird\u2019s work [17] another technique is used for selecting the points to be projected: for each connected component the midpoint of the bottom side of the bounding box is projected."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60023835,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "ee1decd6020b5cf63d1d6a4ea7cfde1ab5309279",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-skew-angle-of-printed-documents-Baird",
            "title": {
                "fragments": [],
                "text": "The skew angle of printed documents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Baird [43] presents a detailed description of a method for the analysis"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Projection Profiles Baird [43] b/w 300dpi single column chars, words, text lines text only; detached chars; one text direction freely shaped regions; baseline computation; line spacing and text size may vary; skew up to 5 is dealt Ha et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59912183,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "16b474bea5eb0dea3f63d3f5ff2ef4e1b9eee8b8",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Global-to-Local-Layout-Analysis-Baird",
            "title": {
                "fragments": [],
                "text": "Global-to-Local Layout Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 144
                            }
                        ],
                        "text": "They are grouped into the following classes related to the adopted segmentation technique: smearing technique [53], projection profile analysis [54, 55, 56, 57, 58, 59], texture based or local analysis [60, 61], and analysis of the background structure [62, 63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[54, 84] b/w 300 dpi xy-cut cleaned docs; no skew; segmentation into columns, blocks, text lines Projection Profiles Pavlidis Zhou [59] b/w 300dpi right polygons blocks surrounded by straight white streams; fixed parameters skew up to 15 is dealt; segmentation into rectangular blocks"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 54
                            }
                        ],
                        "text": "A popular approach to page segmentation, described in [54, 84], is the Recursive X-Y Cuts, RXYC, algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59683040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc45263226de157763006aef70b681dbac744dcc",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "HIERARCHICAL-REPRESENTATION-OF-OPTICALLY-SCANNED-Nagy-Seth",
            "title": {
                "fragments": [],
                "text": "HIERARCHICAL REPRESENTATION OF OPTICALLY SCANNED DOCUMENTS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398550688"
                        ],
                        "name": "L. O'Gorman",
                        "slug": "L.-O'Gorman",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "O'Gorman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. O'Gorman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[3] and the survey of methods by Jain and Yu in [4]), along with the tutorial text by O\u2019Gorman and Kasturi [5], we believe that an attempt to provide a reasoned systematization of the field can be of great interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6575783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c43505cc450b647f6f3b4a6f9004ae7d414e2bea",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Document-Image-Analysis-O'Gorman-Kasturi",
            "title": {
                "fragments": [],
                "text": "Document Image Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication of Newspaper Image Blocks Using Texture Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision, Graphics and Image Processing"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 90
                            }
                        ],
                        "text": "Several algorithms have been speci cally designed for the binarization of document images [7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Segmentation of Document Im-  ages"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transaction on Pattern Analysis and Machine Intelligence,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "Techniques using the well-known Hough transform [39, 40] have been explored by several authors and are based on the observations that a distinguishing feature for text is the alignment of characters and that text lines of a document are usually parallel each other."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine Vision: Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Algorithms, Practicalities. Aca-  demic Press"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "As reported in Haralick\u2019s review [2], Derrien-Peden [108] defines a frame based system to determine the logical structure of scientific and technical documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Frame-based System for Macro-typographical Structure Analysis in Scientific Papers"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 1st International Conference on Document Analysis and Recognition"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[44, 45] b/w right polygons text blocks text only, cleaned docs; one text direction; spacing constraints skew up to 5 is dealt; no input parameters"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Baird [45] describes a segmentation technique based on a previous work [44], in which the structure of the document background is analyzed in order to determine the geometric page layout."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image Segmentation using Shape-Directed Covers"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 10th International Conference on Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document  Analysis at DFKI. Part 1: Image Anlysis and Text Recognition. Tech-  nical Report RR-95-02, German Research Center for Arti cial Intelli-  gence (DKFI)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Spitz [23] describes a data reduction technique which works directly on ccitt 4 compressed images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 217
                            }
                        ],
                        "text": "3 Skew estimation Most of the skew estimation techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection pro les [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Skew Determination in CCITT Group 4 Compressed Doc-  ument Images"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the Symposium on Document Analysis and  Information Retrieval, pages 11{25, Las Vegas"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication of Binary Document Images into Textual or Nontextual Data Blocks using Neural Network Models"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Experiments have been performed on a data set, from the University of Washington Database [86]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English Document Database Standard"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 2nd International Conference on Document Analysis and Recognition"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Fast and E cient  Method For Extracting Text Paragraphs and Graphics from Uncon-  strained Documents"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 11th International Conference on  Pattern Recognition, pages 272{276, The Hague"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DAFS: Document Attribute Format Specification"
            },
            "venue": {
                "fragments": [],
                "text": "DAFS: Document Attribute Format Specification"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 90
                            }
                        ],
                        "text": "Several algorithms have been speci cally designed for the binarization of document images [7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pietik\u007fainen. Adap-  tive Document Binarization"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of the 4th International Con-  ference on Document Analysis and Recognition,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adobe Systems Inc. Portable Document Format Reference Manual"
            },
            "venue": {
                "fragments": [],
                "text": "Adobe Systems Inc. Portable Document Format Reference Manual"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adobe Systems Inc. PostScript Language Reference Manual"
            },
            "venue": {
                "fragments": [],
                "text": "Adobe Systems Inc. PostScript Language Reference Manual"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 140
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Smearing Johnston [48] b/w rectangular blocks text blocks cleaned docs; no skew; known char size; interblock gaps greater than char size non text may be present"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "One of the first approaches to text/non-text discrimination was presented by Johnston [48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SHORT NOTE: Printed Text Discrimination"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Graphics and Image Processing"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 195
                            }
                        ],
                        "text": "Although some OCR systems were developed o ering gray level input as an option in order to improve their recognition accuracy [1], most OCR commercial systems work, at present, on bilevel inputs [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modena. Review of the State of  the Art in Optical Character Recognition. Part 1: Machine Printed  Documents"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report #9607-03,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Background Structure in Document Images In Advances in Structural and Syntactic Pattern Recognition, pages 253{269"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientiic"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "As reported in Haralick's review [2], Derrien-Peden [108] denes a frame based system to determine the logical structure of scienti c and technical documents."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Frame-based System for Macro-typographical  Structure Analysis in Scienti c Papers"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of the 1st Inter-  national Conference on Document Analysis and Recognition,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "For example, a column of text is semantically different from a caption, and a footnote is different from a paragraph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Review of the State of the Art in Optical Character Recognition. Part 1: Machine Printed Documents"
            },
            "venue": {
                "fragments": [],
                "text": "Review of the State of the Art in Optical Character Recognition. Part 1: Machine Printed Documents"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Spitz [23] describes a data reduction technique which works directly on ccitt 4 compressed images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Skew Determination in CCITT Group 4 Compressed Document Images"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the Symposium on Document Analysis and Information Retrieval"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "The Minimum Spanning Tree (MST) method described by Dias [42], is based on a work of Ittner [81], which employs the MST to detect the text lines orientation (horizontal or vertical) of a document."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Inference of Textline Orientation"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the Second Annual Symposium on Document Analysis and Information Retrieval"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "RLSA received great interest, principally because of its easy implementation, and has been employed, with some modi cations, in several systems to perform the segmentation phase [18, 82, 83, 79, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Rule-Based System For Docu-  ment Image Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of the 10th International Confer-  ence on Pattern Recognition,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Page Segmentation and Classification. CVGIP: Graphical Models and Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Page Segmentation and Classification. CVGIP: Graphical Models and Image Processing"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "in order to improve their recognition accuracy [1], most OCR commercial systems work, at present, on bilevel inputs [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Review of the State of the Art in Optical Character Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Part 1: Machine Printed Documents. Technical Report #9607-03, IRST, Trento, Italy, June"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Page Segmentation without Rectangle Assumption The Hague"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 11th International Conference on Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "The text-based approach was the first proposed in the literature [88, 89, 90]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Preliminary Evaluation of Automatic Zoning"
            },
            "venue": {
                "fragments": [],
                "text": "A Preliminary Evaluation of Automatic Zoning"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] and the survey of methods by Jain and Yu in [4]), along with the tutorial text by O'Gorman and Kasturi [5], we believe that an attempt to provide a reasoned systematization of the eld can be of great interest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Document Process-  ing: a Survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition, 29(12):1931{1952"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "are devoted to documents which contain only textual information [29, 42, 21, 43, 44, 45, 46, 47], or text mixed with some non text elements [48, 49, 50, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[46] present a simple method for page segmentation and classi cation into words, text lines, paragraphs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[46] b/w 300dpi single column words, text lines, paragraphs text only, cleaned docs; no skew; hierarchic spacing protocol determination of text orientation Parodi Piccioli [51] b/w 80dpi freely shaped text lines one text direction skew up to 6 is dealt; non text may be present Table 3: Some features of text segmentation algorithms: input type of the documents (binary, gray-level) and working resolution, layout structure, searched text regions, assumptions and limitations, advantages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document Page Decompo-  sition by the Bounding-Box Projection Technique"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of the  3th International Conference on Document Analysis and Recognition,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From Image to Desktop Publishing: a Complete Document Understanding System. Working paper"
            },
            "venue": {
                "fragments": [],
                "text": "From Image to Desktop Publishing: a Complete Document Understanding System. Working paper"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 82
                            }
                        ],
                        "text": "component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Another approach where the layout segmentation is posed as a texture segmentation problem is described by Jain and Zhong [75]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Page Layout Segmentation based on Texture Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document Analysis at DFKI"
            },
            "venue": {
                "fragments": [],
                "text": "Part 1: Image Anlysis and Text Recognition. Technical Report RR-95-02, German Research Center for Artificial Intelligence (DKFI), Kaiserslautern, Germany, March"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Techniques using the well-known Hough transform [39, 40] have been explored by several authors and are based on the observations that a distinguishing feature for text is the alignment of characters and that text lines of a document are usually parallel each other."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods and means for recognizing complex patterns"
            },
            "venue": {
                "fragments": [],
                "text": "US Patent #3"
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Fast and Efficient Method For Extracting Text Paragraphs and Graphics from Unconstrained Documents The Hague"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 11th International Conference on Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[107] have defined the FDL (Form Definition Language), a lisp-like language based on rectangular regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A knowledgebased segmentation method for document understanding"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 8th International Conference on Pattern Recognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "The described techniques are based on connected component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Separation of  Text"
            },
            "venue": {
                "fragments": [],
                "text": "Graphic and Picture Segments in Printed Material. In E.S.  Gelsema and L.N. Kanal, editors, \"Pattern Recognition in Practice\",  pages 213{221. North-Holland, Amsterdam"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[107] have de ned the FDL (Form De nition Language), a lisp-like language based on rectangular regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A knowledge-  based segmentation method for document understanding"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of  the 8th International Conference on Pattern Recognition, pages 745{  748, Paris, France"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Experiments have been performed on a data set, from the University of Washington Database [86]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Haralick. English Document Database  Standard"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of the 2nd International Conference on Document  Analysis and Recognition,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An experimental system for ooce document handling and text recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 9th International Conference on Pattern Recognition"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "\u2026techniques can be divided into the following main classes according to the basic approach they adopt: analysis of projection profiles [16, 17, 18, 19, 20], Hough transform [21, 22, 23, 24, 25, 26, 27], connected components clustering [28, 29, 30, 26], and correlation between lines [31, 32,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "In Ciardiello et al. work [18], only a selected subregion (one with high density of black pixels per row) of the document image is projected; the function to be maximized is the mean square deviation of the profile."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An experimental system for office document handling and text recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 9th International Conference on Pattern Recognition"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DAFS: Document Attribute Format Speciication"
            },
            "venue": {
                "fragments": [],
                "text": "DAFS: Document Attribute Format Speciication"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 82
                            }
                        ],
                        "text": "component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Separation of Text, Graphic and Picture Segments in Printed Material"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition in Practice"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 86
                            }
                        ],
                        "text": "To overcome this limitation a region-based approach has been more recently introduced [91, 92, 47, 93, 94, 95]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document Page Decomposi-  tion Using Bounding Boxes of Connected Components of Black Pix-  els. In IS&T/SPIE Symposium on Electronic Imaging Science and  IRST  66  Technology, Document Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 55
                            }
                        ],
                        "text": "1), is used in the system proposed by Haralick's group [105]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From  Image to Desktop Publishing: a Complete Document Understanding  System"
            },
            "venue": {
                "fragments": [],
                "text": "Working paper"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Frame-based System for Macro-typographical Structure Analysis in Scientiic Papers"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the 1st International Conference on Document Analysis and Recognition"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "The described techniques are based on connected component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "freely shaped text, pictures (graphics with the Fourier method) spacing constraints multiple skews may be present Texture or Local Analysis Sauvola Pietik\u007f ainen [74] b/w rectangular blocks text, pictures, background cleaned docs; no skew Jain Zhong [75] g."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "A similar method is presented by Sauvola and Pietik\u007fainen [74]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pietik\u007fainen. Page Segmentation and Classi cation  using fast Feature Extraction and Connectivity Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of  the 3th International Conference on Document Analysis and Recogni-  tion,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "The described techniques are based on connected component analysis [31, 66, 67, 68], smearing [71], and texture or local analysis [73, 74, 75, 76]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiscale Segmen-  tation of Unstructured Document Pages Using Soft Decision Integra-  tion"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelli-  gence,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Algorithms for approximate string matching. Information and Control"
            },
            "venue": {
                "fragments": [],
                "text": "Algorithms for approximate string matching. Information and Control"
            },
            "year": 1985
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 63,
            "methodology": 72
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 149,
        "totalPages": 15
    },
    "page_url": "https://www.semanticscholar.org/paper/Geometric-Layout-Analysis-Techniques-for-Document-a-Cattoni-Coianiz/cc71c69bb1eb5ed3b82a10347a841cb3a27e2d58?sort=total-citations"
}