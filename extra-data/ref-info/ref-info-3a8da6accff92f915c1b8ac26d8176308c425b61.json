{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13399651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2363f4d5027b10954e1b3124a31218cd9199bab",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Analysis of videos of human-object interactions involves understanding human movements, locating and recognizing objects and observing the effects of human movements on those objects. While each of these can be conducted independently, recognition improves when interactions between these elements are considered. Motivated by psychological studies of human perception, we present a Bayesian approach which unifies the inference processes involved in object classification and localization, action understanding and perception of object reaction. Traditional approaches for object classification and action understanding have relied on shape features and movement analysis respectively. By placing object classification and localization in a video interpretation framework, we can localize and classify objects which are either hard to localize due to clutter or hard to recognize due to lack of discriminative features. Similarly, by applying context on human movements from the objects on which these movements impinge and the effects of these movements, we can segment and recognize actions which are either too subtle to perceive or too hard to recognize using motion features alone."
            },
            "slug": "Objects-in-Action:-An-Approach-for-Combining-Action-Gupta-Davis",
            "title": {
                "fragments": [],
                "text": "Objects in Action: An Approach for Combining Action Understanding and Object Perception"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A Bayesian approach is presented which unifies the inference processes involved in object classification and localization, action understanding and perception of object reaction, and which can segment and recognize actions which are either too subtle to perceive or too hard to recognize using motion features alone."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32713089"
                        ],
                        "name": "Darnell J. Moore",
                        "slug": "Darnell-J.-Moore",
                        "structuredName": {
                            "firstName": "Darnell",
                            "lastName": "Moore",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darnell J. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144449603"
                        ],
                        "name": "M. Hayes",
                        "slug": "M.-Hayes",
                        "structuredName": {
                            "firstName": "Monson",
                            "lastName": "Hayes",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hayes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15378359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f090df944fceaf52cda458ed6116802cd36ca08e",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to exploit human motion and object context to perform action recognition and object classification. Towards this end, we introduce a framework for recognizing actions and objects by measuring image-, object- and action-based information from video. Hidden Markov models are combined with object context to classify hand actions, which are aggregated by a Bayesian classifier to summarize activities. We also use Bayesian methods to differentiate the class of unknown objects by evaluating detected actions along with low-level, extracted object features. Our approach is appropriate for locating and classifying objects under a variety of conditions including full occlusion. We show experiments where both familiar and previously unseen objects are recognized using action and context information."
            },
            "slug": "Exploiting-human-actions-and-object-context-for-Moore-Essa",
            "title": {
                "fragments": [],
                "text": "Exploiting human actions and object context for recognition tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work introduces a framework for recognizing actions and objects by measuring image-, object- and action-based information from video, which is appropriate for locating and classifying objects under a variety of conditions including full occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46802197"
                        ],
                        "name": "H. Helbig",
                        "slug": "H.-Helbig",
                        "structuredName": {
                            "firstName": "Hannah",
                            "lastName": "Helbig",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Helbig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143984784"
                        ],
                        "name": "M. Graf",
                        "slug": "M.-Graf",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Graf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Graf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144288961"
                        ],
                        "name": "M. Kiefer",
                        "slug": "M.-Kiefer",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Kiefer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kiefer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 604829,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cd8e35ab04495fd6f22a85af5f72b2e1770932b3",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "It is typically assumed that perception for action and object recognition are subserved by functionally and neuroanatomically distinct processing streams in the brain. However, recent evidence challenges this classical view and suggests an interaction between both visual processing streams. While previous studies showed an influence of object perception on action-related tasks, we investigated whether action representations facilitate visual object recognition. In order to address this question, two briefly displayed masked objects were sequentially presented, either affording congruent or incongruent motor interactions. We found superior naming accuracy for object pairs with congruent as compared to incongruent motor interactions (Experiment 1). This action priming effect indicates that action representations can facilitate object recognition. We further investigated the nature of the representations underlying this action priming effect. The effect was absent when the prime stimulus was presented as a word (Experiment 2). Thus, the action priming effect seems to rely on action representations specified by visual object information. Our findings suggest that processes of object-directed action influence object recognition."
            },
            "slug": "The-role-of-action-representations-in-visual-object-Helbig-Graf",
            "title": {
                "fragments": [],
                "text": "The role of action representations in visual object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The findings suggest that processes of object-directed action influence object recognition, and the nature of the representations underlying this action priming effect seems to rely on action representations specified by visual object information."
            },
            "venue": {
                "fragments": [],
                "text": "Experimental Brain Research"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3193405"
                        ],
                        "name": "Roman Filipovych",
                        "slug": "Roman-Filipovych",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Filipovych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roman Filipovych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559941"
                        ],
                        "name": "Eraldo Ribeiro",
                        "slug": "Eraldo-Ribeiro",
                        "structuredName": {
                            "firstName": "Eraldo",
                            "lastName": "Ribeiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eraldo Ribeiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12832858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd0cb0134f670a3bd77d52644298810fd30a674c",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a solution to the novel problem of recognizing primitive actor-object interactions from videos. Here, we introduce the concept of actor-object states. Our method is based on the observation that at the moment of physical contact, both the motion and the appearance of actors are constrained by the target object. We propose a probabilistic framework that automatically learns models in such constrained states. We use joint probability distributions to represent both actor and object appearances as well as their intrinsic spatio-temporal configurations. Finally, we demonstrate the applicability of our approach on series of human-object interaction classification experiments."
            },
            "slug": "Recognizing-primitive-interactions-by-exploring-Filipovych-Ribeiro",
            "title": {
                "fragments": [],
                "text": "Recognizing primitive interactions by exploring actor-object states"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A probabilistic framework is proposed that automatically learns models in constrained states of actor-object interactions from videos based on the observation that at the moment of physical contact, both the motion and the appearance of actors are constrained by the target object."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145206607"
                        ],
                        "name": "C. Rao",
                        "slug": "C.-Rao",
                        "structuredName": {
                            "firstName": "Cen",
                            "lastName": "Rao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858702"
                        ],
                        "name": "A. Yilmaz",
                        "slug": "A.-Yilmaz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Yilmaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yilmaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2769833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1128c0cd1e504d555c57cb39cdd7b6be399eb5a7",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Analysis of human perception of motion shows that information for representing the motion is obtained from the dramatic changes in the speed and direction of the trajectory. In this paper, we present a computational representation of human action to capture these dramatic changes using spatio-temporal curvature of 2-D trajectory. This representation is compact, view-invariant, and is capable of explaining an action in terms of meaningful action units called dynamic instants and intervals. A dynamic instant is an instantaneous entity that occurs for only one frame, and represents an important change in the motion characteristics. An interval represents the time period between two dynamic instants during which the motion characteristics do not change. Starting without a model, we use this representation for recognition and incremental learning of human actions. The proposed method can discover instances of the same action performed by differentpeople from different view points. Experiments on 47 actions performed by 7 individuals in an environment with no constraints shows the robustness of the proposed method."
            },
            "slug": "View-Invariant-Representation-and-Recognition-of-Rao-Yilmaz",
            "title": {
                "fragments": [],
                "text": "View-Invariant Representation and Recognition of Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper presents a computational representation of human action to capture these dramatic changes using spatio-temporal curvature of 2-D trajectory that is compact, view-invariant, and capable of explaining an action in terms of meaningful action units called dynamic instants and intervals."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48275599"
                        ],
                        "name": "R. Mann",
                        "slug": "R.-Mann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Mann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Mann et al. [30] presented an approach for understanding observations of interacting objects using Newtonian mechanics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1082646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10a3f14faf79ec12d04f834abf8fea63a891232c",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding observations of image sequences requires one to reason about qualitative scene dynamics. For example, on observing a hand lifting a cup, we may infer that an 'active' hand is applying an upwards force (by grasping) on a 'passive' cup. In order to perform such reasoning, we require an ontology that describes object properties and the generation and transfer of forces in the scene. Such an ontology should include, for example: the presence of gravity, the presence of a ground plane, whether objects are 'active' or 'passive', whether objects are contacting and/or attached to other objects, and so on. In this work we make these ideas precise by presenting an implemented computational system that derives symbolic force-dynamic descriptions from video sequences. \nOur approach to scene dynamics is based on an analysis of the Newtonian mechanics of a simplified scene model. The critical requirement is that, given image sequences, we can obtain estimates for the shape and motion of the objects in the scene. To do this, we assume that the objects can be approximated by a two-dimensional 'layered' scene model. The input to our system consists of a set of polygonal outlines along with estimates for their velocities and accelerations, obtained from a view-based tracker. Given such input, we present a system that extracts force-dynamic descriptions for the image sequence. We provide computational examples to demonstrate that our ontology is sufficiently rich to describe a wide variety of image sequences. \nThis work makes three central contributions. First, we provide an ontology suitable for describing object properties and the generation and transfer of forces in the scene. Second, we provide a computational procedure to test the feasibility of such interpretations by reducing the problem to a feasibility test in linear programming. Finally, we provide a theory of preference ordering between multiple interpretations along with an efficient computational procedure to determine maximal elements in such orderings."
            },
            "slug": "Computational-Perception-of-Scene-Dynamics-Mann-Jepson",
            "title": {
                "fragments": [],
                "text": "Computational Perception of Scene Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work provides an ontology suitable for describing object properties and the generation and transfer of forces in the scene, and provides a computational procedure to test the feasibility of such interpretations by reducing the problem to a feasibility test in linear programming."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2483785"
                        ],
                        "name": "Patrick Peursum",
                        "slug": "Patrick-Peursum",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Peursum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Peursum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145227675"
                        ],
                        "name": "G. West",
                        "slug": "G.-West",
                        "structuredName": {
                            "firstName": "Geoff",
                            "lastName": "West",
                            "middleNames": [
                                "A.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. West"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143761093"
                        ],
                        "name": "S. Venkatesh",
                        "slug": "S.-Venkatesh",
                        "structuredName": {
                            "firstName": "Svetha",
                            "lastName": "Venkatesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Venkatesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2104873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddc37d14d12b81afae1d3e44864766ca01721cd5",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional methods of object recognition are reliant on shape and so are very difficult to apply in cluttered, wide angle and low detail views such as surveillance scenes. To address this, a method of indirect object recognition is proposed, where human activity is used to infer both the location and identity of objects. No shape analysis is necessary. The concept is dubbed 'interaction signatures', since the premise is that a human interacts with objects in ways characteristic of the function of that object - for example, a person sits in a chair and drinks from a cup. The human-centred approach means that recognition is possible in low detail views and is largely invariant to the shape of objects within the same functional class. This paper implements a Bayesian network for classifying region patches with object labels, building upon our previous work in automatically segmenting and recognising a human's interactions with the objects. Experiments show that interaction signatures can successfully find and label objects in low detail views and are equally effective at recognising test objects that differ markedly in appearance from the training objects."
            },
            "slug": "Combining-image-regions-and-human-activity-for-in-Peursum-West",
            "title": {
                "fragments": [],
                "text": "Combining image regions and human activity for indirect object recognition in indoor wide-angle views"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A Bayesian network is implemented for classifying region patches with object labels, building upon the previous work in automatically segmenting and recognising a human's interactions with the objects."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3324054"
                        ],
                        "name": "C. Urgesi",
                        "slug": "C.-Urgesi",
                        "structuredName": {
                            "firstName": "Cosimo",
                            "lastName": "Urgesi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Urgesi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46454765"
                        ],
                        "name": "V. Moro",
                        "slug": "V.-Moro",
                        "structuredName": {
                            "firstName": "Valentina",
                            "lastName": "Moro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Moro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5345635"
                        ],
                        "name": "M. Candidi",
                        "slug": "M.-Candidi",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Candidi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Candidi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2829523"
                        ],
                        "name": "S. Aglioti",
                        "slug": "S.-Aglioti",
                        "structuredName": {
                            "firstName": "Salvatore",
                            "lastName": "Aglioti",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Aglioti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "\u2026of human-object interactions, that integrates information from perceptual tasks such as scene analysis, human motion/pose estimation,1 manipulable object detection, and \u201cobject reaction\u201d determination.2 While each of these tasks can be conducted independently, recognition rates improve\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17254342,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "024db3488239fb0f540ec30e56e6f970205c4c30",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The human visual system is highly tuned to perceive actual motion as well as to extrapolate dynamic information from static pictures of objects or creatures captured in the middle of motion. Processing of implied motion activates higher-order visual areas that are also involved in processing biological motion. Imagery and observation of actual movements performed by others engenders selective activation of motor and premotor areas that are part of a mirror-neuron system matching action observation and execution. By using single-pulse transcranial magnetic stimulation, we found that the mere observation of static snapshots of hands suggesting a pincer grip action induced an increase in corticospinal excitability as compared with observation of resting, relaxed hands, or hands suggesting a completed action. This facilitatory effect was specific for the muscle that would be activated during actual execution of the observed action. We found no changes in responsiveness of the tested muscles during observation of nonbiological entities with (e.g., waterfalls) or without (e.g., icefalls) implied motion. Thus, extrapolation of motion information concerning human actions induced a selective activation of the motor system. This indicates that overlapping motor regions are engaged in the visual analysis of physical and implied body actions. The absence of motor evoked potential modulation during observation of end posture stimuli may indicate that the observation\u2013execution matching system is preferentially activated by implied, ongoing but not yet completed actions."
            },
            "slug": "Mapping-Implied-Body-Actions-in-the-Human-Motor-Urgesi-Moro",
            "title": {
                "fragments": [],
                "text": "Mapping Implied Body Actions in the Human Motor System"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "By using single-pulse transcranial magnetic stimulation, it is found that the mere observation of static snapshots of hands suggesting a pincer grip action induced an increase in corticospinal excitability as compared with observation of resting, relaxed hands, or hands suggest a completed action."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779136"
                        ],
                        "name": "S. Dickinson",
                        "slug": "S.-Dickinson",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dickinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Recognition rates of target objects were higher when the priming object was used in a similar action as the target object."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121993192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bc67ba8025995068c9808be1d9aa50c14a420cb",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present an approach to function-based object recognition that reasons about the functionality of an object's intuitive parts. We extend the popular \"recognition by parts\" shape recognition framework to support \"recognition by functional parts,\" by combining a set of functional primitives and their relations with a set of abstract volumetric shape primitives and their relations. Previous approaches have relied on more global object features, often ignoring the problem of object segmentation and thereby restricting themselves to range images of unoccluded scenes. We show how these shape primitives and relations can be easily recovered from superquadric ellipsoids which, in turn, can be recovered from either range or intensity images of occluded scenes, Furthermore, the proposed framework supports both unexpected (bottom-up) object recognition and expected (top-down) object recognition, We demonstrate the approach on a simple domain by recognizing a restricted class of hand-tools from 2-D images."
            },
            "slug": "Recognition-by-Functional-Parts-Rivlin-Dickinson",
            "title": {
                "fragments": [],
                "text": "Recognition by Functional Parts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how these shape primitives and relations can be easily recovered from superquadric ellipsoids which, in turn, can be recovered from either range or intensity images of occluded scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403679665"
                        ],
                        "name": "Scott H. Johnson-Frey",
                        "slug": "Scott-H.-Johnson-Frey",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Johnson-Frey",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott H. Johnson-Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47049016"
                        ],
                        "name": "Farah R. Maloof",
                        "slug": "Farah-R.-Maloof",
                        "structuredName": {
                            "firstName": "Farah",
                            "lastName": "Maloof",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Farah R. Maloof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399402067"
                        ],
                        "name": "R. Newman-Norlund",
                        "slug": "R.-Newman-Norlund",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Newman-Norlund",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Newman-Norlund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3595346"
                        ],
                        "name": "C. Farrer",
                        "slug": "C.-Farrer",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Farrer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farrer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34027754"
                        ],
                        "name": "S. Inati",
                        "slug": "S.-Inati",
                        "structuredName": {
                            "firstName": "Souheil",
                            "lastName": "Inati",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Inati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3047772"
                        ],
                        "name": "Scott T. Grafton",
                        "slug": "Scott-T.-Grafton",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Grafton",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott T. Grafton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3239572,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "790ed61241ab596761e3353d9826ada08c048b82",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Actions-or-Hand-Object-Interactions-Human-Inferior-Johnson-Frey-Maloof",
            "title": {
                "fragments": [],
                "text": "Actions or Hand-Object Interactions? Human Inferior Frontal Cortex and Action Observation"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152835649"
                        ],
                        "name": "Li Zhang",
                        "slug": "Li-Zhang",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143847264"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Using this approach, however, we are unable to distinguish between objects that have the same shape but a different dominant color; for example, a cricket ball (often red or white in color) as opposed to a tennis ball (often yellow in color)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8496483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86f918a351efd6e0c283dd9b526c9da2330f18c3",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for detecting and tracking humans. Different from most of the previous work, we focus on humans with extensive pose articulations, under situations where there is typically only a single camera, multiple humans are present and the image resolution is low. In our method pose clusters are learned from an embedded silhouette manifold. A set of object detectors, each of which corresponds to one pose cluster, are trained based on a novel Object-Weighted Appearance Model. A probabilistic pose-based transition model is used to track multiple objects within a sliding window buffer, making use of the detection responses. The track segments in the sliding windows are connected sequentially into full trajectories. Experiments on a set of challenging surveillance videos are presented; these show good performance of our approach compared to standard pedestrian detectors, under difficult conditions."
            },
            "slug": "Detection-and-Tracking-of-Multiple-Humans-with-Pose-Zhang-Wu",
            "title": {
                "fragments": [],
                "text": "Detection and Tracking of Multiple Humans with Extensive Pose Articulation"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work describes a method for detecting and tracking humans with extensive pose articulations, under situations where there is typically only a single camera, multiple humans are present and the image resolution is low."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779136"
                        ],
                        "name": "S. Dickinson",
                        "slug": "S.-Dickinson",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dickinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206588874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68fcd229f3a739a3a428d58df4bbc7cf39a59aa3",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to function-based object recognition that reasons about the functionality of an object's initiative parts. We extend the popular \"recognition by parts\" shape recognition framework to support \"recognition, by functional parts\", by combining a set of functional primitives and their relations with a set of abstract volumetric shape primitives and their relations. Previous approaches have relied on more global object features, often ignoring the problem of object segmentation, and thereby restricting themselves to range images of unoccluded scenes. We show how these shape primitives and relations can be easily recovered from superquadric ellipsoids which, in turn, can be recovered from either range or intensity images of occluded scenes. Furthermore, the proposed framework supports both unexpected (bottom-up) object recognition and expected (top-down) object recognition. We demonstrate the approach on, a simple domain by recognizing a restricted class of hand-tools from 2-D images.<<ETX>>"
            },
            "slug": "Recognition-by-functional-parts-[function-based-Rivlin-Dickinson",
            "title": {
                "fragments": [],
                "text": "Recognition by functional parts [function-based object recognition]"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An approach to function-based object recognition that reasons about the functionality of an object's initiative parts and supports both unexpected (bottom-up) object recognition and expected (top-down)object recognition is presented."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50626295"
                        ],
                        "name": "J. Sullivan",
                        "slug": "J.-Sullivan",
                        "structuredName": {
                            "firstName": "Josephine",
                            "lastName": "Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sullivan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153120475"
                        ],
                        "name": "S. Carlsson",
                        "slug": "S.-Carlsson",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Carlsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carlsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16234776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa96efb495cbf73da18737cdaa2200d597015476",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Human activity can be described as a sequence of 3D body postures. The traditional approach to recognition and 3D reconstruction of human activity has been to track motion in 3D, mainly using advanced geometric and dynamic models. In this paper we reverse this process. View based activity recognition serves as an input to a human body location tracker with the ultimate goal of 3D reanimation in mind. We demonstrate that specific human actions can be detected from single frame postures in a video sequence. By recognizing the image of a person's posture as corresponding to a particular key frame from a set of stored key frames, it is possible to map body locations from the key frames to actual frames. This is achieved using a shape matching algorithm based on qualitative similarity that computes point to point correspondence between shapes, together with information about appearance. As the mapping is from fixed key frames, our tracking does not suffer from the problem of having to reinitialise when it gets lost. It is effectively a closed loop. We present experimental results both for recognition and tracking for a sequence of a tennis player."
            },
            "slug": "Recognizing-and-Tracking-Human-Action-Sullivan-Carlsson",
            "title": {
                "fragments": [],
                "text": "Recognizing and Tracking Human Action"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that specific human actions can be detected from single frame postures in a video sequence and identified using a shape matching algorithm based on qualitative similarity that computes point to point correspondence between shapes, together with information about appearance."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6152006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4081e007d7eced95cc618164e976a80d44ff5f4e",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
            },
            "slug": "Putting-Objects-in-Perspective-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Putting Objects in Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper provides a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint by allowing probabilistic object hypotheses to refine geometry and vice-versa."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775451"
                        ],
                        "name": "Z. Kourtzi",
                        "slug": "Z.-Kourtzi",
                        "structuredName": {
                            "firstName": "Zoe",
                            "lastName": "Kourtzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Kourtzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931482"
                        ],
                        "name": "N. Kanwisher",
                        "slug": "N.-Kanwisher",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Kanwisher",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kanwisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "\u2026interpretation of human-object interactions, that integrates information from perceptual tasks such as scene analysis, human motion/pose estimation,1 manipulable object detection, and \u201cobject reaction\u201d determination.2 While each of these tasks can be conducted independently, recognition rates\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1695292,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "c9bac57750ec2e079c75cc3d852d153a6a03fb2c",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A still photograph of an object in motion may convey dynamic information about the position of the object immediately before and after the photograph was taken (implied motion). Medial temporal/medial superior temporal cortex (MT/MST) is one of the main brain regions engaged in the perceptual analysis of visual motion. In two experiments we examined whether MT/MST is also involved in representing implied motion from static images. We found stronger functional magnetic resonance imaging (fMRI) activation within MT/MST during viewing of static photographs with implied motion compared to viewing of photographs without implied motion. These results suggest that brain regions involved in the visual analysis of motion are also engaged in processing implied dynamic information from static images."
            },
            "slug": "Activation-in-Human-MT/MST-by-Static-Images-with-Kourtzi-Kanwisher",
            "title": {
                "fragments": [],
                "text": "Activation in Human MT/MST by Static Images with Implied Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "fMRI activation within MT/MST is found during viewing of static photographs with implied motion compared to viewing of photographs without implied motion, suggesting that brain regions involved in the visual analysis of motion are also engaged in processing implied dynamic information from static images."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2524694"
                        ],
                        "name": "K. Nelissen",
                        "slug": "K.-Nelissen",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Nelissen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nelissen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144450212"
                        ],
                        "name": "G. Luppino",
                        "slug": "G.-Luppino",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Luppino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Luppino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3341228"
                        ],
                        "name": "W. Vanduffel",
                        "slug": "W.-Vanduffel",
                        "structuredName": {
                            "firstName": "Wim",
                            "lastName": "Vanduffel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vanduffel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460061"
                        ],
                        "name": "G. Rizzolatti",
                        "slug": "G.-Rizzolatti",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Rizzolatti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rizzolatti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1826285"
                        ],
                        "name": "G. Orban",
                        "slug": "G.-Orban",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Orban",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orban"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18812275,
            "fieldsOfStudy": [
                "Biology",
                "Psychology",
                "Medicine"
            ],
            "id": "bfd8873ef9bb44e9c2a80d7e2fb60b889786023e",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Observation of actions performed by others activates monkey ventral premotor cortex, where action meaning, but not object identity, is coded. In a functional MRI (fMRI) study, we investigated whether other monkey frontal areas respond to actions performed by others. Observation of a hand grasping objects activated four frontal areas: rostral F5 and areas 45B, 45A, and 46. Observation of an individual grasping an object also activated caudal F5, which indicates different degrees of action abstraction in F5. Observation of shapes activated area 45, but not premotor F5. Convergence of object and action information in area 45 may be important for full comprehension of actions."
            },
            "slug": "Observing-Others:-Multiple-Action-Representation-in-Nelissen-Luppino",
            "title": {
                "fragments": [],
                "text": "Observing Others: Multiple Action Representation in the Frontal Lobe"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Investigation of whether other monkey frontal areas respond to actions performed by others activates monkey ventral premotor cortex, where action meaning, but not object identity, is coded."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2914469"
                        ],
                        "name": "V. Gallese",
                        "slug": "V.-Gallese",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Gallese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Gallese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824336"
                        ],
                        "name": "L. Fadiga",
                        "slug": "L.-Fadiga",
                        "structuredName": {
                            "firstName": "Luciano",
                            "lastName": "Fadiga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fadiga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2419400"
                        ],
                        "name": "L. Fogassi",
                        "slug": "L.-Fogassi",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Fogassi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fogassi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460061"
                        ],
                        "name": "G. Rizzolatti",
                        "slug": "G.-Rizzolatti",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Rizzolatti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rizzolatti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7520532,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0ef550dacb89fb655f252e5b17dbd5d643eb5ac1",
            "isKey": false,
            "numCitedBy": 4419,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We recorded electrical activity from 532 neurons in the rostral part of inferior area 6 (area F5) of two macaque monkeys. Previous data had shown that neurons of this area discharge during goal-directed hand and mouth movements. We describe here the properties of a newly discovered set of F5 neurons (\"mirror neurons', n = 92) all of which became active both when the monkey performed a given action and when it observed a similar action performed by the experimenter. Mirror neurons, in order to be visually triggered, required an interaction between the agent of the action and the object of it. The sight of the agent alone or of the object alone (three-dimensional objects, food) were ineffective. Hand and the mouth were by far the most effective agents. The actions most represented among those activating mirror neurons were grasping, manipulating and placing. In most mirror neurons (92%) there was a clear relation between the visual action they responded to and the motor response they coded. In approximately 30% of mirror neurons the congruence was very strict and the effective observed and executed actions corresponded both in terms of general action (e.g. grasping) and in terms of the way in which that action was executed (e.g. precision grip). We conclude by proposing that mirror neurons form a system for matching observation and execution of motor actions. We discuss the possible role of this system in action recognition and, given the proposed homology between F5 and human Brocca's region, we posit that a matching system, similar to that of mirror neurons exists in humans and could be involved in recognition of actions as well as phonetic gestures."
            },
            "slug": "Action-recognition-in-the-premotor-cortex.-Gallese-Fadiga",
            "title": {
                "fragments": [],
                "text": "Action recognition in the premotor cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proposed that mirror neurons form a system for matching observation and execution of motor actions, similar to that of mirror neurons exists in humans and could be involved in recognition of actions as well as phonetic gestures."
            },
            "venue": {
                "fragments": [],
                "text": "Brain : a journal of neurology"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938732"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36665589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79ad336e3ab6059c64af58b230c0058a4c46675f",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Motion-based recognition deals with the recognition of an object and/or its motion, based on motion in a series of images. In this approach, a sequence containing a large number of frames is used to extract motion information. The advantage is that a longer sequence leads to recognition of higher level motions, like walking or running, which consist of a complex and coordinated series of events. Unlike much previous research in motion, this approach does not require explicit reconstruction of shape from the images prior to recognition. This book provides the state-of-the-art in this rapidly developing discipline. It consists of a collection of invited chapters by leading researchers in the world covering various aspects of motion-based recognition including lipreading, gesture recognition, facial expression recognition, gait analysis, cyclic motion detection, and activity recognition. Audience: This volume will be of interest to researchers and post- graduate students whose work involves computer vision, robotics and image processing."
            },
            "slug": "Motion-Based-Recognition-Jain-Shah",
            "title": {
                "fragments": [],
                "text": "Motion-Based Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This book is a collection of invited chapters by leading researchers in the world covering various aspects of motion-based recognition including lipreading, gesture recognition, facial expression recognition, gait analysis, cyclic motion detection, and activity recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Imaging and Vision"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858702"
                        ],
                        "name": "A. Yilmaz",
                        "slug": "A.-Yilmaz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Yilmaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yilmaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18724425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "487c37dce9b93d48f753ab2ec3fc997edb5639ce",
            "isKey": false,
            "numCitedBy": 490,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose to model an action based on both the shape and the motion of the performing object. When the object performs an action in 3D, the points on the outer boundary of the object are projected as 2D (x, y) contour in the image plane. A sequence of such 2D contours with respect to time generates a spatiotemporal volume (STV) in (x, y, t), which can be treated as 3D object in the (x, y, t) space. We analyze STV by using the differential geometric surface properties to identify action descriptors capturing both spatial and temporal properties. A set of action descriptors is called an action sketch. The first step in our approach is to generate STV by solving the point correspondence problem between consecutive frames. The correspondences are determined using a two-step graph theoretical approach. After the STV is generated, actions descriptors are computed by analyzing the differential geometric properties of STV. Finally, using these descriptors, we perform action recognition, which is also formulated as graph theoretical problem. Several experimental results are presented to demonstrate our approach."
            },
            "slug": "Actions-sketch:-a-novel-action-representation-Yilmaz-Shah",
            "title": {
                "fragments": [],
                "text": "Actions sketch: a novel action representation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper proposes to model an action based on both the shape and the motion of the performing object, and generates STV by solving the point correspondence problem between consecutive frames using a two-step graph theoretical approach."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700569"
                        ],
                        "name": "T. Moeslund",
                        "slug": "T.-Moeslund",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Moeslund",
                            "middleNames": [
                                "Baltzer"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Moeslund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144046599"
                        ],
                        "name": "A. Hilton",
                        "slug": "A.-Hilton",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Hilton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48378203"
                        ],
                        "name": "V. Kr\u00fcger",
                        "slug": "V.-Kr\u00fcger",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Kr\u00fcger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kr\u00fcger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9815253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4e16d64b77d133b7382440a08091d64008dd923",
            "isKey": false,
            "numCitedBy": 2738,
            "numCiting": 520,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-of-advances-in-vision-based-human-motion-Moeslund-Hilton",
            "title": {
                "fragments": [],
                "text": "A survey of advances in vision-based human motion capture and analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6153430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5331349557fababfac48d47e49b44583e3bd5f6",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes. The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator. Each object category then has its own distribution over these parts, which are shared between objects. We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters. Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available. We also extend this hierarchical framework to scenes containing multiple objects"
            },
            "slug": "Learning-hierarchical-models-of-scenes,-objects,-Sudderth-Torralba",
            "title": {
                "fragments": [],
                "text": "Learning hierarchical models of scenes, objects, and parts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available and this hierarchical probabilistic model is extended to scenes containing multiple objects."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279024"
                        ],
                        "name": "D. Bub",
                        "slug": "D.-Bub",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bub",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143902101"
                        ],
                        "name": "M. Masson",
                        "slug": "M.-Masson",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Masson",
                            "middleNames": [
                                "E",
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Masson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5936974,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "10dd3568039d08816dd0bac65aa5714afcc5da3d",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Background: Theories of embodied knowledge argue that the representation and recruitment of motor processes may be important for deriving the meaning of many linguistic and perceptual elements. Aims: We examined the conditions under which gestural knowledge associated with manipulable objects is evoked. Methods & Procedures: A priming paradigm was used in which an object was presented in advance of a photograph of a hand gesture that participants were to mimic. On related trials, the target gesture was the same as the gesture typically used to interact with the object prime. On unrelated trials, the target gesture was not related to the object. In another set of experiments, a Stroop-like paradigm was used in which participants learned to produce manual responses to colour cues. After training, coloured photographs of manipulable objects were presented. The colour-cued gesture was either one typically used with the object or was unrelated to it. Outcomes & Results: In the priming experiments, response latencies were shorter in the related condition, but only when participants also made an identification response to the object prime. In the Stroop experiments, interference effects indicated that gestures to colour were affected by gestural knowledge associated with the object. Conclusions: These results indicate that conceptual representations of manipulable objects include specific forms of gestural knowledge that are automatically evoked when observers attend to an object."
            },
            "slug": "Gestural-knowledge-evoked-by-objects-as-part-of-Bub-Masson",
            "title": {
                "fragments": [],
                "text": "Gestural knowledge evoked by objects as part of conceptual representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3306372"
                        ],
                        "name": "S. Vitaladevuni",
                        "slug": "S.-Vitaladevuni",
                        "structuredName": {
                            "firstName": "Shiv",
                            "lastName": "Vitaladevuni",
                            "middleNames": [
                                "Naga",
                                "Prasad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vitaladevuni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942975"
                        ],
                        "name": "Vili Kellokumpu",
                        "slug": "Vili-Kellokumpu",
                        "structuredName": {
                            "firstName": "Vili",
                            "lastName": "Kellokumpu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vili Kellokumpu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Using features such as time to accelerate, peak velocity, and magnitude of acceleration and deceleration, the likelihoods of reach movements can be computed from hand trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "While one can use independent models for tracking the two hands, this could lead to identity exchange and lost tracks during occlusions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7733292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7379f94cf61a58aed672e2cb3d311570169912de",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Common movements like reaching, striking, etc. observed during surveillance have highly variable target locations. This puts appearance-based techniques at a disadvantage for modelling and recognizing them. Psychological studies indicate that these actions are ballistic in nature. Their trajectories have simple structures and are determined to a great degree by the starting and ending positions. We present an approach for movement recognition that explicitly considers their ballistic nature. This enables the decoupling of recognition from the movement's trajectory, allowing generalization over a range of target-positions. A given movement is first analyzed to determine if it is ballistic. Ballistic movements are further classified into reaching, striking, etc. The proposed approach was tested with motion capture data obtained from the CMU MoCap database"
            },
            "slug": "Ballistic-Hand-Movements-Vitaladevuni-Kellokumpu",
            "title": {
                "fragments": [],
                "text": "Ballistic Hand Movements"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents an approach for movement recognition that explicitly considers their ballistic nature, enabling the decoupling of recognition from the movement's trajectory, allowing generalization over a range of target-positions."
            },
            "venue": {
                "fragments": [],
                "text": "AMDO"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460061"
                        ],
                        "name": "G. Rizzolatti",
                        "slug": "G.-Rizzolatti",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Rizzolatti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rizzolatti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824336"
                        ],
                        "name": "L. Fadiga",
                        "slug": "L.-Fadiga",
                        "structuredName": {
                            "firstName": "Luciano",
                            "lastName": "Fadiga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fadiga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2914469"
                        ],
                        "name": "V. Gallese",
                        "slug": "V.-Gallese",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Gallese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Gallese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2419400"
                        ],
                        "name": "L. Fogassi",
                        "slug": "L.-Fogassi",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Fogassi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fogassi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18998331,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "031acf0078a4e1b5343939ce07acda6a7d795a07",
            "isKey": false,
            "numCitedBy": 4387,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Premotor-cortex-and-the-recognition-of-motor-Rizzolatti-Fadiga",
            "title": {
                "fragments": [],
                "text": "Premotor cortex and the recognition of motor actions."
            },
            "venue": {
                "fragments": [],
                "text": "Brain research. Cognitive brain research"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143847264"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "The cortical responses for goal directed actions are different from the responses evoked when the same action is executed but without the presence of the object."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de1ac0baea1f907388ff5fe9fa22f25f406e2ca6",
            "isKey": false,
            "numCitedBy": 881,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a method for human detection in crowded scene from static images. An individual human is modeled as an assembly of natural body parts. We introduce edgelet features, which are a new type of silhouette oriented features. Part detectors, based on these features, are learned by a boosting method. Responses of part detectors are combined to form a joint likelihood model that includes cases of multiple, possibly inter-occluded humans. The human detection problem is formulated as maximum a posteriori (MAP) estimation. We show results on a commonly used previous dataset as well as new data sets that could not be processed by earlier methods."
            },
            "slug": "Detection-of-multiple,-partially-occluded-humans-in-Wu-Nevatia",
            "title": {
                "fragments": [],
                "text": "Detection of multiple, partially occluded humans in a single image by Bayesian combination of edgelet part detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The human detection problem is formulated as maximum a posteriori (MAP) estimation, and edgelet features are introduced, which are a new type of silhouette oriented features that are learned by a boosting method."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143891655"
                        ],
                        "name": "Hao Jiang",
                        "slug": "Hao-Jiang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680726"
                        ],
                        "name": "M. S. Drew",
                        "slug": "M.-S.-Drew",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Drew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S. Drew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051053953"
                        ],
                        "name": "Ze-Nian Li",
                        "slug": "Ze-Nian-Li",
                        "structuredName": {
                            "firstName": "Ze-Nian",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ze-Nian Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1878790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3bf24beedff1fd3128e37d17a2777b93ec56a5e",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider the problem of describing the action being performed by human figures in still images. We will attack this problem using an unsupervised learning approach, attempting to discover the set of action classes present in a large collection of training images. These action classes will then be used to label test images. Our approach uses the coarse shape of the human figures to match pairs of images. The distance between a pair of images is computed using a linear programming relaxation technique. This is a computationally expensive process, and we employ a fast pruning method to enable its use on a large collection of images. Spectral clustering is then performed using the resulting distances. We present clustering and image labeling results on a variety of datasets."
            },
            "slug": "Unsupervised-Discovery-of-Action-Classes-Wang-Jiang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Discovery of Action Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper will attack the problem of describing the action being performed by human figures in still images using an unsupervised learning approach, attempting to discover the set of action classes present in a large collection of training images."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5540810"
                        ],
                        "name": "Patric Bach",
                        "slug": "Patric-Bach",
                        "structuredName": {
                            "firstName": "Patric",
                            "lastName": "Bach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patric Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2488692"
                        ],
                        "name": "G. Knoblich",
                        "slug": "G.-Knoblich",
                        "structuredName": {
                            "firstName": "G\u00fcnther",
                            "lastName": "Knoblich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Knoblich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10601104"
                        ],
                        "name": "T. Gunter",
                        "slug": "T.-Gunter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Gunter",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gunter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2671714"
                        ],
                        "name": "A. Friederici",
                        "slug": "A.-Friederici",
                        "structuredName": {
                            "firstName": "Angela",
                            "lastName": "Friederici",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Friederici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144374107"
                        ],
                        "name": "W. Prinz",
                        "slug": "W.-Prinz",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Prinz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Prinz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14517347,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "09b8423c75f7f0b89e7bb548b66bf6fb75a73bf2",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "A perceived action can be understood only when information about the action carried out and the objects used are taken into account. It was investigated how spatial and functional information contributes to establishing these relations. Participants observed static frames showing a hand wielding an instrument and a potential target object of the action. The 2 elements could either match or mismatch, spatially or functionally. Participants were required to judge only 1 of the 2 relations while ignoring the other. Both irrelevant spatial and functional mismatches affected judgments of the relevant relation. Moreover, the functional relation provided a context for the judgment of the spatial relation but not vice versa. The results are discussed in respect to recent accounts of action understanding."
            },
            "slug": "Action-comprehension:-deriving-spatial-and-Bach-Knoblich",
            "title": {
                "fragments": [],
                "text": "Action comprehension: deriving spatial and functional relations."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It was investigated how spatial and functional information contributes to establishing these relations and found that the functional relation provided a context for the judgment of the spatial relation but not vice versa."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Human perception and performance"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "With the same neurons involved in execution and perception, a link between object recognition and action understanding has been established [38] in humans."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2461997,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adb6e3edb417206f15e76da3c5833ec4134dc6ca",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufficient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or \"informative\" background(context). We present a \"shape-aware\" model which utilizes contour information for efficient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity."
            },
            "slug": "A-\"Shape-Aware\"-Model-for-semi-supervised-Learning-Gupta-Shi",
            "title": {
                "fragments": [],
                "text": "A \"Shape Aware\" Model for semi-supervised Learning of Objects and its Context"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work argues that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufficient for representing context, and presents a \"shape-aware\" model which utilizes contour information for efficient and accurate labeling of features in the image."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Each scene object node corresponds to a class of scene objects and is represented by the probability of presence of that object class across the image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11664336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "869171b2f56cfeaa9b81b2626cb4956fea590a57",
            "isKey": false,
            "numCitedBy": 6523,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."
            },
            "slug": "Modeling-the-Shape-of-the-Scene:-A-Holistic-of-the-Oliva-Torralba",
            "title": {
                "fragments": [],
                "text": "Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744602"
                        ],
                        "name": "Y. Kuniyoshi",
                        "slug": "Y.-Kuniyoshi",
                        "structuredName": {
                            "firstName": "Yasuo",
                            "lastName": "Kuniyoshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kuniyoshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9714706"
                        ],
                        "name": "M. Shimozaki",
                        "slug": "M.-Shimozaki",
                        "structuredName": {
                            "firstName": "Moriaki",
                            "lastName": "Shimozaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shimozaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62447254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31a00715d14d1d3a4754cee24bf2a1dc986353bd",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An artificial neural network model for visual recognition of actions is proposed. In contrast with the existing gesture recognition systems, our model learns to recognize \"true\" actions, i.e. object-directed actions with causal chains of events, such as \"He threw the ball at the window and broke it\". The core mechanism of our model consists of a triplet of parallel self-organizing networks; The first pair of networks learn and recognize \"spatial relationship\" and \"movement patterns\", whose output is integrated by the third \"temporal context\" network."
            },
            "slug": "A-self-organizing-neural-model-for-context-based-Kuniyoshi-Shimozaki",
            "title": {
                "fragments": [],
                "text": "A self-organizing neural model for context-based action recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An artificial neural network model for visual recognition of actions that learns to recognize \"true\" actions, i.e. object-directed actions with causal chains of events, such as \"He threw the ball at the window and broke it\"."
            },
            "venue": {
                "fragments": [],
                "text": "First International IEEE EMBS Conference on Neural Engineering, 2003. Conference Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82910116"
                        ],
                        "name": "H. Murase",
                        "slug": "H.-Murase",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Murase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "With the same neurons involved in execution and perception, a link between object recognition and action understanding has been established [38] in humans."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15455066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "024fb7a47f121d93415f8e2be6343c5975db90de",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of automatically learning object models for recognition and pose estimation. In contrast to the traditional approach, we formulate the recognition problem as one of matching visual appearance than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illummation conditions. While shape and reflectance are intrinsic properties of an object and are constant, pose and illumination vary from scene to scene. We present a new compact representation of object appearance that is parametrized by pose and illumination. For each object of interest, a large set of Images is obtained by automatically varying pose and illumination. This large image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the object is represented as a hypersurface. Given an unknown input image, the recognition system projects the image onto the eigenspace. The object is recognized based on the hypersurface it lies on. The exact position of the projection on the hypersurface determines the object's pose in the image. We have conducted experiments usmg several objects with complex appearance characteristics. These results suggest the proposed appearance representation to be a valuable tool for a variety of machine vision applications."
            },
            "slug": "Learning-Object-Models-from-Appearance-Murase-Nayar",
            "title": {
                "fragments": [],
                "text": "Learning Object Models from Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents a new compact representation of object appearance that is parametrized by pose and illumination and suggests the proposed appearance representation to be a valuable tool for a variety of machine vision applications."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145961735"
                        ],
                        "name": "L. Chao",
                        "slug": "L.-Chao",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Chao",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404253"
                        ],
                        "name": "Alex Martin",
                        "slug": "Alex-Martin",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Martin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Chao and Martin [8] showed that when\nhumans see manipulable objects, there is cortical activity in the region that corresponds to action execution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14550376,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "fa1fd612d16eb3b08c81924044322bb2af89bf7b",
            "isKey": false,
            "numCitedBy": 1200,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We used fMRI to examine the neural response in frontal and parietal cortices associated with viewing and naming pictures of different categories of objects. Because tools are commonly associated with specific hand movements, we predicted that pictures of tools, but not other categories of objects, would elicit activity in regions of the brain that store information about motor-based properties. We found that viewing and naming pictures of tools selectively activated the left ventral premotor cortex (BA 6). Single-unit recording studies in monkeys have shown that neurons in the rostral part of the ventral premotor cortex (canonical F5 neurons) respond to the visual presentation of graspable objects, even in the absence of any subsequent motor activity. Thus, the left ventral premotor region that responded selectively to tools in the current study may be the human homolog of the monkey canonical F5 area. Viewing and naming tools also selectively activated the left posterior parietal cortex (BA 40). This response is similar to the firing of monkey anterior intraparietal neurons to the visual presentation of graspable objects. In humans and monkeys, there appears to be a close link between manipulable objects and information about the actions associated with their use. The selective activation of the left posterior parietal and left ventral premotor cortices by pictures of tools suggests that the ability to recognize and identify at least one category of objects (tools) may depend on activity in specific sites of the ventral and dorsal visual processing streams."
            },
            "slug": "Representation-of-Manipulable-Man-Made-Objects-in-Chao-Martin",
            "title": {
                "fragments": [],
                "text": "Representation of Manipulable Man-Made Objects in the Dorsal Stream"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The left ventral premotor region that responded selectively to tools in the current study may be the human homolog of the monkey canonical F5 area, which responds to the visual presentation of graspable objects."
            },
            "venue": {
                "fragments": [],
                "text": "NeuroImage"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29264,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "If a detector consists of L levels, but only the first lw levels classify a window w as containing an object, then the overall likelihood is approximated by\nP \u00f0O \u00bc fobj; wgjeO\u00de Ylw\ni\u00bc1 Pi\u00f0w\u00de\nYL\nj\u00bclw\u00fe1 \u00f0Ptj\u00de: \u00f01\u00de"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2277383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd9ab441df8b24f473a3635370c69620b00c1e60",
            "isKey": false,
            "numCitedBy": 2423,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images."
            },
            "slug": "Pictorial-Structures-for-Object-Recognition-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Pictorial Structures for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A computationally efficient framework for part-based modeling and recognition of objects, motivated by the pictorial structure models introduced by Fischler and Elschlager, that allows for qualitative descriptions of visual appearance and is suitable for generic recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697005"
                        ],
                        "name": "L. Vaina",
                        "slug": "L.-Vaina",
                        "structuredName": {
                            "firstName": "Lucia",
                            "lastName": "Vaina",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vaina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710160"
                        ],
                        "name": "M. Jaulent",
                        "slug": "M.-Jaulent",
                        "structuredName": {
                            "firstName": "Marie-Christine",
                            "lastName": "Jaulent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jaulent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21139585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f763ac19d50f02eabb86520408a31001ab56a27",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "For an intelligent entity to carry out tasks in the real world, perceived three\u2010dimensional shapes are transformed into objects identified by their functional category which makes explicit the roles or uses of objects in actions. This study describes an approach to the recognition of functions that combines ideas about representations of shapes, concepts, and object categories, with goal requirements of actions. A particular conceptual model of the compatibility between objects and actions is introduced, the outline of the solution is given, and the experimental domain of hand actions and of objects useful in such actions is described; the solution is currently under implementation and computational verifications. the article is organized as follows: first, the computational definition of the problem of functional recognition and a comprehensive theoretical framework for it is given and, second, the relation between primary functions of hand\u2010manipulable objects and auxillary functions is discussed and modeled in the framework of fuzzy sets and possibility theory."
            },
            "slug": "Object-structure-and-action-requirements:-A-model-Vaina-Jaulent",
            "title": {
                "fragments": [],
                "text": "Object structure and action requirements: A compatibility model for functional recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This study describes an approach to the recognition of functions that combines ideas about representations of shapes, concepts, and object categories, with goal requirements of actions, in the framework of fuzzy sets and possibility theory."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Intell. Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13251789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e523721feebeaee18e487607b7d0920ac6cd3b4",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual classifiers for object recognition from weakly labeled data requires determining correspondence between image regions and semantic object classes. Most approaches use co-occurrence of \"nouns\" and image features over large datasets to determine the correspondence, but many correspondence ambiguities remain. We further constrain the correspondence problem by exploiting additional language constructs to improve the learning process from weakly labeled data. We consider both \"prepositions\" and \"comparative adjectives\" which are used to express relationships between objects. If the models of such relationships can be determined, they help resolve correspondence ambiguities. However, learning models of these relationships requires solving the correspondence problem. We simultaneously learn the visual features defining \"nouns\" and the differential visual features defining such \"binary-relationships\" using an EM-based approach."
            },
            "slug": "Beyond-Nouns:-Exploiting-Prepositions-and-for-Gupta-Davis",
            "title": {
                "fragments": [],
                "text": "Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning Visual Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work simultaneously learns the visual features defining \"nouns\" and the differentialVisual features defining such \"binary-relationships\" using an EM-based approach and constrain the correspondence problem by exploiting additional language constructs to improve the learning process from weakly labeled data."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50853059"
                        ],
                        "name": "Anurag Mittal",
                        "slug": "Anurag-Mittal",
                        "structuredName": {
                            "firstName": "Anurag",
                            "lastName": "Mittal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anurag Mittal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "If a detector consists of L levels, but only the first lw levels classify a window w as containing an object, then the overall likelihood is approximated by\nP \u00f0O \u00bc fobj; wgjeO\u00de Ylw\ni\u00bc1 Pi\u00f0w\u00de\nYL\nj\u00bclw\u00fe1 \u00f0Ptj\u00de: \u00f01\u00de"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5557711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb71b12eb985ffac8a518fcec3f343785b7f224b",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic initialization and tracking of human pose is an important task in visual surveillance. We present a part-based approach that incorporates a variety of constraints in a unified framework. These constraints include the kinematic constraints between parts that are physically connected to each other, the occlusion of one part by another, and the high correlation between the appearance of certain parts, such as the arms. The location probability distribution of each part is determined by evaluating appropriate likelihood measures. The graphical (nontree) structure representing the interdependencies between parts is utilized to \"connect\" such part distributions via nonparametric belief propagation. Methods are also developed to perform this optimization efficiently in the large space of pose configurations."
            },
            "slug": "Constraint-Integration-for-Efficient-Multiview-Pose-Gupta-Mittal",
            "title": {
                "fragments": [],
                "text": "Constraint Integration for Efficient Multiview Pose Estimation with Self-Occlusions"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work presents a part-based approach that incorporates a variety of constraints in a unified framework that incorporates the kinematic constraints between parts that are physically connected to each other, the occlusion of one part by another, and the high correlation between the appearance of certain parts, such as the arms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2318192"
                        ],
                        "name": "Zoran Duric",
                        "slug": "Zoran-Duric",
                        "structuredName": {
                            "firstName": "Zoran",
                            "lastName": "Duric",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoran Duric"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1987067"
                        ],
                        "name": "J. Fayman",
                        "slug": "J.-Fayman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Fayman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fayman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Recognition rates of target objects were higher when the priming object was used in a similar action as the target object."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11205635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db84b1133aa92773f4cc7814aa314cf76410bb05",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In order for a robot to operate autonomously in its environment, it must be able to perceive its environment and take actions based on these perceptions. Recognizing the functionalities of objects is an important component of this ability. In this paper, we look into a new area of functionality recognition: determining the function of an object from its motion. Given a sequence of images of a known object performing some function, we attempt to determine what that function is. We show that the motion of an object, when combined with information about the object and its normal uses, provides us with strong constraints on possible functions that the object might be performing."
            },
            "slug": "Function-From-Motion-Duric-Fayman",
            "title": {
                "fragments": [],
                "text": "Function From Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the motion of an object, when combined with information about the object and its normal uses, provides us with strong constraints on possible functions that the object might be performing."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403834669"
                        ],
                        "name": "Gutemberg Guerra-Filho",
                        "slug": "Gutemberg-Guerra-Filho",
                        "structuredName": {
                            "firstName": "Gutemberg",
                            "lastName": "Guerra-Filho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gutemberg Guerra-Filho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3415312"
                        ],
                        "name": "Cornelia Fermuller",
                        "slug": "Cornelia-Fermuller",
                        "structuredName": {
                            "firstName": "Cornelia",
                            "lastName": "Fermuller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cornelia Fermuller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697493"
                        ],
                        "name": "Y. Aloimonos",
                        "slug": "Y.-Aloimonos",
                        "structuredName": {
                            "firstName": "Yiannis",
                            "lastName": "Aloimonos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aloimonos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12179082,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7734696e6ff12c81f61b1ccac9085654fa3a25e1",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a roadmap to a Human Activity Language (HAL) for symbolic manipulation of visual and motor information in a sensory-motor system model. The visual perception subsystem translates a visual representation of action into our visuo-motor language. One instance of this perception process could be achieved by a Motion Capture system. We captured almost 90 different human actions in order to have empirical data that could validate and support our embodied language for movement and activity. The embodiment of the language serves as the interface between visual perception and the motor subsystem. The visuomotor language is defined using a linguistic approach. In phonology, we define basic atomic segments that are used to compose human activity. Phonological rules are modeled as a finite automaton. In morphology, we study how visuomotor phonemes are combined to form strings representing human activity and to generate a higher-level morphological grammar. This compact grammar suggests the existence of lexical units working as visuo-motor subprograms. In syntax, we present a model for visuo-motor sentence construction where the subject corresponds to the active joints (noun) modified by a posture (adjective). A verbal phrase involves the representation of the human activity (verb) and timing coordination among different joints (adverb)."
            },
            "slug": "Discovering-a-Language-for-Human-Activity-1-Guerra-Filho-Fermuller",
            "title": {
                "fragments": [],
                "text": "Discovering a Language for Human Activity 1"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A roadmap to a Human Activity Language (HAL) for symbolic manipulation of visual and motor information in a sensory-motor system model and a model for visuomotor sentence construction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6909858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef4209ed288ef38fecdfae2409bce78633386c10",
            "isKey": false,
            "numCitedBy": 821,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a first attempt to classify events in static images by integrating scene and object categorizations. We define an event in a static image as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. Our goal is to classify the event in the image as well as to provide a number of semantic labels to the objects and scene environment within the image. For example, given a rowing scene, our algorithm recognizes the event as rowing by classifying the environment as a lake and recognizing the critical objects in the image as athletes, rowing boat, water, etc. We achieve this integrative and holistic recognition through a generative graphical model. We have assembled a highly challenging database of 8 widely varied sport events. We show that our system is capable of classifying these event classes at 73.4% accuracy. While each component of the model contributes to the final recognition, using scene or objects alone cannot achieve this performance."
            },
            "slug": "What,-where-and-who-Classifying-events-by-scene-and-Li-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "What, where and who? Classifying events by scene and object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper uses a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification and proposes a first attempt to classify events in static images by integrating scene and object categorizations."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984136"
                        ],
                        "name": "A. Agarwal",
                        "slug": "A.-Agarwal",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "This test density is compared to the color model of every object category using the Kullback-Leibler distance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 302682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3845d9e62540b8e2406343f801e026b562299ae0",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a learning based method for recovering 3D human body pose from single images and monocular image sequences. Our approach requires neither an explicit body model nor prior labelling of body pans in the image. Instead, it recovers pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes. For robustness against local silhouette segmentation errors, silhouette shape is encoded by histogram-of-shape-contexts descriptors. For the main regression, we evaluate both regularized least squares and relevance vector machine (RVM) regressors over both linear and kernel bases. The RVM's provide much sparser regressors without compromising performance, and kernel bases give a small but worthwhile improvement in performance. For realism and good generalization with respect to viewpoints, we train the regressors on images resynthesized from real human motion capture data, and test it both quantitatively on similar independent test data, and qualitatively on a real image sequence. Mean angular errors of 6-7 degrees are obtained - a factor of 3 better than the current state of the art for the much simpler upper body problem."
            },
            "slug": "3D-human-pose-from-silhouettes-by-relevance-vector-Agarwal-Triggs",
            "title": {
                "fragments": [],
                "text": "3D human pose from silhouettes by relevance vector regression"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work describes a learning based method for recovering 3D human body pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes, and results are a factor of 3 better than the current state of the art for the much simpler upper body problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46597039"
                        ],
                        "name": "P. Sinha",
                        "slug": "P.-Sinha",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sinha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9982531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "There is general consensus that context can be a rich source of information about an object's identity, location and scale. However the issue of how to formalize centextual influences is still largely open. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties. We represent global context information in terms of the spatial layout of spectral components. The resulting scheme serves as an effective procedure for context driven focus of attention and scale-selection on real-world scenes. Based on a simple holistic analysis of an image, the scheme is able to accurately predict object locations and sizes."
            },
            "slug": "Statistical-context-priming-for-object-detection-Torralba-Sinha",
            "title": {
                "fragments": [],
                "text": "Statistical Context Priming for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple probabilistic framework for modeling the relationship between context and object properties is introduced, representing global context information in terms of the spatial layout of spectral components and serving as an effective procedure for context driven focus of attention and scale-selection on real-world scenes."
            },
            "venue": {
                "fragments": [],
                "text": "ICCV"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775451"
                        ],
                        "name": "Z. Kourtzi",
                        "slug": "Z.-Kourtzi",
                        "structuredName": {
                            "firstName": "Zoe",
                            "lastName": "Kourtzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Kourtzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "We present a Bayesian approach for the interpretation of human-object interactions, that integrates information from perceptual tasks such as scene analysis, human motion/pose estimation,1 manipulable object detection, and \u201cobject reaction\u201d determination.2 While each of these tasks can be conducted\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1981175,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "5949dc83c70a5525db73deb8b8bf924208153b57",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\u2018But-still,-it-moves\u2019-Kourtzi",
            "title": {
                "fragments": [],
                "text": "\u2018But still, it moves\u2019"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11573252"
                        ],
                        "name": "T. Chen",
                        "slug": "T.-Chen",
                        "structuredName": {
                            "firstName": "Trista",
                            "lastName": "Chen",
                            "middleNames": [
                                "Pei-chun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35709566"
                        ],
                        "name": "Francine R. Chen",
                        "slug": "Francine-R.-Chen",
                        "structuredName": {
                            "firstName": "Francine",
                            "lastName": "Chen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francine R. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145698468"
                        ],
                        "name": "Don Kimber",
                        "slug": "Don-Kimber",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Kimber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don Kimber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7472216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7c22e1267b590f0f7dbb25cd28d2a6340c5fc49",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Current approaches to pose estimation and tracking can be classified into two categories: generative and discriminative. While generative approaches can accurately determine human pose from image observations, they are computationally expensive due to search in the high dimensional human pose space. On the other hand, discriminative approaches do not generalize well, but are computationally efficient. We present a hybrid model that combines the strengths of the two in an integrated learning and inference framework. We extend the Gaussian process latent variable model (GPLVM) to include an embedding from observation space (the space of image features) to the latent space. GPLVM is a generative model, but the inclusion of this mapping provides a discriminative component, making the model observation driven. Observation Driven GPLVM (OD-GPLVM) not only provides a faster inference approach, but also more accurate estimates (compared to GPLVM) in cases where dynamics are not sufficient for the initialization of search in the latent space. We also extend OD-GPLVM to learn and estimate poses from parameterized actions/gestures. Parameterized gestures are actions which exhibit large systematic variation in joint angle space for different instances due to difference in contextual variables. For example, the joint angles in a forehand tennis shot are function of the height of the ball (Figure 2). We learn these systematic variations as a function of the contextual variables. We then present an approach to use information from scene/objects to provide context for human pose estimation for such parameterized actions."
            },
            "slug": "Context-and-observation-driven-latent-variable-for-Gupta-Chen",
            "title": {
                "fragments": [],
                "text": "Context and observation driven latent variable model for human pose estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work extends the Gaussian process latent variable model (GPLVM) to include an embedding from observation space (the space of image features) to the latent space, and presents a hybrid model that combines the strengths of the two in an integrated learning and inference framework."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398347979"
                        ],
                        "name": "Manuel J. Mar\u00edn-Jim\u00e9nez",
                        "slug": "Manuel-J.-Mar\u00edn-Jim\u00e9nez",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Mar\u00edn-Jim\u00e9nez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manuel J. Mar\u00edn-Jim\u00e9nez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Using this approach, however, we are unable to distinguish between objects that have the same shape but a different dominant color; for example, a cricket ball (often red or white in color) as opposed to a tennis ball (often yellow in color)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2845360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7b6bd15f32ec49906e3500cac1abd7ed6a7c01a",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this paper is to estimate 2D human pose as a spatial configuration of body parts in TV and movie video shots. Such video material is uncontrolled and extremely challenging. We propose an approach that progressively reduces the search space for body parts, to greatly improve the chances that pose estimation will succeed. This involves two contributions: (i) a generic detector using a weak model of pose to substantially reduce the full pose search space; and (ii) employing 'grabcut' initialized on detected regions proposed by the weak model, to further prune the search space. Moreover, we also propose (Hi) an integrated spatio- temporal model covering multiple frames to refine pose estimates from individual frames, with inference using belief propagation. The method is fully automatic and self-initializing, and explains the spatio-temporal volume covered by a person moving in a shot, by soft-labeling every pixel as belonging to a particular body part or to the background. We demonstrate upper-body pose estimation by an extensive evaluation over 70000 frames from four episodes of the TV series Buffy the vampire slayer, and present an application to full- body action recognition on the Weizmann dataset."
            },
            "slug": "Progressive-search-space-reduction-for-human-pose-Ferrari-Mar\u00edn-Jim\u00e9nez",
            "title": {
                "fragments": [],
                "text": "Progressive search space reduction for human pose estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach that progressively reduces the search space for body parts, to greatly improve the chances that pose estimation will succeed, and an integrated spatio- temporal model covering multiple frames to refine pose estimates from individual frames, with inference using belief propagation."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145771244"
                        ],
                        "name": "Andrew D. Wilson",
                        "slug": "Andrew-D.-Wilson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Wilson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew D. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7469544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d838b7e02ecfafdd1ed81ac0f70d9996b4bdf20",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for the representation, recognition, and interpretation of parameterized gesture is presented. By parameterized gesture we mean gestures that exhibit a systematic spatial variation; one example is a point gesture where the relevant parameter is the two-dimensional direction. Our approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states. Using a linear model of dependence, we formulate an expectation-maximization (EM) method for training the parametric HMM. During testing, a similar EM algorithm simultaneously maximizes the output likelihood of the PHMM for the given sequence and estimates the quantifying parameters. Using visually derived and directly measured three-dimensional hand position measurements as input, we present results that demonstrate the recognition superiority of the PHMM over standard HMM techniques, as well as greater robustness in parameter estimation with respect to noise in the input features. Finally, we extend the PHMM to handle arbitrary smooth (nonlinear) dependencies. The nonlinear formulation requires the use of a generalized expectation-maximization (GEM) algorithm for both training and the simultaneous recognition of the gesture and estimation of the value of the parameter. We present results on a pointing gesture, where the nonlinear approach permits the natural spherical coordinate parameterization of pointing direction."
            },
            "slug": "Parametric-Hidden-Markov-Models-for-Gesture-Wilson-Bobick",
            "title": {
                "fragments": [],
                "text": "Parametric Hidden Markov Models for Gesture Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states by forming an expectation-maximization (EM) method for training the parametric HMM."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1954793"
                        ],
                        "name": "C. Galleguillos",
                        "slug": "C.-Galleguillos",
                        "structuredName": {
                            "firstName": "Carolina",
                            "lastName": "Galleguillos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Galleguillos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766844"
                        ],
                        "name": "Eric Wiewiora",
                        "slug": "Eric-Wiewiora",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wiewiora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Wiewiora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 749550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4d13788112f0fec457d31e1f7de9a53bbcec8e6",
            "isKey": false,
            "numCitedBy": 717,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In the task of visual object categorization, semantic context can play the very important role of reducing ambiguity in objects' visual appearance. In this work we propose to incorporate semantic object context as a post-processing step into any off-the-shelf object categorization model. Using a conditional random field (CRF) framework, our approach maximizes object label agreement according to contextual relevance. We compare two sources of context: one learned from training data and another queried from Google Sets. The overall performance of the proposed framework is evaluated on the PASCAL and MSRC datasets. Our findings conclude that incorporating context into object categorization greatly improves categorization accuracy."
            },
            "slug": "Objects-in-Context-Rabinovich-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to incorporate semantic object context as a post-processing step into any off-the-shelf object categorization model using a conditional random field (CRF) framework, which maximizes object label agreement according to contextual relevance."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808816"
                        ],
                        "name": "Jianxin Wu",
                        "slug": "Jianxin-Wu",
                        "structuredName": {
                            "firstName": "Jianxin",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxin Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754526"
                        ],
                        "name": "Adebola Osuntogun",
                        "slug": "Adebola-Osuntogun",
                        "structuredName": {
                            "firstName": "Adebola",
                            "lastName": "Osuntogun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adebola Osuntogun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144075899"
                        ],
                        "name": "Tanzeem Choudhury",
                        "slug": "Tanzeem-Choudhury",
                        "structuredName": {
                            "firstName": "Tanzeem",
                            "lastName": "Choudhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanzeem Choudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3041721"
                        ],
                        "name": "Matthai Philipose",
                        "slug": "Matthai-Philipose",
                        "structuredName": {
                            "firstName": "Matthai",
                            "lastName": "Philipose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthai Philipose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10822863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7e13004dcc904591ade1f471b09a86870da0b69",
            "isKey": false,
            "numCitedBy": 360,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to activity recognition based on detecting and analyzing the sequence of objects that are being manipulated by the user. In domains such as cooking, where many activities involve similar actions, object-use information can be a valuable cue. In order for this approach to scale to many activities and objects, however, it is necessary to minimize the amount of human-labeled data that is required for modeling. We describe a method for automatically acquiring object models from video without any explicit human supervision. Our approach leverages sparse and noisy readings from RFID tagged objects, along with common-sense knowledge about which objects are likely to be used during a given activity, to bootstrap the learning process. We present a dynamic Bayesian network model which combines RFID and video data to jointly infer the most likely activity and object labels. We demonstrate that our approach can achieve activity recognition rates of more than 80% on a real-world dataset consisting of 16 household activities involving 33 objects with significant background clutter. We show that the combination of visual object recognition with RFID data is significantly more effective than the RFID sensor alone. Our work demonstrates that it is possible to automatically learn object models from video of household activities and employ these models for activity recognition, without requiring any explicit human labeling."
            },
            "slug": "A-Scalable-Approach-to-Activity-Recognition-based-Wu-Osuntogun",
            "title": {
                "fragments": [],
                "text": "A Scalable Approach to Activity Recognition based on Object Use"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is demonstrated that it is possible to automatically learn object models from video of household activities and employ these models for activity recognition, without requiring any explicit human labeling."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5215202"
                        ],
                        "name": "R. Marteniuk",
                        "slug": "R.-Marteniuk",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Marteniuk",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marteniuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154341"
                        ],
                        "name": "C. MacKenzie",
                        "slug": "C.-MacKenzie",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "MacKenzie",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. MacKenzie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2581461"
                        ],
                        "name": "M. Jeannerod",
                        "slug": "M.-Jeannerod",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Jeannerod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jeannerod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795438"
                        ],
                        "name": "S. Ath\u00e8nes",
                        "slug": "S.-Ath\u00e8nes",
                        "structuredName": {
                            "firstName": "Sylvie",
                            "lastName": "Ath\u00e8nes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ath\u00e8nes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4041451"
                        ],
                        "name": "C. Dugas",
                        "slug": "C.-Dugas",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Dugas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dugas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "We implemented a variant of [12] for estimating the 2D pose of the upper body."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5485220,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b2efa997cc8550eeb166423de4e3c6996461688b",
            "isKey": false,
            "numCitedBy": 682,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "AUSTKA< T The underlying pirn-esses in movement organization and control were studied by varying the conditions under which arm movements were made. The three-dimensional movement trajectories of the following conditions were contrasted: pointing to a target with the index linger versus grasping a disk the same size as the target, grasping a fragile object versus a soil resilient object, and grasping a disk either to throw into a large box or place into a light litting well. Results showed that the arm trajectories, as represented by the resultant velocity profile of the wrist, varied considerably in their shape with the main factor being when peak velocity was reached as a function of the total duration of the movement. It appeared that when task demands required greater precision, the main deceleration phase of the trajectory was increased in duration. These results do not support a movement production mechanism that has access to an abstract representation of a base velocity profile and that creates trajectories by a simple scaling procedure in the temporal domain. Rather, the results support a view of movement production as relatively specific to the past experience of the performer and the constraints of the task. Ix's processus a la base du controlc el de ('organisation du mouvement out etc etudies en variant les conditions sous lesquelles les mouvements du bras etaient executes. Les trajectoires de mouvemenl tridimcnsionncl des conditions suivantes etaient niises en contraste: pointer une cible avee l'index versus saisir un disque de la meme taille que la cible; saisir un objet fragile versus un object elastique. mou; ct, saisir un disque suit pour le lancer dans une grnnde boitc soit le placer dans un puits bien ajustc. Les resultats indiquent que les trajectoires du bras, lelles que represenlees par le prolil de velocite resultant du poignel, varicnt considerablement dans leur forme avec le facteur principal apparaissant au moment ou 1c pic de velocite etait atteinl en tant que function de la duiee tolale du mouvement. Lorsque la tache requiert line plus grande precision la phase de deceleration principale dc la trajecloire augmente en duree. Cos resultats n'appuient pas un mccanisme dc production du mouvemenl ayant acccs a line representation abslraitc d'un prolil dc velocite dc base et creant des trajectoires par un procedc d'echelonncment simple dans le domaine temporcl. Cos resultats supported! plulol I \"idee d'unc production de mouvement coiiunc relativement specilique ii lexperience passcc du \u2026"
            },
            "slug": "Constraints-on-human-arm-movement-trajectories.-Marteniuk-MacKenzie",
            "title": {
                "fragments": [],
                "text": "Constraints on human arm movement trajectories."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results support a view of movement production as relatively specific to the past experience of the performer and the constraints of the task, and support a movement production mechanism that has access to an abstract representation of a base velocity profile."
            },
            "venue": {
                "fragments": [],
                "text": "Canadian journal of psychology"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144238410"
                        ],
                        "name": "Qiang Zhu",
                        "slug": "Qiang-Zhu",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39369497"
                        ],
                        "name": "Mei-Chen Yeh",
                        "slug": "Mei-Chen-Yeh",
                        "structuredName": {
                            "firstName": "Mei-Chen",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mei-Chen Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766349"
                        ],
                        "name": "K. Cheng",
                        "slug": "K.-Cheng",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815078"
                        ],
                        "name": "S. Avidan",
                        "slug": "S.-Avidan",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Avidan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Avidan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "We do this using the graphical model shown in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7800101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05fe01b57b3ba58dc5029c068a48567b55018ea5",
            "isKey": false,
            "numCitedBy": 1568,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We integrate the cascade-of-rejectors approach with the Histograms of Oriented Gradients (HoG) features to achieve a fast and accurate human detection system. The features used in our system are HoGs of variable-size blocks that capture salient features of humans automatically. Using AdaBoost for feature selection, we identify the appropriate set of blocks, from a large set of possible blocks. In our system, we use the integral image representation and a rejection cascade which significantly speed up the computation. For a 320 \u00d7 280 image, the system can process 5 to 30 frames per second depending on the density in which we scan the image, while maintaining an accuracy level similar to existing methods."
            },
            "slug": "Fast-Human-Detection-Using-a-Cascade-of-Histograms-Zhu-Yeh",
            "title": {
                "fragments": [],
                "text": "Fast Human Detection Using a Cascade of Histograms of Oriented Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work integrates the cascade-of-rejectors approach with the Histograms of Oriented Gradients features to achieve a fast and accurate human detection system that can process 5 to 30 frames per second depending on the density in which the image is scanned, while maintaining an accuracy level similar to existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144423856"
                        ],
                        "name": "Anna Bosch",
                        "slug": "Anna-Bosch",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062941326"
                        ],
                        "name": "X. Mu\u00f1oz",
                        "slug": "X.-Mu\u00f1oz",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Mu\u00f1oz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Mu\u00f1oz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "Gallese et al. [15] showed that movement analysis in humans depends on the presence of objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17584818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d175a196816e44c08928ad05e30fd774468d69aa",
            "isKey": false,
            "numCitedBy": 1366,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the problem of classifying images by the object categories they contain in the case of a large number of object categories. To this end we combine three ingredients: (i) shape and appearance representations that support spatial pyramid matching over a region of interest. This generalizes the representation of Lazebnik et al., (2006) from an image to a region of interest (ROI), and from appearance (visual words) alone to appearance and local shape (edge distributions); (ii) automatic selection of the regions of interest in training. This provides a method of inhibiting background clutter and adding invariance to the object instance 's position; and (iii) the use of random forests (and random ferns) as a multi-way classifier. The advantage of such classifiers (over multi-way SVM for example) is the ease of training and testing. Results are reported for classification of the Caltech-101 and Caltech-256 data sets. We compare the performance of the random forest/ferns classifier with a benchmark multi-way SVM classifier. It is shown that selecting the ROI adds about 5% to the performance and, together with the other improvements, the result is about a 10% improvement over the state of the art for Caltech-256."
            },
            "slug": "Image-Classification-using-Random-Forests-and-Ferns-Bosch-Zisserman",
            "title": {
                "fragments": [],
                "text": "Image Classification using Random Forests and Ferns"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that selecting the ROI adds about 5% to the performance and, together with the other improvements, the result is about a 10% improvement over the state of the art for Caltech-256."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577358"
                        ],
                        "name": "P. Srinivasan",
                        "slug": "P.-Srinivasan",
                        "structuredName": {
                            "firstName": "Praveen",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16635607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49927656ede0c75af22ca73dcf4abba028839650",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Analyzing videos of human activities involves not only recognizing actions (typically based on their appearances), but also determining the story/plot of the video. The storyline of a video describes causal relationships between actions. Beyond recognition of individual actions, discovering causal relationships helps to better understand the semantic meaning of the activities. We present an approach to learn a visually grounded storyline model of videos directly from weakly labeled data. The storyline model is represented as an AND-OR graph, a structure that can compactly encode storyline variation across videos. The edges in the AND-OR graph correspond to causal relationships which are represented in terms of spatio-temporal constraints. We formulate an Integer Programming framework for action recognition and storyline extraction using the storyline model and visual groundings learned from training data."
            },
            "slug": "Understanding-videos,-constructing-plots-learning-a-Gupta-Srinivasan",
            "title": {
                "fragments": [],
                "text": "Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An Integer Programming framework for action recognition and storyline extraction using the storyline model and visual groundings learned from training data is formulated."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145771244"
                        ],
                        "name": "Andrew D. Wilson",
                        "slug": "Andrew-D.-Wilson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Wilson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew D. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18658011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abb3fff3759b92a76eeb25e086df0595dd9f6dc3",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A state-based technique for the representation and recognition of gesture is presented. We define a gesture to be a sequence of states in a measurement or configuration space. For a given gesture, these states are used to capture both the repeatability and variability evidenced in a training set of example trajectories. Using techniques for computing a prototype trajectory of an ensemble of trajectories, we develop methods for defining configuration states along the prototype and for recognizing gestures from an unsegmented, continuous stream of sensor data. The approach is illustrated by application to a range of gesture-related sensory data: the two-dimensional movements of a mouse input device, the movement of the hand measured by a magnetic spatial position and orientation sensor, and, lastly, the changing eigenvector projection coefficients computed from an image sequence."
            },
            "slug": "A-State-Based-Approach-to-the-Representation-and-of-Bobick-Wilson",
            "title": {
                "fragments": [],
                "text": "A State-Based Approach to the Representation and Recognition of Gesture"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A state-based technique for the representation and recognition of gesture is presented, using techniques for computing a prototype trajectory of an ensemble of trajectories and for defining configuration states along the prototype and for recognizing gestures from an unsegmented, continuous stream of sensor data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "On the other hand, the importance of action in perceiving and recognizing objects (especially manipulable objects like tools) has been shown [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1332241,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "94a830e61c5f5bba19938efb83031a6509817d8c",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-world object category can be viewed as a characteristic configuration of its parts, that are themselves simpler, smaller (sub)categories. Recognition of a category can therefore be made easier by detecting its constituent subcategories and combing these detection results. Given a set of training images, each labeled by an object category contained in it, we present an approach to learning: (1) Taxonomy defined by recursive sharing of subcategories by multiple image categories; (2) Subcategory relevance as the degree of evidence a subcategory offers for the presence of its parent; (3) Likelihood that the image contains a subcategory; and (4) Prior that a subcategory occurs. The images are represented as points in a feature space spanned by confidences in the occurrences of the subcategories. The subcategory relevances are estimated as weights, necessary to rescale the corresponding axes of the feature space so that the images with the same label are closer to each other than to those with different labels. When a new image is encountered, the learned taxonomy, relevances, likelihoods, and priors are used by a linear classifier to categorize the image. On the challenging Caltech-256 dataset, the proposed approach significantly outperforms the best categorizations reported. This result is significant in that it not only demonstrates the advantages of exploiting subcategory taxonomy for recognition, but also suggests that a feature space spanned by part properties, instead of direct object properties, allows for linear separation of image classes."
            },
            "slug": "Learning-subcategory-relevances-for-category-Todorovic-Ahuja",
            "title": {
                "fragments": [],
                "text": "Learning subcategory relevances for category recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes an approach to learning that demonstrates the advantages of exploiting subcategory taxonomy for recognition, and suggests that a feature space spanned by part properties, instead of direct object properties, allows for linear separation of image classes."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 419324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a4300efb6895695205dfc1b74e124f9fea6aff2",
            "isKey": false,
            "numCitedBy": 413,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not. We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities. We present a conditional random field for jointly solving the tasks of object detection and scene classification."
            },
            "slug": "Using-the-Forest-to-See-the-Trees:-A-Graphical-and-Murphy-Torralba",
            "title": {
                "fragments": [],
                "text": "Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a conditional random field for jointly solving the tasks of object detection and scene classification, and proposes to use the scene context as an extra source of (global) information, to help resolve local ambiguities."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144429686"
                        ],
                        "name": "James W. Davis",
                        "slug": "James-W.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108258399"
                        ],
                        "name": "Hui Gao",
                        "slug": "Hui-Gao",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49183400"
                        ],
                        "name": "V. S. Kannappan",
                        "slug": "V.-S.-Kannappan",
                        "structuredName": {
                            "firstName": "Vignesh",
                            "lastName": "Kannappan",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. S. Kannappan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17431838,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5129b388c272ea681581cdffa0c068b555f7596c",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an expressive feature model for recognizing the performance effort of human actions. A set of low and high effort examples for an action are initially factored into its three-mode principal components, followed by a learning phase to compute the expressive features required to bring the model estimation of effort into agreement with perceptual judgements. The approach is demonstrated using real and illusory movements."
            },
            "slug": "A-three-mode-expressive-feature-model-of-action-Davis-Gao",
            "title": {
                "fragments": [],
                "text": "A three-mode expressive feature model of action effort"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An expressive feature model for recognizing the performance effort of human actions is presented, followed by a learning phase to compute the expressive features required to bring the model estimation of effort into agreement with perceptual judgements."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Motion and Video Computing, 2002. Proceedings."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144258209"
                        ],
                        "name": "L. Stark",
                        "slug": "L.-Stark",
                        "structuredName": {
                            "firstName": "Louise",
                            "lastName": "Stark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "Recognition rates of target objects were higher when the priming object was used in a similar action as the target object."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30322860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c70bdb311324b7618342c8ad3aba04661b2e1f2",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The work which demonstrates the feasibility of a different approach to 3-D object recognition is described. The authors construct a definition of a generic object category, such as a chair, in terms of the function required of the object. This definition is based on qualitative reasoning about 3-D shape, and does not imply any particular geometric or structural model for an object. Thus, this approach has the potential to lead to recognition systems of much greater generality than current CAD-based or model-based approaches. >"
            },
            "slug": "Generic-recognition-through-qualitative-reasoning-Stark-Bowyer",
            "title": {
                "fragments": [],
                "text": "Generic recognition through qualitative reasoning about 3-D shape and object function"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The authors construct a definition of a generic object category in terms of the function required of the object, based on qualitative reasoning about 3-D shape, which has the potential to lead to recognition systems of much greater generality than current CAD-based or model-based approaches."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144721252"
                        ],
                        "name": "H. Nagel",
                        "slug": "H.-Nagel",
                        "structuredName": {
                            "firstName": "Hans-Hellmut",
                            "lastName": "Nagel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nagel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41370692,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fe3dadb17d8ad21b85f5d1a99f551e61edff7439",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-image-sequences-towards-conceptual-Nagel",
            "title": {
                "fragments": [],
                "text": "From image sequences towards conceptual descriptions"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144735785"
                        ],
                        "name": "Matthew A. Brown",
                        "slug": "Matthew-A.-Brown",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brown",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew A. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565371"
                        ],
                        "name": "P. P\u00e9rez",
                        "slug": "P.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P\u00e9rez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 158
                            }
                        ],
                        "text": "We sample pixels from training images of the manipulable objects and build a 3D model in the RGB space.\npModel\u00f0r; g; b\u00de \u00bc 1\nN\nXN i\u00bc1 K r\u00f0r ri\u00deK g\u00f0g gi\u00deK b\u00f0b bi\u00de: \u00f07\u00de\nGiven a test image, we first use the shape-based classifier to detect potential object candidates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 632396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddf8e27c6686a8ee276a709f0bf98f32f7d41252",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of interactive foreground/background segmentation in still images is of great practical importance in image editing. The state of the art in interactive segmentation is probably represented by the graph cut algorithm of Boykov and Jolly (ICCV 2001). Its underlying model uses both colour and contrast information, together with a strong prior for region coherence. Estimation is performed by solving a graph cut problem for which very efficient algorithms have recently been developed. However the model depends on parameters which must be set by hand and the aim of this work is for those constants to be learned from image data."
            },
            "slug": "Interactive-Image-Segmentation-Using-an-Adaptive-Blake-Rother",
            "title": {
                "fragments": [],
                "text": "Interactive Image Segmentation Using an Adaptive GMMRF Model"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Estimation is performed by solving a graph cut problem for which very efficient algorithms have recently been developed, however the model depends on parameters which must be set by hand and the aim of this work is for those constants to be learned from image data."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "With the same neurons involved in execution and perception, a link between object recognition and action understanding has been established [38] in humans."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12650942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af220e04f193ed2a44e89e2cfd45a4e28ab35a52",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of finding point correspondences in images by way of an approach to template matching that is robust under affine distortions. This is achieved by applying \"geometric blur\" to both the template and the image, resulting in a fall-off in similarity that is close to linear in the norm of the distortion between the template and the image. Results in wide baseline stereo correspondence, face detection, and feature correspondence are included."
            },
            "slug": "Geometric-blur-for-template-matching-Berg-Malik",
            "title": {
                "fragments": [],
                "text": "Geometric blur for template matching"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work addresses the problem of finding point correspondences in images by way of an approach to template matching that is robust under affine distortions by applying \"geometric blur\" to both the template and the image, resulting in a fall-off in similarity."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838643"
                        ],
                        "name": "M. Smyth",
                        "slug": "M.-Smyth",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Smyth",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Smyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145251942"
                        ],
                        "name": "A. Wing",
                        "slug": "A.-Wing",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Wing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wing"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "Using features such as time to accelerate, peak velocity, and magnitude of acceleration and deceleration, the likelihoods of reach movements can be computed from hand trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "We implemented a variant of [12] for estimating the 2D pose of the upper body."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 142312195,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3f4d3e8ab6b426bb964d944151649e8b951b2486",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Psychology-of-human-movement-Smyth-Wing",
            "title": {
                "fragments": [],
                "text": "The Psychology of human movement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144601452"
                        ],
                        "name": "A. Hasman",
                        "slug": "A.-Hasman",
                        "structuredName": {
                            "firstName": "Arie",
                            "lastName": "Hasman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hasman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "Manipulation movements provide contextual information about the type of object being acted on and object class provides contextual information on possible interactions with them, depending on affordances and function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "A scene is mainly characterized as a place in which we can move [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61503518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a90c0588d1bf00e349abbb00dde009b6760866a",
            "isKey": false,
            "numCitedBy": 1114,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems:-of-Hasman",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems: Networks of plausible inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2918740"
                        ],
                        "name": "A. Vezhnevets",
                        "slug": "A.-Vezhnevets",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Vezhnevets",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vezhnevets"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238388"
                        ],
                        "name": "Vladimir Vezhnevets",
                        "slug": "Vladimir-Vezhnevets",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vezhnevets",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vladimir Vezhnevets"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "We classify each patch as belonging to one of the N scene object classes, using an adaboost-based classifier [55] based on features such as HOG, histograms of each color channel (eight bins each in color channel), and histograms of edge distance map values within the neighborhood."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16793346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18cb7cc7e60e1c67ea9c4a880d8b14053b9c68e3",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is a technique of combining a set weak classifiers to form one high-performance prediction rule. Boosting was successfully applied to solve the problems of object detection, text analysis, data mining and etc. The most and widely used boosting algorithm is AdaBoost and its later more effective variations Gentle and Real AdaBoost. In this article we propose a new boosting algorithm, which produces less generalization error compared to mentioned algorithms at the cost of somewhat higher training error."
            },
            "slug": "\u2018-Modest-AdaBoost-\u2019-\u2013-Teaching-AdaBoost-to-Better-Vezhnevets-Vezhnevets",
            "title": {
                "fragments": [],
                "text": "\u2018 Modest AdaBoost \u2019 \u2013 Teaching AdaBoost to Generalize Better"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new boosting algorithm is proposed, which produces less generalization error compared to mentioned algorithms at the cost of somewhat higher training error."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071978015"
                        ],
                        "name": "\u5927\u897f \u4ec1",
                        "slug": "\u5927\u897f-\u4ec1",
                        "structuredName": {
                            "firstName": "\u5927\u897f",
                            "lastName": "\u4ec1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5927\u897f \u4ec1"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 204167032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c36143936e3048818d881d9c5ebcae68e88ad70",
            "isKey": false,
            "numCitedBy": 582,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pearl,-J.-(1988,-second-printing-1991).-Reasoning-\u5927\u897f",
            "title": {
                "fragments": [],
                "text": "Pearl, J. (1988, second printing 1991). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan-Kaufmann."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 238926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8185a04652b9ff239d56958f2127e60bae850c5",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Graphical-Model-For-Recognizing-Scenes-and-Objects.-Murphy-Torralba",
            "title": {
                "fragments": [],
                "text": "Graphical Model For Recognizing Scenes and Objects."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2003"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Since the observed image shape of a human, changes significantly with articulation, viewpoint, and illumination, it is infeasible to train a single human detector for all shapes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. Graphicon"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Graphicon"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Influence of parameters on the system performance. (a) Scene . (b) w Scene . (c) Number of manipulable objects. (d) Dimensionality of pose features"
            },
            "venue": {
                "fragments": [],
                "text": "Fig"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Model for Scenes and Objects"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Conf. Neural Information Processing Systems"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "With the same neurons involved in execution and perception, a link between object recognition and action understanding has been established [38] in humans."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "We do this using the graphical model shown in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "We sample pixels from training images of the manipulable objects and build a 3D model in the RGB space.\npModel\u00f0r; g; b\u00de \u00bc 1\nN\nXN i\u00bc1 K r\u00f0r ri\u00deK g\u00f0g gi\u00deK b\u00f0b bi\u00de: \u00f07\u00de\nGiven a test image, we first use the shape-based classifier to detect potential object candidates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Histogram of Oriented Gradients for Fast Human Detection"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib"
            },
            "venue": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Role of Action Representation in Visual Object"
            },
            "venue": {
                "fragments": [],
                "text": "Experimental Brain Research"
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 47,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 71,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Observing-Human-Object-Interactions:-Using-Spatial-Gupta-Kembhavi/3a8da6accff92f915c1b8ac26d8176308c425b61?sort=total-citations"
}