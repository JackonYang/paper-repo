{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698689"
                        ],
                        "name": "N. Jojic",
                        "slug": "N.-Jojic",
                        "structuredName": {
                            "firstName": "Nebojsa",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jojic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "This simple approach is similar to the unsupervised algorithm from [17], though it avoids an explicit registration step by leveraging ground-truth bounding boxes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5489503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79bd5e61e20594c13a90e66293e1abb4179935a2",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of learning object class models and object segmentations from unannotated images. We introduce LOCUS (learning object classes with unsupervised segmentation) which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose. A key aspect of this model is that the object appearance is allowed to vary from image to image, allowing for significant within-class variation. By iteratively updating the belief in the object's position, size, segmentation and pose, LOCUS avoids making hard decisions about any of these quantities and so allows for each to be refined at any stage. We show that LOCUS successfully learns an object class model from unlabeled images, whilst also giving segmentation accuracies that rival existing supervised methods. Finally, we demonstrate simultaneous recognition and segmentation in novel images using the learned models for a number of object classes, as well as unsupervised object discovery and tracking in video."
            },
            "slug": "LOCUS:-learning-object-classes-with-unsupervised-Winn-Jojic",
            "title": {
                "fragments": [],
                "text": "LOCUS: learning object classes with unsupervised segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "LOCUS (learning object classes with unsupervised segmentation) is introduced which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose, allowing for significant within-class variation."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "There are a number of approaches that combine recognition with segmentation [12, 15, 11, 18, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Similar ideas have also proved useful for category-level recognition [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2900658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "398bff5abb9c35b2de63bf6b5ecae244c082ca6e",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 203,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis is concerned with the problem of visual object categorization, that is of reeognizing unseen-before objects, localizing them in cluttered real-worid images, and assigning the correct category label. This capability is one of the core compe\u00ac tencies of the human visual system. Yet, Computer vision Systems are still far from reaching a comparable level of Performance.Moreover,Computer visionresearch has in the past mainly focused on the simpler and more specific problem of identifying known objects under novel viewing conditions. The visual categorization problem is closely linked to the task of figure-ground segmentation, that is of dividing the image into an object and a non-objeet part. Historically, figure-ground segmentation has often been seen as an important and even necessary preprocessing step for object recognition. However, purely bottomup approacheshave so far been unable to yield segmentationsof sufficient quality, so that most current recognition approacheshave been designed to work independently from segmentation. In contrast,this thesis considers object categorization and figure-ground segmen\u00ac tation as two interleaved processes that closely collaborate towards a common goal. The core part of our work is a probabilisticformulation which integrates both capabilities into a common framework. As shown in our experiments, the tight coupling between those two processes allows them to profit from each other and improve their individual Performances. The resulting approach can detect categorical objects in novel images and automatically compute a segmentationfor them. This segmenta\u00ac tion is then used to again improve recognition by allowing the System to focus its effort on object pixels and discard misleading influencesfrom the background. In addition to improving the recognition Performance for individual hypotheses, the top-down segmentation also allows to determine exactly from where a hypoth\u00ac esis draws its support. We use this information to design a hypothesis verification stage based on the MDL principle that resolves ambiguities between overlapping hypotheseson a per-pixel level and factorsout the effects of partialocclusion. Altogether, this procedureconstitutes a novel mechanismin object detection that allows to analyze scenes containing multiple objects in a principled manner. Our results show that it presents an improvement over conventional criteria based on bounding box overlap and permitsmore aecurate aeeeptancedecisions. Our approach is based on a highly flexible implicit representation for object shape that can combine the information of local parts observed on different training exam\u00ac ples and interpolate between the correspondingobjects. As a result, the proposed method can learn object modeis already from few training examples and achieve competitive object detection Performance with training sets that are between one and two orders of magnitude smaller than those used in comparable Systems. An extensive evaluation on several large data sets shows that the system is applicable to many different object categories, including both rigid and articulated objects."
            },
            "slug": "Interleaved-Object-Categorization-and-Segmentation-Leibe-Schiele",
            "title": {
                "fragments": [],
                "text": "Interleaved Object Categorization and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This thesis considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal and develops a probabilistic formulation which integrates both capabilities into a common framework that allows to analyze scenes containing multiple objects in a principled manner."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116733876"
                        ],
                        "name": "Liang Zhao",
                        "slug": "Liang-Zhao",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Our work is more related in approach to [19] and the concurrent work of [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16930539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca4aa5c6bf7dd6c92fa748b0e3efb12cb413e31d",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a closely coupled object detection and segmentation algorithm for enhancing both processes in a cooperative and iterative manner. Figure-ground segmentation reduces the effect of background clutter on template matching; the matched template provides shape constraints on segmentation. More precisely, we estimate the probability of each pixel belonging to the foreground by a weighted sum of the estimates based on shape and color alone. The weight on the shape-based estimate is related to the probability that a familiar object is present and is updated dynamically so that we enforce shape constraints only where the object is present. Experiments on detecting people in images of cluttered scenes demonstrate that the proposed algorithm improves both segmentation and detection. More accurate object boundaries are extracted; higher object detection rates and lower false alarm rates are achieved than performing the two processes separately or sequentially."
            },
            "slug": "Closely-coupled-object-detection-and-segmentation-Zhao-Davis",
            "title": {
                "fragments": [],
                "text": "Closely coupled object detection and segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A closely coupled object detection and segmentation algorithm for enhancing both processes in a cooperative and iterative manner is proposed, which improves both segmentation and detection."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107881808"
                        ],
                        "name": "Stella X. Yu",
                        "slug": "Stella-X.-Yu",
                        "structuredName": {
                            "firstName": "Stella",
                            "lastName": "Yu",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stella X. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33731953"
                        ],
                        "name": "R. Gross",
                        "slug": "R.-Gross",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "There are a number of approaches that combine recognition with segmentation [12, 15, 11, 18, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1761618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e0e519dbf9aa58e153fb7c887935a371e24d7e3",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Segmentation and recognition have long been treated as two separate processes. We propose a mechanism based on spectral graph partitioning that readily combine the two processes into one. A part-based recognition system detects object patches, supplies their partial segmentations as well as knowledge about the spatial configurations of the object. The goal of patch grouping is to find a set of patches that conform best to the object configuration, while the goal of pixel grouping is to find a set of pixels that have the best low-level feature similarity. Through pixel-patch interactions and between-pateh competition encoded in the solution space, these two processes are realized in one joint optimization problem. The globally optima] partition is obtained by solving a constrained eigenvalue problem. We demonstrate that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection."
            },
            "slug": "Concurrent-Object-Recognition-and-Segmentation-by-Yu-Gross",
            "title": {
                "fragments": [],
                "text": "Concurrent Object Recognition and Segmentation by Graph Partitioning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that the resulting object segmentation eliminates false positives for the part detection, while overcoming occlusion and weak contours for the low-level edge detection."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 4
                            }
                        ],
                        "text": "For ViolaJones, we prematurely truncated the last 5 stages of the cascade of classifiers from the OpenCV implementation [2]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 39
                            }
                        ],
                        "text": "%) over established approaches such as ViolaJones and DalalTriggs on a variety of benchmark datasets including the PASCAL challenge, LabelMe, and the INRIAPerson dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 111
                            }
                        ],
                        "text": "When learning the prior/classifier, we use the same canonical window size as the baseline detector (except for ViolaJones, we used twice the size due to to the enlarged window context)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "In this work, we use well-known state-of-the-art baselines \u2013 ViolaJones [16] for finding faces and DalalTriggs [3] for finding pedestrians and cars."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 63
                            }
                        ],
                        "text": "These results are impressive given the quality of our baseline ViolaJones detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 43
                            }
                        ],
                        "text": "The shape priors and shape classifiers for ViolaJones frontal/profile face detector were trained from the LabelMe\ntraining set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 107
                            }
                        ],
                        "text": "Acknowledgments: Thanks to Navneet Dalal and Derek Hoiem for discussions and code and Xiaofeng Ren for his ViolaJones implementation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2715202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63",
            "isKey": true,
            "numCitedBy": 17885,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection."
            },
            "slug": "Rapid-object-detection-using-a-boosted-cascade-of-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Rapid object detection using a boosted cascade of simple features"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates and the introduction of a new image representation called the \"integral image\" which allows the features used by the detector to be computed very quickly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6075144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb444dc25bab36a8e273ed654d49e3841905e5af",
            "isKey": false,
            "numCitedBy": 1349,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. \n \nHigh classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow)."
            },
            "slug": "TextonBoost:-Joint-Appearance,-Shape-and-Context-Shotton-Winn",
            "title": {
                "fragments": [],
                "text": "TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-class Object Recognition and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently, is proposed, which is used for automatic visual recognition and semantic segmentation of photographs."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 215
                            }
                        ],
                        "text": "We show that such a strategy leads to stateof-the-art performance (10-20% improvement over existing systems) across a number of difficult datasets, such as the Pascal VOC2006 challenge [4], the INRIAPerson database [3], and LabelMe [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 35
                            }
                        ],
                        "text": "The shape prior/classifier for the DalalTriggs person detector was trained on the INRIAPerson training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 38
                            }
                        ],
                        "text": "Finally, the prior/classifier for the DalalTriggs car detector was trained using the PascalCar training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 54
                            }
                        ],
                        "text": "%) over established approaches such as ViolaJones and DalalTriggs on a variety of benchmark datasets including the PASCAL challenge, LabelMe, and the INRIAPerson dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 4
                            }
                        ],
                        "text": "For DalalTriggs, we lowered the bias threshold in the linear SVM by 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "In this work, we use well-known state-of-the-art baselines \u2013 ViolaJones [16] for finding faces and DalalTriggs [3] for finding pedestrians and cars."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 117
                            }
                        ],
                        "text": "We do not bother enlarging the window if the baseline detector already uses an enlarged window with context (as does DalalTriggs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 67
                            }
                        ],
                        "text": "Such window-based classifiers perform quite well in practice - the DalalTriggs detector yields the top score across many object classes (including people and cars) from the PASCAL 2006 Visual Object Challenge [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": true,
            "numCitedBy": 29264,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1752880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cca9200d9da958b7f90eab901b2f30c04f1e0e9c",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a Bayesian framework for parsing images into their constituent visual patterns. The parsing algorithm optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language. The algorithm constructs the parsing graph and re-configures it dynamically using a set of moves, which are mostly reversible Markov chain jumps. This computational framework integrates two popular inference approaches\u2014generative (top-down) methods and discriminative (bottom-up) methods. The former formulates the posterior probability in terms of generative models for images defined by likelihood functions and priors. The latter computes discriminative probabilities based on a sequence (cascade) of bottom-up tests/filters. In our Markov chain algorithm design, the posterior probability, defined by the generative models, is the invariant (target) probability for the Markov chain, and the discriminative probabilities are used to construct proposal probabilities to drive the Markov chain. Intuitively, the bottom-up discriminative probabilities activate top-down generative models. In this paper, we focus on two types of visual patterns\u2014generic visual patterns, such as texture and shading, and object patterns including human faces and text. These types of patterns compete and cooperate to explain the image and so image parsing unifies image segmentation, object detection, and recognition (if we use generic visual patterns only then image parsing will correspond to image segmentation (Tu and Zhu, 2002. IEEE Trans. PAMI, 24(5):657\u2013673). We illustrate our algorithm on natural images of complex city scenes and show examples where image segmentation can be improved by allowing object specific knowledge to disambiguate low-level segmentation cues, and conversely where object detection can be improved by using generic visual patterns to explain away shadows and occlusions."
            },
            "slug": "Image-Parsing:-Unifying-Segmentation,-Detection,-Tu-Chen",
            "title": {
                "fragments": [],
                "text": "Image Parsing: Unifying Segmentation, Detection, and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A Bayesian framework for parsing images into their constituent visual patterns that optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 232
                            }
                        ],
                        "text": "We show that such a strategy leads to stateof-the-art performance (10-20% improvement over existing systems) across a number of difficult datasets, such as the Pascal VOC2006 challenge [4], the INRIAPerson database [3], and LabelMe [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "%) over established approaches such as ViolaJones and DalalTriggs on a variety of benchmark datasets including the PASCAL challenge, LabelMe, and the INRIAPerson dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "LabelMeFaces: Since our procedure relies on computing a segmentation using color cues, we cannot use standard benchmarks for face detection (which are grayscale)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "LabelMePerson and LabelMeCars: We also performed experiments using the LabelMe test data from [8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 29
                            }
                        ],
                        "text": "We plot the PR curve for the LabelMe test set in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 106
                            }
                        ],
                        "text": "The shape priors and shape classifiers for ViolaJones frontal/profile face detector were trained from the LabelMe\ntraining set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 60
                            }
                        ],
                        "text": "Instead, we collected all the face and head detections from LabelMe (as of Aug 2006), re-verifying all detections manually."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": true,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719838"
                        ],
                        "name": "W. Grimson",
                        "slug": "W.-Grimson",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Grimson",
                            "middleNames": [
                                "Eric",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grimson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 121
                            }
                        ],
                        "text": "The strategy of detection followed by verification dates back to the hypothesis-and-test paradigm for model-based vision [9, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26307597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5324a91be1faec335a1c6665b01db842f65c7dc",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Model-based recognition methods generally use ad hoc techniques to decide whether or not a model of an object matches a given scene. The most common such technique is to set an empirically determined threshold on the fraction of model features that must be matched to data features. Conditions under which to accept a match as correct are rigorously derived. The analysis is based on modeling the recognition process as a statistical occupancy problem. This model makes the assumption that pairings of object and data features can be characterized as a random process with a uniform distribution. The authors present a number of examples illustrating that real image data are well approximated by such a random process. Using a statistical occupancy model, they derive an expression for the probability that a randomly occurring match will account for a given fraction of the features of a particular object. This expression is a function of the number of model features, the number of data features, and bounds on the degree of sensor noise. It provides a means of setting a threshold such that the probability of a random match is very small. >"
            },
            "slug": "On-the-Verification-of-Hypothesized-Matches-in-Grimson-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "On the Verification of Hypothesized Matches in Model-Based Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors derive an expression for the probability that a randomly occurring match will account for a given fraction of the features of a particular object, a function of the number of model features, theNumber of data features, and bounds on the degree of sensor noise."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "LabelMePerson and LabelMeCars: We also performed experiments using the LabelMe test data from [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "This suggests that it is indeed difficult to recognize small objects using local information \u2013 as eloquently argued in [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "We score Person and Car detection on LabelMe using the dataset from [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6152006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4081e007d7eced95cc618164e976a80d44ff5f4e",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
            },
            "slug": "Putting-Objects-in-Perspective-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Putting Objects in Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper provides a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint by allowing probabilistic object hypotheses to refine geometry and vice-versa."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856452"
                        ],
                        "name": "Jonathan Rihan",
                        "slug": "Jonathan-Rihan",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Rihan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Rihan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Our work is more related in approach to [19] and the concurrent work of [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 160
                            }
                        ],
                        "text": "Role of prior pfg: Typically, shape priors for graphcuts appear explicitly in the unary term (usually in the form of an unsigned distance function to a contour [13, 11])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9359385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2514ce8da21d54ed925034d7173d4a2fef3c6607",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel, simple and efficient method for face segmentation which works by coupling face detection and segmentation in a single framework. We use the OBJCUT [1] formulation that allows for a smooth combination of object detection and Markov Random Field for segmentation, to produce a real-time face segmentation. It should be noted that our algorithm is extremely efficient and runs in real time."
            },
            "slug": "OBJCUT-for-Face-Detection-Rihan-Kohli",
            "title": {
                "fragments": [],
                "text": "OBJCUT for Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper proposes a novel, simple and efficient method for face segmentation which works by coupling face detection and segmentation in a single framework using the OBJCUT formulation."
            },
            "venue": {
                "fragments": [],
                "text": "ICVGIP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "This can be efficiently solved with a min-cut max-flow algorithm [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "We segment the pixels within the enlarged window into object (foreground) versus background using a binary graphcut [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Possible extensions would be to make the term by contrast dependent [1] or category-specific (tuned by say, cross validation)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2430892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3120324069ec20eed853d3f9bbbceb32e4173b93",
            "isKey": true,
            "numCitedBy": 3913,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion."
            },
            "slug": "Fast-approximate-energy-minimization-via-graph-cuts-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Fast approximate energy minimization via graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed, and generates a labeling such that there is no expansion move that decreases the energy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862374"
                        ],
                        "name": "U. Grenander",
                        "slug": "U.-Grenander",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Grenander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Grenander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2661433"
                        ],
                        "name": "Yunshyong Chow",
                        "slug": "Yunshyong-Chow",
                        "structuredName": {
                            "firstName": "Yunshyong",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunshyong Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963249"
                        ],
                        "name": "D. M. Keenan",
                        "slug": "D.-M.-Keenan",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keenan",
                            "middleNames": [
                                "Macrae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Keenan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 105
                            }
                        ],
                        "text": "Shape models in computer vision have a storied history, dating back at least to the deformable models of [5, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195592621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac891dff9effb37aeb5800406addc8f71f86b0b6",
            "isKey": false,
            "numCitedBy": 407,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The text develops a global shape model and applies it to the analysis of real pictures acquired with a visible light camera under varying conditions of optical degradation. Computational feasibility of the algorithms derived from this model is achieved by analytical means. The aim is to develop methods for image understanding based on structured restoration, for example automatic detection of abnormalities. The limits of applicability of the algorithms are also traced by making the optical degradations more and more severe until the algorithms no longer succeed in their task. This book is suitable for an advanced undergraduate or graduate seminar in pattern theory, or as an accompanying book for applied probability, computer vision or pattern recognition."
            },
            "slug": "Hands:-A-Pattern-Theoretic-Study-of-Biological-Grenander-Chow",
            "title": {
                "fragments": [],
                "text": "Hands: A Pattern Theoretic Study of Biological Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The text develops a global shape model and applies it to the analysis of real pictures acquired with a visible light camera under varying conditions of optical degradation to develop methods for image understanding based on structured restoration, for example automatic detection of abnormalities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61615905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a2ed19ac684022aa3186887cd4893484ab8f80c",
            "isKey": false,
            "numCitedBy": 2169,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006). Details of the challenge, data, and evaluation are presented. Participants in the challenge submitted descriptions of their methods, and these have been included verbatim. This document should be considered preliminary, and subject to change."
            },
            "slug": "The-PASCAL-visual-object-classes-challenge-2006-Everingham-Zisserman",
            "title": {
                "fragments": [],
                "text": "The PASCAL visual object classes challenge 2006 (VOC2006) results"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465976"
                        ],
                        "name": "M. Fischler",
                        "slug": "M.-Fischler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Fischler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394928"
                        ],
                        "name": "R. Elschlager",
                        "slug": "R.-Elschlager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Elschlager",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elschlager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 105
                            }
                        ],
                        "text": "Shape models in computer vision have a storied history, dating back at least to the deformable models of [5, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14554383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "719da2a0ddd38e78151e1cb2db31703ea8b2e490",
            "isKey": false,
            "numCitedBy": 1527,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "slug": "The-Representation-and-Matching-of-Pictorial-Fischler-Elschlager",
            "title": {
                "fragments": [],
                "text": "The Representation and Matching of Pictorial Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The primary problem dealt with in this paper is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720184"
                        ],
                        "name": "G. Bradski",
                        "slug": "G.-Bradski",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Bradski",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bradski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "For ViolaJones, we prematurely truncated the last 5 stages of the cascade of classifiers from the OpenCV implementation [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62653534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "947fe67547cb0291ce9bcf03bcfb933246f5404c",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Based-Computer-Vision-with-Intels-Open-Bradski",
            "title": {
                "fragments": [],
                "text": "Learning-Based Computer Vision with Intels Open Source Computer Vision Library"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Acknowledgments: Thanks to Navneet Dalal and Derek Hoiem for discussions and code and Xiaofeng Ren for his ViolaJones implementation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "There are a number of approaches that combine recognition with segmentation [12, 15, 11, 18, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CVPR"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "In this way, our approach follows the spirit of [15, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "There are a number of approaches that combine recognition with segmentation [12, 15, 11, 18, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Textonboost: Joint appearance"
            },
            "venue": {
                "fragments": [],
                "text": "shape and context modeling for mulit-class object recognition and segmentation. In ECCV"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 121
                            }
                        ],
                        "text": "The strategy of detection followed by verification dates back to the hypothesis-and-test paradigm for model-based vision [9, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object recognition using alignment"
            },
            "venue": {
                "fragments": [],
                "text": "ICCV, page 102"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "There are a number of approaches that combine recognition with segmentation [12, 15, 11, 18, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 160
                            }
                        ],
                        "text": "Role of prior pfg: Typically, shape priors for graphcuts appear explicitly in the unary term (usually in the form of an unsigned distance function to a contour [13, 11])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Objcut"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "Scoring: For all our experiments, we precisely followed the evaluation methodology outlined in the PASCAL challenge [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 185
                            }
                        ],
                        "text": "We show that such a strategy leads to stateof-the-art performance (10-20% improvement over existing systems) across a number of difficult datasets, such as the Pascal VOC2006 challenge [4], the INRIAPerson database [3], and LabelMe [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 209
                            }
                        ],
                        "text": "Such window-based classifiers perform quite well in practice - the DalalTriggs detector yields the top score across many object classes (including people and cars) from the PASCAL 2006 Visual Object Challenge [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and L"
            },
            "venue": {
                "fragments": [],
                "text": "V. Gool. The pascal visual object classes challenge 2006 "
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "In this way, our approach follows the spirit of [15, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "There are a number of approaches that combine recognition with segmentation [12, 15, 11, 18, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image parsing: Unifying segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "detection, and recognition. In Proc ICCV"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Using-Segmentation-to-Verify-Object-Hypotheses-Ramanan/03cf107beae8394c685cd3997c8c4740e2468a7c?sort=total-citations"
}