{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7513025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones. This paper follows up on proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples. It extends them to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor. The advantage of the extension is that it allows predicting the label for a new example without having to solve again a linear system of dimension 'n' (the number of unlabeled and labeled training examples), which can cost O(n^3). Experiments show that the extension works well, in the sense of predicting a label close to the one that would have been obtained if the test example had been included in the unlabeled set. This relatively efficient function induction procedure can also be used when 'n' is large to approximate the solution by writing it only in terms of a kernel expansion with 'm' Keywords: non-parametric models, classification, regression, semi-supervised learning, modeles non parametriques, classification, regression, apprentissage semi-supervise"
            },
            "slug": "Efficient-Non-Parametric-Function-Induction-in-Delalleau-Bengio",
            "title": {
                "fragments": [],
                "text": "Efficient Non-Parametric Function Induction in Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Experiments show that the proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples are extended to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6035769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec7c68427a26f812532b1c913c68fcf84b7de58e",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks."
            },
            "slug": "Beyond-the-point-cloud:-from-transductive-to-Sindhwani-Niyogi",
            "title": {
                "fragments": [],
                "text": "Beyond the point cloud: from transductive to semi-supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper constructs a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS) that allow the structure of the RKHS to reflect the underlying geometry of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34574606"
                        ],
                        "name": "Michinari Momma",
                        "slug": "Michinari-Momma",
                        "structuredName": {
                            "firstName": "Michinari",
                            "lastName": "Momma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michinari Momma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14610843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f92a45f6ff5fb5c3f1afc96628428ad1b96d7962",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A crucial problem in machine learning is to choose an appropriate representation of data, in a way that emphasizes the relations we are interested in. In many cases this amounts to finding a suitable metric in the data space. In the supervised case, Linear Discriminant Analysis (LDA) can be used to find an appropriate subspace in which the data structure is apparent. Other ways to learn a suitable metric are found in [6] and [11]. However recently significant attention has been devoted to the problem of learning a metric in the semi-supervised case. In particular the work by Xing et al. [15] has demonstrated how semi-definite programming (SDP) can be used to directly learn a distance measure that satisfies constraints in the form of side-information. They obtain a significant increase in clustering performance with the new representation. The approach is very interesting, however, the computational complexity of the method severely limits its applicability to real machine learning tasks. In this paper we present an alternative solution for dealing with the problem of incorporating side-information. This side-information specifies pairs of examples belonging to the same class. The approach is based on LDA, and is solved by the efficient eigenproblem. The performance reached is very similar, but the complexity is only O(d 3) instead of O(d 6) where d is the dimensionality of the data. We also show how our method can be extended to deal with more general types of side-information."
            },
            "slug": "Efficiently-Learning-the-Metric-with-Bie-Momma",
            "title": {
                "fragments": [],
                "text": "Efficiently Learning the Metric with Side-Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative solution for dealing with the problem of incorporating side-information is presented, based on LDA, and is solved by the efficient eigenproblem, and can be extended to deal with more general types of side- information."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714122"
                        ],
                        "name": "M. Rwebangira",
                        "slug": "M.-Rwebangira",
                        "structuredName": {
                            "firstName": "Mugizi",
                            "lastName": "Rwebangira",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rwebangira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37341510"
                        ],
                        "name": "Rajashekar Reddy",
                        "slug": "Rajashekar-Reddy",
                        "structuredName": {
                            "firstName": "Rajashekar",
                            "lastName": "Reddy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajashekar Reddy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 807019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4563e793ef639480e915e34b2f4788a27f9c344",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In many application domains there is a large amount of unlabeled data but only a very limited amount of labeled training data. One general approach that has been explored for utilizing this unlabeled data is to construct a graph on all the data points based on distance relationships among examples, and then to use the known labels to perform some type of graph partitioning. One natural partitioning to use is the minimum cut that agrees with the labeled data (Blum & Chawla, 2001), which can be thought of as giving the most probable label assignment if one views labels as generated according to a Markov Random Field on the graph. Zhu et al. (2003) propose a cut based on a relaxation of this field, and Joachims (2003) gives an algorithm based on finding an approximate min-ratio cut.In this paper, we extend the mincut approach by adding randomness to the graph structure. The resulting algorithm addresses several short-comings of the basic mincut approach, and can be given theoretical justification from both a Markov random field perspective and from sample complexity considerations. In cases where the graph does not have small cuts for a given classification problem, randomization may not help. However, our experiments on several datasets show that when the structure of the graph supports small cuts, this can result in highly accurate classifiers with good accuracy/coverage tradeoffs. In addition, we are able to achieve good performance with a very simple graph-construction procedure."
            },
            "slug": "Semi-supervised-learning-using-randomized-mincuts-Blum-Lafferty",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning using randomized mincuts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The experiments on several datasets show that when the structure of the graph supports small cuts, this can result in highly accurate classifiers with good accuracy/coverage tradeoffs, and can be given theoretical justification from both a Markov random field perspective and from sample complexity considerations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802711"
                        ],
                        "name": "Yves Grandvalet",
                        "slug": "Yves-Grandvalet",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Grandvalet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Grandvalet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7890982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad67ccee45b801b0138016e2f44a566344e77320",
            "isKey": false,
            "numCitedBy": 1274,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution benefits from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the \"cluster assumption\". Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces."
            },
            "slug": "Semi-supervised-Learning-by-Entropy-Minimization-Grandvalet-Bengio",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Learning by Entropy Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This framework, which motivates minimum entropy regularization, enables to incorporate unlabeled data in the standard supervised learning, and includes other approaches to the semi-supervised problem as particular or limiting cases."
            },
            "venue": {
                "fragments": [],
                "text": "CAP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744439"
                        ],
                        "name": "J. Suykens",
                        "slug": "J.-Suykens",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Suykens",
                            "middleNames": [
                                "A.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suykens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750713"
                        ],
                        "name": "B. Moor",
                        "slug": "B.-Moor",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Moor",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17214020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1a911771db07fbd9bcad863c3afbbeb0495c600",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Most machine learning algorithms are designed either for supervised or for unsupervised learning, notably classification and clustering. Practical problems in bioinformatics and in vision however show that this setting often is an oversimplification of reality. While label information is of course invaluable in most cases, it would be a huge waste to ignore the information on the cluster structure that is present in an (often much larger) unlabeled sample set. Several recent contributions deal with this topic: given partially labeled data, exploit all information available. In this paper, we present an elegant and efficient algorithm that allows to deal with very general types of label constraints in class learning problems. The approach is based on spectral clustering, and leads to an efficient algorithm based on the simple eigenvalue problem."
            },
            "slug": "Learning-from-General-Label-Constraints-Bie-Suykens",
            "title": {
                "fragments": [],
                "text": "Learning from General Label Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An elegant and efficient algorithm is presented that allows to deal with very general types of label constraints in class learning problems, based on spectral clustering, and leads to an efficient algorithm based on the simple eigenvalue problem."
            },
            "venue": {
                "fragments": [],
                "text": "SSPR/SPR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7635678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach."
            },
            "slug": "Semi-Supervised-Support-Vector-Machines-Bennett-Demiriz",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A general S3VM model is proposed that minimizes both the misclassification error and the function capacity based on all the available data that can be converted to a mixed-integer program and then solved exactly using integer programming."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5896838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e626de498fd5638456af1943443cb7ca5e12435d",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including Support Vector Machines and Regularized Least Squares can be obtained as special cases. We utilize properties of Reproducing Kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework."
            },
            "slug": "Manifold-Regularization-:-A-Geometric-Framework-for-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Manifold Regularization : A Geometric Framework for Learning from Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner is focused on and properties of Reproducing Kernel Hilbert spaces are utilized to prove new Representer theorems that provide theoretical basis for the algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 8
                            }
                        ],
                        "text": ", 1998, Blum and Mitchell, 1998, Collins and Singer, 1999, Joachims, 1999]. Note that, to our knowledge, Merz et al. [1992] were the first to use the term \u201csemisupervised\u201d for classification with both labeled and unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17133491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed3c324be93f30797e0f71d5f5fb5417cdd790bc",
            "isKey": false,
            "numCitedBy": 806,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classification functions are naturally defined only on the submanifold in question rather than the total ambient space. Using the Laplace-Beltrami operator one produces a basis (the Laplacian Eigenmaps) for a Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples are required. Once such a basis is obtained, training can be performed using the labeled data set.Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace-Beltrami operator by the graph Laplacian. We provide details of the algorithm, its theoretical justification, and several practical applications for image, speech, and text classification."
            },
            "slug": "Semi-Supervised-Learning-on-Riemannian-Manifolds-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning on Riemannian Manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An algorithmic framework to classify a partially labeled data set in a principled manner and models the manifold using the adjacency graph for the data and approximates the Laplace-Beltrami operator by the graph Laplacian."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34277946"
                        ],
                        "name": "D. Pierce",
                        "slug": "D.-Pierce",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pierce",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pierce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13999155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df3520f52fcf42b4f10ed4b35b3b3f9cd050f290",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between co-trained classifiers and fully supervised classifiers trained on a labeled version of all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks."
            },
            "slug": "Limitations-of-Co-Training-for-Natural-Language-Pierce-Cardie",
            "title": {
                "fragments": [],
                "text": "Limitations of Co-Training for Natural Language Learning from Large Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels and proposes a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144444438"
                        ],
                        "name": "Massih-Reza Amini",
                        "slug": "Massih-Reza-Amini",
                        "structuredName": {
                            "firstName": "Massih-Reza",
                            "lastName": "Amini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massih-Reza Amini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741426"
                        ],
                        "name": "P. Gallinari",
                        "slug": "P.-Gallinari",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gallinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gallinari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5391022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df7f8e4deade0da8428f9d6f38e2d2400cb77f07",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised learning has recently emerged as a new paradigm in the machine learning community. It aims at exploiting simultaneously labeled and unlabeled data for classification. We introduce here a new semi-supervised algorithm. Its originality is that it relies on a discriminative approach to semi-supervised learning rather than a generative approach, as it is usually the case. We present in details this algorithm for a logistic classifier and show that it can be interpreted as an instance of the Classification Expectation Maximization algorithm. We also provide empirical results on two data sets for sentence classification tasks and analyze the behavior of our methods."
            },
            "slug": "Semi-Supervised-Logistic-Regression-Amini-Gallinari",
            "title": {
                "fragments": [],
                "text": "Semi Supervised Logistic Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A new semi-supervised algorithm that relies on a discriminative approach to semi- supervised learning rather than a generative approach, which can be interpreted as an instance of the Classification Expectation Maximization algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145780619"
                        ],
                        "name": "David J. Miller",
                        "slug": "David-J.-Miller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309473"
                        ],
                        "name": "H. S. Uyar",
                        "slug": "H.-S.-Uyar",
                        "structuredName": {
                            "firstName": "Hasan",
                            "lastName": "Uyar",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. S. Uyar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17425751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c608ec27a937361122d178b38b6b7387440b58eb",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We address statistical classifier design given a mixed training set consisting of a small labelled feature set and a (generally larger) set of unlabelled features. This situation arises, e.g., for medical images, where although training features may be plentiful, expensive expertise is required to extract their class labels. We propose a classifier structure and learning algorithm that make effective use of unlabelled data to improve performance. The learning is based on maximization of the total data likelihood, i.e. over both the labelled and unlabelled data subsets. Two distinct EM learning algorithms are proposed, differing in the EM formalism applied for unlabelled data. The classifier, based on a joint probability model for features and labels, is a \"mixture of experts\" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training. The scope of application for the new method is greatly extended by the observation that test data, or any new data to classify, is in fact additional, unlabelled data - thus, a combined learning/classification operation - much akin to what is done in image segmentation - can be invoked whenever there is new data to classify. Experiments with data sets from the UC Irvine database demonstrate that the new learning algorithms and structure achieve substantial performance gains over alternative approaches."
            },
            "slug": "A-Mixture-of-Experts-Classifier-with-Learning-Based-Miller-Uyar",
            "title": {
                "fragments": [],
                "text": "A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A classifier structure and learning algorithm that make effective use of unlabelled data to improve performance and is a \"mixture of experts\" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1052837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125842668eab7decac136db8a59d392dc5e4e395",
            "isKey": false,
            "numCitedBy": 3711,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks."
            },
            "slug": "Semi-Supervised-Learning-Using-Gaussian-Fields-and-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model, and methods to incorporate class priors and the predictions of classifiers obtained by supervised learning are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39713260"
                        ],
                        "name": "Tilman Lange",
                        "slug": "Tilman-Lange",
                        "structuredName": {
                            "firstName": "Tilman",
                            "lastName": "Lange",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tilman Lange"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40004085"
                        ],
                        "name": "Martin H. C. Law",
                        "slug": "Martin-H.-C.-Law",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Law",
                            "middleNames": [
                                "H.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin H. C. Law"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6667160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63db4ce22fe80ed50665da89bbb695d7ed448782",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification problems abundantly arise in many computer vision tasks eing of supervised, semi-supervised or unsupervised nature. Even when class labels are not available, a user still might favor certain grouping solutions over others. This bias can be expressed either by providing a clustering criterion or cost function and, in addition to that, by specifying pairwise constraints on the assignment of objects to classes. In this work, we discuss a unifying formulation for labelled and unlabelled data that can incorporate constrained data for model fitting. Our approach models the constraint information by the maximum entropy principle. This modeling strategy allows us (i) to handle constraint violations and soft constraints, and, at the same time, (ii) to speed up the optimization process. Experimental results on face classification and image segmentation indicates that the proposed algorithm is computationally efficient and generates superior groupings when compared with alternative techniques."
            },
            "slug": "Learning-with-constrained-and-unlabelled-data-Lange-Law",
            "title": {
                "fragments": [],
                "text": "Learning with constrained and unlabelled data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work discusses a unifying formulation for labelled and unlabelled data that can incorporate constrained data for model fitting that is computationally efficient and generates superior groupings when compared with alternative techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745410"
                        ],
                        "name": "Maria-Florina Balcan",
                        "slug": "Maria-Florina-Balcan",
                        "structuredName": {
                            "firstName": "Maria-Florina",
                            "lastName": "Balcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria-Florina Balcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143781496"
                        ],
                        "name": "Ke Yang",
                        "slug": "Ke-Yang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 895559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eaa981275145025c5064107c9ec7e58fec39f6b",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Co-training is a method for combining labeled and unlabeled data when examples can be thought of as containing two distinct sets of features. It has had a number of practical successes, yet previous theoretical analyses have needed very strong assumptions on the data that are unlikely to be satisfied in practice. \n \nIn this paper, we propose a much weaker \"expansion\" assumption on the underlying data distribution, that we prove is sufficient for iterative co-training to succeed given appropriately strong PAC-learning algorithms on each feature set, and that to some extent is necessary as well. This expansion assumption in fact motivates the iterative nature of the original co-training algorithm, unlike stronger assumptions (such as independence given the label) that allow a simpler one-shot co-training to succeed. We also heuristically analyze the effect on performance of noise in the data. Predicted behavior is qualitatively matched in synthetic experiments on expander graphs."
            },
            "slug": "Co-Training-and-Expansion:-Towards-Bridging-Theory-Balcan-Blum",
            "title": {
                "fragments": [],
                "text": "Co-Training and Expansion: Towards Bridging Theory and Practice"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A much weaker \"expansion\" assumption on the underlying data distribution is proposed, that is proved to be sufficient for iterative co-training to succeed given appropriately strong PAC-learning algorithms on each feature set, and that to some extent is necessary as well."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791498"
                        ],
                        "name": "R. Ghani",
                        "slug": "R.-Ghani",
                        "structuredName": {
                            "firstName": "Rayid",
                            "lastName": "Ghani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ghani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7464925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "847546aa1cbcb017cb16041cba4927b76f57461a",
            "isKey": false,
            "numCitedBy": 1061,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been signi cant interest in supervised learning algorithms that combine labeled and unlabeled data for text learning tasks. The co-training setting [1] applies to datasets that have a natural separation of their features into two disjoint sets. We demonstrate that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not. When a natural split does not exist, co-training algorithms that manufacture a feature split may out-perform algorithms not using a split. These results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classi ers."
            },
            "slug": "Analyzing-the-effectiveness-and-applicability-of-Nigam-Ghani",
            "title": {
                "fragments": [],
                "text": "Analyzing the effectiveness and applicability of co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is demonstrated that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not and may out-perform algorithms not using a split."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40632403"
                        ],
                        "name": "Sugato Basu",
                        "slug": "Sugato-Basu",
                        "structuredName": {
                            "firstName": "Sugato",
                            "lastName": "Basu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sugato Basu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47695762"
                        ],
                        "name": "M. Bilenko",
                        "slug": "M.-Bilenko",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Bilenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bilenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207155154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01d5bf24c0b35d9a234d534bf69924fa16201dee",
            "isKey": false,
            "numCitedBy": 849,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints, i.e., pairs of instances labeled as belonging to same or different clusters. In recent years, a number of algorithms have been proposed for enhancing clustering quality by employing such supervision. Such methods use the constraints to either modify the objective function, or to learn the distance measure. We propose a probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields (HMRFs) that provides a principled framework for incorporating supervision into prototype-based clustering. The model generalizes a previous approach that combines constraints and Euclidean distance learning, and allows the use of a broad range of clustering distortion measures, including Bregman divergences (e.g., Euclidean distance and I-divergence) and directional similarity measures (e.g., cosine similarity). We present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the HMRF model. Experimental results on several text data sets demonstrate the advantages of the proposed framework."
            },
            "slug": "A-probabilistic-framework-for-semi-supervised-Basu-Bilenko",
            "title": {
                "fragments": [],
                "text": "A probabilistic framework for semi-supervised clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields (HMRFs) that provides a principled framework for incorporating supervision into prototype-based clustering and experimental results demonstrate the advantages of the proposed framework."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144848317"
                        ],
                        "name": "Glenn Fung",
                        "slug": "Glenn-Fung",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Fung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glenn Fung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5320604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fced0bfd90bd624876762dd6bfacb992e5ed3b27",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A concave minimization approach is proposed for classifying unlabeled data based on the following ideas: (i) A small representative percentage (5% to 10%) of the unlabeled data is chosen by a clustering algorithm and given to an expert or oracle to label, (ii) A linear support vector machine is trained using the small labeled sample while simultaneously assigning the remaining bulk of the unlabeled dataset to one of two classes so as to maximize the margin (distance) between the two bounding planes that determine the separating plane midway between them. This latter problem is formulated as a concave minimization problem on a polyhedral set for which a stationary point is quickly obtained by solving a few (5 to 7) linear programs. Such stationary points turn out to be very effective as evidenced by our computational results which show that clustered concave minimization yields: (a) Test set improvement as high as 20.4% over a linear support vector machine trained on a correspondingly small but randomly chosen subset that is labeled by an expert. (b) Test set correctness averaged to within 5.1% when compared to that of a completely supervised linear support vector machine trained on the entire dataset which has been labeled by an expert."
            },
            "slug": "Semi-superyised-support-vector-machines-for-data-Fung-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Semi-superyised support vector machines for unlabeled data classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Computational results show that clustered concave minimization yields test set improvement as high as 20.4% over a linear support vector machine trained on a correspondingly small but randomly chosen subset that is labeled by an expert."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14283441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2c5d2cafc35856832f2b478790f0af119baab92",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We believe that the cluster assumption is key to successful semi-supervised learning. Based on this, we propose three semi-supervised algorithms: 1. deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM; 2. optimizing the Transductive SVM objective function, which places the decision boundary in low density regions, by gradient descent; 3. combining the first two to make maximum use of the cluster assumption. We compare with state of the art algorithms and demonstrate superior accuracy for the latter two methods."
            },
            "slug": "Semi-Supervised-Classification-by-Low-Density-Chapelle-Zien",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Classification by Low Density Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Three semi-supervised algorithms are proposed: deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM, and optimizing the Transductive SVM objective function by gradient descent."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18355414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7145a4aa8146162b93d3af48772bdbae606b1dc",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient learning with partially labeled data involves extracting structure from large unlabeled set and combining this information with limited labeled examples. A typical albeit unstated assumption in this context associates separable clusters in the unlabeled set with unique but unknown labels. When this assumption is valid, labeled examples are needed only to the extent that they can facilitate labeling of the clusters. We capture and formalize this intuition in a conditional probability model where soft clusters serve to regularize the labeling of the unlabeled examples. Clustering is achieved by defining a Markov diffusion process (cf. Tishby and Slonim, NIPS 2000). The associated time scale of this process determines the effective size of the clusters and is chosen through a margin based criterion that guarantees unambiguous classification of examples. We relate the time scale to the mixing time of the Markov process and extend the basic idea by combining multiple time scales to maximize classification accuracy. We demonstrate the performance of the approach on both real and synthetic datasets. 1 Representation based on Markov diffusion To achieve good learning performance, the data must be encoded in a suitable representation matched to the learning algorithm. Typically, we are provided data points in a space, and a distance metric that measures pairwise similarity between points. The provided distance metric is often quite accurate locally, as it is relatively easy to characterize small perturbations in the data. However, over larger distances, the given metric is frequently inadequate, and hurts the performance of the many learning algorithms that rely on global distances. Fortunately, in problems with many data points (with or without labels), we can use the locally accurate metric to construct an improved global distance measure that reflects the density of the data. For example, the data may lie on a submanifold of the space, revealed by the density, and we should measure distances along the manifold. Intuitively, the distances are smaller in directions of high density, and larger in low-density directions. We define a Markov diffusion process based on the locally accurate metric. The local metric defines probabilities of transitioning between two nearby points in one timestep, and we construct the global distance as the probability of transitioning between two points in t timesteps. Thus, we consider all the paths of length t on this graph. Formally, consider a set of points fx1; : : : ;xNg that need not be labeled. Construct a graph whose nodes correspond to data points, and whose undirected edges correspond to one step transitions. We only allow such immediate transitions from a point to its neighbors. Specifically, for each point, connect it with an undirected edge to its K nearest neighbors. Points in high density centers can be neighbors of many points and end up with more than K edges. Self-transitions back to the point itself are also included. Let the probability of transitioning from a state i at time t to a neighbor k at time t+ 1 be"
            },
            "slug": "Clustering-and-efficient-use-of-unlabeled-examples-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Clustering and efficient use of unlabeled examples"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work relates the time scale to the mixing time of the Markov process and extend the basic idea by combining multiple time scales to maximize classification accuracy, and demonstrates the performance of the approach on both real and synthetic datasets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118237765"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kanal",
                            "lastName": "Nigam",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15161133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c56af1eb05cfb6ec81e84a6924fb46cb202747",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "One key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately. This dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers. By assuming that documents are created by a parametric generative model, Expectation-Maximization (EM) finds local maximum a posteriori models and classifiers from all the data\u2014labeled and unlabeled. These generative models do not capture all the intricacies of text; however on some domains this technique substantially improves classification accuracy, especially when labeled data are sparse. \nTwo problems arise from this basic approach. First, unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated. In this case the assumptions can be made more representative in two ways: by modeling sub-topic class structure, and by modeling super-topic hierarchical class relationships. By doing so, model probability and classification accuracy come into correspondence, allowing unlabeled data to improve classification performance. The second problem is that even with a representative model, the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data. Here, limited labeled data provide EM initializations that lead to low-probability models. Performance can be significantly improved by using active learning to select high-quality initializations, and by using alternatives to EM that avoid low-probability local maxima."
            },
            "slug": "Using-unlabeled-data-to-improve-text-classification-Nigam-Mitchell",
            "title": {
                "fragments": [],
                "text": "Using unlabeled data to improve text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 566534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67f3e2d4823b783fb6e2969f6fe01757f077dba3",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nBuilding an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful.\n\n\nRESULTS\nIn this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency.\n\n\nAVAILABILITY\nSource code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider.\n\n\nSUPPLEMENTARY INFORMATION\nwww.kyb.tuebingen.mpg.de/bs/people/weston/semiprot."
            },
            "slug": "Semi-supervised-Protein-Classification-Using-Weston-Leslie",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Protein Classification Using Cluster Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work develops simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences and achieves equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14313123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36f7805632939e610f9474a7d5868f503de7197e",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Considerable progress was recently made on semi-supervised learning, which differs from the traditional supervised learning by additionally exploring the information of the unlabeled examples. However, a disadvantage of many existing methods is that it does not generalize to unseen inputs. This paper suggests a space of basis functions to perform semi-supervised inductive learning. As a nice property, the proposed method allows efficient training and can easily handle new test points. We validate the method based on both toy data and real world data sets."
            },
            "slug": "Semi-supervised-Induction-with-Basis-Functions-Yu",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Induction with Basis Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A space of basis functions to perform semi-supervised inductive learning is suggested and the proposed method allows efficient training and can easily handle new test points."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47695762"
                        ],
                        "name": "M. Bilenko",
                        "slug": "M.-Bilenko",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Bilenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bilenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40632403"
                        ],
                        "name": "Sugato Basu",
                        "slug": "Sugato-Basu",
                        "structuredName": {
                            "firstName": "Sugato",
                            "lastName": "Basu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sugato Basu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16157513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "197e3a315c57c9278876d95b7e522700aa112886",
            "isKey": false,
            "numCitedBy": 908,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised clustering employs a small amount of labeled data to aid unsupervised learning. Previous work in the area has utilized supervised data in one of two approaches: 1) constraint-based methods that guide the clustering algorithm towards a better grouping of the data, and 2) distance-function learning methods that adapt the underlying similarity metric used by the clustering algorithm. This paper provides new methods for the two approaches as well as presents a new semi-supervised clustering algorithm that integrates both of these techniques in a uniform, principled framework. Experimental results demonstrate that the unified approach produces better clusters than both individual approaches as well as previously proposed semi-supervised clustering algorithms."
            },
            "slug": "Integrating-constraints-and-metric-learning-in-Bilenko-Basu",
            "title": {
                "fragments": [],
                "text": "Integrating constraints and metric learning in semi-supervised clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results demonstrate that the unified approach produces better clusters than both individual approaches as well as previously proposed semi-supervised clustering algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349519"
                        ],
                        "name": "Joel Ratsaby",
                        "slug": "Joel-Ratsaby",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Ratsaby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Ratsaby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144694846"
                        ],
                        "name": "S. Venkatesh",
                        "slug": "S.-Venkatesh",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Venkatesh",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Venkatesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17561403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53dcb8199cda481d67663efd29f0d80f6f29bf32",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "1 INTRODUCTION We investigate the tradeoff between labeled The classical problem of learning a classification rule and unlabeled sample complexities in learning can be stated as follows: patterns from classes \" 1 \" and a classification rule for a parametric two-class \" 2 \" (or \" states of nature \") appear with probabilities problem. In the problem considered, a sam-P 1 = P and P2 = 1 \u2013 p, respectively; the pattern classes ple of m labeled examples and n unlabeled ex-are represented by feature vectors x in a common N-amples generated from a two-class, N-variate dimensional Euclidean space R N, the patterns of class Gaussian mixture is provided together with \" i \" distributed according to the class-conditional prob-side information specifying the parametric form ability density fi (x) (i = 1, 2), Labeled pairs (x, y) E of the probability densities. The class means RN x {1,2} are assumed generated according to the fol-and a priori class probabilities are, however, lowing mechanism: a pattern class (or \" label \") y e {1,2} unknown parameters. In this framework we is first drawn randomly according to the distribution of use the maximum likelihood estimation method classes {p 1, P2}; a corresponding random feature vector to estimate the unknown parameters and uti-x c RN is then drawn according to the class-conditional lize rates of convergence of uniform strong laws density fv. In the supervised learning scenario, a labeled to determine the tradeoff between error rate m-sample { (xj, y j), 1 < j < m } is acquired by inde-and sample complexity. In particular, we show pendent sampling from the distribution of pairs (x, y). that for the algorithm used, the misclassifi-Using the sample, the objective is to construct a deci-mation probability deviates from the minimal sion rule which when presented with a random pattern Bayes error rate by Cl(N3/5n-' /5) + Cl(e-cm) x drawn from the mixture density where N is the dimension of the feature space, f(x) = plfl (x) + P2f2(x) m is the number of labeled examples, n is the number of unlabeled examples, and c is a pos-produces a label which disagrees with the true class it ive constant. of origin by a probability P~,,O, close to the minimal pB~yw error rate. Formally this learning problem can be formulated in the framework of the Probably Approximately Correct (PAC) learning model (cf. [9, 10]) as follows: Given e \u2026"
            },
            "slug": "Learning-from-a-mixture-of-labeled-and-unlabeled-Ratsaby-Venkatesh",
            "title": {
                "fragments": [],
                "text": "Learning from a mixture of labeled and unlabeled examples with parametric side information"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The tradeoff between labeled and unlabeled sample complexities in learning is investigated and pendent sampling from the distribution of pairs (x, y) is shown."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8924778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7d471970467a99bec4bce34c7dba5ef6745ad06",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge."
            },
            "slug": "The-Curse-of-Highly-Variable-Functions-for-Local-Bengio-Delalleau",
            "title": {
                "fragments": [],
                "text": "The Curse of Highly Variable Functions for Local Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A series of theoretical arguments are presented supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15295061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4c631b245971eb5fde9cbd6d3575e4ac4e41c49",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Assignment methods are at the heart of many algorithms for unsupervised learning and clustering -- in particular, the well-known K-means and Expectation-Maximizatian (EM) algorithms. In this work, we study several different methods of assignment, including the \"hard\" assignments used by K-means and the \"soft\" assignments used by EM. While it is known that K-means minimizes the distortion on the data and EM maximizes the likelihood, little is known about the systematic differences of behavior between the two algorithms. Here we shed light on these differences via an information-theoretic analysis. The cornerstone of our results is a simple decomposition of the expected distortion, showing that K-means (and its extension for inferring general parametric densities from unlabeled sample data) must implicitly manage a trade-off between how similar the data assigned to each cluster are, and how the data are balanced among the clusters. How well the data are balanced is measured by the entropy of the partition defined by the hard assignments. In addition to letting us predict and verify systematic differences between K-means and EM on specific examples, the decomposition allows us to give a rather general argument showing that K-means will consistently find densities with less \"overlap\" than EM. We also study a third natural assignment method that we call posterior assignment, that is close in spirit to the soft assignments of EM, but leads to a surprisingly different algorithm."
            },
            "slug": "An-Information-Theoretic-Analysis-of-Hard-and-Soft-Kearns-Mansour",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple decomposition of the expected distortion is shown, showing that K-means (and its extension for inferring general parametric densities from unlabeled sample data) must implicitly manage a trade-off between how similar the data assigned to each cluster are, and how the data are balanced among the clusters."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791498"
                        ],
                        "name": "R. Ghani",
                        "slug": "R.-Ghani",
                        "structuredName": {
                            "firstName": "Rayid",
                            "lastName": "Ghani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ghani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6901292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa88c3e8781722cd55e2ad29209eff0a84144f85",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised learning techniques for text classi cation often require a large number of labeled examples to learn accurately. One way to reduce the amount of labeled data required is to develop algorithms that can learn e ectively from a small number of labeled examples augmented with a large number of unlabeled examples. Current text learning techniques for combining labeled and unlabeled, such as EM and Co-Training, are mostly applicable for classi cation tasks with a small number of classes and do not scale up well for large multiclass problems. In this paper, we develop a framework to incorporate unlabeled data in the Error-Correcting Output Coding (ECOC) setup by rst decomposing multiclass problems into multiple binary problems and then using Co-Training to learn the individual binary classi cation problems. We show that our method is especially useful for text classi cation tasks involving a large number of categories and outperforms other semi-supervised learning techniques such as EM and Co-Training. In addition to being highly accurate, this method utilizes the hamming distance from ECOC to provide high-precision results. We also present results with algorithms other than co-training in this framework and show that co-training is uniquely suited to work well within ECOC."
            },
            "slug": "Combining-Labeled-and-Unlabeled-Data-for-MultiClass-Ghani",
            "title": {
                "fragments": [],
                "text": "Combining Labeled and Unlabeled Data for MultiClass Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper develops a framework to incorporate unlabeled data in the Error-Correcting Output Coding (ECOC) setup by first decomposing multiclass problems into multiple binary problems and then using Co-Training to learn the individual binary classi cation problems."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701847"
                        ],
                        "name": "L. Ghaoui",
                        "slug": "L.-Ghaoui",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Ghaoui",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ghaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1113875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0948365ef39ef153e61e9569ade541cf881c7c2a",
            "isKey": false,
            "numCitedBy": 2470,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem."
            },
            "slug": "Learning-the-Kernel-Matrix-with-Semidefinite-Lanckriet-Cristianini",
            "title": {
                "fragments": [],
                "text": "Learning the Kernel Matrix with Semidefinite Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques and leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709927"
                        ],
                        "name": "R. Maclin",
                        "slug": "R.-Maclin",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Maclin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maclin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5124171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e1e5668889ffb40110704260547c35af052839e",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive semi-supervised ensemble method, ASSEMBLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data. ASSEMBLE alternates between assigning \"pseudo-classes\" to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data. Mathematically, this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data. Unlike alternative approaches, ASSEMBLE does not require a semi-supervised learning method for the base classifier. ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems. ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition. In addition, strong results on several benchmark datasets using both decision trees and neural networks support the proposed method."
            },
            "slug": "Exploiting-unlabeled-data-in-ensemble-methods-Bennett-Demiriz",
            "title": {
                "fragments": [],
                "text": "Exploiting unlabeled data in ensemble methods"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "An adaptive semi-supervised ensemble method, ASSEMBLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data and can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723884"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 917949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89c3aed3e1219985c4a832f09adcd1df7096bf60",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study a special kind of learning problem in which each training instance is given a set of (or distribution over) candidate class labels and only one of the candidate labels is the correct one. Such a problem can occur, e.g., in an information retrieval setting where a set of words is associated with an image, or if classes labels are organized hierarchically. We propose a novel discriminative approach for handling the ambiguity of class labels in the training examples. The experiments with the proposed approach over five different UCI datasets show that our approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label. In contrast, naive methods degrade rapidly as more ambiguity is introduced into the labels."
            },
            "slug": "Learning-with-Multiple-Labels-Jin-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Learning with Multiple Labels"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a novel discriminative approach for handling the ambiguity of class labels in the training examples and shows that the approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1966951"
                        ],
                        "name": "Yasutoshi Yajima",
                        "slug": "Yasutoshi-Yajima",
                        "structuredName": {
                            "firstName": "Yasutoshi",
                            "lastName": "Yajima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasutoshi Yajima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425259"
                        ],
                        "name": "T. Hoshiba",
                        "slug": "T.-Hoshiba",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Hoshiba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hoshiba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18614115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49599ebab0d31e12e9f9ff90cd064d29370ca0ad",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present new approaches for semi-supervised learning based on the formulations of SVMs for the conventional supervised setting. The manifold structure of the data points given by the graph Laplacian can be taken into account in a efficient way. The proposed optimization problems fully enjoy the sparse structure of the graph Laplacian, which enables us to optimize the problems with a large number of data points in a practical amount of computational time. Some results of experiments showing the performance of our approaches are presented."
            },
            "slug": "Optimization-approaches-for-semi-supervised-Yajima-Hoshiba",
            "title": {
                "fragments": [],
                "text": "Optimization approaches for semi-supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "New approaches for semi-supervised learning based on the formulations of SVMs for the conventional supervised setting are presented, which fully enjoy the sparse structure of the graph Laplacian, which enables them to optimize the problems with a large number of data points in a practical amount of computational time."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth International Conference on Machine Learning and Applications (ICMLA'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17137268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f83288f12b1ea85f3166be427d04a77c04de0330",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world classification problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classification of such structured variables. In this paper, we investigate structured classification in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points."
            },
            "slug": "Maximum-Margin-Semi-Supervised-Learning-for-Altun-McAllester",
            "title": {
                "fragments": [],
                "text": "Maximum Margin Semi-Supervised Learning for Structured Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and derives a maximum-margin formulation of semi-supervised learning for structured variables."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6789724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38a49f2d906b48a36ab4baca448298666a9ec259",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classification functions are naturally defined only on the sub-manifold in question rather than the total ambient space. Using the Laplace Beltrami operator one produces a basis for a Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples are required. Once a basis is obtained, training can be performed using the labeled data set. Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace Beltrami operator by the graph Laplacian. Practical applications to image and text classification are considered."
            },
            "slug": "Using-manifold-structure-for-partially-labelled-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Using manifold structure for partially labelled classification"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An algorithmic framework to classify a partially labeled data set in a principled manner under the assumption that the data lie on a submanifold in a high dimensional space is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8390207"
                        ],
                        "name": "Mikhail Bilenko and Sugato Basu",
                        "slug": "Mikhail-Bilenko-and-Sugato-Basu",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Basu",
                            "middleNames": [
                                "Bilenko",
                                "and",
                                "Sugato"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Bilenko and Sugato Basu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 191896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38d86e8c19ec63c3a665ccf4f3d8179fe9620ad6",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, a number of methods have been proposed for semi-supervised clustering that employ supervision in the form of pairwise constraints. We describe a probabilistic model for semisupervised clustering based on Hidden Markov Random Fields (HMRFs) that incorporates relational supervision. The model leads to an EMstyle clustering algorithm, the E-step of which requires collective assignment of instances to cluster centroids under the constraints. We evaluate three known techniques for such collective assignment: belief propagation, linear programming relaxation, and iterated conditional modes (ICM). The first two methods attempt to globally approximate the optimal assignment, while ICM is a greedy method. Experimental results indicate that global methods outperform the greedy approach when relational supervision is limited, while their benefits diminish as more pairwise constraints are provided."
            },
            "slug": "A-Comparison-of-Inference-Techniques-for-Clustering-Basu",
            "title": {
                "fragments": [],
                "text": "A Comparison of Inference Techniques for Semi-supervised Clustering with Hidden Markov Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work describes a probabilistic model for semisupervised clustering based on Hidden Markov Random Fields that incorporates relational supervision and evaluates three known techniques for such collective assignment: belief propagation, linear programming relaxation, and iterated conditional modes (ICM)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48584242"
                        ],
                        "name": "M. K\u00e4\u00e4ri\u00e4inen",
                        "slug": "M.-K\u00e4\u00e4ri\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "K\u00e4\u00e4ri\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. K\u00e4\u00e4ri\u00e4inen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14784619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1d94dff8762a0f4c26fd9f3b1f0debe53161995",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two new methods for obtaining generalization error bounds in a semi-supervised setting. Both methods are based on approximating the disagreement probability of pairs of classifiers using unlabeled data. The first method works in the realizable case. It suggests how the ERM principle can be refined using unlabeled data and has provable optimality guarantees when the number of unlabeled examples is large. Furthermore, the technique extends easily to cover active learning. A downside is that the method is of little use in practice due to its limitation to the realizable case. The idea in our second method is to use unlabeled data to transform bounds for randomized classifiers into bounds for simpler deterministic classifiers. As a concrete example of how the general method works in practice, we apply it to a bound based on cross-validation. The result is a semi-supervised bound for classifiers learned based on all the labeled data. The bound is easy to implement and apply and should be tight whenever cross-validation makes sense. Applying the bound to SVMs on the MNIST benchmark data set gives results that suggest that the bound may be tight enough to be useful in practice."
            },
            "slug": "Generalization-Error-Bounds-Using-Unlabeled-Data-K\u00e4\u00e4ri\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Generalization Error Bounds Using Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Two new methods for obtaining generalization error bounds in a semi-supervised setting based on approximating the disagreement probability of pairs of classifiers using unlabeled data are presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2341726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5662f02cee0afea188e1d59443b60263d4f7b92a",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A semi-supervised clustering algorithm is proposed that combines the benefits of supervised and unsupervised learning methods. The approach allows unlabeled data with no known class to be used to improve classification accuracy. The objective function of an unsupervised technique, e.g. K-means clustering, is modified to minimize both the cluster dispersion of the input attributes and a measure of cluster impurity based on the class labels. Minimizing the cluster dispersion of the examples is a form of capacity control to prevent overfitting. For the the output labels, impurity measures from decision tree algorithms such as the Gini index can be used. A genetic algorithm optimizes the objective function to produce clusters. Experimental results show that using class information improves the generalization ability compared to unsupervised methods based only on the input attributes."
            },
            "slug": "Semi-Supervised-Clustering-Using-Genetic-Algorithms-Demiriz-Bennett",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Clustering Using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results show that using class information improves the generalization ability compared to unsupervised methods based only on the input attributes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123887468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4c0dee1b1bfe2ba9fa87ebb526df2a288011533",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms based on local kernels are sensitive to the curse of dimensionality. These include local manifold learning algorithms such as Isomap and LLE, support vector classifiers with Gaussian or other local kernels, and graph-based semisupervised learning algorithms using a local similarity function. These algorithms are shown to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. There is a large class of data distributions for which non-local solutions could be expressed compactly and potentially be learned with few examples, but which will require a large number of local bases and therefore a large number of training examples when using a local learning algorithm."
            },
            "slug": "The-Curse-of-Dimensionality-for-Local-Kernel-Bengio-Delalleau",
            "title": {
                "fragments": [],
                "text": "The Curse of Dimensionality for Local Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A series of theoretical arguments support the claim that a large class of modern learning algorithms based on local kernels are sensitive to the curse of dimensionality, including local manifold learning algorithms such as Isomap and LLE, support vector classifiers with Gaussian or other local kernels, and graph-based semisupervised learning algorithms using a local similarity function."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312432"
                        ],
                        "name": "T. N. Lal",
                        "slug": "T.-N.-Lal",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lal",
                            "middleNames": [
                                "Navin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. N. Lal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 508435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46770a8e7e2af28f5253e5961f709be74e34c1f6",
            "isKey": false,
            "numCitedBy": 3895,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data."
            },
            "slug": "Learning-with-Local-and-Global-Consistency-Zhou-Bousquet",
            "title": {
                "fragments": [],
                "text": "Learning with Local and Global Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2071866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8db95dbd08e4ee64fb258e5380e78cfa507ed94d",
            "isKey": false,
            "numCitedBy": 1611,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE---though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm's performance---both successes and failures---and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction."
            },
            "slug": "Think-Globally,-Fit-Locally:-Unsupervised-Learning-Saul-Roweis",
            "title": {
                "fragments": [],
                "text": "Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifold"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data, is described and several extensions that enhance its performance are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2282762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc36397e1fef5c922d64e88211a7e08ecc64759",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies."
            },
            "slug": "Discriminative-Probabilistic-Models-for-Relational-Taskar-Abbeel",
            "title": {
                "fragments": [],
                "text": "Discriminative Probabilistic Models for Relational Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach is presented, showing how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058177533"
                        ],
                        "name": "Simon Tong",
                        "slug": "Simon-Tong",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Tong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7806109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8797f1d253c75669d96e6fcceda2be3f8534e1d",
            "isKey": false,
            "numCitedBy": 3138,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings."
            },
            "slug": "Support-Vector-Machine-Active-Learning-with-to-Text-Tong-Koller",
            "title": {
                "fragments": [],
                "text": "Support Vector Machine Active Learning with Applications to Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental results showing that employing the active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings are presented."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748188"
                        ],
                        "name": "Nicolas Chapados",
                        "slug": "Nicolas-Chapados",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Chapados",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Chapados"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15281230,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4f2b9a5953de46b48fc076f2821c1b6bb6da7861",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Metric-based methods have recently been introduced for model selection and regularization, often yielding very significant improvements over the alternatives tried (including cross-validation). All these methods require unlabeled data over which to compare functions and detect gross differences in behavior away from the training points. We introduce three new extensions of the metric model selection methods and apply them to feature selection. The first extension takes advantage of the particular case of time-series data in which the task involves prediction with a horizon h. The idea is to use at t the h unlabeled examples that precede t for model selection. The second extension takes advantage of the different error distributions of cross-validation and the metric methods: cross-validation tends to have a larger variance and is unbiased. A hybrid combining the two model selection methods is rarely beaten by any of the two methods. The third extension deals with the case when unlabeled data is not available at all, using an estimated input density. Experiments are described to study these extensions in the context of capacity control and feature subset selection."
            },
            "slug": "Extensions-to-Metric-Based-Model-Selection-Bengio-Chapados",
            "title": {
                "fragments": [],
                "text": "Extensions to Metric-Based Model Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Three new extensions of the metric model selection methods are introduced and applied to feature selection, one of which takes advantage of the particular case of time-series data in which the task involves prediction with a horizon h."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8451324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d8fdf390063e83f72e6321f55c708a206725aee",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We emphasize the need for input-dependent regularization in the context of conditional density models (also: discriminative models) like Gaussian process predictors. This can be achieved by a simple modification of the standard Bayesian data generation model un- derlying these techniques. Specifically, we allow the latent target function to be a- priori dependent on the distribution of the input points. While the standard genera- tion model results in robust predictors, data with missing labels is ignored, which can be wasteful if relevant prior knowledge is avail- able. We show that discriminative mod- els like Fisher kernel discriminants and Co- Training classifiers can be regarded as (ap- proximate) Bayesian inference techniques un- der the modified generation model, and that the template Co-Training algorithm is related to a variant of the well-known Expectation- Maximization (EM) technique. We propose a template EM algorithm for the modified generation model which can be regarded as generalization of Co-Training."
            },
            "slug": "Input-dependent-Regularization-of-Conditional-Seeger",
            "title": {
                "fragments": [],
                "text": "Input-dependent Regularization of Conditional Density Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that discriminative mod- els like Fisher kernel discriminants and Co- Training classifiers can be regarded as Bayesian inference techniques un- der the modified generation model, and that the template Co-Training algorithm is related to a variant of the well-known Expectation- Maximization technique."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 859162,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c0ece611643cfb8f3a23e4802c754ea583ebe37",
            "isKey": false,
            "numCitedBy": 1013,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \"seed\" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context inwhich it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98)."
            },
            "slug": "Unsupervised-Models-for-Named-Entity-Classification-Collins-Singer",
            "title": {
                "fragments": [],
                "text": "Unsupervised Models for Named Entity Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \"seed\" rules, gaining leverage from natural redundancy in the data."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 686980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2de29049d62de925cf709024b92774cd82b0a5a",
            "isKey": false,
            "numCitedBy": 3072,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%."
            },
            "slug": "Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Text Classification from Labeled and Unlabeled Documents using EM"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents, and presents two extensions to the algorithm that improve classification accuracy under these conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50263663"
                        ],
                        "name": "D. Boswell",
                        "slug": "D.-Boswell",
                        "structuredName": {
                            "firstName": "Dustin",
                            "lastName": "Boswell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boswell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18986102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "isKey": false,
            "numCitedBy": 2113,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines (SVM\u2019s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM\u2019s introduce the notion of a \u201ckernel induced feature space\u201d which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM\u2019s is that the higher-dimensional space doesn\u2019t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system\u2019s likelihood to perform well on unseen data) of SVM\u2019s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM\u2019s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM\u2019s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than \u201cyes/no\u201d classification)."
            },
            "slug": "Introduction-to-Support-Vector-Machines-Boswell",
            "title": {
                "fragments": [],
                "text": "Introduction to Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Support Vector Machines (SVM\u2019s) are intuitive, theoretically wellfounded, and have shown to be practically successful."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716876"
                        ],
                        "name": "O. Dekel",
                        "slug": "O.-Dekel",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Dekel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dekel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 124060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff0c2ec329cc36e4c78b147a3921099175633e5b",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Label ranking is the task of inferring a total order over a predefined set of labels for each given instance. We present a general framework for batch learning of label ranking functions from supervised data. We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent. This enables us to accommodate a variety of ranking problems. In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels. Special cases of our setting are multilabel categorization and hierarchical classification. We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration. The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus."
            },
            "slug": "Log-Linear-Models-for-Label-Ranking-Dekel-Manning",
            "title": {
                "fragments": [],
                "text": "Log-Linear Models for Label Ranking"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents a general boosting-based learning algorithm for the label ranking problem and proves a lower bound on the progress of each boosting iteration."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2255318"
                        ],
                        "name": "R. Liere",
                        "slug": "R.-Liere",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Liere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Liere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729906"
                        ],
                        "name": "Prasad Tadepalli",
                        "slug": "Prasad-Tadepalli",
                        "structuredName": {
                            "firstName": "Prasad",
                            "lastName": "Tadepalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasad Tadepalli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7530337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80ef14d2a1b8c7efbf45bedae9d001fe5446c7de",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In many real-world domains, supervised learning requires a large number of training examples. In this paper, we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning. Our approach is similar to the Query by Committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label. Our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant. We report here on experiments using a committee of Winnowbased learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by l-2 orders of magnitude. 1. Hntroduction"
            },
            "slug": "Active-Learning-with-Committees-for-Text-Liere-Tadepalli",
            "title": {
                "fragments": [],
                "text": "Active Learning with Committees for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper reports on experiments using a committee of Winnowbased learners and demonstrates that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by l-2 orders of magnitude."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 623918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "141e6c1dd532504611266d08458dbe2a0dbb4e98",
            "isKey": false,
            "numCitedBy": 1597,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes."
            },
            "slug": "Multiple-kernel-learning,-conic-duality,-and-the-Bach-Lanckriet",
            "title": {
                "fragments": [],
                "text": "Multiple kernel learning, conic duality, and the SMO algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Experimental results are presented that show that the proposed novel dual formulation of the QCQP as a second-order cone programming problem is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16025939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9755f9993553131e5cc796d34ecaa624fe0ddffa",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been increasing interest in using unlabeled data for classiica-tion. However, whether these unlabeled data are truly useful is still under debate. In order to have a better understanding of relevant issues, it is worthwhile to precisely formulate the problem and carefully analyze the value of unlabeled data under certain learning models. In this paper, we approach this problem from the statistical point of view, where we assume that a correct model of the underlying distribution is given. We demonstrate that Fisher information matrices can be used to judge the asymp-totic value of unlabeled data. We apply this methodology to both \\passive partially supervised learning\" and \\active learning\", and draw conclusions from this analysis. Experiments will be provided to support our claims."
            },
            "slug": "The-Value-of-Unlabeled-Data-for-Classification-Zhang",
            "title": {
                "fragments": [],
                "text": "The Value of Unlabeled Data for Classification Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that Fisher information matrices can be used to judge the asymp-totic value of unlabeled data and this methodology is applied to both passive partially supervised learning and active learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1208015,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "22a2a99643109ba39ad427a41deaa30fb36bcf12",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The 2-class transduction problem, as formulated by Vapnik [1], involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points. In this form, the problem has exponential computational complexity in the size of the working set. So far it has been attacked by means of integer programming techniques [2] that do not scale to reasonable problem sizes, or by local search procedures [3]. In this paper we present a relaxation of this task based on semi-definite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set. The results are very encouraging for mid sized data sets, however the cost is still too high for large scale problems, due to the high dimensional search space. To this end, we restrict the feasible region by introducing an approximation based on solving an eigenproblem. With this approximation, the computational cost of the algorithm is such that problems with more than 1000 points can be treated."
            },
            "slug": "Convex-Methods-for-Transduction-Bie-Cristianini",
            "title": {
                "fragments": [],
                "text": "Convex Methods for Transduction"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents a relaxation of the 2-class transduction problem based on semi-definite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879453"
                        ],
                        "name": "V. Castelli",
                        "slug": "V.-Castelli",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Castelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Castelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35473938,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f67e9a6bd7c688f1c9c653584a4fa1f9c7fda2a6",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-exponential-value-of-labeled-samples-Castelli-Cover",
            "title": {
                "fragments": [],
                "text": "On the exponential value of labeled samples"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6789514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces some generalizations of Vapnik's (1982) method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a \"large margin\". This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (1971) dimension measured on the sample."
            },
            "slug": "Structural-Risk-Minimization-Over-Data-Dependent-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization Over Data-Dependent Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A result is presented that allows one to trade off errors on the training sample against improved generalization performance, and a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716445"
                        ],
                        "name": "F. Southey",
                        "slug": "F.-Southey",
                        "structuredName": {
                            "firstName": "Finnegan",
                            "lastName": "Southey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Southey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2605508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094271a91510105dd01eb3b8cf97a5f210b1c938",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general approach to model selection and regularization that exploits unlabeled data to adaptively control hypothesis complexity in supervised learning tasks. The idea is to impose a metric structure on hypotheses by determining the discrepancy between their predictions across the distribution of unlabeled data. We show how this metric can be used to detect untrustworthy training error estimates, and devise novel model selection strategies that exhibit theoretical guarantees against over-fitting (while still avoiding under-fitting). We then extend the approach to derive a general training criterion for supervised learning\u2014yielding an adaptive regularization method that uses unlabeled data to automatically set regularization parameters. This new criterion adjusts its regularization level to the specific set of training data received, and performs well on a variety of regression and conditional density estimation tasks. The only proviso for these methods is that sufficient unlabeled training data be available."
            },
            "slug": "Metric-Based-Methods-for-Adaptive-Model-Selection-Schuurmans-Southey",
            "title": {
                "fragments": [],
                "text": "Metric-Based Methods for Adaptive Model Selection and Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A general approach to model selection and regularization that exploits unlabeled data to adaptively control hypothesis complexity in supervised learning tasks and derives a general training criterion for supervised learning that adjusts its regularization level to the specific set of training data received, and performs well on a variety of regression and conditional density estimation tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39651651"
                        ],
                        "name": "Jean-Fran\u00e7ois Paiement",
                        "slug": "Jean-Fran\u00e7ois-Paiement",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Paiement",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Fran\u00e7ois Paiement"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120888"
                        ],
                        "name": "M. Ouimet",
                        "slug": "M.-Ouimet",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Ouimet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ouimet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1944221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfa15801bf4e7bf610d38dc86da62b83e2ddedcb",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this letter, we show a direct relation between spectral embedding methods and kernel principal components analysis and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density. Whereas spectral embedding methods provided only coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nystrm formula) for multidimensional scaling (MDS), spectral clustering, Laplacian eigenmaps, locally linear embedding (LLE), and Isomap. The analysis provides, for all such spectral embedding methods, the definition of a loss function, whose empirical average is minimized by the traditional algorithms. The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn. Experiments with LLE, Isomap, spectral clustering, and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding."
            },
            "slug": "Learning-Eigenfunctions-Links-Spectral-Embedding-Bengio-Delalleau",
            "title": {
                "fragments": [],
                "text": "Learning Eigenfunctions Links Spectral Embedding and Kernel PCA"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A direct relation is shown between spectral embedding methods and kernel principal components analysis and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2643381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1a2d203733208deda7427c8e20318334193d9d7",
            "isKey": false,
            "numCitedBy": 3026,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many \"plausible\" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider \"similar.\" For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \u211dn, learns a distance metric over \u211dn that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance."
            },
            "slug": "Distance-Metric-Learning-with-Application-to-with-Xing-Ng",
            "title": {
                "fragments": [],
                "text": "Distance Metric Learning with Application to Clustering with Side-Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \ufffd\u201dn, learns a distance metric over \u211dn that respects these relationships."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104537710"
                        ],
                        "name": "J. MacQueen",
                        "slug": "J.-MacQueen",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "MacQueen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. MacQueen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6278891,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed",
            "isKey": false,
            "numCitedBy": 24211,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called 'k-means,' appears to give partitions which are reasonably efficient in the sense of within-class variance. That is, if p is the probability mass function for the population, S = {S1, S2, * *, Sk} is a partition of EN, and ui, i = 1, 2, * , k, is the conditional mean of p over the set Si, then W2(S) = ff=ISi f z u42 dp(z) tends to be low for the partitions S generated by the method. We say 'tends to be low,' primarily because of intuitive considerations, corroborated to some extent by mathematical analysis and practical computational experience. Also, the k-means procedure is easily programmed and is computationally economical, so that it is feasible to process very large samples on a digital computer. Possible applications include methods for similarity grouping, nonlinear prediction, approximating multivariate distributions, and nonparametric tests for independence among several variables. In addition to suggesting practical classification methods, the study of k-means has proved to be theoretically interesting. The k-means concept represents a generalization of the ordinary sample mean, and one is naturally led to study the pertinent asymptotic behavior, the object being to establish some sort of law of large numbers for the k-means. This problem is sufficiently interesting, in fact, for us to devote a good portion of this paper to it. The k-means are defined in section 2.1, and the main results which have been obtained on the asymptotic behavior are given there. The rest of section 2 is devoted to the proofs of these results. Section 3 describes several specific possible applications, and reports some preliminary results from computer experiments conducted to explore the possibilities inherent in the k-means idea. The extension to general metric spaces is indicated briefly in section 4. The original point of departure for the work described here was a series of problems in optimal classification (MacQueen [9]) which represented special"
            },
            "slug": "Some-methods-for-classification-and-analysis-of-MacQueen",
            "title": {
                "fragments": [],
                "text": "Some methods for classification and analysis of multivariate observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3035742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79f93de1c06dfed28df4de7c2cc9d3822c2f401",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents probabilistic modeling methods to solve the problem of discriminating between five facial orientations with very little labeled data. Three models are explored. The first model maintains no inter-pixel dependencies, the second model is capable of modeling a set of arbitrary pair-wise dependencies, and the last model allows dependencies only between neighboring pixels. We show that for all three of these models, the accuracy of the learned models can be greatly improved by augmenting a small number of labeled training images with a large set of unlabeled images using Expectation-Maximization. This is important because it is often difficult to obtain image labels, while many unlabeled images are readily available. Through a large set of empirical tests, we examine the benefits of unlabeled data for each of the models. By using only two randomly selected labeled examples per class, we can discriminate between the five facial orientations with an accuracy of 94%; with six labeled examples, we achieve an accuracy of 98%."
            },
            "slug": "Probabilistic-Modeling-for-Face-Orientation-from-Baluja",
            "title": {
                "fragments": [],
                "text": "Probabilistic Modeling for Face Orientation Discrimination: Learning from Labeled and Unlabeled Data"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6027413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49b8dff62cccc26023c876460234bf29084a382f",
            "isKey": false,
            "numCitedBy": 734,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case."
            },
            "slug": "Transductive-Learning-via-Spectral-Graph-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Learning via Spectral Graph Partitioning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes an algorithm that robustly achieves good generalization performance and that can be trained efficiently, and shows a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1460876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63287d3220fe96d5cbf73067545abbb88cc180a6",
            "isKey": false,
            "numCitedBy": 426,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%."
            },
            "slug": "Learning-to-Classify-Text-from-Labeled-and-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Learning to Classify Text from Labeled and Unlabeled Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents, and an algorithm is introduced based on the combination of Expectation-Maximization with a naive Bayes classifier."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40632403"
                        ],
                        "name": "Sugato Basu",
                        "slug": "Sugato-Basu",
                        "structuredName": {
                            "firstName": "Sugato",
                            "lastName": "Basu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sugato Basu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144205696"
                        ],
                        "name": "A. Banerjee",
                        "slug": "A.-Banerjee",
                        "structuredName": {
                            "firstName": "Arindam",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2852345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95a2ade2834ce6a85d0e8e6b82c81deb34cd115d",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised clustering uses a small amount of supervised data to aid unsupervised learning. One typical approach specifies a limited number of must-link and cannotlink constraints between pairs of examples. This paper presents a pairwise constrained clustering framework and a new method for actively selecting informative pairwise constraints to get improved clustering performance. The clustering and active learning methods are both easily scalable to large datasets, and can handle very high dimensional data. Experimental and theoretical results confirm that this active querying of pairwise constraints significantly improves the accuracy of clustering when given a relatively small amount of supervision."
            },
            "slug": "Active-Semi-Supervision-for-Pairwise-Constrained-Basu-Banerjee",
            "title": {
                "fragments": [],
                "text": "Active Semi-Supervision for Pairwise Constrained Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental and theoretical results confirm that this active querying of pairwise constraints significantly improves the accuracy of clustering when given a relatively small amount of supervision."
            },
            "venue": {
                "fragments": [],
                "text": "SDM"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6392609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56d957ec64a7ab2ac63c1856af5db3f9beb0dab6",
            "isKey": false,
            "numCitedBy": 738,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of \u201crobust\u201d learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model, a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given access to an oracle providing estimates of probabilities over the sample space of random examples. One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant\u2019s model, with a noise rate approaching the informationtheoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant\u2019s model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known."
            },
            "slug": "Efficient-noise-tolerant-learning-from-statistical-Kearns",
            "title": {
                "fragments": [],
                "text": "Efficient noise-tolerant learning from statistical queries"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper formalizes a new but related model of learning from statistical queries, and demonstrates the generality of the statistical query model, showing that practically every class learnable in Valiant\u2019s model and its variants can also be learned in the new model (and thus can be learning in the presence of noise)."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729828"
                        ],
                        "name": "A. Garg",
                        "slug": "A.-Garg",
                        "structuredName": {
                            "firstName": "Ashutosh",
                            "lastName": "Garg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9342526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9aaf60736a8e51f3550decab7fde991db597fd37",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic classifiers are developed by assuming generative models which are product distributions over the original attribute space (as in naive Bayes) or more involved spaces (as in general Bayesian networks). While this paradigm has been shown experimentally successful on real world applications, despite vastly simplified probabilistic assumptions, the question of why these approaches work is still open. \n \nThis paper resolves this question.We show that almost all joint distributions with a given set of marginals (i.e., all distributions that could have given rise to the classifier learned) or, equivalently, almost all data sets that yield this set of marginals, are very close (in terms of distributional distance) to the product distribution on the marginals; the number of these distributions goes down exponentially with their distance from the product distribution. Consequently, as we show, for almost all joint distributions with this set of marginals, the penalty incurred in using the marginal distribution rather than the true one is small. In addition to resolving the puzzle surrounding the success of probabilistic classifiers our results contribute to understanding the tradeoffs in developing probabilistic classifiers and will help in developing better classifiers."
            },
            "slug": "Understanding-Probabilistic-Classifiers-Garg-Roth",
            "title": {
                "fragments": [],
                "text": "Understanding Probabilistic Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Almost all joint distributions with a given set of marginals, are very close (in terms of distributional distance) to the product distribution on the marginals; the number of these distributions goes down exponentially with their distance from the product Distribution."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4829,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 402174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35d56a2f2f41864c512d5c54a16378cd9b625157",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a measure of clustering quality or accuracy that is appropriate in situations where it is desirable to evaluate a clustering algorithm by somehow comparing the clusters it produces with ``ground truth' consisting of classes assigned to the patterns by manual means or some other means in whose veracity there is confidence. Such measures are refered to as ``external'. Our measure also has the characteristic of allowing clusterings with different numbers of clusters to be compared in a quantitative and principled way. Our evaluation scheme quantitatively measures how useful the cluster labels of the patterns are as predictors of their class labels. In cases where all clusterings to be compared have the same number of clusters, the measure is equivalent to the mutual information between the cluster labels and the class labels. In cases where the numbers of clusters are different, however, it computes the reduction in the number of bits that would be required to encode (compress) the class labels if both the encoder and decoder have free acccess to the cluster labels. To achieve this encoding the estimated conditional probabilities of the class labels given the cluster labels must also be encoded. These estimated probabilities can be seen as a model for the class labels and their associated code length as a model cost."
            },
            "slug": "An-Information-Theoretic-External-Cluster-Validity-Dom",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic External Cluster-Validity Measure"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A measure of clustering quality or accuracy that is appropriate in situations where it is desirable to evaluate a clustering algorithm by somehow comparing the clusters it produces with ``ground truth' consisting of classes assigned to the patterns by manual means or some other means in whose veracity there is confidence is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1761253,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7de4569c7353030fec21bbb38c06323dd69f777c",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate a principle for classification with the knowledge of the marginal distribution over the data points (unlabeled data). The principle is cast in terms of Tikhonov style regularization where the regularization penalty articulates the way in which the marginal density should constrain otherwise unrestricted conditional distributions. Specifically, the regularization penalty penalizes any information introduced between the examples and labels beyond what is provided by the available labeled examples. The work extends Szummer and Jaakkola's information regularization (NIPS 2002) to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation. We show in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound. We illustrate the regularization principle in practice by restricting the class of conditional distributions to be logistic regression models and constructing the regularization penalty from a finite set of unlabeled examples."
            },
            "slug": "On-Information-Regularization-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "On Information Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The work extends Szummer and Jaakkola's information regularization to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation, and shows in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60532258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "248a297d786228a183fcae64023092660550fcd2",
            "isKey": false,
            "numCitedBy": 1753,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Based on ideas from Support Vector Machines (SVMs), Learning To Classify Text Using Support Vector Machines presents a new approach to generating text classifiers from examples. The approach combines high performance and efficiency with theoretical understanding and improved robustness. In particular, it is highly effective without greedy heuristic components. The SVM approach is computationally efficient in training and classification, and it comes with a learning theory that can guide real-world applications. Learning To Classify Text Using Support Vector Machines gives a complete and detailed description of the SVM approach to learning text classifiers, including training algorithms, transductive text classification, efficient performance estimation, and a statistical learning model of text classification. In addition, it includes an overview of the field of text classification, making it self-contained even for newcomers to the field. This book gives a concise introduction to SVMs for pattern recognition, and it includes a detailed description of how to formulate text-classification tasks for machine learning."
            },
            "slug": "Learning-to-classify-text-using-support-vector-and-Joachims",
            "title": {
                "fragments": [],
                "text": "Learning to classify text using support vector machines - methods, theory and algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book gives a concise introduction to SVMs for pattern recognition, and it includes a detailed description of how to formulate text-classification tasks for machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "The Kluwer international series in engineering and computer science"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145536952"
                        ],
                        "name": "J. Kandola",
                        "slug": "J.-Kandola",
                        "structuredName": {
                            "firstName": "Jaz",
                            "lastName": "Kandola",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kandola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10304437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb8dac27694a14b9b1ff9ef1fe04387bc14468f1",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results."
            },
            "slug": "Spectral-Kernel-Methods-for-Clustering-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Spectral Kernel Methods for Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper introduces new algorithms for unsupervised learning based on the use of a kernel matrix, and shows how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150220964"
                        ],
                        "name": "Monperrus Martin",
                        "slug": "Monperrus-Martin",
                        "structuredName": {
                            "firstName": "Monperrus",
                            "lastName": "Martin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Monperrus Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14850798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc1d132c79a72dba386dc47750a49b0c29b54568",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation suggests to explore non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions. A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data (on learning handwritten character image rotations), where a local non-parametric method fails."
            },
            "slug": "Non-Local-Manifold-Tangent-Learning-Bengio-Martin",
            "title": {
                "fragments": [],
                "text": "Non-Local Manifold Tangent Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "It is claimed and presented arguments that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400556488"
                        ],
                        "name": "Aharon Bar-Hillel",
                        "slug": "Aharon-Bar-Hillel",
                        "structuredName": {
                            "firstName": "Aharon",
                            "lastName": "Bar-Hillel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aharon Bar-Hillel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774536"
                        ],
                        "name": "T. Hertz",
                        "slug": "T.-Hertz",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Hertz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787804"
                        ],
                        "name": "N. Shental",
                        "slug": "N.-Shental",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shental",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shental"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789171"
                        ],
                        "name": "D. Weinshall",
                        "slug": "D.-Weinshall",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Weinshall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Weinshall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6865208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0029cf474bce70cfab2e944c4be01f99e741f1f4",
            "isKey": false,
            "numCitedBy": 504,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of learning distance metrics using side-information in the form of groups of \"similar\" points. We propose to use the RCA algorithm, which is a simple and efficient algorithm for learning a full ranked Mahalanobis metric (Shental et al., 2002). We first show that RCA obtains the solution to an interesting optimization problem, founded on an information theoretic basis. If the Mahalanobis matrix is allowed to be singular, we show that Fisher's linear discriminant followed by RCA is the optimal dimensionality reduction algorithm under the same criterion. We then show how this optimization problem is related to the criterion optimized by another recent algorithm for metric learning (Xing et al., 2002), which uses the same kind of side information. We empirically demonstrate that learning a distance metric using the RCA algorithm significantly improves clustering performance, similarly to the alternative algorithm. Since the RCA algorithm is much more efficient and cost effective than the alternative, as it only uses closed form expressions of the data, it seems like a preferable choice for the learning of full rank Mahalanobis distances."
            },
            "slug": "Learning-Distance-Functions-using-Equivalence-Bar-Hillel-Hertz",
            "title": {
                "fragments": [],
                "text": "Learning Distance Functions using Equivalence Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is empirically demonstrate that learning a distance metric using the RCA algorithm significantly improves clustering performance, similarly to the alternative algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746222"
                        ],
                        "name": "\u00c9. Tardos",
                        "slug": "\u00c9.-Tardos",
                        "structuredName": {
                            "firstName": "\u00c9va",
                            "lastName": "Tardos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9. Tardos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16241328,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d0794c1a57cad9c35028427e6c084642346c720f",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "In a traditional classification problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified; this issue is one of the principal motivations for the framework of Markov random fields, and it arises in areas such as image processing, biometry: and document analysis. In its most basic form, this style of analysis seeks a classification that optimizes a combinatorial function consisting of assignment costs-based on the individual choice of label we make for each object-and separation costs-based on the pair of choices we make for two \"related\" objects. We formulate a general classification problem of this type, the metric labeling problem; we show that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields. From the perspective of combinatorial optimization, our problem can be viewed as a substantial generalization of the multiway cut problem, and equivalent to a type of uncapacitated quadratic assignment problem. We provide the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type. Our main result is an O(log k log log k)-approximation algorithm for the metric labeling problem, with respect to an arbitrary metric on a set of k labels, and an arbitrary weighted graph of relationships on a set of objects. For the special case in which the labels are endowed with the uniform metric-all distances are the same-our methods provide a 2-approximation."
            },
            "slug": "Approximation-algorithms-for-classification-with-Kleinberg-Tardos",
            "title": {
                "fragments": [],
                "text": "Approximation algorithms for classification problems with pairwise relationships: metric labeling and Markov random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work provides the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type, the metric labeling problem, and shows that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields."
            },
            "venue": {
                "fragments": [],
                "text": "40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444268"
                        ],
                        "name": "R. Lordo",
                        "slug": "R.-Lordo",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Lordo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lordo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20604466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b993cfb0321fc546ca6265dcd7859c0c72e2ee25",
            "isKey": false,
            "numCitedBy": 995,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "This book consists of a collection of new wavelet techniques that can be used to explore several types of statistical problems, including but not limited to nonparametric regression, density estimation, and change-point problems. It provides an easy but solid introduction to available wavelet tools from an applied point of view. The main focus of this book seems to be development of the applied aspects of statistical functional estimation using a variety of wavelet methods. This book is intended as a reference for advanced undergraduateto graduate-level courses. It relies exclusively on several worked-out examples and provides step-by-step methods for most of the illustrated examples. This is de\u008e nitely a plus point for readers who want to get their hands dirty with wavelet tools. In fact, the reader may want to download the S-PLUS codes from the Web site cited by the author. The introductory chapters provide an overview of essential theory of wavelets, and these results are used throughout the text. Among the basic topics covered in this book are dyadic wavelets, time-frequency localization, wavelet transforms, frames, spline wavelets, orthonormal wavelet bases, and wavelet packets. In addition, the author presents generalizations and extensions to twodimensional wavelets and translation-invariant wavelet smoothing. The book requires a background in undergraduate calculus, linear algebra, and basic statistical theory. The \u201cmeat\u201d part of the book lies in its Chapter 4, in which the author presents several \u201cwavelet features and examples.\u201d The essential theory on wavelet decomposition and reconstruction is presented in a manner that is easy to follow and does not require substantial knowledge of advanced theory of functional analysis. The author then presents fundamental concepts of \u008e lter representation and time-frequency localization. Finally, all of the aforementioned topics are then illustrated via several wavelet examples. Chapters 6 and 7 provide essential concepts for any researcher who is interested in statistical inference for wavelet-based models but is not necessarily an expert in either. The book has achieved its goal of presenting basic wavelet concepts in an understandable way to an audience familiar with the basic theory of statistics. The central theme of the book seems to focus on analyzing several statistical models using ready-made wavelet tools. The material covered in each chapter sometimes seems limited; emphasis on geometrical appeal to wavelets is not addressed. In summary, Essential Wavelets for Statistical Applications and Data Analysis does a good job of presenting wavelet tools to explain various aspects of statistical modeling. A very nice aspect of this book is that it provides a Web site reference that contains most additional resources, such as S-PLUS codes that were used to generate graphics used in the book. However, the book lacks the elegant geometrical approach of wavelet methods, but that is partially the nature of the material. Such fundamental geometrical concepts might turn out to be dif\u008e cult to follow for the beginners. Unfortunately, most books on wavelets are primarily accessible to research statisticians. This book presents basic and advanced concepts of wavelets in a way that is accessible to anyone with only a fundamental knowledge of statistical and mathematical theory. The reader may also want to read a book by Vidakovic (1999) that also presents ideas similar to those developed in this one. I liked the book very much and would not hesitate to recommend its usage in a classroom as reference book for a course on statistical inference based on wavelet models."
            },
            "slug": "Learning-from-Data:-Concepts,-Theory,-and-Methods-Lordo",
            "title": {
                "fragments": [],
                "text": "Learning from Data: Concepts, Theory, and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This book consists of a collection of new wavelet techniques that can be used to explore several types of statistical problems, including but not limited to nonparametric regression, density estimation, and change-point problems."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7668712"
                        ],
                        "name": "Fabio Gagliardi Cozman",
                        "slug": "Fabio-Gagliardi-Cozman",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Cozman",
                            "middleNames": [
                                "Gagliardi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Gagliardi Cozman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49641404"
                        ],
                        "name": "I. Cohen",
                        "slug": "I.-Cohen",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156170"
                        ],
                        "name": "M. C. Cirelo",
                        "slug": "M.-C.-Cirelo",
                        "structuredName": {
                            "firstName": "Marcelo",
                            "lastName": "Cirelo",
                            "middleNames": [
                                "Cesar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Cirelo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16974352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db9203b1a30b2edf51fdac23f661a99672459a85",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the performance of semi-supervised learning of mixture models. We show that unlabeled data can lead to an increase in classification error even in situations where additional labeled data would decrease classification error. We present a mathematical analysis of this \"degradation\" phenomenon and show that it is due to the fact that bias may be adversely affected by unlabeled data. We discuss the impact of these theoretical results to practical situations."
            },
            "slug": "Semi-Supervised-Learning-of-Mixture-Models-Cozman-Cohen",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning of Mixture Models"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper analyzes the performance of semi-supervised learning of mixture models and shows that unlabeled data can lead to an increase in classification error even in situations where additional labeled data would decrease classification error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054056341"
                        ],
                        "name": "B. Fischer",
                        "slug": "B.-Fischer",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145677861"
                        ],
                        "name": "V. Roth",
                        "slug": "V.-Roth",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5766565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98cc2fe063031b2682d468d449666062f4e4c06",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering aims at extracting hidden structure in dataset. While the problem of finding compact clusters has been widely studied in the literature, extracting arbitrarily formed elongated structures is considered a much harder problem. In this paper we present a novel clustering algorithm which tackles the problem by a two step procedure: first the data are transformed in such a way that elongated structures become compact ones. In a second step, these new objects are clustered by optimizing a compactness-based criterion. The advantages of the method over related approaches are threefold: (i) robustness properties of compactness-based criteria naturally transfer to the problem of extracting elongated structures, leading to a model which is highly robust against outlier objects; (ii) the transformed distances induce a Mercer kernel which allows us to formulate a polynomial approximation scheme to the generally NP-hard clustering problem; (iii) the new method does not contain free kernel parameters in contrast to methods like spectral clustering or mean-shift clustering."
            },
            "slug": "Clustering-with-the-Connectivity-Kernel-Fischer-Roth",
            "title": {
                "fragments": [],
                "text": "Clustering with the Connectivity Kernel"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel clustering algorithm which tackles the problem by a two step procedure: first the data are transformed in such a way that elongated structures become compact ones, and in a second step these new objects are clustered by optimizing a compactness-based criterion."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1402489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddb2567f1e3630529b05c19c4fe99720de4b481b",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a directed graph in which some of the nodes are labeled, we investigate the question of how to exploit the link structure of the graph to infer the labels of the remaining unlabeled nodes. To that extent we propose a regularization framework for functions defined over nodes of a directed graph that forces the classification function to change slowly on densely linked subgraphs. A powerful, yet computationally simple classification algorithm is derived within the proposed framework. The experimental evaluation on real-world Web classification problems demonstrates encouraging results that validate our approach."
            },
            "slug": "Semi-supervised-Learning-on-Directed-Graphs-Zhou-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Learning on Directed Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A regularization framework for functions defined over nodes of a directed graph that forces the classification function to change slowly on densely linked subgraphs is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144205696"
                        ],
                        "name": "A. Banerjee",
                        "slug": "A.-Banerjee",
                        "structuredName": {
                            "firstName": "Arindam",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783667"
                        ],
                        "name": "I. Dhillon",
                        "slug": "I.-Dhillon",
                        "structuredName": {
                            "firstName": "Inderjit",
                            "lastName": "Dhillon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dhillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34724702"
                        ],
                        "name": "Joydeep Ghosh",
                        "slug": "Joydeep-Ghosh",
                        "structuredName": {
                            "firstName": "Joydeep",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joydeep Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072326"
                        ],
                        "name": "S. Sra",
                        "slug": "S.-Sra",
                        "structuredName": {
                            "firstName": "Suvrit",
                            "lastName": "Sra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7581749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11de3f9770b08484b28597deb17714fb107caafe",
            "isKey": false,
            "numCitedBy": 824,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difficult clustering tasks in high-dimensional spaces."
            },
            "slug": "Clustering-on-the-Unit-Hypersphere-using-von-Banerjee-Dhillon",
            "title": {
                "fragments": [],
                "text": "Clustering on the Unit Hypersphere using von Mises-Fisher Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A generative mixture-model approach to clustering directional data based on the von Mises-Fisher distribution, which arises naturally for data distributed on the unit hypersphere, and derives and analyzes two variants of the Expectation Maximization framework for estimating the mean and concentration parameters of this mixture."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068068778"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9947637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffaa313b8da3695627cd9915ca46b8bed24a9f4a",
            "isKey": false,
            "numCitedBy": 568,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The advantages of discriminative learning algorithms and kernel machines are combined with generative modeling using a novel kernel between distributions. In the probability product kernel, data points in the input space are mapped to distributions over the sample space and a general inner product is then evaluated as the integral of the product of pairs of distributions. The kernel is straightforward to evaluate for all exponential family models such as multinomials and Gaussians and yields interesting nonlinear kernels. Furthermore, the kernel is computable in closed form for latent distributions such as mixture models, hidden Markov models and linear dynamical systems. For intractable models, such as switching linear dynamical systems, structured mean-field approximations can be brought to bear on the kernel evaluation. For general distributions, even if an analytic expression for the kernel is not feasible, we show a straightforward sampling method to evaluate it. Thus, the kernel permits discriminative learning methods, including support vector machines, to exploit the properties, metrics and invariances of the generative models we infer from each datum. Experiments are shown using multinomial models for text, hidden Markov models for biological data sets and linear dynamical systems for time series data."
            },
            "slug": "Probability-Product-Kernels-Jebara-Kondor",
            "title": {
                "fragments": [],
                "text": "Probability Product Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The advantages of discriminative learning algorithms and kernel machines are combined with generative modeling using a novel kernel between distributions to exploit the properties, metrics and invariances of the generative models the authors infer from each datum."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49641404"
                        ],
                        "name": "I. Cohen",
                        "slug": "I.-Cohen",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7668712"
                        ],
                        "name": "Fabio Gagliardi Cozman",
                        "slug": "Fabio-Gagliardi-Cozman",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Cozman",
                            "middleNames": [
                                "Gagliardi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Gagliardi Cozman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703601"
                        ],
                        "name": "N. Sebe",
                        "slug": "N.-Sebe",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156170"
                        ],
                        "name": "M. C. Cirelo",
                        "slug": "M.-C.-Cirelo",
                        "structuredName": {
                            "firstName": "Marcelo",
                            "lastName": "Cirelo",
                            "middleNames": [
                                "Cesar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Cirelo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1832450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3268e5e0e0e402df798337a3f8ff9739d4773ae1",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic classification is one of the basic tasks required in any pattern recognition and human computer interaction application. In this paper, we discuss training probabilistic classifiers with labeled and unlabeled data. We provide a new analysis that shows under what conditions unlabeled data can be used in learning to improve classification performance. We also show that, if the conditions are violated, using unlabeled data can be detrimental to classification performance. We discuss the implications of this analysis to a specific type of probabilistic classifiers, Bayesian networks, and propose a new structure learning algorithm that can utilize unlabeled data to improve classification. Finally, we show how the resulting algorithms are successfully employed in two applications related to human-computer interaction and pattern recognition: facial expression recognition and face detection."
            },
            "slug": "Semisupervised-learning-of-classifiers:-theory,-and-Cohen-Cozman",
            "title": {
                "fragments": [],
                "text": "Semisupervised learning of classifiers: theory, algorithms, and their application to human-computer interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new analysis is provided that shows under what conditions unlabeled data can be used in learning to improve classification performance, and how the resulting algorithms are successfully employed in two applications related to human-computer interaction and pattern recognition: facial expression recognition and face detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144500070"
                        ],
                        "name": "Shuchi Chawla",
                        "slug": "Shuchi-Chawla",
                        "structuredName": {
                            "firstName": "Shuchi",
                            "lastName": "Chawla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchi Chawla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5892518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eedbab3ae55fd6a4e7bbc75fcc261293384f883",
            "isKey": false,
            "numCitedBy": 1057,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data."
            },
            "slug": "Learning-from-Labeled-and-Unlabeled-Data-using-Blum-Chawla",
            "title": {
                "fragments": [],
                "text": "Learning from Labeled and Unlabeled Data using Graph Mincuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data is considered."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880237"
                        ],
                        "name": "S. Dasgupta",
                        "slug": "S.-Dasgupta",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Dasgupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dasgupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 280438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a48abd949978432b8a9498f94b029e440bc1b4f3",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The rule-based bootstrapping introduced by Yarowsky, and its co-training variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justifies both the use of confidences \u2014 partial rules and partial labeling of the unlabeled data \u2014 and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for k \u2265 2."
            },
            "slug": "PAC-Generalization-Bounds-for-Co-training-Dasgupta-Littman",
            "title": {
                "fragments": [],
                "text": "PAC Generalization Bounds for Co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new PAC-style bound on generalization error is given which justifies both the use of confidences \u2014 partial rules and partial labeling of the unlabeled data \u2014 and theUse of an agreement-based objective function as suggested by Collins and Singer."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143674326"
                        ],
                        "name": "B. K\u00e9gl",
                        "slug": "B.-K\u00e9gl",
                        "structuredName": {
                            "firstName": "Bal\u00e1zs",
                            "lastName": "K\u00e9gl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. K\u00e9gl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108888783"
                        ],
                        "name": "Ligen Wang",
                        "slug": "Ligen-Wang",
                        "structuredName": {
                            "firstName": "Ligen",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ligen Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15030874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94126a5a7472b1c30fa5799fb97aec630c9d1a13",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose to combine two powerful ideas, boosting and manifold learning. On the one hand, we improve ADABOOST by incorporating knowledge on the structure of the data into base classifier design and selection. On the other hand, we use ADABOOST's efficient learning mechanism to significantly improve supervised and semi-supervised algorithms proposed in the context of manifold learning. Beside the specific manifold-based penalization, the resulting algorithm also accommodates the boosting of a large family of regularized learning algorithms."
            },
            "slug": "Boosting-on-Manifolds:-Adaptive-Regularization-of-K\u00e9gl-Wang",
            "title": {
                "fragments": [],
                "text": "Boosting on Manifolds: Adaptive Regularization of Base Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This paper improves ADABOOST by incorporating knowledge on the structure of the data into base classifier design and selection and uses its efficient learning mechanism to significantly improve supervised and semi-supervised algorithms proposed in the context of manifold learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1948067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d1fd4c63831a8ecdc863ceb37d8775142a1fd1c",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of modern learning tasks involve estimation from heterogeneous information sources. This includes classification with labeled and unlabeled data as well as other problems with analogous structure such as competitive (game theoretic) problems. The associated estimation problems can be typically reduced to solving a set of fixed point equations (consistency conditions). We introduce a general method for combining a preferred information source with another in this setting by evolving continuous paths of fixed points at intermediate allocations. We explicitly identify critical points along the unique paths to either increase the stability of estimation or to ensure a significant departure from the initial source. The homotopy continuation approach is guaranteed to terminate at the second source, and involves no combinatorial effort. We illustrate the power of these ideas both in classification tasks with labeled and unlabeled data, as well as in the context of a competitive (min-max) formulation of DNA sequence motif discovery."
            },
            "slug": "Continuation-Methods-for-Mixing-Heterogenous-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Continuation Methods for Mixing Heterogenous Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work introduces a general method for combining a preferred information source with another in this setting by evolving continuous paths of fixed points at intermediate allocations and identifies critical points along the unique paths to either increase the stability of estimation or to ensure a significant departure from the initial source."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14278367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b3b54848c1bc6ffea2625ce79302abed8e8deb9",
            "isKey": false,
            "numCitedBy": 836,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how a text classifier\u2019s need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents. We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling. Then active learning is combined with ExpectationMaximization in order to \u201cfill in\u201d the class labels of those documents that remain unlabeled. Experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or EM alone."
            },
            "slug": "Employing-EM-and-Pool-Based-Active-Learning-for-McCallum-Nigam",
            "title": {
                "fragments": [],
                "text": "Employing EM and Pool-Based Active Learning for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This paper shows how a text classifier\u2019s need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents by modifying the Query-by-Committee method of active learning to use it for explicitly estimating document density when selecting examples for labeling."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770007"
                        ],
                        "name": "M. Goldszmidt",
                        "slug": "M.-Goldszmidt",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Goldszmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldszmidt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 930676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c76ac0a39577760d4aaf6fce98327543ec64a560",
            "isKey": false,
            "numCitedBy": 4537,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection."
            },
            "slug": "Bayesian-Network-Classifiers-Friedman-Geiger",
            "title": {
                "fragments": [],
                "text": "Bayesian Network Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Tree Augmented Naive Bayes (TAN) is single out, which outperforms naive Bayes, yet at the same time maintains the computational simplicity and robustness that characterize naive Baye."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058177533"
                        ],
                        "name": "Simon Tong",
                        "slug": "Simon-Tong",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Tong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8223567,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "06a154b63c9e49840ded076ebe9e9915ea672e99",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the notion of restricted Bayes optimal classifiers . These classifiers attempt to combine the flexibility of the generative approach to classification with the high accuracy associated with discriminative learning. They first create a model of the joint distribution over class labels and features. Instead of choosing the decision boundary induced directly from the model, they restrict the allowable types of decision boundaries and learn the one that minimizes the probability of misclassification relative to the estimated joint distribution. In this paper, we investigate two particular instantiations of this approach. The first uses a non-parametric density estimator \u2014 Parzen Windows with Gaussian kernels \u2014 and hyperplane decision boundaries. We show that the resulting classifier is asymptotically equivalent to a maximal margin hyperplane classifier, a highly successful discriminative classifier. We therefore provide an alternative justification for maximal margin hyperplane classifiers. The second instantiation uses a mixture of Gaussians as the estimated density; in experiments on real-world data, we show that this approach allows data with missing values to be handled in a principled manner, leading to improved performance over regular discriminative approaches."
            },
            "slug": "Restricted-Bayes-Optimal-Classifiers-Tong-Koller",
            "title": {
                "fragments": [],
                "text": "Restricted Bayes Optimal Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper investigates two particular instantiations of the notion of restricted Bayes optimal classifiers, and shows that the first uses a non-parametric density estimator \u2014 Parzen Windows with Gaussian kernels \u2014 and hyperplane decision boundaries and is asymptotically equivalent to a maximal margin hyperplane classifier, a highly successful discriminative classifier."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5947209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "445cac87f39f44f128db8b0a48032fea845fec16",
            "isKey": false,
            "numCitedBy": 698,
            "numCiting": 123,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the mixtures-of-trees model, a probabilistic model for discrete multidimensional domains. Mixtures-of-trees generalize the probabilistic trees of Chow and Liu (1968) in a different and complementary direction to that of Bayesian networks. We present efficient algorithms for learning mixtures-of-trees models in maximum likelihood and Bayesian frameworks. We also discuss additional efficiencies that can be obtained when data are \"sparse,\" and we present data structures and algorithms that exploit such sparseness. Experimental results demonstrate the performance of the model for both density estimation and classification. We also discuss the sense in which tree-based classifiers perform an implicit form of feature selection, and demonstrate a resulting insensitivity to irrelevant attributes."
            },
            "slug": "Learning-with-Mixtures-of-Trees-Meil\u0103-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning with Mixtures of Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The mixtures-of-trees model, a probabilistic model for discrete multidimensional domains, is described and the sense in which tree-based classifiers perform an implicit form of feature selection is discussed, and a resulting insensitivity to irrelevant attributes is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388508441"
                        ],
                        "name": "F. P\u00e9rez-Cruz",
                        "slug": "F.-P\u00e9rez-Cruz",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "P\u00e9rez-Cruz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. P\u00e9rez-Cruz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5655061,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "a82b669eadad938d4d6e7276bad6dddd9de16d6f",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nIn drug discovery a key task is to identify characteristics that separate active (binding) compounds from inactive (non-binding) ones. An automated prediction system can help reduce resources necessary to carry out this task.\n\n\nRESULTS\nTwo methods for prediction of molecular bioactivity for drug design are introduced and shown to perform well in a data set previously studied as part of the KDD (Knowledge Discovery and Data Mining) Cup 2001. The data is characterized by very few positive examples, a very large number of features (describing three-dimensional properties of the molecules) and rather different distributions between training and test data. Two techniques are introduced specifically to tackle these problems: a feature selection method for unbalanced data and a classifier which adapts to the distribution of the the unlabeled test data (a so-called transductive method). We show both techniques improve identification performance and in conjunction provide an improvement over using only one of the techniques. Our results suggest the importance of taking into account the characteristics in this data which may also be relevant in other problems of a similar type."
            },
            "slug": "Feature-selection-and-transduction-for-prediction-Weston-P\u00e9rez-Cruz",
            "title": {
                "fragments": [],
                "text": "Feature selection and transduction for prediction of molecular bioactivity for drug design"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Two methods for prediction of molecular bioactivity for drug design are introduced and shown to perform well in a data set previously studied as part of the KDD Cup 2001, characterized by very few positive examples, a very large number of features and rather different distributions between training and test data."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686971"
                        ],
                        "name": "T. Graepel",
                        "slug": "T.-Graepel",
                        "structuredName": {
                            "firstName": "Thore",
                            "lastName": "Graepel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Graepel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234984"
                        ],
                        "name": "R. Herbrich",
                        "slug": "R.-Herbrich",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Herbrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herbrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743272"
                        ],
                        "name": "K. Obermayer",
                        "slug": "K.-Obermayer",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Obermayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Obermayer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2164365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6bdd20d5c028020da4ce68266ceca56b3587f46",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Transduction is an inference principle that takes a training sample and aims at estimating the values of a function at given points contained in the so-called working sample as opposed to the whole of input space for induction. Transduction provides a confidence measure on single predictions rather than classifiers - a feature particularly important for risk-sensitive applications. The possibly infinite number of functions is reduced to a finite number of equivalence classes on the working sample. A rigorous Bayesian analysis reveals that for standard classification loss we cannot benefit from considering more than one test point at a time. The probability of the label of a given test point is determined as the posterior measure of the corresponding subset of hypothesis space. We consider the PAC setting of binary classification by linear discriminant functions (perceptrons) in kernel space such that the probability of labels is determined by the volume ratio in version space. We suggest to sample this region by an ergodic billiard. Experimental results on real world data indicate that Bayesian Transduction compares favourably to the well-known Support Vector Machine, in particular if the posterior probability of labellings is used as a confidence measure to exclude test points of low confidence."
            },
            "slug": "Bayesian-Transduction-Graepel-Herbrich",
            "title": {
                "fragments": [],
                "text": "Bayesian Transduction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results on real world data indicate that Bayesian Transduction compares favourably to the well-known Support Vector Machine, in particular if the posterior probability of labellings is used as a confidence measure to exclude test points of low confidence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879453"
                        ],
                        "name": "V. Castelli",
                        "slug": "V.-Castelli",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Castelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Castelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123359088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbf110e1e786d71b974392c018a64ec06eb08bef",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We attempt to discover the role and relative value of labeled and unlabeled samples in reducing the probability of error of the classification of a sample based on the previous observation of labeled and unlabeled data. We assume that the underlying densities belong to a regular family that generates identifiable mixtures. The unlabeled observations, under the above conditions, carry information about the statistical model and therefore can be effectively used to construct a decision rule. When the training set contains an infinite number of unlabeled samples, the first labeled observation reduces the probability of error to within a factor of two of the Bayes risk. Moreover subsequent labeled samples yield exponential convergence of the probability of classification error to the Bayes risk. We argue that labeled samples are exponentially more valuable than unlabeled samples and identify the (exponent as the Bhatthacharyya distance."
            },
            "slug": "The-relative-value-of-labeled-and-unlabeled-samples-Castelli-Cover",
            "title": {
                "fragments": [],
                "text": "The relative value of labeled and unlabeled samples in pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is argued that labeled samples are exponentially more valuable than unlabeled samples and identify the (exponent as the Bhatthacharyya distance)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. IEEE International Symposium on Information Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2788842"
                        ],
                        "name": "Cyril Goutte",
                        "slug": "Cyril-Goutte",
                        "structuredName": {
                            "firstName": "Cyril",
                            "lastName": "Goutte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cyril Goutte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2131960"
                        ],
                        "name": "Herv\u00e9 D\u00e9jean",
                        "slug": "Herv\u00e9-D\u00e9jean",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "D\u00e9jean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Herv\u00e9 D\u00e9jean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732180"
                        ],
                        "name": "\u00c9ric Gaussier",
                        "slug": "\u00c9ric-Gaussier",
                        "structuredName": {
                            "firstName": "\u00c9ric",
                            "lastName": "Gaussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9ric Gaussier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683776"
                        ],
                        "name": "Nicola Cancedda",
                        "slug": "Nicola-Cancedda",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Cancedda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicola Cancedda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2822140"
                        ],
                        "name": "J. Renders",
                        "slug": "J.-Renders",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Renders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Renders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9980409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f59b055a7df1b6bf7cb8f92ca8020af0ad47bdec",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of using partially labelled data, eg large collections were only little data is annotated, for extracting biological entities. Our approach relies on a combination of probabilistic models, which we use to model the generation of entities and their context, and kernel machines, which implement powerful categorisers based on a similarity measure and some labelled data. This combination takes the form of the so-called Fisher kernels which implement a similarity based on an underlying probabilistic model. Such kernels are compared with transductive inference, an alternative approach to combining labelled and unlabelled data, again coupled with Support Vector Machines. Experiments are performed on a database of abstracts extracted from Medline."
            },
            "slug": "Combining-Labelled-and-Unlabelled-Data:-A-Case-on-Goutte-D\u00e9jean",
            "title": {
                "fragments": [],
                "text": "Combining Labelled and Unlabelled Data: A Case Study on Fisher Kernels and Transductive Inference for Biological Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work addresses the problem of using partially labelled data, eg large collections were only little data is annotated, for extracting biological entities by using probabilistic models and kernel machines, which implement powerful categorisers based on a similarity measure and some labelled data."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17875902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dd9743183f07b7653cc0335fcc1042aa71032c6",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "There is much current interest in kernel methods for classi cation re gression PCA and other linear methods of data analysis Kernel methods may be particularly valuable for problems in which the input data is not readily described by explicit feature vectors One such problem is where input data consists of symbol sequences of di erent lengths and the re lationships between sequences are best captured by dynamic alignment scores This paper shows that the scores produced by certain dynamic align ment algorithms for sequences are in fact valid kernel functions This is proved by expressing the alignment scores explicitly as dot products Alignment kernels are potentially applicable to biological sequence data speech data and time series data The kernel construction may be extended from pair HMMs to pair probabilistic context free grammars Introduction Linear Methods using Kernel Functions Introduction Linear Methods using Kernel Functions In many types of machine learning the learner is given a training set of cases or examples a al A A denotes the set of all possible cases cases may be vectors pieces of text biological sequences sentences etc For supervised learning the cases are accompanied by a set of corresponding labels or values y yl The cases are mapped to feature vectors x xl X where the X is a real vector space termed the feature space The mapping from A to X is denoted by so that xi ai Sometimes the cases are given as feature vectors to start with in which case may be the identity mapping otherwise denotes the method of assigning numeric feature values to a case Once a feature vector xi has been de ned for each case ai it becomes pos sible to apply a wide range of linear methods such as support vector machines linear regression principal components analysis PCA and k means cluster analysis As shown in Vap for SV machines in for example Wah for linear re gression and in SSM for PCA and k means cluster analysis the calculations for all of these linear methods may be carried out using a dual rather than a primal formulation of the problem For example in linear least squares regression the primal formulation is to nd a coe cient vector that minimises kX yk whereX is the design matrix an l by d matrix in which the ith row is xi and each xi has d elements If l is larger than d the usual method of nding is to solve the normal equations XX Xy This requires the solution of a set of linear equations with coe cients given by the d d matrix XX The dual formulation is to nd a coe cient vector that minimises kXX yk so that one coe cient i is found for each case vector xi This requires the solution of a set of linear equations with coe cients given by the l l matrix XX Both methods lead to the same predicted value y for a new case x If there are more cases than features that is if l d the primal method is more economical because the d d matrix XX is smaller than the l l matrix XX For example if there are cases each described by a vector of measurements then the primal method requires solving a by system of linear equations while the dual method requires solving a by system which will have rank at most For such a problem the dual method has no advantage The potential advantage of the dual method for regression is that it can be applied to very large feature vectors The coe cient matrix XX contains the dot products of pairs of feature vectors the ijth element of XX is xi xj In the dual calculation it is only dot products of feature vectors that are used feature vectors never appear on their own As the feature vectors xi ai appear only in dot products it is often possible to avoid computing the feature vectors and to compute dot products directly in some economical fashion from the case descriptions ai instead A kernel is a function k that computes a dot product of feature vectors from the corresponding cases Applying Linear Methods to Structured Objects De nition A kernel is a function k such that for all a b A"
            },
            "slug": "Dynamic-Alignment-Kernels-Watkins",
            "title": {
                "fragments": [],
                "text": "Dynamic Alignment Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the scores produced by certain dynamic align ment algorithms for sequences are in fact valid kernel functions, proved by expressing the alignment scores explicitly as dot products."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ba566223e426677d12a9a18418c023a4deec77e",
            "isKey": false,
            "numCitedBy": 13126,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5841767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28d89b941cefaeeb7e7af8a5ea4f9153bbb43bff",
            "isKey": false,
            "numCitedBy": 720,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nDuring the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data.\n\n\nRESULTS\nThis paper describes a computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Each dataset is represented via a kernel function, which defines generalized similarity relationships between pairs of entities, such as genes or proteins. The kernel representation is both flexible and efficient, and can be applied to many different types of data. Furthermore, kernel functions derived from different types of data can be combined in a straightforward fashion. Recent advances in the theory of kernel methods have provided efficient algorithms to perform such combinations in a way that minimizes a statistical loss function. These methods exploit semidefinite programming techniques to reduce the problem of finding optimizing kernel combinations to a convex optimization problem. Computational experiments performed using yeast genome-wide datasets, including amino acid sequences, hydropathy profiles, gene expression data and known protein-protein interactions, demonstrate the utility of this approach. A statistical learning algorithm trained from all of these data to recognize particular classes of proteins--membrane proteins and ribosomal proteins--performs significantly better than the same algorithm trained on any single type of data.\n\n\nAVAILABILITY\nSupplementary data at http://noble.gs.washington.edu/proj/sdp-svm"
            },
            "slug": "A-statistical-framework-for-genomic-data-fusion-Lanckriet-Bie",
            "title": {
                "fragments": [],
                "text": "A statistical framework for genomic data fusion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes a computational framework for integrating and drawing inferences from a collection of genome-wide measurements represented via a kernel function, which defines generalized similarity relationships between pairs of entities, such as genes or proteins."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121009"
                        ],
                        "name": "J. Puzicha",
                        "slug": "J.-Puzicha",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Puzicha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Puzicha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2696305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ec00abf9ff66d6f16378978faf907b047834cbb",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling and predicting co-occurrences of events is a fundamental problem of unsupervised learning. In this contribution we develop a statistical framework for analyzing co-occurrence data in a general setting where elementary observations are joint occurrences of pairs of abstract objects from two finite sets. The main challenge for statistical models in this context is to overcome the inherent data sparseness and to estimate the probabilities for pairs which were rarely observed or even unobserved in a given sample set. Moreover, it is often of considerable interest to extract grouping structure or to find a hierarchical data organization. A novel family of mixture models is proposed which explain the observed data by a finite number of shared aspects or clusters. This provides a common framework for statistical inference and structure discovery and also includes several recently proposed models as special cases. Adopting the maximum likelihood principle, EM algorithms are derived to fit the model parameters. We develop improved versions of EM which largely avoid overfitting problems and overcome the inherent locality of EM--based optimization. Among the broad variety of possible applications, e.g., in information retrieval, natural language processing, data mining, and computer vision, we have chosen document retrieval, the statistical analysis of noun/adjective co-occurrence and the unsupervised segmentation of textured images to test and evaluate the proposed algorithms."
            },
            "slug": "Statistical-Models-for-Co-occurrence-Data-Hofmann-Puzicha",
            "title": {
                "fragments": [],
                "text": "Statistical Models for Co-occurrence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A statistical framework for analyzing co-occurrence data in a general setting where elementary observations are joint occurrences of pairs of abstract objects from two finite sets is developed and a novel family of mixture models is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145536952"
                        ],
                        "name": "J. Kandola",
                        "slug": "J.-Kandola",
                        "structuredName": {
                            "firstName": "Jaz",
                            "lastName": "Kandola",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kandola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17466014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36aa0d0936b2cf128c646c36a1981807b5a27aaf",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction."
            },
            "slug": "On-Kernel-Target-Alignment-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "On Kernel-Target Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function, is introduced, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on a test set, giving improved classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121955983"
                        ],
                        "name": "C.J. Merz",
                        "slug": "C.J.-Merz",
                        "structuredName": {
                            "firstName": "C.J.",
                            "lastName": "Merz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C.J. Merz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31000092"
                        ],
                        "name": "D. St. Clair",
                        "slug": "D.-St.-Clair",
                        "structuredName": {
                            "firstName": "D.C.",
                            "lastName": "St. Clair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. St. Clair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797964"
                        ],
                        "name": "W. Bond",
                        "slug": "W.-Bond",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bond",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bond"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58041158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45c68989c3f6e47dec8fdc10110337a15f09af51",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive resonance theory (ART) algorithms represent a class of neural network architectures which self-organize stable recognition categories in response to arbitrary sequences of input patterns. The authors discuss incorporation of supervision into one of these architectures, ART2. Results of numerical experiments indicate that this new semi-supervised version of ART2 (SMART2) outperformed ART for classification problems. The results and analysis of runs on several data sets by SMART2, ART2, and backpropagation are analyzed. The test accuracy of SMART2 was similar to that of backpropagation. However, SMART2 network structures are easier to interpret than the corresponding structures produced by backpropagation.<<ETX>>"
            },
            "slug": "SeMi-supervised-adaptive-resonance-theory-(SMART2)-Merz-Clair",
            "title": {
                "fragments": [],
                "text": "SeMi-supervised adaptive resonance theory (SMART2)"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Results of numerical experiments indicate that this new semi-supervised version of ART2 (SMART2) outperformed ART for classification problems, and network structures are easier to interpret than the corresponding structures produced by backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18543237,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "15252afe259894bd3d0f306f29eca5e90ab05eac",
            "isKey": false,
            "numCitedBy": 869,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The classification problem is considered in which an outputvariable y assumes discrete values with respectiveprobabilities that depend upon the simultaneous values of a set of input variablesx = {x_1,....,x_n}. At issue is how error in the estimates of theseprobabilities affects classification error when the estimates are used ina classification rule. These effects are seen to be somewhat counterintuitive in both their strength and nature. In particular the bias andvariance components of the estimation error combine to influenceclassification in a very different way than with squared error on theprobabilities themselves. Certain types of (very high) bias can becanceled by low variance to produce accurate classification. This candramatically mitigate the effect of the bias associated with some simpleestimators like \u201cnaive\u201d Bayes, and the bias induced by thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy such simple methods are often competitive with and sometimes superiorto more sophisticated ones for classification, and why\u201cbagging/aggregating\u201d classifiers can often improveaccuracy. These results also suggest simple modifications to theseprocedures that can (sometimes dramatically) further improve theirclassification performance."
            },
            "slug": "On-Bias,-Variance,-0/1\u2014Loss,-and-the-Friedman",
            "title": {
                "fragments": [],
                "text": "On Bias, Variance, 0/1\u2014Loss, and the Curse-of-Dimensionality"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work candramatically mitigate the effect of the bias associated with some simpleestimators like \u201cnaive\u201d Bayes, and the bias induced by the curse-of-dimensionality on nearest-neighbor procedures."
            },
            "venue": {
                "fragments": [],
                "text": "Data Mining and Knowledge Discovery"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2255275,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b99bd29e2e90e4ed7dbdccf352cd56c359f70f0b",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Polynomial-time-algorithms-for-learning-neural-nets-Baum",
            "title": {
                "fragments": [],
                "text": "Polynomial time algorithms for learning neural nets"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409749316"
                        ],
                        "name": "S. Ben-David",
                        "slug": "S.-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736744"
                        ],
                        "name": "A. Itai",
                        "slug": "A.-Itai",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Itai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Itai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738470"
                        ],
                        "name": "E. Kushilevitz",
                        "slug": "E.-Kushilevitz",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Kushilevitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kushilevitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 9629052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2c2ea2b2a95dd73acabe78eac13ef67a0e1e0f3",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of learning by distances is presented. In this model a concept is a point in a metric space. At each step of the learning process the student guesses a hypothesis and receives from the teacher an approximation of its distance to the target. A notion of a distance measuring the proximity of a hypothesis to the correct answer is common to many models of learnability. By focusing on this fundamental aspect we discover some general and simple tools for the analysis of learnability tasks. As a corollary we present new learning algorithms for Valiant?s PAC scenario with any given distribution. These algorithms can learn any PAC-learnable class and, in some cases, settle for significantly less information than the usual labeled examples. Insight gained by the new model is applied to show that every class of subsets C that has a finite VC-dimension is PAC-learnable with respect to any fixed distribution. Previously known results of this nature were subject to complicated measurability constraints."
            },
            "slug": "Learning-by-distances-Ben-David-Itai",
            "title": {
                "fragments": [],
                "text": "Learning by distances"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A model of learning by distances is presented and Insight gained is applied to show that every class of subsets C that has a finite VC-dimension is PAC-learnable with respect to any fixed distribution."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 291166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f70d8019618a7c1dfc8118711a2e72ea5a510e7",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Can we detect low dimensional structure in high dimensional data sets of images? In this paper, we propose an algorithm for unsupervised learning of image manifolds by semidefinite programming. Given a data set of images, our algorithm computes a low dimensional representation of each image with the property that distances between nearby images are preserved. More generally, it can be used to analyze high dimensional data that lies on or near a low dimensional manifold. We illustrate the algorithm on easily visualized examples of curves and surfaces, as well as on actual images of faces, handwritten digits, and solid objects."
            },
            "slug": "Unsupervised-Learning-of-Image-Manifolds-by-Weinberger-Saul",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Image Manifolds by Semidefinite Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm for unsupervised learning of image manifolds by semidefinite programming that computes a low dimensional representation of each image with the property that distances between nearby images are preserved."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686744"
                        ],
                        "name": "N. Kasabov",
                        "slug": "N.-Kasabov",
                        "structuredName": {
                            "firstName": "Nikola",
                            "lastName": "Kasabov",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kasabov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9922157"
                        ],
                        "name": "Shaoning Pang",
                        "slug": "Shaoning-Pang",
                        "structuredName": {
                            "firstName": "Shaoning",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoning Pang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14162967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f1c1e02bca7c8c85531727b238a9684c1f67c5c",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a novel transductive support vector machine (TSVM) model and compares it with the traditional inductive SVM on a key problem in bioinformatics - promoter recognition. While inductive reasoning is concerned with the development of a model (a function) to approximate data from the whole problem space (induction), and consecutively using this model to predict output values for a new input vector (deduction), in the transductive inference systems a model is developed for every new input vector based on some closest to the new vector data from an existing database and this model is used to predict only the output for this vector. The TSVM outperforms by far the inductive SVM models applied on the same problems. Analysis is given on the advantages and disadvantages of the TSVM. Hybrid TSVM-evolving connections systems are discussed as directions for future research."
            },
            "slug": "Transductive-support-vector-machines-and-in-for-Kasabov-Pang",
            "title": {
                "fragments": [],
                "text": "Transductive support vector machines and applications in bioinformatics for promoter recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel transductive support vector machine (TSVM) model is introduced and it is compared with the traditional inductive SVM on a key problem in bioinformatics - promoter recognition."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5430060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1097ebc8459336b3f425ce4684f7463c10b50818",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a principle for semi-supervised learning based on optimizing the rate of communicating labels for unlabeled points with side information. The side information is expressed in terms of identities of sets of points or regions with the purpose of biasing the labels in each region to be the same. The resulting regularization objective is convex, has a unique solution, and the solution can be found with a pair of local propagation operations on graphs induced by the regions. We analyze the properties of the algorithm and demonstrate its performance on document classification tasks."
            },
            "slug": "Distributed-Information-Regularization-on-Graphs-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Distributed Information Regularization on Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A principle for semi-supervised learning based on optimizing the rate of communicating labels for unlabeled points with side information that has a unique solution and demonstrates its performance on document classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2731006,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ca8219c2a7753872ac5343c68140014d57470fef",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification with partially labeled data requires using a large number of unlabeled examples (or an estimated marginal P(x)), to further constrain the conditional P(y|x) beyond a few available labeled examples. We formulate a regularization approach to linking the marginal and the conditional in a general way. The regularization penalty measures the information that is implied about the labels over covering regions. No parametric assumptions are required and the approach remains tractable even for continuous marginal densities P(x). We develop algorithms for solving the regularization problem for finite covers, establish a limiting differential equation, and exemplify the behavior of the new regularization approach in simple cases."
            },
            "slug": "Information-Regularization-with-Partially-Labeled-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Information Regularization with Partially Labeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A regularization approach to linking the marginal and the conditional in a general way is formulated and the regularization penalty measures the information that is implied about the labels over covering regions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913364"
                        ],
                        "name": "Boaz Leskes",
                        "slug": "Boaz-Leskes",
                        "structuredName": {
                            "firstName": "Boaz",
                            "lastName": "Leskes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boaz Leskes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049789"
                        ],
                        "name": "L. Torenvliet",
                        "slug": "L.-Torenvliet",
                        "structuredName": {
                            "firstName": "Leen",
                            "lastName": "Torenvliet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Torenvliet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 576672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "007e5e29f6f06ac03ad51514d3e842a7d1dd2968",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-value-of-agreement-a-new-boosting-algorithm-Leskes-Torenvliet",
            "title": {
                "fragments": [],
                "text": "The value of agreement a new boosting algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1954599,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b145c84cb7a7c691cbd6ced92195b098be11ba7b",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density estimator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models."
            },
            "slug": "Non-Local-Manifold-Parzen-Windows-Bengio-Larochelle",
            "title": {
                "fragments": [],
                "text": "Non-Local Manifold Parzen Windows"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a non-local non-parametric density estimator that builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49641404"
                        ],
                        "name": "I. Cohen",
                        "slug": "I.-Cohen",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703601"
                        ],
                        "name": "N. Sebe",
                        "slug": "N.-Sebe",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7668712"
                        ],
                        "name": "Fabio Gagliardi Cozman",
                        "slug": "Fabio-Gagliardi-Cozman",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Cozman",
                            "middleNames": [
                                "Gagliardi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Gagliardi Cozman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156170"
                        ],
                        "name": "M. C. Cirelo",
                        "slug": "M.-C.-Cirelo",
                        "structuredName": {
                            "firstName": "Marcelo",
                            "lastName": "Cirelo",
                            "middleNames": [
                                "Cesar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Cirelo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 546373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d08b632cbfa3197a53cfbae6816f8534c7e7c843",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding human emotions is one of the necessary skills for the computer to interact intelligently with human users. The most expressive way humans display emotions is through facial expressions. In this paper, we report on several advances we have made in building a system for classification of facial expressions from continuous video input. We use Bayesian network classifiers for classifying expressions from video. One of the motivating factor in using the Bayesian network classifiers is their ability to handle missing data, both during inference and training. In particular, we are interested in the problem of learning with both labeled and unlabeled data. We show that when using unlabeled data to learn classifiers, using correct modeling assumptions is critical for achieving improved classification performance. Motivated by this, we introduce a classification driven stochastic structure search algorithm for learning the structure of Bayesian network classifiers. We show that with moderate size labeled training sets and large amount of unlabeled data, our method can utilize unlabeled data to improve classification performance. We also provide results using the Naive Bayes (NB) and the Tree-Augmented Naive Bayes (TAN) classifiers, showing that the two can achieve good performance with labeled training sets, but perform poorly when unlabeled data are added to the training set."
            },
            "slug": "Learning-Bayesian-network-classifiers-for-facial-Cohen-Sebe",
            "title": {
                "fragments": [],
                "text": "Learning Bayesian network classifiers for facial expression recognition both labeled and unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A classification driven stochastic structure search algorithm for learning the structure of Bayesian network classifiers is introduced, and it is shown that with moderate size labeled training sets and large amount of unlabeled data, the method can utilize unlabeling data to improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787804"
                        ],
                        "name": "N. Shental",
                        "slug": "N.-Shental",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shental",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shental"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400556488"
                        ],
                        "name": "Aharon Bar-Hillel",
                        "slug": "Aharon-Bar-Hillel",
                        "structuredName": {
                            "firstName": "Aharon",
                            "lastName": "Bar-Hillel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aharon Bar-Hillel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774536"
                        ],
                        "name": "T. Hertz",
                        "slug": "T.-Hertz",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Hertz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789171"
                        ],
                        "name": "D. Weinshall",
                        "slug": "D.-Weinshall",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Weinshall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Weinshall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 95794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "448f8d87f1709ffbdec46a7d5881f8c1f924d0e0",
            "isKey": false,
            "numCitedBy": 305,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Density estimation with Gaussian Mixture Models is a popular generative technique used also for clustering. We develop a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure. Equivalence constraints are defined on pairs of data points, indicating whether the points arise from the same source (positive constraints) or from different sources (negative constraints). Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others. For the estimation of model parameters we present a closed form EM procedure which handles positive constraints, and a Generalized EM procedure using a Markov net which handles negative constraints. Using publicly available data sets we demonstrate that such side information can lead to considerable improvement in clustering tasks, and that our algorithm is preferable to two other suggested methods using the same type of side information."
            },
            "slug": "Computing-Gaussian-Mixture-Models-with-EM-Using-Shental-Bar-Hillel",
            "title": {
                "fragments": [],
                "text": "Computing Gaussian Mixture Models with EM Using Equivalence Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work develops a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure, and demonstrates that such side information can lead to considerable improvement in clustering tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821511"
                        ],
                        "name": "Gyora M. Benedek",
                        "slug": "Gyora-M.-Benedek",
                        "structuredName": {
                            "firstName": "Gyora",
                            "lastName": "Benedek",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyora M. Benedek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736744"
                        ],
                        "name": "A. Itai",
                        "slug": "A.-Itai",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Itai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Itai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33054388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17fcc0a08921e2e1a908e823ad80b176871d8a4d",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learnability-with-Respect-to-Fixed-Distributions-Benedek-Itai",
            "title": {
                "fragments": [],
                "text": "Learnability with Respect to Fixed Distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39651651"
                        ],
                        "name": "Jean-Fran\u00e7ois Paiement",
                        "slug": "Jean-Fran\u00e7ois-Paiement",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Paiement",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Fran\u00e7ois Paiement"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120888"
                        ],
                        "name": "M. Ouimet",
                        "slug": "M.-Ouimet",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Ouimet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ouimet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6894357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f39fe2659603f4194fd638d1a2e17985415c3bb",
            "isKey": false,
            "numCitedBy": 1066,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data."
            },
            "slug": "Out-of-Sample-Extensions-for-LLE,-Isomap,-MDS,-and-Bengio-Paiement",
            "title": {
                "fragments": [],
                "text": "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified framework for extending Local Linear Embedding, Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling as well as for Spectral Clustering is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": false,
            "numCitedBy": 33435,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784986"
                        ],
                        "name": "V. Koltchinskii",
                        "slug": "V.-Koltchinskii",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Koltchinskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltchinskii"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34590277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "329b2bc0d9ee04038af0f65a366d389da71e6b9c",
            "isKey": false,
            "numCitedBy": 367,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We suggest a penalty function to be used in various problems of structural risk minimization. This penalty is data dependent and is based on the sup-norm of the so-called Rademacher process indexed by the underlying class of functions (sets). The standard complexity penalties, used in learning problems and based on the VC-dimensions of the classes, are conservative upper bounds (in a probabilistic sense, uniformly over the set of all underlying distributions) for the penalty we suggest. Thus, for a particular distribution of training examples, one can expect better performance of learning algorithms with the data-driven Rademacher penalties. We obtain oracle inequalities for the theoretical risk of estimators, obtained by structural minimization of the empirical risk with Rademacher penalties. The inequalities imply some form of optimality of the empirical risk minimizers. We also suggest an iterative approach to structural risk minimization with Rademacher penalties, in which the hierarchy of classes is not given in advance, but is determined in the data-driven iterative process of risk minimization. We prove probabilistic oracle inequalities for the theoretical risk of the estimators based on this approach as well."
            },
            "slug": "Rademacher-penalties-and-structural-risk-Koltchinskii",
            "title": {
                "fragments": [],
                "text": "Rademacher penalties and structural risk minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work suggests a penalty function to be used in various problems of structural risk minimization, based on the sup-norm of the so-called Rademacher process indexed by the underlying class of functions (sets), and obtains oracle inequalities for the theoretical risk of estimators, obtained by structural minimization of the empirical risk withRademacher penalties."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36547165"
                        ],
                        "name": "Lei Wang",
                        "slug": "Lei-Wang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699006"
                        ],
                        "name": "K. Chan",
                        "slug": "K.-Chan",
                        "structuredName": {
                            "firstName": "Kap",
                            "lastName": "Chan",
                            "middleNames": [
                                "Luk"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109086595"
                        ],
                        "name": "Zhihua Zhang",
                        "slug": "Zhihua-Zhang",
                        "structuredName": {
                            "firstName": "Zhihua",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhihua Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5808505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f1a795ccd957ea80e9e2fd8b56bf0355fde414d",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of image retrieval with SVM active learning is known to be poor when started with few labeled images only. In this paper, the problem is solved by incorporating the unlabelled images into the bootstrapping of the learning process. In this work, the initial SVM classifier is trained with the few labeled images and the unlabelled images randomly selected from the image database. Both theoretical analysis and experimental results show that by incorporating unlabelled images in the bootstrapping, the efficiency of SVM active learning can be improved, and thus improves the overall retrieval performance."
            },
            "slug": "Bootstrapping-SVM-active-learning-by-incorporating-Wang-Chan",
            "title": {
                "fragments": [],
                "text": "Bootstrapping SVM active learning by incorporating unlabelled images for image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By incorporating unlabelled images in the bootstrapping of the learning process, the efficiency of SVM active learning can be improved, and thus improves the overall retrieval performance."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35730151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd1f14e7531220c39fad8f86985cce7b283f035d",
            "isKey": false,
            "numCitedBy": 4629,
            "numCiting": 184,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "slug": "Kernel-Methods-for-Pattern-Analysis-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Kernel Methods for Pattern Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "venue": {
                "fragments": [],
                "text": "ICTAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9743839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e6779bb55f7fbed5684ded55df51747ea678a84",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems."
            },
            "slug": "Partially-labeled-classification-with-Markov-random-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Partially labeled classification with Markov random walks"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work combines a limited number of labeled examples with a Markov random walk representation over the unlabeled examples and develops and compares several estimation criteria/algorithms suited to this representation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8192534"
                        ],
                        "name": "Jiayuan Huang",
                        "slug": "Jiayuan-Huang",
                        "structuredName": {
                            "firstName": "Jiayuan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiayuan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16721854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df95ae968cb0b722143f6000fa0dc7ce21cc35e2",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered. The time complexity of the algorithm derived from this framework is nearly linear due to recently developed numerical techniques. In the absence of labeled instances, this framework can be utilized as a spectral clustering method for directed graphs, which generalizes the spectral clustering approach for undirected graphs. We have applied our framework to real-world web classification problems and obtained encouraging results."
            },
            "slug": "Learning-from-labeled-and-unlabeled-data-on-a-graph-Zhou-Huang",
            "title": {
                "fragments": [],
                "text": "Learning from labeled and unlabeled data on a directed graph"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered, which generalizes the spectral clustering approach for undirected graphs."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120888"
                        ],
                        "name": "M. Ouimet",
                        "slug": "M.-Ouimet",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Ouimet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ouimet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17946226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bb257fa1311abe601c3c2270610ecf50aeea89d",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral dimensionality reduction methods and spectral clustering methods require computation of the principal eigenvectors of an n \u00d7 n matrix where n is the number of examples. Following up on previously proposed techniques to speed-up kernel methods by focusing on a subset of m examples, we study a greedy selection procedure for this subset, based on the featurespace distance between a candidate example and the span of the previously chosen ones. In the case of kernel PCA or spectral clustering this reduces computation to O(m2n). For the same computational complexity, we can also compute the feature space projection of the non-selected examples on the subspace spanned by the selected examples, to estimate the embedding function based on all the data, which yields considerably better estimation of the embedding function. This algorithm can be formulated in an online setting and we can bound the error on the approximation of the Gram matrix."
            },
            "slug": "Greedy-Spectral-Embedding-Ouimet-Bengio",
            "title": {
                "fragments": [],
                "text": "Greedy Spectral Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A greedy selection procedure for this subset of m examples, based on the featurespace distance between a candidate example and the span of the previously chosen ones, to estimate the embedding function based on all the data."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2882099"
                        ],
                        "name": "Jesper Vedelsby",
                        "slug": "Jesper-Vedelsby",
                        "structuredName": {
                            "firstName": "Jesper",
                            "lastName": "Vedelsby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesper Vedelsby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5846986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "910688d01c01856dd20715907af44157de8d3d1d",
            "isKey": false,
            "numCitedBy": 1971,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme."
            },
            "slug": "Neural-Network-Ensembles,-Cross-Validation,-and-Krogh-Vedelsby",
            "title": {
                "fragments": [],
                "text": "Neural Network Ensembles, Cross Validation, and Active Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown how to estimate the optimal weights of the ensemble members using unlabeled data and how the ambiguity can be used to select new training data to be labeled in an active learning scheme."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067947117"
                        ],
                        "name": "Andrew Y. Ng",
                        "slug": "Andrew-Y.-Ng",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Ng",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Y. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9086884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2671b151fad7e176176b35d425b2b6356ff4595",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "When documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy. The U.S. patent database and Yahoo are two examples. This paper shows that the accuracy of a naive Bayes text classi er can be signi cantly improved by taking advantage of a hierarchy of classes. We adopt an established statistical technique called shrinkage that smoothes parameter estimates of a data-sparse child with its parent in order to obtain more robust parameter estimates. The approach is also employed in deleted interpolation, a technique for smoothing n-grams in language modeling for speech recognition. Our method scales well to large data sets, with numerous categories in large hierarchies. Experimental results on three real-world data sets from UseNet, Yahoo, and corporate web pages show improved performance, with a reduction in error up to 29% over the traditional at classi er."
            },
            "slug": "Improving-Text-Classification-by-Shrinkage-in-a-of-McCallum-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Improving Text Classification by Shrinkage in a Hierarchy of Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows that the accuracy of a naive Bayes text classi er can be improved by taking advantage of a hierarchy of classes, and adopts an established statistical technique called shrinkage that smoothes parameter estimates of a data-sparse child with its parent in order to obtain more robust parameter estimates."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143870001"
                        ],
                        "name": "T. Krishnan",
                        "slug": "T.-Krishnan",
                        "structuredName": {
                            "firstName": "Thriyambakam",
                            "lastName": "Krishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Krishnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122530182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d50991b693fc23edda316fb1487f114f6cc6706",
            "isKey": false,
            "numCitedBy": 6113,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The first unified account of the theory, methodology, and applications of the EM algorithm and its extensionsSince its inception in 1977, the Expectation-Maximization (EM) algorithm has been the subject of intense scrutiny, dozens of applications, numerous extensions, and thousands of publications. The algorithm and its extensions are now standard tools applied to incomplete data problems in virtually every field in which statistical methods are used. Until now, however, no single source offered a complete and unified treatment of the subject.The EM Algorithm and Extensions describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts. Employing numerous examples, Geoffrey McLachlan and Thriyambakam Krishnan examine applications both in evidently incomplete data situations-where data are missing, distributions are truncated, or observations are censored or grouped-and in a broad variety of situations in which incompleteness is neither natural nor evident. They point out the algorithm's shortcomings and explain how these are addressed in the various extensions.Areas of application discussed include: Regression Medical imaging Categorical data analysis Finite mixture analysis Factor analysis Robust statistical modeling Variance-components estimation Survival analysis Repeated-measures designs For theoreticians, practitioners, and graduate students in statistics as well as researchers in the social and physical sciences, The EM Algorithm and Extensions opens the door to the tremendous potential of this remarkably versatile statistical tool."
            },
            "slug": "The-EM-algorithm-and-extensions-McLachlan-Krishnan",
            "title": {
                "fragments": [],
                "text": "The EM algorithm and extensions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The EM Algorithm and Extensions describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts, opening the door to the tremendous potential of this remarkably versatile statistical tool."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123333909"
                        ],
                        "name": "M.I. Jordan",
                        "slug": "M.I.-Jordan",
                        "structuredName": {
                            "firstName": "M.I.",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M.I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207178945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d98d0d1900b13b87aa4ffd6b69c046beb63f0434",
            "isKey": false,
            "numCitedBy": 3901,
            "numCiting": 303,
            "paperAbstract": {
                "fragments": [],
                "text": "The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances \u2014 including the key problems of computing marginals and modes of probability distributions \u2014 are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms \u2014 among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations \u2014 can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "slug": "Graphical-Models,-Exponential-Families,-and-Wainwright-Jordan",
            "title": {
                "fragments": [],
                "text": "Graphical Models, Exponential Families, and Variational Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786338"
                        ],
                        "name": "Seong-Bae Park",
                        "slug": "Seong-Bae-Park",
                        "structuredName": {
                            "firstName": "Seong-Bae",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Bae Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692756"
                        ],
                        "name": "Byoung-Tak Zhang",
                        "slug": "Byoung-Tak-Zhang",
                        "structuredName": {
                            "firstName": "Byoung-Tak",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Byoung-Tak Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8311911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d3e1386e9e3f5db44c20e22c52325b96e2bf0ae",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Most document classification systems consider only the distribution of content words of the documents, ignoring the syntactic information underlying the documents though it is also an important factor. In this paper, we present an approach for classifying large scale unstructured documents by incorporating both lexical and syntactic information of documents. For this purpose, we use the co-training algorithm, a partially supervised learning algorithm, in which two separated views for the training data are employed and the small number of labeled data are augmented by a large number of unlabeled data. Since both lexical and syntactic information can play roles of separated views for the unstructured documents, the co-training algorithm enhances the performance of document classification using both of them and a large number of unlabeled documents. The experimental results on Reuters-21578 corpus and TREC-7 filtering documents show the effectiveness of unlabeled documents and the use of both lexical and syntactic information."
            },
            "slug": "Large-Scale-Unstructured-Document-Classification-Park-Zhang",
            "title": {
                "fragments": [],
                "text": "Large Scale Unstructured Document Classification Using Unlabeled Data and Syntactic Information"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The co-training algorithm is used, a partially supervised learning algorithm, in which two separated views for the training data are employed and the small number of labeled data are augmented by a large number of unlabeled data."
            },
            "venue": {
                "fragments": [],
                "text": "PAKDD"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061023110"
                        ],
                        "name": "D. Herrmann",
                        "slug": "D.-Herrmann",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Herrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Herrmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14679413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2afea0ad74ed804e2eb419c5a1a64b3d9c5cfc9a",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate data based procedures for selecting the kernel when learning with Support Vector Machines. We provide generalization error bounds by estimating the Rademacher complexities of the corresponding function classes. In particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors, i.e., we allow to vary the spectrum and keep the eigenvectors fix. This bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel. However, optimizing the margin over such classes leads to overfitting. We thus propose a suitable way of constraining the class. We use an efficient algorithm to solve the resulting optimization problem, present preliminary experimental results, and compare them to an alignment-based approach."
            },
            "slug": "On-the-Complexity-of-Learning-the-Kernel-Matrix-Bousquet-Herrmann",
            "title": {
                "fragments": [],
                "text": "On the Complexity of Learning the Kernel Matrix"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A suitable way of constraining the class is proposed and an efficient algorithm is used to solve the resulting optimization problem, preliminary experimental results are presented, and an alignment-based approach is compared."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2702042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c70a0a39a686bf80b76cb1b77f9eef156f6432d",
            "isKey": false,
            "numCitedBy": 11152,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds."
            },
            "slug": "A-Study-of-Cross-Validation-and-Bootstrap-for-and-Kohavi",
            "title": {
                "fragments": [],
                "text": "A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results indicate that for real-word datasets similar to the authors', the best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 447055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e0bb90262160c26d8c9ec216716d57122c8672",
            "isKey": false,
            "numCitedBy": 688,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data--that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof."
            },
            "slug": "The-Bayesian-Structural-EM-Algorithm-Friedman",
            "title": {
                "fragments": [],
                "text": "The Bayesian Structural EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper extends Structural EM to deal directly with Bayesian model selection and proves the convergence of the resulting algorithm and shows how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752379"
                        ],
                        "name": "D. Opitz",
                        "slug": "D.-Opitz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Opitz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Opitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12093784,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec174f3b57706e6482ad2d00b3cf7f278b6bd22f",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural-network ensembles have been shown to be very accurate classification techniques. Previous work has shown that an effective ensemble should consist of networks that are not only highly correct, but ones that make their errors on different parts of the input space as well. Most existing techniques, however, only indirectly address the problem of creating such a set of networks. In this paper we present a technique called ADDEMUP that uses genetic algorithms to directly search for an accurate and diverse set of trained networks. ADDEMUP works by first creating an initial population, then uses genetic operators to continually create new networks, keeping the set of networks that are as accurate as possible while disagreeing with each other as much as possible. Experiments on three DNA problems show that ADDEMUP is able to generate a set of trained networks that is more accurate than several existing approaches. Experiments also show that ADDEMUP is able to effectively incorporate prior knowledge, if available, to improve the quality of its ensemble."
            },
            "slug": "Generating-Accurate-and-Diverse-Members-of-a-Opitz-Shavlik",
            "title": {
                "fragments": [],
                "text": "Generating Accurate and Diverse Members of a Neural-Network Ensemble"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A technique called ADDEMUP is presented that uses genetic algorithms to directly search for an accurate and diverse set of trained networks and is able to effectively incorporate prior knowledge, if available, to improve the quality of its ensemble."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12265049,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "87c780cf10c2ea642468c827b3877aa0355329d1",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper identifies a broad class of nonlinear dimensionality reduced (NLDR) problems where the exact local isometry between an extrinsically curved data manifold M and a low-dimensional parameterization space can be recovered from a finite set of high-dimensional point sampels. The method, Geodesic Nullsapce Analysis (GNA), rests on two results: First, the exact isometric parameterization of a local point clique on M haas an algebraic reduction to arc-length integrations when the ambient-space embedding of M is locally a product of planar quadrics. Second, the locally isometric global parameterization lies in the left invariant subspace of a linearizing operator that averages the nullspace projectors of the local parameterizations. We show how to use the GNA operator for denosing, dimensionality reduction, and resynthesis of both the original data and of new samples, making such s\u0308ubmanifold\u0308methods an attractive alternative to subspace methods in data analysis. British Machine Vision Conference (BMVC) This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c \u00a9Mitsubishi Electric Research Laboratories, Inc., 2004 201 Broadway, Cambridge, Massachusetts 02139"
            },
            "slug": "From-Subspaces-to-Submanifolds-Brand",
            "title": {
                "fragments": [],
                "text": "From Subspaces to Submanifolds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14021663,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e6da0ec3d6d9d65274eb5673e81766df9ea16298",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an algorithm for estimating the values of a function at a set of test points xl+1,..., xl+m given a set of training points (x1, Y1),...,(xl, yl) without estimating (as an intermediate step) the regression function. We demonstrate that this direct (transductive) way for estimating values of the regression (or classification in pattern recognition) can be more accurate than the traditional one based on two steps, first estimating the function and then calculating the values of this function at the points of interest."
            },
            "slug": "Transductive-Inference-for-Estimating-Values-of-Chapelle-Vapnik",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Estimating Values of Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This direct way for estimating values of the regression (or classification in pattern recognition) can be more accurate than the traditional one based on two steps, first estimating the function and then calculating the values of this function at the points of interest."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709874"
                        ],
                        "name": "A. Frieze",
                        "slug": "A.-Frieze",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Frieze",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Frieze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144632403"
                        ],
                        "name": "R. Kannan",
                        "slug": "R.-Kannan",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kannan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kannan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15479137,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "66f36f30144f6ac56208c7f712c5cfb1f8c9b2d0",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. In this paper we consider the problem of learning a linear threshold function (a half-space in n dimensions, also called a ``perceptron''). Methods for solving this problem generally fall into two categories. In the absence of noise, this problem can be formulated as a Linear Program and solved in polynomial time with the Ellipsoid Algorithm or Interior Point methods. Alternatively, simple greedy algorithms such as the Perceptron Algorithm are often used in practice and have certain provable noise-tolerance properties; but their running time depends on a separation parameter, which quantifies the amount of ``wiggle room'' available for a solution, and can be exponential in the description length of the input. In this paper we show how simple greedy methods can be used to find weak hypotheses (hypotheses that correctly classify noticeably more than half of the examples) in polynomial time, without dependence on any separation parameter. Suitably combining these hypotheses results in a polynomial-time algorithm for learning linear threshold functions in the PAC model in the presence of random classification noise. (Also, a polynomial-time algorithm for learning linear threshold functions in the Statistical Query model of Kearns.) Our algorithm is based on a new method for removing outliers in data. Specifically, for any set S of points in Rn , each given to b bits of precision, we show that one can remove only a small fraction of S so that in the remaining set T , for every vector v , maxx \u2208 T(v . x)2\u2264 poly(n,b)Ex \u2208 T(v . x)2; i.e., for any hyperplane through the origin, the maximum distance (squared) from a point in T to the plane is at most polynomially larger than the average. After removing these outliers, we are able to show that a modified version of the Perceptron Algorithm finds a weak hypothesis in polynomial time, even in the presence of random classification noise."
            },
            "slug": "A-Polynomial-Time-Algorithm-for-Learning-Noisy-Blum-Frieze",
            "title": {
                "fragments": [],
                "text": "A Polynomial-Time Algorithm for Learning Noisy Linear Threshold Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how simple greedy methods can be used to find weak hypotheses (hypotheses that correctly classify noticeably more than half of the examples) in polynomial time, without dependence on any separation parameter."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 37th Conference on Foundations of Computer Science"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053645486"
                        ],
                        "name": "Junnichi Takeuchi",
                        "slug": "Junnichi-Takeuchi",
                        "structuredName": {
                            "firstName": "Junnichi",
                            "lastName": "Takeuchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junnichi Takeuchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102741823"
                        ],
                        "name": "K. M. Warmuth",
                        "slug": "K.-M.-Warmuth",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "Manfred"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5445767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ef6462998c5cf2775a97c6f090233a65cc8509a",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of efficient learning of probabilistic concepts (p-concepts) and more generally stochastic rules in the sense defined by Kearns and Schapire [6] and by Yamanishi [18]. Their models extend the PAC-learning model of Valiant [16] to the learning scenario in which the target concept or function is stochastic rather than deterministic as in Valiant\u2019s original model. In this paper, we consider the learnability of stochastic rules with respect to the classic \u2018Kullback-Leibler divergence\u2019 (KL divergence) as well as the quadratic distance as the distance measure between the rules. First, we show that the notion of polynomial time learnability of p-concepts and stochastic rules with fixed range size using the KL divergence is in fact equivalent to the same notion using the quadratic distance, and hence any of the distances considered in [6] and [18]: the quadratic, variation, and Hellinger distances. As a corollary, it follows that a wide range of classes of p-concepts which were shown to be polynomially learnable with respect to the quadratic distance in [6] are also learnable with respect to the KL divergence. The sample and time complexity of algorithms that would be obtained by the above general equivalence, however, are far from optimal. We present a polynomial learning algorithm with reasonable sample and time complexity for the important class of convex linear combinations of stochastic rules. We also develop a simple and versatile technique for obtaining sample complexity bounds for learning classes of stochastic rules with respect to the KL-divergence and quadratic distance, and apply them to produce bounds for the classes of probabilistic finite state acceptors (automata), probabilistic decision lists, and convex linear combinations. key words: PAC-learning, KL-divergence, quadratic-distance, stochastic rules, p-concepts"
            },
            "slug": "Polynomial-Learnability-of-Stochastic-Rules-with-to-Abe-Takeuchi",
            "title": {
                "fragments": [],
                "text": "Polynomial Learnability of Stochastic Rules with Respect to the KL-Divergence and Quadratic Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper shows that the notion of polynomial time learnability of p-concepts and stochastic rules with fixed range size using the KL divergence is in fact equivalent to the same notion using the quadratic distance, and hence any of the distances considered in [6] and [18]: the quadRatic, variation, and Hellinger distances."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6635519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4abd4e51705e74f1739bd3a1e47ac10e45f6468b",
            "isKey": false,
            "numCitedBy": 1171,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage."
            },
            "slug": "Regularization-Algorithms-for-Learning-That-Are-to-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15008961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a4ca461fa847e8433bab67e7bfe4620371c1f77",
            "isKey": false,
            "numCitedBy": 1488,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of unlabeled data to help labeled data in cl ssification. We propose a simple iterative algorithm, label pro pagation, to propagate labels through the dataset along high density are as d fined by unlabeled data. We analyze the algorithm, show its solution , and its connection to several other algorithms. We also show how to lear n p ameters by minimum spanning tree heuristic and entropy minimiz ation, and the algorithm\u2019s ability to perform feature selection. Expe riment results are promising."
            },
            "slug": "Learning-from-labeled-and-unlabeled-data-with-label-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Learning from labeled and unlabeled data with label propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A simple iterative algorithm to propagate labels through the dataset along high density are as d fined by unlabeled data is proposed and its solution is analyzed, and its connection to several other algorithms is analyzed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14336127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e45c2420e6dc59ba6d357fb0c996ebf43c861560",
            "isKey": false,
            "numCitedBy": 1619,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis."
            },
            "slug": "Exploiting-Generative-Models-in-Discriminative-Jaakkola-Haussler",
            "title": {
                "fragments": [],
                "text": "Exploiting Generative Models in Discriminative Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686671"
                        ],
                        "name": "L. Csat\u00f3",
                        "slug": "L.-Csat\u00f3",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Csat\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Csat\u00f3"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118736313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c51abe724a072e916fabb86a138161f08f8f687a",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been an increased interest in applying non-parametric methods to real-world problems. Significant research has been devoted to Gaussian processes (GPs) due to their increased flexibility when compared with parametric models. These methods use Bayesian learning, which generally leads to analytically intractable posteriors. This thesis proposes a two-step solution to construct a probabilistic approximation to the posterior. In the first step we adapt the Bayesian online learning to GPs: the final approximation to the posterior is the result of propagating the first and second moments of intermediate posteriors obtained by combining a new example with the previous approximation. The propagation of em functional forms is solved by showing the existence of a parametrisation to posterior moments that uses combinations of the kernel function at the training points, transforming the Bayesian online learning of functions into a parametric formulation. The drawback is the prohibitive quadratic scaling of the number of parameters with the size of the data, making the method inapplicable to large datasets. The second step solves the problem of the exploding parameter size and makes GPs applicable to arbitrarily large datasets. The approximation is based on a measure of distance between two GPs, the KL-divergence between GPs. This second approximation is with a constrained GP in which only a small subset of the whole training dataset is used to represent the GP. This subset is called the em Basis Vector, or BV set and the resulting GP is a sparse approximation to the true posterior. As this sparsity is based on the KL-minimisation, it is probabilistic and independent of the way the posterior approximation from the first step is obtained. We combine the sparse approximation with an extension to the Bayesian online algorithm that allows multiple iterations for each input and thus approximating a batch solution. The resulting sparse learning algorithm is a generic one: for different problems we only change the likelihood. The algorithm is applied to a variety of problems and we examine its performance both on more classical regression and classification tasks and to the data-assimilation and a simple density estimation problems."
            },
            "slug": "Gaussian-processes:iterative-sparse-approximations-Csat\u00f3",
            "title": {
                "fragments": [],
                "text": "Gaussian processes:iterative sparse approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis proposes a two-step solution to construct a probabilistic approximation to the posterior of Gaussian processes, and combines the sparse approximation with an extension to the Bayesian online algorithm that allows multiple iterations for each input and thus approximating a batch solution."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38551815"
                        ],
                        "name": "Anat Levin",
                        "slug": "Anat-Levin",
                        "structuredName": {
                            "firstName": "Anat",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anat Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11538768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "602b209fee5a93832a094be3c6e340b25abc8def",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "One significant challenge in the construction of visual detection systems is the acquisition of sufficient labeled data. We describe a new technique for training visual detectors which requires only a small quantity of labeled data, and then uses unlabeled data to improve performance over time. Unsupervised improvement is based on the cotraining framework of Blum and Mitchell, in which two disparate classifiers are trained simultaneously. Unlabeled examples which are confidently labeled by one classifier are added, with labels, to the training set of the other classifier. Experiments are presented on the realistic task of automobile detection in roadway surveillance video. In this application, cotraining reduces the false positive rate by a factor of 2 to 11 from the classifier trained with labeled data alone."
            },
            "slug": "Unsupervised-improvement-of-visual-detectors-using-Levin-Viola",
            "title": {
                "fragments": [],
                "text": "Unsupervised improvement of visual detectors using cotraining"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new technique for training visual detectors which requires only a small quantity of labeled data, and then uses unlabeled data to improve performance over time is described, based on the cotraining framework of Blum and Mitchell."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2374498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "008abebf4a9404db9050c9d2fbca769f4faf3ca6",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed."
            },
            "slug": "Learning-by-Transduction-Gammerman-Vovk",
            "title": {
                "fragments": [],
                "text": "Learning by Transduction"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work describes a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 930666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7eeea351d07ef9cc2d572d857e944ab59ee7c61b",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian committee machine (BCM) is a novel approach to combining estimators that were trained on different data sets. Although the BCM can be applied to the combination of any kind of estimators, the main foci are gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data. Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator. The BCM also provides a new solution for on-line learning with potential applications to data mining. We apply the BCM to systems with fixed basis functions and discuss its relationship to gaussian process regression. Finally, we show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input-dependent combination of estimators."
            },
            "slug": "A-Bayesian-Committee-Machine-Tresp",
            "title": {
                "fragments": [],
                "text": "A Bayesian Committee Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 683036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22f0579f212dfb568fbda317cba67c8654d84ccd",
            "isKey": false,
            "numCitedBy": 3143,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These test sare compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 52 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set."
            },
            "slug": "Approximate-Statistical-Tests-for-Comparing-Dietterich",
            "title": {
                "fragments": [],
                "text": "Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task and measures the power (ability to detect algorithm differences when they do exist) of these tests."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209630"
                        ],
                        "name": "Behzad M. Shahshahani",
                        "slug": "Behzad-M.-Shahshahani",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahshahani",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Behzad M. Shahshahani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 94
                            }
                        ],
                        "text": "Later, this one component per class setting has been extended to several components per class [Shahshahani and Landgrebe, 1994] and further generalized by Miller and Uyar [1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11081015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "334867ed99a0af07d8a53dae4f7fdeffffdecc09",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the use of unlabeled sam- ples in reducing the problem of small training sample size that can severely affect the recognition rate of classifiers when the dimensionality of the multispectral data is high. We show that by using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated. Furthermore, by ex- periments, we show that by using additional unlabeled samples more representative estimates can be obtained. We also pro- pose a semiparametric method for incorporating the training (Le., labeled) and unlabeled samples simultaneously into the parameter estimation process."
            },
            "slug": "The-effect-of-unlabeled-samples-in-reducing-the-and-Shahshahani-Landgrebe",
            "title": {
                "fragments": [],
                "text": "The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated and therefore more representative estimates can be obtained."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Geosci. Remote. Sens."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1925579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b83396caf4762c906530c9219a9e4dd0658232b0",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-general-lower-bound-on-the-number-of-examples-for-Ehrenfeucht-Haussler",
            "title": {
                "fragments": [],
                "text": "A general lower bound on the number of examples needed for learning"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703070"
                        ],
                        "name": "Alexander G. Gray",
                        "slug": "Alexander-G.-Gray",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gray",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander G. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14961407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93c473358b063cec9bb9ea7cb3036bbce7b76493",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present efficient algorithms for all-point-pairs problems, or 'N-body'- like problems, which are ubiquitous in statistical learning. We focus on six examples, including nearest-neighbor classification, kernel density estimation, outlier detection, and the two-point correlation. These include any problem which abstractly requires a comparison of each of the N points in a dataset with each other point and would naively be solved using N2 distance computations. In practice N is often large enough to make this infeasible. We present a suite of new geometric techniques which are applicable in principle to any 'N-body' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM's. Our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation, even for small datasets. We are aware of no exact algorithms for these problems which are more efficient either empirically or theoretically. In addition, our framework yields simple and elegant algorithms. It also permits two important generalizations beyond the standard all-point-pairs problems, which are more difficult. These are represented by our final examples, the multiple two-point correlation and the notorious n-point correlation."
            },
            "slug": "'N-Body'-Problems-in-Statistical-Learning-Gray-Moore",
            "title": {
                "fragments": [],
                "text": "'N-Body' Problems in Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A suite of new geometric techniques which are applicable in principle to any 'N-body' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM's are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1985596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204f6148bc6aba37eb5a7c5686d80547a99425b1",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy (Omohundro 1992). The process begins with a maximum likelihood HMM that directly encodes the training data. Successively more general models are produced by merging HMM states. A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing. The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability. We discuss a variety of possible priors for HMMs, as well as a number of approximations which improve the computational efficiency of the algorithm. We studied three applications to evaluate the procedure. The first compares the merging algorithm with the standard Baum-Welch approach in inducing simple finite-state languages from small, positive-only training samples. We found that the merging procedure is more robust and accurate, particularly with a small amount of training data. The second application uses labelled speech data from the TIMIT database to build compact, multiple-pronunciation word models that can be used in speech recognition. Finally, we describe how the algorithm was incorporated in an operational speech understanding system, where it is combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models."
            },
            "slug": "Best-first-Model-Merging-for-Hidden-Markov-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Best-first Model Merging for Hidden Markov Model Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy, and how the algorithm was incorporated in an operational speech understanding system, where it was combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14591650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "isKey": false,
            "numCitedBy": 3047,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more."
            },
            "slug": "Transductive-Inference-for-Text-Classification-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Text Classification using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An analysis of why Transductive Support Vector Machines are well suited for text classi cation is presented, and an algorithm for training TSVMs, handling 10,000 examples and more is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791498"
                        ],
                        "name": "R. Ghani",
                        "slug": "R.-Ghani",
                        "structuredName": {
                            "firstName": "Rayid",
                            "lastName": "Ghani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ghani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7976232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b913876b5b7621332e6981e368017b95af06c2af",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a framework to incorporate unlabeled data in the error-correcting output coding (ECOC) setup by decomposing multiclass problems into multiple binary problems and then use co-training to learn the individual binary classification problems. We show that our method is especially useful for classification tasks involving a large number of categories where co-training doesn't perform very well by itself and when combined with ECOC, outperforms several other algorithms that combine labeled and unlabeled data for text classification in terms of accuracy, precision-recall tradeoff, and efficiency."
            },
            "slug": "Combining-labeled-and-unlabeled-data-for-text-with-Ghani",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data for text classification with a large number of categories"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work develops a framework to incorporate unlabeled data in the error-correcting output coding (ECOC) setup by decomposing multiclass problems into multiple binary problems and then using co-training to learn the individual binary classification problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2001 IEEE International Conference on Data Mining"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2697855"
                        ],
                        "name": "M. Ringuette",
                        "slug": "M.-Ringuette",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Ringuette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ringuette"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16894634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9fd1a7ae0322d417ab2d32017e373dd50efc063",
            "isKey": false,
            "numCitedBy": 745,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the use of inductive learning to categorize natural language documents into predeened content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it diicult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classiier and a decision tree learning algorithm on two text categorization data sets. We nd that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial preeltering of features, connrming the results found by Almuallim and Dietterich on artiicial data sets. We also demonstrate the impact of the time-varying nature of category deenitions."
            },
            "slug": "A-comparison-of-two-learning-algorithms-for-text-Lewis-Ringuette",
            "title": {
                "fragments": [],
                "text": "A comparison of two learning algorithms for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives, and the stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783667"
                        ],
                        "name": "I. Dhillon",
                        "slug": "I.-Dhillon",
                        "structuredName": {
                            "firstName": "Inderjit",
                            "lastName": "Dhillon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dhillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944330"
                        ],
                        "name": "D. Modha",
                        "slug": "D.-Modha",
                        "structuredName": {
                            "firstName": "Dharmendra",
                            "lastName": "Modha",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Modha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2286629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "155d81d9f764de34c7234f0e3dc37a5bd297edee",
            "isKey": false,
            "numCitedBy": 1408,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Unlabeled document collections are becoming increasingly common and available; mining such data sets represents a major contemporary challenge. Using words as features, text documents are often represented as high-dimensional and sparse vectors\u2013a few thousand dimensions and a sparsity of 95 to 99% is typical. In this paper, we study a certain spherical k-means algorithm for clustering such document vectors. The algorithm outputs k disjoint clusters each with a concept vector that is the centroid of the cluster normalized to have unit Euclidean norm. As our first contribution, we empirically demonstrate that, owing to the high-dimensionality and sparsity of the text data, the clusters produced by the algorithm have a certain \u201cfractal-like\u201d and \u201cself-similar\u201d behavior. As our second contribution, we introduce concept decompositions to approximate the matrix of document vectors; these decompositions are obtained by taking the least-squares approximation onto the linear subspace spanned by all the concept vectors. We empirically establish that the approximation errors of the concept decompositions are close to the best possible, namely, to truncated singular value decompositions. As our third contribution, we show that the concept vectors are localized in the word space, are sparse, and tend towards orthonormality. In contrast, the singular vectors are global in the word space and are dense. Nonetheless, we observe the surprising fact that the linear subspaces spanned by the concept vectors and the leading singular vectors are quite close in the sense of small principal angles between them. In conclusion, the concept vectors produced by the spherical k-means algorithm constitute a powerful sparse and localized \u201cbasis\u201d for text data sets."
            },
            "slug": "Concept-Decompositions-for-Large-Sparse-Text-Data-Dhillon-Modha",
            "title": {
                "fragments": [],
                "text": "Concept Decompositions for Large Sparse Text Data Using Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The concept vectors produced by the spherical k-means algorithm constitute a powerful sparse and localized \u201cbasis\u201d for text data sets and are localized in the word space, are sparse, and tend towards orthonormality."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6243824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d474299d7a51b89a1d7394d426cf881a89b8013d",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>"
            },
            "slug": "Efficient-distribution-free-learning-of-concepts-Kearns-Schapire",
            "title": {
                "fragments": [],
                "text": "Efficient distribution-free learning of probabilistic concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated, and an underlying theory of learning p-concepts is developed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15872360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9d7f589a3d368a3701832e28d90ca09ec9e5577",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic grouping and segmentation of images remains a challenging problem in computer vision. Recently, a number of authors have demonstrated good performance on this task using methods that are based on eigenvectors of the affinity matrix. These approaches are extremely attractive in that they are based on simple eigendecomposition algorithms whose stability is well understood. Nevertheless, the use of eigendecompositions in the context of segmentation is far from well understood. In this paper we give a unified treatment of these algorithms, and show the close connections between them while highlighting their distinguishing features. We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings. Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms. We illustrate our analysis with results on real and synthetic images."
            },
            "slug": "Segmentation-using-eigenvectors:-a-unifying-view-Weiss",
            "title": {
                "fragments": [],
                "text": "Segmentation using eigenvectors: a unifying view"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified treatment of eigenvectors of block matrices based on eigendecompositions in the context of segmentation is given, and close connections between them are shown while highlighting their distinguishing features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207705714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f31d46fdeb84cb848c1e3f19c3182f0b481963d2",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an entropic prior for multinomial parameter estimation problems and solve for its maximum a posteriori (MAP) estimator. The prior is a bias for maximally structured and minimally ambiguous models. In conditional probability models with hidden state, iterative MAP estimation drives weakly supported parameters toward extinction, effectively turning them off. Thus, structure discovery is folded into parameter estimation. We then establish criteria for simplifying a probabilistic model's graphical structure by trimming parameters and states, with a guarantee that any such deletion will increase the posterior probability of the model. Trimming accelerates learning by sparsifying the model. All operations monotonically and maximally increase the posterior probability, yielding structure-learning algorithms only slightly slower than parameter estimation via expectation-maximization and orders of magnitude faster than search-based structure induction. When applied to hidden Markov model training, the resulting models show superior generalization to held-out test data. In many cases the resulting models are so sparse and concise that they are interpretable, with hidden states that strongly correlate with meaningful categories."
            },
            "slug": "Structure-Learning-in-Conditional-Probability-via-Brand",
            "title": {
                "fragments": [],
                "text": "Structure Learning in Conditional Probability Models via an Entropic Prior and Parameter Extinction"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "An entropic prior is introduced for multinomial parameter estimation problems and the resulting models show superior generalization to held-out test data, and a guarantee that any such deletion will increase the posterior probability of the model."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11672931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa7a32d9ce76cd016cf21d4f956e19d90e87b0dc",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, andthe manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximizationalgorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "slug": "Operations-for-Learning-with-Graphical-Models-Buntine",
            "title": {
                "fragments": [],
                "text": "Operations for Learning with Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709706"
                        ],
                        "name": "Minghua Deng",
                        "slug": "Minghua-Deng",
                        "structuredName": {
                            "firstName": "Minghua",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghua Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7412694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "087e8dc46e9ef192e828a2287e153425173e1532",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods provide a principled framework in which to represent many types of data, including vectors, strings, trees and graphs. As such, these methods are useful for drawing inferences about biological phenomena. We describe a method for combining multiple kernel representations in an optimal fashion, by formulating the problem as a convex optimization problem that can be solved using semidefinite programming techniques. The method is applied to the problem of predicting yeast protein functional classifications using a support vector machine (SVM) trained on five types of data. For this problem, the new method performs better than a previously-described Markov random field method, and better than the SVM trained on any single type of data."
            },
            "slug": "Kernel-Based-Data-Fusion-and-Its-Application-to-in-Lanckriet-Deng",
            "title": {
                "fragments": [],
                "text": "Kernel-Based Data Fusion and Its Application to Protein Function Prediction in Yeast"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method for combining multiple kernel representations in an optimal fashion is described, by formulating the problem as a convex optimization problem that can be solved using semidefinite programming techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Pacific Symposium on Biocomputing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 356393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32b21a53f527c8ed00f28006df5285164d830912",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "An important aspect of clustering algorithms is whether the partitions constructed on finite samples converge to a useful clustering of the whole data space as the sample size increases. This paper investigates this question for normalized and unnormalized versions of the popular spectral clustering algorithm. Surprisingly, the convergence of unnormalized spectral clustering is more difficult to handle than the normalized case. Even though recently some first results on the convergence of normalized spectral clustering have been obtained, for the unnormalized case we have to develop a completely new approach combining tools from numerical integration, spectral and perturbation theory, and probability. It turns out that while in the normalized case, spectral clustering usually converges to a nice partition of the data space, in the unnormalized case the same only holds under strong additional assumptions which are not always satisfied. We conclude that our analysis gives strong evidence for the superiority of normalized spectral clustering. It also provides a basis for future exploration of other Laplacian-based methods."
            },
            "slug": "Limits-of-Spectral-Clustering-Luxburg-Bousquet",
            "title": {
                "fragments": [],
                "text": "Limits of Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper investigates whether the partitions constructed on finite samples converge to a useful clustering of the whole data space as the sample size increases and concludes that while in the normalized case, spectral clustering usually converges to a nice partition of the data space, in the unnormalized case the same only holds under strong additional assumptions which are not always satisfied."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30883040"
                        ],
                        "name": "S. Ganesalingam",
                        "slug": "S.-Ganesalingam",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Ganesalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ganesalingam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123486552,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0bf02f847185037588051b853f245c8e243c5830",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Mixtures of distributions, in particular the normal distribution, have been used extensively as models in a wide variety of important practical situations where the population of interest may be considered to consist of two or more subpopulations mixed in varying proportions. The problem of decomposing such a mixture of distributions is of considerable interest and utility. Two commonly used clustering methods based on maximum likelihood are considered in the context of the classification problem where observations of unknown origin belong to one of the two possible populations. The basic assumptions and associated properties of the two methods are contrasted and illustrated by a series of simulations under two different sampling schemes, namely the mixture sampling scheme and the separate sampling scheme. A case study is presented to demonstrate the basic differences between these two methods."
            },
            "slug": "Classification-and-Mixture-Approaches-to-Clustering-Ganesalingam",
            "title": {
                "fragments": [],
                "text": "Classification and Mixture Approaches to Clustering Via Maximum Likelihood"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1798409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f40aff4f7dc431d1da223744c63b67b49acacc6",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Many unsupervised algorithms for nonlinear dimensionality reduction, such as locally linear embedding (LLE) and Laplacian eigenmaps, are derived from the spectral decompositions of sparse matrices. While these algorithms aim to preserve certain proximity relations on average, their embeddings are not explicitly designed to preserve local features such as distances or angles. In this paper, we show how to construct a low dimensional embedding that maximally preserves angles between nearby data points. The embedding is derived from the bottom eigenvectors of LLE and/or Laplacian eigenmaps by solving an additional (but small) problem in semidefinite programming, whose size is independent of the number of data points. The solution obtained by semidefinite programming also yields an estimate of the data's intrinsic dimensionality. Experimental results on several data sets demonstrate the merits of our approach."
            },
            "slug": "Analysis-and-extension-of-spectral-methods-for-Sha-Saul",
            "title": {
                "fragments": [],
                "text": "Analysis and extension of spectral methods for nonlinear dimensionality reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows how to construct a low dimensional embedding that maximally preserves angles between nearby data points by solving an additional problem in semidefinite programming, whose size is independent of the number of data points."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444394"
                        ],
                        "name": "E. Ziegel",
                        "slug": "E.-Ziegel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ziegel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ziegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46701966,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "e41ba5dc12c79a64dfa905c0328f95976252ffe0",
            "isKey": false,
            "numCitedBy": 12393,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research. Chapter 12 concludes the book with some commentary about the scienti\u008e c contributions of MTS. The Taguchi method for design of experiment has generated considerable controversy in the statistical community over the past few decades. The MTS/MTGS method seems to lead another source of discussions on the methodology it advocates (Montgomery 2003). As pointed out by Woodall et al. (2003), the MTS/MTGS methods are considered ad hoc in the sense that they have not been developed using any underlying statistical theory. Because the \u201cnormal\u201d and \u201cabnormal\u201d groups form the basis of the theory, some sampling restrictions are fundamental to the applications. First, it is essential that the \u201cnormal\u201d sample be uniform, unbiased, and/or complete so that a reliable measurement scale is obtained. Second, the selection of \u201cabnormal\u201d samples is crucial to the success of dimensionality reduction when OAs are used. For example, if each abnormal item is really unique in the medical example, then it is unclear how the statistical distance MD can be guaranteed to give a consistent diagnosis measure of severity on a continuous scale when the larger-the-better type S/N ratio is used. Multivariate diagnosis is not new to Technometrics readers and is now becoming increasingly more popular in statistical analysis and data mining for knowledge discovery. As a promising alternative that assumes no underlying data model, The Mahalanobis\u2013Taguchi Strategy does not provide suf\u008e cient evidence of gains achieved by using the proposed method over existing tools. Readers may be very interested in a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods. Overall, although the idea of MTS/MTGS is intriguing, this book would be more valuable had it been written in a rigorous fashion as a technical reference. There is some lack of precision even in several mathematical notations. Perhaps a follow-up with additional theoretical justi\u008e cation and careful case studies would answer some of the lingering questions."
            },
            "slug": "The-Elements-of-Statistical-Learning-Ziegel",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research, and a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": false,
            "numCitedBy": 12182,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153856933"
                        ],
                        "name": "M. Brown",
                        "slug": "M.-Brown",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863326"
                        ],
                        "name": "I. Mian",
                        "slug": "I.-Mian",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Mian",
                            "middleNames": [
                                "Saira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5233893"
                        ],
                        "name": "K. Sj\u00f6lander",
                        "slug": "K.-Sj\u00f6lander",
                        "structuredName": {
                            "firstName": "Kimmen",
                            "lastName": "Sj\u00f6lander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sj\u00f6lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2160404,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "5d28fc1a4027d23cc9e4ad8555361d48940e9be8",
            "isKey": false,
            "numCitedBy": 2003,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMMs) are applied to the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated on the globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the SWISS-PROT 22 database for other sequences that are members of the given protein family, or contain the given domain. The HMM produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three-dimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appears to have a slight advantage over PROFILESEARCH in terms of lower rates of false negatives and false positives, even though the HMM is trained using only unaligned sequences, whereas PROFILESEARCH requires aligned training sequences. Our results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling. This region has been suggested to contain the functional domains that are typical or essential for all L-type calcium channels regardless of whether they couple to ryanodine receptors, conduct ions or both."
            },
            "slug": "Hidden-Markov-models-in-computational-biology.-to-Krogh-Brown",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models in computational biology. Applications to protein modeling."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the alpha-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50742419"
                        ],
                        "name": "David A. Cohn",
                        "slug": "David-A.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Cohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53900406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc86aa7d81cfac9aaec4b095e817ce7bb1f1ef78",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to clustering based on the observa- tion that \"it is easier to criticize than to construct.\" Our approach of semi- supervised clustering allows a user to iteratively provide feedback to a clus- tering algorithm. The feedback is incorporated in the form of constraints, which the clustering algorithm attempts to satisfy on future iterations. These constraints allow the user to guide the clusterer toward clusterings of the data that the user finds more useful. We demonstrate semi-supervised clustering with a system that learns to cluster news stories from a Reuters data set. 1"
            },
            "slug": "Semi-Supervised-Clustering-with-User-Feedback-Cohn-Caruana",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Clustering with User Feedback"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work presents an approach to clustering based on the observation that \"it is easier to criticize than to construct\" and demonstrates semi-supervised clustering with a system that learns to cluster news stories from a Reuters data set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4332326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c85b7fe70dda0adbbd7630e2a341a904c74fbd2",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "THE standard form of back-propagation learning1 is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other (Fig. la). The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "slug": "Self-organizing-neural-network-that-discovers-in-Becker-Hinton",
            "title": {
                "fragments": [],
                "text": "Self-organizing neural network that discovers surfaces in random-dot stereograms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors' simulations show that when the learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458509"
                        ],
                        "name": "Irina Matveeva",
                        "slug": "Irina-Matveeva",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Matveeva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irina Matveeva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44352521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5c6ea2f23fe8d3e986c4c99e83a90c204538619",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of labeling a partially labeled graph. This setting may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings. It is also of potential practical importance, when the data is abundant, but labeling is expensive or requires human assistance."
            },
            "slug": "Regularization-and-Semi-supervised-Learning-on-Belkin-Matveeva",
            "title": {
                "fragments": [],
                "text": "Regularization and Semi-supervised Learning on Large Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work considers the problem of labeling a partially labeled graph, which may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2917091"
                        ],
                        "name": "John Dunagan",
                        "slug": "John-Dunagan",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dunagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Dunagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 52808490,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "acceeb8d55b2e90f8d72269546df2b1dab6cfc2b",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-outlier-removal-in-high-dimensional-spaces-Dunagan-Vempala",
            "title": {
                "fragments": [],
                "text": "Optimal outlier removal in high-dimensional spaces"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712692"
                        ],
                        "name": "F. Odone",
                        "slug": "F.-Odone",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Odone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Odone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690976"
                        ],
                        "name": "L. Rosasco",
                        "slug": "L.-Rosasco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Rosasco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rosasco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10368334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f666be963a921837340b63867637b39fb28141b",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In this lecture we introduce a class of learning algorithms, collectively called manifold regularization algorithms, suited for predicting/classifying data embedded in high-dimensional spaces. We introduce manifold regularization in the framework of semi-supervised learning, a generalization of the supervised learning setting in which our training set may consist of unlabeled as well as labeled examples."
            },
            "slug": "Manifold-Regularization-Odone-Rosasco",
            "title": {
                "fragments": [],
                "text": "Manifold Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This lecture introduces manifold regularization in the framework of semi-supervised learning, a generalization of the supervised learning setting in which the training set may consist of unlabeled as well as labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6541629"
                        ],
                        "name": "K. Wagstaff",
                        "slug": "K.-Wagstaff",
                        "structuredName": {
                            "firstName": "Kiri",
                            "lastName": "Wagstaff",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wagstaff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144476811"
                        ],
                        "name": "Seth Rogers",
                        "slug": "Seth-Rogers",
                        "structuredName": {
                            "firstName": "Seth",
                            "lastName": "Rogers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seth Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830204"
                        ],
                        "name": "Stefan Schr\u00f6dl",
                        "slug": "Stefan-Schr\u00f6dl",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schr\u00f6dl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Schr\u00f6dl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13491515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bacca0993a3f51649a6bb8dbb093fc8d8481ad4",
            "isKey": false,
            "numCitedBy": 2612,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering is traditionally viewed as an unsupervised method for data analysis. However, in some cases information about the problem domain is available in addition to the data instances themselves. In this paper, we demonstrate how the popular k-means clustering algorithm can be protably modied to make use of this information. In experiments with articial constraints on six data sets, we observe improvements in clustering accuracy. We also apply this method to the real-world problem of automatically detecting road lanes from GPS data and observe dramatic increases in performance."
            },
            "slug": "Constrained-K-means-Clustering-with-Background-Wagstaff-Cardie",
            "title": {
                "fragments": [],
                "text": "Constrained K-means Clustering with Background Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper demonstrates how the popular k-means clustering algorithm can be protably modied to make use of information about the problem domain that is available in addition to the data instances themselves."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807167"
                        ],
                        "name": "R. Kuang",
                        "slug": "R.-Kuang",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Kuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042413"
                        ],
                        "name": "Eugene Ie",
                        "slug": "Eugene-Ie",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Ie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Ie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48884646"
                        ],
                        "name": "Ke Wang",
                        "slug": "Ke-Wang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896417"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058219787"
                        ],
                        "name": "Mahira Siddiqi",
                        "slug": "Mahira-Siddiqi",
                        "structuredName": {
                            "firstName": "Mahira",
                            "lastName": "Siddiqi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mahira Siddiqi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14032548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "572487e1968d1717fb3f1f92c9e50e6a5c6fa87e",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce novel profile-based string kernels for use with support vector machines (SVMs) for the problems of protein classification and remote homology detection. These kernels use probabilistic profiles, such as those produced by the PSI-BLAST algorithm, to define position-dependent mutation neighborhoods along protein sequences for inexact matching of k-length subsequences (\"k-mers\") in the data. By use of an efficient data structure, the kernels are fast to compute once the profiles have been obtained. For example, the time needed to run PSI-BLAST in order to build the profiles is significantly longer than both the kernel computation time and the SVM training time. We present remote homology detection experiments based on the SCOP database where we show that profile-based string kernels used with SVM classifiers strongly outperform all recently presented supervised SVM methods. We also show how we can use the learned SVM classifier to extract \"discriminative sequence motifs\" - short regions of the original profile that contribute almost all the weight of the SVM classification score - and show that these discriminative motifs correspond to meaningful structural features in the protein data. The use of PSI-BLAST profiles can be seen as a semi-supervised learning technique, since PSI-BLAST leverages unlabeled data from a large sequence database to build more informative profiles. Recently presented \"cluster kernels \" give general semi-supervised methods for improving SVM protein classification performance. We show that our profile kernel results are comparable to cluster kernels while providing much better scalability to large datasets."
            },
            "slug": "Profile-based-string-kernels-for-remote-homology-Kuang-Ie",
            "title": {
                "fragments": [],
                "text": "Profile-based string kernels for remote homology detection and motif extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Novel profile-based string kernels for use with support vector machines (SVMs) for the problems of protein classification and remote homology detection are introduced and it is shown that the profile kernel results are comparable to cluster kernels while providing much better scalability to large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709446"
                        ],
                        "name": "R. Rosipal",
                        "slug": "R.-Rosipal",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Rosipal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosipal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13904435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87b7455b4a1ebd6f354cf23233435f81215f25d1",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of studying the properties of con\ufb02gurations of points embeddedin a metric space has long been a central task in pattern recognition, buthas acquired even greater importance after the recent introduction of kernel-based learning methods. These methods work by virtually embedding generaltypes of data in a vector space, and then analyzing the properties of theresulting data cloud. While a number of techniques for this task have beendeveloped in \ufb02elds as diverse as multivariate statistics, neural networks, andsignal processing, many of them show an underlying unity. In this chapterwe describe a large class of pattern analysis methods based on the use ofgeneralized eigenproblems, which reduce to solving the equation Aw ="
            },
            "slug": "Eigenproblems-in-Pattern-Recognition-Bie-Cristianini",
            "title": {
                "fragments": [],
                "text": "Eigenproblems in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter describes a large class of pattern analysis methods based on the use of generalized eigenproblems, which reduce to solving the equation Aw = Aw + 1."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6541629"
                        ],
                        "name": "K. Wagstaff",
                        "slug": "K.-Wagstaff",
                        "structuredName": {
                            "firstName": "Kiri",
                            "lastName": "Wagstaff",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wagstaff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61337545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2db54f5a422fbb0e93489ef77350f457ac610009",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "One goal of research in artificial intelligence is to automate tasks that currently require human expertise; this automation is important because it saves time and brings problems that were previously too large to be solved into the feasible domain. Data analysis, or the ability to identify meaningful patterns and trends in large volumes of data, is an important task that falls into this category. Clustering algorithms are a particularly useful group of data analysis tools. These methods are used, for example, to analyze satellite images of the Earth to identify and categorize different land and foliage types or to analyze telescopic observations to determine what distinct types of astronomical bodies exist and to categorize each observation. However, most existing clustering methods apply general similarity techniques rather than making use of problem-specific information. \nThis dissertation first presents a novel method for converting existing clustering algorithms into constrained clustering algorithms. The resulting methods are able to accept domain-specific information in the form of constraints on the output clusters. At the most general level, each constraint is an instance-level statement about a pair of items in the data set that indicates a preference for being placed into the same cluster, or, alternatively, into different clusters. The constrained clustering algorithms developed and presented in this dissertation enforce each constraint according to the strength of that preference. \nThe second major contribution of this dissertation is the application of constrained clustering algorithms to diverse, significant, challenging real-world problems. We observe that the additional domain knowledge, when combined with the algorithms' ability to enforce that knowledge, produces improvements on a variety of tasks. The problem domains include automated map refinement, natural language processing, and automated data analysis of Hubble Space Telescope observations."
            },
            "slug": "Intelligent-Clustering-with-Instance-Level-Wagstaff-Cardie",
            "title": {
                "fragments": [],
                "text": "Intelligent Clustering with Instance-Level Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that the additional domain knowledge, when combined with the algorithms' ability to enforce that knowledge, produces improvements on a variety of tasks, including automated map refinement, natural language processing, and automated data analysis of Hubble Space Telescope observations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143931058"
                        ],
                        "name": "Li Liao",
                        "slug": "Li-Liao",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1111309,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "0ec64d06ff98230bc6b4c94bbe5212d479dfbbe9",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "One key element in understanding the molecular machinery of the cell is to understand the meaning, or function, of each protein encoded in the genome. A very successful means of inferring the function of a previously unannotated protein is via sequence similarity with one or more proteins whose functions are already known. Currently, one of the most powerful such homology detection methods is the SVM-Fisher method of Jaakkola, Diekhans and Haussler (ISMB 2000). This method combines a generative, profile hidden Markov model (HMM) with a discriminative classification algorithm known as a support vector machine (SVM). The current work presents an alternative method for SVM-based protein classification. The method, SVM-pairwise, uses a pairwise sequence similarity algorithm such as Smith-Waterman in place of the HMM in the SVM-Fisher method. The resulting algorithm, when tested on its ability to recognize previously unseen families from the SCOP database, yields significantly better remote protein homology detection than SVM-Fisher, profile HMMs and PSI-BLAST."
            },
            "slug": "Combining-pairwise-sequence-similarity-and-support-Liao-Noble",
            "title": {
                "fragments": [],
                "text": "Combining pairwise sequence similarity and support vector machines for remote protein homology detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The current work presents an alternative method for SVM-based protein classification that uses a pairwise sequence similarity algorithm such as Smith-Waterman in place of the HMM in the S VM-Fisher method, and yields significantly better remote protein homology detection."
            },
            "venue": {
                "fragments": [],
                "text": "RECOMB '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87339519"
                        ],
                        "name": "M. Kanehisa",
                        "slug": "M.-Kanehisa",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Kanehisa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kanehisa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8629843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d74bfa3505b5350a7c84732f3d51cf9fd66bd8b3",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm to extract features from high-dimensional gene expression profiles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic pathways. Motivated by the intuition that biologically relevant features are likely to exhibit smoothness with respect to the graph topology, the algorithm involves encoding the graph and the set of expression profiles into kernel functions, and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel Hilbert spaces. \n \nFunction prediction experiments for the genes of the yeast S. Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classifier uses the vector of features instead of the original expression profile to predict the functional class of a gene."
            },
            "slug": "Graph-Driven-Feature-Extraction-From-Microarray-and-Vert-Kanehisa",
            "title": {
                "fragments": [],
                "text": "Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This approach is validated by showing a consistent increase in performance when a state-of-the-art classifier uses the vector of features instead of the original expression profile to predict the functional class of a gene."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042413"
                        ],
                        "name": "Eugene Ie",
                        "slug": "Eugene-Ie",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Ie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Ie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8692923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c76bc6de969fe67a4845a7d8a2fecabe6dfce87c",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a novel multi-class classification method based on output codes for the problem of classifying a sequence of amino acids into one of many known protein structural classes, called folds. Our method learns relative weights between one-vs-all classifiers and encodes information about the protein structural hierarchy for multi-class prediction. Our code weighting approach significantly improves on the standard one-vs-all method for the fold recognition problem. In order to compare against widely used methods in protein sequence analysis, we also test nearest neighbor approaches based on the PSI-BLAST algorithm. Our code weight learning algorithm strongly outperforms these PSI-BLAST methods on every structure recognition problem we consider."
            },
            "slug": "Multi-class-protein-fold-recognition-using-adaptive-Ie-Weston",
            "title": {
                "fragments": [],
                "text": "Multi-class protein fold recognition using adaptive codes"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A novel multi-class classification method based on output codes for the problem of classifying a sequence of amino acids into one of many known protein structural classes, called folds, which significantly improves on the standard one-vs-all method for the fold recognition problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158281"
                        ],
                        "name": "S. Fralick",
                        "slug": "S.-Fralick",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Fralick",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11609879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76d4b7fd734f987dc87c9aafdbcc7e1af99ad8a5",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An important problem in pattern recognition or signal detection is the recognition of a pattern that is completely characterized statistically except for a finite set of unknown parameters. If a machine is required to solve such a problem on a number of occasions, it is possible to take advantage of this repetition. One can design a machine that will extract more and more of the pertinent information about these unknown parameters as it recognizes the patterns and readjusts itself to be more selective to them; the machine improves in performance as it gains experience on the problem. This paper presents a model suitable for many such problems and evolves a solution in the form of a machine that \"learns\" to solve the problem without external aid. Such machines are said to \"learn without a teacher.\" The Bayes solution to the model problem requires the computation of the a posteriori probability density of the unknown parameters. A recursive equation for this density is derived. This equation describes the structure of a relatively simple system of finite size that may be realized in a delay-feedback form. The application of the model and the synthesis of a learning system are illustrated by the derivation of a receiver for the detection of signals of unknown amplitude in white Gaussian noise."
            },
            "slug": "Learning-to-recognize-patterns-without-a-teacher-Fralick",
            "title": {
                "fragments": [],
                "text": "Learning to recognize patterns without a teacher"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a model suitable for many problems and evolves a solution in the form of a machine that \"learns\" to solve the problem without external aid, said to \"learn without a teacher\"."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108002908"
                        ],
                        "name": "Yongyue Zhang",
                        "slug": "Yongyue-Zhang",
                        "structuredName": {
                            "firstName": "Yongyue",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongyue Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144431498"
                        ],
                        "name": "M. Brady",
                        "slug": "M.-Brady",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brady"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2162210436"
                        ],
                        "name": "Stephen M. Smith",
                        "slug": "Stephen-M.-Smith",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smith",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen M. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11832552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daf7d7de26196ecb6204f5b936c117e9f3c60ca6",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The finite mixture (FM) model is the most commonly used model for statistical segmentation of brain MR images because of its simple mathematical form and the piecewise constant nature of ideal brain MR images. However, being a histogram-based model, the FM has an intrinsic limitation -- no spatial information is taken into account. This causes the FM model to work only on well-defined images with low noise level. In this paper, we propose a novel hidden Markov random field (HMRF) model, which is a stochastic process generated by a Markov random field whose state sequence cannot be observed directly but which can be observed through observations. Mathematically, it can be shown that the FM model is a degenerate version of the HMRF model. The advantage of the HMRF model derives from the way in which the spatial information is encoded through the mutual influences of neighboring sites. To fit the HMRF model, an expectation-maximization (EM) algorithm is used. We show that by incorporating both the HMRF model and the EM algorithm into an HMRF-EM framework, an accurate and robust segmentation can be achieved, which is demonstrated by comparison experiments with the FM model-based segmentation.\u00a9 (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."
            },
            "slug": "Hidden-Markov-random-field-model-for-segmentation-Zhang-Brady",
            "title": {
                "fragments": [],
                "text": "Hidden Markov random field model for segmentation of brain MR image"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "By incorporating both the HMRF model and the EM algorithm into an H MRF-EM framework, an accurate and robust segmentation can be achieved, which is demonstrated by comparison experiments with the FM model-based segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Medical Imaging: Image Processing"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17429533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31b92f3ad473e6744e965411cdc529473c729c9e",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss approaches to transduction based on graph cut cost functions. More specifically, we focus on the normalized cut, which is the cost function of choice in many clustering applications, notably in image segmentation. Since optimizing the normalized cut cost is an NP-complete problem, much of the research attention so far has gone to relaxing the problem of normalized cut clustering to tractable problems, producing so far a spectral relaxation and a more recently a tighter but computationally much tougher semi-definite programming (SDP) relaxation. In this paper we deliver two main contributions: first, we show how an alternative SDP relaxation yields a much more tractable optimization problem, and we show how scalability and speed can further be increased by making a principled approximation. Second, we show how it is possible to efficiently optimize the normalized cut cost in a transduction setting using our newly proposed approaches. Positive empirical results are reported."
            },
            "slug": "Convex-Transduction-with-the-Normalized-Cut-Bie",
            "title": {
                "fragments": [],
                "text": "Convex Transduction with the Normalized Cut"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper shows how an alternative SDP relaxation yields a much more tractable optimization problem, and shows how scalability and speed can be increased by making a principled approximation to efficiently optimize the normalized cut cost in a transduction setting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58641170,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "403d2ea231a788cc8c2f7b3af6766d47256c70cc",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a tutorial overview of several geometric methods for feature extraction and dimensional reduction. We divide the methods into projective methods and methods that model the manifold on which the data lies. For projective methods, we review projection pursuit, principal component analysis (PCA), kernel PCA, probabilistic PCA, and oriented PCA; and for the manifold methods, we review multidimensional scaling (MDS), landmark MDS, Isomap, locally linear embedding, Laplacian eigenmaps and spectral clustering. The Nystrom method, which links several of the algorithms, is also reviewed. The goal is to provide a self-contained review of the concepts and mathematics underlying these algorithms."
            },
            "slug": "Geometric-Methods-for-Feature-Extraction-and-Burges",
            "title": {
                "fragments": [],
                "text": "Geometric Methods for Feature Extraction and Dimensional Reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The goal is to provide a self-contained review of the concepts and mathematics underlying these algorithms that are used for feature extraction and dimensional reduction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807167"
                        ],
                        "name": "R. Kuang",
                        "slug": "R.-Kuang",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Kuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6351814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78324730bcc942644421f045141be2c70407424a",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce several new families of string kernels designed in particular for use with support vector machines (SVMs) for classification of protein sequence data. These kernels \u2013 restricted gappy kernels, substitution kernels, and wildcard kernels \u2013 are based on feature spaces indexed by k-length subsequences from the string alphabet \u03a3 (or the alphabet augmented by a wildcard character), and hence they are related to the recently presented (k,m)-mismatch kernel and string kernels used in text classification. However, for all kernels we define here, the kernel value K(x,y) can be computed in O(c K (|x| + |y|)) time, where the constant c K depends on the parameters of the kernel but is independent of the size |\u03a3| of the alphabet. Thus the computation of these kernels is linear in the length of the sequences, like the mismatch kernel, but we improve upon the parameter-dependent constant \\(c_K = k^{m+1} |\\Sigma|^m\\) of the mismatch kernel. We compute the kernels efficiently using a recursive function based on a trie data structure and relate our new kernels to the recently described transducer formalism. Finally, we report protein classification experiments on a benchmark SCOP dataset, where we show that our new faster kernels achieve SVM classification performance comparable to the mismatch kernel and the Fisher kernel derived from profile hidden Markov models."
            },
            "slug": "Fast-Kernels-for-Inexact-String-Matching-Leslie-Kuang",
            "title": {
                "fragments": [],
                "text": "Fast Kernels for Inexact String Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Several new families of string kernels designed in particular for use with support vector machines (SVMs) for classification of protein sequence data are introduced, and it is shown that these new faster kernels achieve SVM classification performance comparable to the mismatch kernel and the Fisher kernel derived from profile hidden Markov models."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7339371,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "e4fb6fe40ea0760ad8c3cf044edf6b6145deae44",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this article is to develop a framework for large margin classification in metric spaces. We want to find a generalization of linear decision functions for metric spaces and define a corresponding notion of margin such that the decision function separates the training points with a large margin. It will turn out that using Lipschitz functions as decision functions, the inverse of the Lipschitz constant can be interpreted as the size of a margin. In order to construct a clean mathematical setup we isometrically embed the given metric space into a Banach space and the space of Lipschitz functions into its dual space. Our approach leads to a general large margin algorithm for classification in metric spaces. To analyze this algorithm, we first prove a representer theorem. It states that there exists a solution which can be expressed as linear combination of distances to sets of training points. Then we analyze the Rademacher complexity of some Lipschitz function classes. The generality of the Lipschitz approach can be seen from the fact that several well-known algorithms are special cases of the Lipschitz algorithm, among them the support vector machine, the linear programming machine, and the 1-nearest neighbor classifier."
            },
            "slug": "Distance-Based-Classification-with-Lipschitz-Luxburg-Bousquet",
            "title": {
                "fragments": [],
                "text": "Distance-Based Classification with Lipschitz Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The goal of this article is to find a generalization of linear decision functions for metric spaces and define a corresponding notion of margin such that the decision function separates the training points with a large margin."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144205696"
                        ],
                        "name": "A. Banerjee",
                        "slug": "A.-Banerjee",
                        "structuredName": {
                            "firstName": "Arindam",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2945239"
                        ],
                        "name": "S. Merugu",
                        "slug": "S.-Merugu",
                        "structuredName": {
                            "firstName": "Srujana",
                            "lastName": "Merugu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Merugu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783667"
                        ],
                        "name": "I. Dhillon",
                        "slug": "I.-Dhillon",
                        "structuredName": {
                            "firstName": "Inderjit",
                            "lastName": "Dhillon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dhillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34724702"
                        ],
                        "name": "Joydeep Ghosh",
                        "slug": "Joydeep-Ghosh",
                        "structuredName": {
                            "firstName": "Joydeep",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joydeep Ghosh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8197416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f312852b31a76bc09b59aca6e4017a9382f9cb39",
            "isKey": false,
            "numCitedBy": 1591,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "A wide variety of distortion functions, such as squared Euclidean distance, Mahalanobis distance, Itakura-Saito distance and relative entropy, have been used for clustering. In this paper, we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences. The proposed algorithms unify centroid-based parametric clustering approaches, such as classical kmeans , the Linde-Buzo-Gray (LBG) algorithm and information-theoretic clustering, which arise by special choices of the Bregman divergence. The algorithms maintain the simplicity and scalability of the classical kmeans algorithm, while generalizing the method to a large class of clustering loss functions. This is achieved by first posing the hard clustering problem in terms of minimizing the loss in Bregman information, a quantity motivated by rate distortion theory, and then deriving an iterative algorithm that monotonically decreases this loss. In addition, we show that there is a bijection between regular exponential families and a large class of Bregman divergences, that we call regular Bregman divergences. This result enables the development of an alternative interpretation of an efficient EM scheme for learning mixtures of exponential family distributions, and leads to a simple soft clustering algorithm for regular Bregman divergences. Finally, we discuss the connection between rate distortion theory and Bregman clustering and present an information theoretic analysis of Bregman clustering algorithms in terms of a trade-off between compression and loss in Bregman information."
            },
            "slug": "Clustering-with-Bregman-Divergences-Banerjee-Merugu",
            "title": {
                "fragments": [],
                "text": "Clustering with Bregman Divergences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes and analyzes parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences, and shows that there is a bijection between regular exponential families and a largeclass of BRegman diverGences, that is called regular Breg man divergence."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879453"
                        ],
                        "name": "V. Castelli",
                        "slug": "V.-Castelli",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Castelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Castelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 27
                            }
                        ],
                        "text": "Related is the analysis in [Castelli and Cover, 1996] in which the classconditional densities are known but the class priors are not."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1389637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22834aa74138de7f4da42fb9dfb480cef4e7b177",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We observe a training set Q composed of l labeled samples {(X/sub 1/,/spl theta//sub 1/),...,(X/sub l/, /spl theta//sub l/)} and u unlabeled samples {X/sub 1/',...,X/sub u/'}. The labels /spl theta//sub i/ are independent random variables satisfying Pr{/spl theta//sub i/=1}=/spl eta/, Pr{/spl theta//sub i/=2}=1-/spl eta/. The labeled observations X/sub i/ are independently distributed with conditional density f/sub /spl theta/i/(/spl middot/) given /spl theta//sub i/. Let (X/sub 0/,/spl theta//sub 0/) be a new sample, independently distributed as the samples in the training set. We observe X/sub 0/ and we wish to infer the classification /spl theta//sub 0/. In this paper we first assume that the distributions f/sub 1/(/spl middot/) and f/sub 2/(/spl middot/) are given and that the mixing parameter is unknown. We show that the relative value of labeled and unlabeled samples in reducing the risk of optimal classifiers is the ratio of the Fisher informations they carry about the parameter /spl eta/. We then assume that two densities g/sub 1/(/spl middot/) and g/sub 2/(/spl middot/) are given, but we do not know whether g/sub 1/(/spl middot/)=f/sub 1/(/spl middot/) and g/sub 2/(/spl middot/)=f/sub 2/(/spl middot/) or if the opposite holds, nor do we know /spl eta/. Thus the learning problem consists of both estimating the optimum partition of the observation space and assigning the classifications to the decision regions. Here, we show that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples."
            },
            "slug": "The-relative-value-of-labeled-and-unlabeled-samples-Castelli-Cover",
            "title": {
                "fragments": [],
                "text": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34867643"
                        ],
                        "name": "J. Hanley",
                        "slug": "J.-Hanley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hanley",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140137"
                        ],
                        "name": "B. McNeil",
                        "slug": "B.-McNeil",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "McNeil",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. McNeil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10511727,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bc37489e7173c75be152b2fcea35191c68847ab2",
            "isKey": false,
            "numCitedBy": 18361,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A representation and interpretation of the area under a receiver operating characteristic (ROC) curve obtained by the \"rating\" method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen non-diseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already well-studied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for the approximate magnitude of the sampling variability, i.e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining the size of the sample required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in the accuracy of diagnostic techniques."
            },
            "slug": "The-meaning-and-use-of-the-area-under-a-receiver-Hanley-McNeil",
            "title": {
                "fragments": [],
                "text": "The meaning and use of the area under a receiver operating characteristic (ROC) curve."
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A representation and interpretation of the area under a receiver operating characteristic (ROC) curve obtained by the \"rating\" method, or by mathematical predictions based on patient characteristics, is presented and it is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a random chosen non-diseased subject."
            },
            "venue": {
                "fragments": [],
                "text": "Radiology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783667"
                        ],
                        "name": "I. Dhillon",
                        "slug": "I.-Dhillon",
                        "structuredName": {
                            "firstName": "Inderjit",
                            "lastName": "Dhillon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dhillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1853389"
                        ],
                        "name": "Yuqiang Guan",
                        "slug": "Yuqiang-Guan",
                        "structuredName": {
                            "firstName": "Yuqiang",
                            "lastName": "Guan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuqiang Guan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18476282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec61645930da9ae52f2e8d2c7354d0ba73a27846",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach to clustering cooccurrence data poses it as an optimization problem in information theory which minimizes the resulting loss in mutual information. A divisive clustering algorithm that monotonically reduces this loss function was recently proposed. We show that sparse high-dimensional data presents special challenges which can result in the algorithm getting stuck at poor local minima. We propose two solutions to this problem: (a) a \"prior\" to overcome infinite relative entropy values as in the supervised Naive Bayes algorithm, and (b) local search to escape local minima. Finally, we combine these solutions to get a robust algorithm that is computationally efficient. We present experimental results to show that the proposed method is effective in clustering document collections and outperform previous information-theoretic clustering approaches."
            },
            "slug": "Information-theoretic-clustering-of-sparse-data-Dhillon-Guan",
            "title": {
                "fragments": [],
                "text": "Information theoretic clustering of sparse cooccurrence data"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes two solutions to the clustering cooccurrence data problem: a \"prior\" to overcome infinite relative entropy values as in the supervised Naive Bayes algorithm, and a local search to escape local minima."
            },
            "venue": {
                "fragments": [],
                "text": "Third IEEE International Conference on Data Mining"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17488612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c66378bcf0c63a25e78c24b544764e7ee073cb5",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classification. This is done by modifying the finite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight, SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modified Huber's loss function and the L1 loss function, and also for solving ordinal regression."
            },
            "slug": "A-Modified-Finite-Newton-Method-for-Fast-Solution-Keerthi-DeCoste",
            "title": {
                "fragments": [],
                "text": "A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classification is developed by modifying the finite Newton method of Mangasarian in several ways."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47695762"
                        ],
                        "name": "M. Bilenko",
                        "slug": "M.-Bilenko",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Bilenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bilenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6745692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7f287b62c5f0bed8e0d44604563c57908133d59",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques."
            },
            "slug": "Adaptive-duplicate-detection-using-learnable-string-Bilenko-Mooney",
            "title": {
                "fragments": [],
                "text": "Adaptive duplicate detection using learnable string similarity measures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes to employ learnable text distance functions for each database field, and shows that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144557047"
                        ],
                        "name": "M. Craven",
                        "slug": "M.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2922396"
                        ],
                        "name": "Dan DiPasquo",
                        "slug": "Dan-DiPasquo",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "DiPasquo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan DiPasquo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682522"
                        ],
                        "name": "Se\u00e1n Slattery",
                        "slug": "Se\u00e1n-Slattery",
                        "structuredName": {
                            "firstName": "Se\u00e1n",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se\u00e1n Slattery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5303928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10dfa97d41f17755257f3d653f4808a30fd7481b",
            "isKey": false,
            "numCitedBy": 526,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-to-construct-knowledge-bases-from-the-Wide-Craven-DiPasquo",
            "title": {
                "fragments": [],
                "text": "Learning to construct knowledge bases from the World Wide Web"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767444"
                        ],
                        "name": "Sarah Zelikovitz",
                        "slug": "Sarah-Zelikovitz",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "Zelikovitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sarah Zelikovitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025731"
                        ],
                        "name": "H. Hirsh",
                        "slug": "H.-Hirsh",
                        "structuredName": {
                            "firstName": "Haym",
                            "lastName": "Hirsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hirsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15389528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "488517c595b07acaa8a4f3b33eb7dac5fc185766",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for improving the classification of short text strings using a combination of labeled training data plus a secondary corpus of unlabeled but related longer documents. We show that such unlabeled background knowledge can greatly decrease error rates, particularly if the number of examples or the size of the strings in the training set is small. This is particularly useful when labeling text is a labor-intensive job and when there is a large amount of information available about a particular problem on the World Wide Web. Our approach views the task as one of information integration using WHIRL, a tool that combines database functionalities with techniques from the information-retrieval literature."
            },
            "slug": "Improving-Short-Text-Classification-Using-Unlabeled-Zelikovitz-Hirsh",
            "title": {
                "fragments": [],
                "text": "Improving Short Text Classification Using Unlabeled Background Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The approach views the task as one of information integration using WHIRL, a tool that combines database functionalities with techniques from the information-retrieval literature, and shows that such unlabeled background knowledge can greatly decrease error rates."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120100599"
                        ],
                        "name": "Xin Liu",
                        "slug": "Xin-Liu",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6465383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43015e9790c812bdc25bf0539b2ee4055a1882a7",
            "isKey": false,
            "numCitedBy": 2969,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports a controlled study with statistical signi cance tests on ve text categorization methods: the Support Vector Machines (SVM), a k-Nearest Neighbor (kNN) classi er, a neural network (NNet) approach, the Linear Leastsquares Fit (LLSF) mapping and a Naive Bayes (NB) classier. We focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the training-set category frequency. Our results show that SVM, kNN and LLSF signi cantly outperform NNet and NB when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are su ciently common (over 300 instances)."
            },
            "slug": "A-re-examination-of-text-categorization-methods-Yang-Liu",
            "title": {
                "fragments": [],
                "text": "A re-examination of text categorization methods"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results show that SVM, kNN and LLSF signi cantly outperform NNet and NB when the number of positive training instances per category are small, and that all the methods perform comparably when the categories are over 300 instances."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624289"
                        ],
                        "name": "A. Beygelzimer",
                        "slug": "A.-Beygelzimer",
                        "structuredName": {
                            "firstName": "Alina",
                            "lastName": "Beygelzimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Beygelzimer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2607124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5396e9858d70f09bf42f251621b5f2816a9cb660",
            "isKey": false,
            "numCitedBy": 824,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer & Lee, 2004b). If the point set has a bounded expansion constant c, which is a measure of the intrinsic dimensionality, as defined in (Karger & Ruhl, 2002), the cover tree data structure can be constructed in O (c6n log n) time. Furthermore, nearest neighbor queries require time only logarithmic in n, in particular O (c12 log n) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets."
            },
            "slug": "Cover-trees-for-nearest-neighbor-Beygelzimer-Kakade",
            "title": {
                "fragments": [],
                "text": "Cover trees for nearest neighbor"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points) that shows speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856488"
                        ],
                        "name": "M. Gribskov",
                        "slug": "M.-Gribskov",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gribskov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gribskov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067645198"
                        ],
                        "name": "Nina L. Robinson",
                        "slug": "Nina-L.-Robinson",
                        "structuredName": {
                            "firstName": "Nina",
                            "lastName": "Robinson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nina L. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10174046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73aaa386f93bbdea33184fdceacdbb9d9bd24cf8",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Use-of-Receiver-Operating-Characteristic-(ROC)-to-Gribskov-Robinson",
            "title": {
                "fragments": [],
                "text": "Use of Receiver Operating Characteristic (ROC) Analysis to Evaluate Sequence Matching"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Chem."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1138467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b8fa3496283d4d808fba9ff62d5f024bcf23be",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability."
            },
            "slug": "Learnability-and-the-Vapnik-Chervonenkis-dimension-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18709,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34724293"
                        ],
                        "name": "Yoshihiro Yamanishi",
                        "slug": "Yoshihiro-Yamanishi",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Yamanishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Yamanishi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11678258,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "addc878150c33c34147ae367c75be736e2b7f1ff",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate the problem of graph inference where part of the graph is known as a supervised learning problem, and propose an algorithm to solve it. The method involves the learning of a mapping of the vertices to a Euclidean space where the graph is easy to infer, and can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of metabolic network reconstruction from genomic data."
            },
            "slug": "Supervised-Graph-Inference-Vert-Yamanishi",
            "title": {
                "fragments": [],
                "text": "Supervised Graph Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work forms the problem of graph inference where part of the graph is known as a supervised learning problem, and proposes an algorithm to solve it that can be formulated as an optimization problem in a reproducing kernel Hilbert space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15128952,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47865b56fee61d9c9ff477f7c79f090cc6663d3a",
            "isKey": false,
            "numCitedBy": 4634,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "may 7th, 1986, Professor A. F. M. Smith in the Chair] SUMMARY A continuous two-dimensional region is partitioned into a fine rectangular array of sites or \"pixels\", each pixel having a particular \"colour\" belonging to a prescribed finite set. The true colouring of the region is unknown but, associated with each pixel, there is a possibly multivariate record which conveys imperfect information about its colour according to a known statistical model. The aim is to reconstruct the true scene, with the additional knowledge that pixels close together tend to have the same or similar colours. In this paper, it is assumed that the local characteristics of the true scene can be represented by a nondegenerate Markov random field. Such information can be combined with the records by Bayes' theorem and the true scene can be estimated according to standard criteria. However, the computational burden is enormous and the reconstruction may reflect undesirable largescale properties of the random field. Thus, a simple, iterative method of reconstruction is proposed, which does not depend on these large-scale characteristics. The method is illustrated by computer simulations in which the original scene is not directly related to the assumed random field. Some complications, including parameter estimation, are discussed. Potential applications are mentioned briefly."
            },
            "slug": "On-the-Statistical-Analysis-of-Dirty-Pictures-Besag",
            "title": {
                "fragments": [],
                "text": "On the Statistical Analysis of Dirty Pictures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2833700"
                        ],
                        "name": "S. Kamvar",
                        "slug": "S.-Kamvar",
                        "structuredName": {
                            "firstName": "Sepandar",
                            "lastName": "Kamvar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kamvar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8968350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51556ef5eb3e1e18540177ce0201a047537d673d",
            "isKey": false,
            "numCitedBy": 605,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an improved method for clustering in the presence of very limited supervisory information, given as pairwise instance constraints. By allowing instance-level constraints to have space-level inductive implications, we are able to successfully incorporate constraints for a wide range of data set types. Our method greatly improves on the previously studied constrained k-means algorithm, generally requiring less than half as many constraints to achieve a given accuracy on a range of real-world data, while also being more robust when over-constrained. We additionally discuss an active learning algorithm which increases the value of constraints even further."
            },
            "slug": "From-Instance-level-Constraints-to-Space-Level-the-Klein-Kamvar",
            "title": {
                "fragments": [],
                "text": "From Instance-level Constraints to Space-Level Constraints: Making the Most of Prior Knowledge in Data Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By allowing instance-level constraints to have space-level inductive implications, this work is able to successfully incorporate constraints for a wide range of data set types, and greatly improves on the previously studied constrained k-means algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144632403"
                        ],
                        "name": "R. Kannan",
                        "slug": "R.-Kannan",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kannan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kannan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16506347,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8d3d973e059d116cbb6a94090f5ed23b94d31b5c",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a polynomial-time algorithm to learn an intersection of a constant number of halfspaces in n dimensions, over the uniform distribution on an n-dimensional ball. The algorithm we present in fact can learn an intersection of an arbitrary (polynomial) number of halfspaces over this distribution, if the subspace spanned by the normal vectors to the bounding hyperplanes has constant dimension. This generalizes previous results for this distribution, in particular a result of E.B. Baum (1990) who showed how to learn an intersection of 2 halfspaces defined by hyperplanes that pass through the origin (his results in fact held for a variety of symmetric distributions). Our algorithm uses estimates of second moments to find vectors in a low-dimensional \"relevant subspace\". We believe that the algorithmic techniques studied here may be useful in other geometric learning applications.<<ETX>>"
            },
            "slug": "Learning-an-intersection-of-k-halfspaces-over-a-Blum-Kannan",
            "title": {
                "fragments": [],
                "text": "Learning an intersection of k halfspaces over a uniform distribution"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The algorithm can learn an intersection of an arbitrary (polynomial) number of halfspaces over this distribution, if the subspace spanned by the normal vectors to the bounding hyperplanes has constant dimension."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107389564"
                        ],
                        "name": "T. Smith",
                        "slug": "T.-Smith",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2398669"
                        ],
                        "name": "M. Waterman",
                        "slug": "M.-Waterman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Waterman",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Waterman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20031248,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "40c5441aad96b366996e6af163ca9473a19bb9ad",
            "isKey": false,
            "numCitedBy": 10037,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Identification-of-common-molecular-subsequences.-Smith-Waterman",
            "title": {
                "fragments": [],
                "text": "Identification of common molecular subsequences."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39581622"
                        ],
                        "name": "James A. Cuff",
                        "slug": "James-A.-Cuff",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cuff",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James A. Cuff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764306"
                        ],
                        "name": "G. Barton",
                        "slug": "G.-Barton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Barton",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6016474,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "a8946a1a1b666edb4fc1b1b4d69a59b2c80015c5",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": "A new dataset of 396 protein domains is developed and used to evaluate the performance of the protein secondary structure prediction algorithms DSC, PHD, NNSSP, and PREDATOR. The maximum theoretical Q3 accuracy for combination of these methods is shown to be 78%. A simple consensus prediction on the 396 domains, with automatically generated multiple sequence alignments gives an average Q3 prediction accuracy of 72.9%. This is a 1% improvement over PHD, which was the best single method evaluated. Segment Overlap Accuracy (SOV) is 75.4% for the consensus method on the 396\u2010protein set. The secondary structure definition method DSSP defines 8 states, but these are reduced by most authors to 3 for prediction. Application of the different published 8\u2010 to 3\u2010state reduction methods shows variation of over 3% on apparent prediction accuracy. This suggests that care should be taken to compare methods by the same reduction method. Two new sequence datasets (CB513 and CB251) are derived which are suitable for cross\u2010validation of secondary structure prediction methods without artifacts due to internal homology. A fully automatic World Wide Web service that predicts protein secondary structure by a combination of methods is available via http://barton.ebi.ac.uk/. Proteins 1999;34:508\u2013519. \u00a9 1999 Wiley\u2010Liss, Inc."
            },
            "slug": "Evaluation-and-improvement-of-multiple-sequence-for-Cuff-Barton",
            "title": {
                "fragments": [],
                "text": "Evaluation and improvement of multiple sequence methods for protein secondary structure prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Two new sequence datasets are derived which are suitable for cross\u2010validation of secondary structure prediction methods without artifacts due to internal homology and application of the different published 8\u2010 to 3\u2010state reduction methods shows variation of over 3% on apparent prediction accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proteins"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15074762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8381c212e081c319e17c65a2138d46ba5f91cd90",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an algorithm for nonlinear dimensionality reduction based on semidefinite programming and kernel matrix factorization. The algorithm learns a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. In earlier work, the kernel matrix was learned by maximizing the variance in feature space while preserving the distances and angles between nearest neighbors. In this paper, adapting recent ideas from semi-supervised learning on graphs, we show that the full kernel matrix can be very well approximated by a product of smaller matrices. Representing the kernel matrix in this way, we can reformulate the semidefinite program in terms of a much smaller submatrix of inner products between randomly chosen landmarks. The new framework leads to order-of-magnitude reductions in computation time and makes it possible to study much larger problems in manifold learning."
            },
            "slug": "Nonlinear-Dimensionality-Reduction-by-Semidefinite-Weinberger-Packer",
            "title": {
                "fragments": [],
                "text": "Nonlinear Dimensionality Reduction by Semidefinite Programming and Kernel Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the full kernel matrix can be very well approximated by a product of smaller matrices, which leads to order-of-magnitude reductions in computation time and makes it possible to study much larger problems in manifold learning."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34628173"
                        ],
                        "name": "K. Tsuda",
                        "slug": "K.-Tsuda",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Tsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tsuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9365291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f82ebf260149d5a30050d0ecc7b084786d7ebdf",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nThe diffusion kernel is a general method for computing pairwise distances among all nodes in a graph, based on the sum of weighted paths between each pair of nodes. This technique has been used successfully, in conjunction with kernel-based learning methods, to draw inferences from several types of biological networks.\n\n\nRESULTS\nWe show that computing the diffusion kernel is equivalent to maximizing the von Neumann entropy, subject to a global constraint on the sum of the Euclidean distances between nodes. This global constraint allows for high variance in the pairwise distances. Accordingly, we propose an alternative, locally constrained diffusion kernel, and we demonstrate that the resulting kernel allows for more accurate support vector machine prediction of protein functional classifications from metabolic and protein-protein interaction networks.\n\n\nAVAILABILITY\nSupplementary results and data are available at noble.gs.washington.edu/proj/maxent"
            },
            "slug": "Learning-kernels-from-biological-networks-by-Tsuda-Noble",
            "title": {
                "fragments": [],
                "text": "Learning kernels from biological networks by maximizing entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that computing the diffusion kernel is equivalent to maximizing the von Neumann entropy, subject to a global constraint on the sum of the Euclidean distances between nodes, and that the resulting kernel allows for more accurate support vector machine prediction of protein functional classifications from metabolic and protein-protein interaction networks."
            },
            "venue": {
                "fragments": [],
                "text": "ISMB/ECCB"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2418588"
                        ],
                        "name": "Sajama",
                        "slug": "Sajama",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sajama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sajama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691155"
                        ],
                        "name": "A. Orlitsky",
                        "slug": "A.-Orlitsky",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Orlitsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Orlitsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10731077,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "485fade71c20ece3f99d68a1321e73226c5edd4b",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Density-based distance metrics have applications in semi-supervised learning, nonlinear interpolation and clustering. We consider density-based metrics induced by Riemannian manifold structures and estimate them using kernel density estimators for the underlying data distribution. We lower bound the rate of convergence of these plug-in path-length estimates and hence of the metric, as the sample size increases. We present an upper bound on the rate of convergence of all estimators of the metric. We also show that the metric can be consistently computed using the shortest path algorithm on a suitably constructed graph on the data samples and lower bound the convergence rate of the computation error. We present experiments illustrating the use of the metrics for semi-supervised classification and non-linear interpolation."
            },
            "slug": "Estimating-and-computing-density-based-distance-Sajama-Orlitsky",
            "title": {
                "fragments": [],
                "text": "Estimating and computing density based distance metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the metric can be consistently computed using the shortest path algorithm on a suitably constructed graph on the data samples and the convergence rate of the computation error is lower bound."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7311285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04ce064505b1635583fa0d9cc07cac7e9ea993cc",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size."
            },
            "slug": "A-comparison-of-event-models-for-naive-bayes-text-McCallum-Nigam",
            "title": {
                "fragments": [],
                "text": "A comparison of event models for naive bayes text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi -variateBernoulli model at any vocabulary size."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1998"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34806014,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6ef350b093ece4621c20a2912f6ca5a3afa30d56",
            "isKey": false,
            "numCitedBy": 536,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A random vector x arises from one of two multivariate normal distributions differing in mean but not covariance. A training set x 1, x 2, \u00b7\u00b7\u00b7 x n of previous cases, along with their correct assignments, is known. These can be used to estimate Fisher's discriminant by maximum likelihood and then to assign x on the basis of the estimated discriminant, a method known as the normal discrimination procedure. Logistic regression does the same thing but with the estimation of Fisher's disriminant done conditionally on the observed values of x 1 x 2, \u00b7\u00b7\u00b7, x n . This article computes the asymptotic relative efficiency of the two procedures. Typically, logistic regression is shown to be between one half and two thirds as effective as normal discrimination for statistically interesting values of the parameters."
            },
            "slug": "The-Efficiency-of-Logistic-Regression-Compared-to-Efron",
            "title": {
                "fragments": [],
                "text": "The Efficiency of Logistic Regression Compared to Normal Discriminant Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48897539"
                        ],
                        "name": "P. J. Huber",
                        "slug": "P.-J.-Huber",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Huber",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. J. Huber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123642812,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "64b4ddcf066597a200423c55652f11ce89780063",
            "isKey": false,
            "numCitedBy": 5267,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proves consistency and asymptotic normality of maximum likelihood (ML) estimators under weaker conditions than usual. In particular, (i) it is not assumed that the true distribution underlying the observations belongs to the parametric family defining the ML estimator, and (ii) the regularity conditions do not involve the second and higher derivatives of the likelihood function. The need for theorems on asymptotic normality of ML estimators subject to (i) and (ii) becomes apparent in connection with robust estimation problems; for instance, if one tries to extend the author's results on robust estimation of a location parameter [4] to multivariate and other more general estimation problems. Wald's classical consistency proof [6] satisfies (ii) and can easily be modified to show that the ML estimator is consistent also in case (i), that is, it converges to the 0o characterized by the property E(logf(x, 0) log f(x, O0)) < 0 for 0 . Oo, where the expectation is taken with respect to the true underlying distribution. Asymptotic normality is more troublesome. Daniels [1] proved asymptotic normality subject to (ii), but unfortunately he overlooked that a crucial step in his proof (the use of the central limit theorem in (4.4)) is incorrect without condition (2.2) of Linnik [5]; this condition seems to be too restrictive for many purposes. In section 4 we shall prove asymptotic normality, assuming that the ML estimator is consistent. For the sake of completeness, sections 2 and 3 contain, therefore, two different sets of sufficient conditions for consistency. Otherwise, these sections are independent of each other. Section 5 presents two examples."
            },
            "slug": "The-behavior-of-maximum-likelihood-estimates-under-Huber",
            "title": {
                "fragments": [],
                "text": "The behavior of maximum likelihood estimates under nonstandard conditions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14278422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42ac8849c5f1090d6341e41fe143d8eddcb8ab50",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the structure of model selection problems via the bias/variance decomposition. In particular, we characterize the essential aspects of a model selection task by the bias and variance profiles it generates over the sequence of hypothesis classes. With this view, we develop a new understanding of complexity-penalization methods: First, the penalty terms can be interpreted as postulating a particular profile for the variances as a function of model complexity\u2014if the postulated and true profiles do not match, then systematic under-fitting or over-fitting results, depending on whether the penalty terms are too large or too small. Second, we observe that it is generally best to penalize according to the true variances of the task, and therefore no fixed penalization strategy is optimal across all problems. We then use this characterization to introduce the notion of easy versus hard model selection problems. Here we show that if the variance profile grows too rapidly in relation to the biases, then standard model selection techniques become prone to significant errors. This can happen, for example, in regression problems where the independent variables are drawn from wide-tailed distributions. To counter this, we discuss a new model selection strategy that dramatically outperforms standard complexity-penalization and hold-out methods on these hard tasks."
            },
            "slug": "Characterizing-the-generalization-performance-of-Schuurmans-Ungar",
            "title": {
                "fragments": [],
                "text": "Characterizing the generalization performance of model selection strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if the variance profile grows too rapidly in relation to the biases, then standard model selection techniques become prone to significant errors and a new model selection strategy is discussed that dramatically outperforms standard complexity-penalization and hold-out methods on these hard tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094435396"
                        ],
                        "name": "Paul Komarek",
                        "slug": "Paul-Komarek",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Komarek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Komarek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10917723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b6f2975f33db74f0ad7954053a80ce95d42eae6",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Although popular and extremely well established in mainstream statistical data analysis, logistic regression is strangely absent in the field of data mining. There are two possible explanations of this phenomenon. First, there might be an assumption that any tool which can only produce linear classification boundaries is likely to be trumped by more modern nonlinear tools. Second, there is a legitimate fear that logistic regression cannot practically scale up to the massive dataset sizes to which modern data mining tools are applied. This paper consists of an empirical examination of the first assumption, and surveys, implements and compares techniques by which logistic regression can be scaled to data with millions of attributes and records. Our results, on a large life sciences dataset, indicate that logistic regression can perform surprisingly well, both statistically and computationally, when compared with an array of more recent classification algorithms."
            },
            "slug": "Fast-Robust-Logistic-Regression-for-Large-Sparse-Komarek-Moore",
            "title": {
                "fragments": [],
                "text": "Fast Robust Logistic Regression for Large Sparse Datasets with Binary Outputs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The results, on a large life sciences dataset, indicate that logistic regression can perform surprisingly well, both statistically and computationally, when compared with an array of more recent classification algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11039454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95ac3a07b0a803b738aa9d630ef9768b65c6ba69",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we follow the same general ideology as in [Gammerman et al., 1998], and describe a new transductive learning algorithm using Support Vector Machines. The algorithm presented provides confidence values for its predicted classifications of new examples. We also obtain a measure of \"credibility\" which serves as an indicator of the reliability of the data upon which we make our prediction. Experiments compare the new algorithm to a standard Support Vector Machine and other transductive methods which use Support Vector Machines, such as Vapnik's margin transduction. Empirical results show that the new algorithm not only produces confidence and credibility measures, but is comparable to, and sometimes exceeds the performance of the other algorithms."
            },
            "slug": "Transduction-with-Confidence-and-Credibility-Saunders-Gammerman",
            "title": {
                "fragments": [],
                "text": "Transduction with Confidence and Credibility"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new transductive learning algorithm using Support Vector Machines is described, which provides confidence values for its predicted classifications of new examples and a measure of \"credibility\" which serves as an indicator of the reliability of the data upon which it makes its prediction."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122865377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99cd988b104202887ad9657b8a61baa7ff0581c1",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We equate nonlinear dimensionality reduction (NLDR) to graph embedding with side information about the vertices, and derive a solution to either problem in the form of a kernel-based mixture of affine maps from the ambient space to the target space. Unlike most spectral NLDR methods, the central eigenproblem can be made relatively small, and the result is a continuous mapping defined over the entire space, not just the datapoints. A demonstration is made to visualizing the distribution of word usages (as a proxy to word meanings) in a sample of the machine learning literature."
            },
            "slug": "Continuous-nonlinear-dimensionality-reduction-by-Brand",
            "title": {
                "fragments": [],
                "text": "Continuous nonlinear dimensionality reduction by kernel Eigenmaps"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This work equate nonlinear dimensionality reduction (NLDR) to graph embedding with side information about the vertices, and derive a solution to either problem in the form of a kernel-based mixture of affine maps from the ambient space to the target space."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 77139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "700666f0c59a4fedc8b08294424c47cb99a8e2ff",
            "isKey": false,
            "numCitedBy": 3124,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically."
            },
            "slug": "On-the-Optimality-of-the-Simple-Bayesian-Classifier-Domingos-Pazzani",
            "title": {
                "fragments": [],
                "text": "On the Optimality of the Simple Bayesian Classifier under Zero-One Loss"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Bayesian classifier is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption, and will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1934343"
                        ],
                        "name": "David Hecherman",
                        "slug": "David-Hecherman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hecherman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Hecherman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764547"
                        ],
                        "name": "M. Sahami",
                        "slug": "M.-Sahami",
                        "structuredName": {
                            "firstName": "Mehran",
                            "lastName": "Sahami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 617436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02adea3455cd7b09e1dac9ddf2637a1e7ae84005",
            "isKey": false,
            "numCitedBy": 1291,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "1. ABSTRACT Text categorization \u2013 the assignment of natural language texts to one or more predefined categories based on their content \u2013 is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1"
            },
            "slug": "Inductive-learning-algorithms-and-representations-Dumais-Platt",
            "title": {
                "fragments": [],
                "text": "Inductive learning algorithms and representations for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A comparison of the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320577"
                        ],
                        "name": "M. Kockelkorn",
                        "slug": "M.-Kockelkorn",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kockelkorn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kockelkorn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2282341"
                        ],
                        "name": "Andreas L\u00fcneburg",
                        "slug": "Andreas-L\u00fcneburg",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "L\u00fcneburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas L\u00fcneburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751348"
                        ],
                        "name": "T. Scheffer",
                        "slug": "T.-Scheffer",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Scheffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Scheffer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15289156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54f10ea30d457fd0c972e8841685dac6cc0d94a1",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Many organizations and companies have to answer large amounts of emails. Often, most of these emails contain variations of relatively few frequently asked questions. We address the problem of predicting which of several frequently used answers a user will choose to respond to an email. Our approach effectively utilizes the data that is typically available in this setting: inbound and outbound emails stored on a server. We take into account that there are no explicit links between inbound and corresponding outbound mails on the server. We map the problem to a semi-supervised classification problem that can be addressed by algorithms such as the transductive support vector machine and multi-view learning. We evaluate our approach using emails sent to a corporate customer service department."
            },
            "slug": "Using-Transduction-and-Multi-view-Learning-to-Kockelkorn-L\u00fcneburg",
            "title": {
                "fragments": [],
                "text": "Using Transduction and Multi-view Learning to Answer Emails"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work addresses the problem of predicting which of several frequently used answers a user will choose to respond to an email, and effectively utilizes the data that is typically available in this setting: inbound and outbound emails stored on a server."
            },
            "venue": {
                "fragments": [],
                "text": "PKDD"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14879317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88816ae492956f3004daa41357166f1181c0c1bf",
            "isKey": false,
            "numCitedBy": 7046,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed."
            },
            "slug": "Laplacian-Eigenmaps-for-Dimensionality-Reduction-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a geometrically motivated algorithm for representing the high-dimensional data that provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015507"
                        ],
                        "name": "Jean-Yves Audibert",
                        "slug": "Jean-Yves-Audibert",
                        "structuredName": {
                            "firstName": "Jean-Yves",
                            "lastName": "Audibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Yves Audibert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33205829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "7d3215f5bde15802fe8fa42e3b7f3faa7eedd983",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method to estimate the intrinsic dimensionality of a submanifold M in Rd from random samples. The method is based on the convergence rates of a certain U-statistic on the manifold. We solve at least partially the question of the choice of the scale of the data. Moreover the proposed method is easy to implement, can handle large data sets and performs very well even for small sample sizes. We compare the proposed method to two standard estimators on several artificial as well as real data sets."
            },
            "slug": "Intrinsic-dimensionality-estimation-of-submanifolds-Hein-Audibert",
            "title": {
                "fragments": [],
                "text": "Intrinsic dimensionality estimation of submanifolds in Rd"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The proposed method to estimate the intrinsic dimensionality of a submanifold M in Rd from random samples is based on the convergence rates of a certain U-statistic on the manifold and is compared to two standard estimators on several artificial as well as real data sets."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102937032"
                        ],
                        "name": "\u5f20\u632f\u8dc3",
                        "slug": "\u5f20\u632f\u8dc3",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u5f20\u632f\u8dc3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5f20\u632f\u8dc3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102443728"
                        ],
                        "name": "\u67e5\u5b8f\u8fdc",
                        "slug": "\u67e5\u5b8f\u8fdc",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u67e5\u5b8f\u8fdc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u67e5\u5b8f\u8fdc"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117752584,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4662b8550a67f3f6ca7d4efe8c4c21acfa2f53e0",
            "isKey": false,
            "numCitedBy": 687,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized da-ta points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approxi-mation for the tangent space at each point, and those tangent spaces are then aligned to give the global coordinates of the data pointswith respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can bequite small in some cases. We illustrate our algorithm using curves and surfaces both in 2D/3D Euclidean spaces and higher dimension-al Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements."
            },
            "slug": "Principal-Manifolds-and-Nonlinear-Dimensionality-\u5f20\u632f\u8dc3-\u67e5\u5b8f\u8fdc",
            "title": {
                "fragments": [],
                "text": "Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new algorithm for manifold learning and nonlinear dimensionality reduction is presented based on a set of unorganized da-ta points sampled with noise from a parameterized manifold, and the local geometry of the manifold is learned by constructing an approxi-mation for the tangent space at each point."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7668712"
                        ],
                        "name": "Fabio Gagliardi Cozman",
                        "slug": "Fabio-Gagliardi-Cozman",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Cozman",
                            "middleNames": [
                                "Gagliardi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Gagliardi Cozman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49641404"
                        ],
                        "name": "I. Cohen",
                        "slug": "I.-Cohen",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1336592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22d96cb7a3dbf56bcabadacd727ec07bf2ed26df",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the effect of unlabeled training data in generative classifiers. We are interested in classification performance when unlabeled data are added to an existing pool of labeled data. We show that unlabeled data can degrade the performance of a classifier when there are discrepancies between modeling assumptions used to build the classifier and the actual model that generates the data; our analysis of this situation explains several seemingly disparate results in the literature."
            },
            "slug": "Unlabeled-Data-Can-Degrade-Classification-of-Cozman-Cohen",
            "title": {
                "fragments": [],
                "text": "Unlabeled Data Can Degrade Classification Performance of Generative Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that unlabeled data can degrade the performance of a classifier when there are discrepancies between modeling assumptions used to build the classifier and the actual model that generates the data."
            },
            "venue": {
                "fragments": [],
                "text": "FLAIRS Conference"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10587410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "138b6767d572e84147da34dd38573b0eff5171b7",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: \"We show that the Gaussian random fields and harmonic energy minimizing function framework for semi-supervised learning can be viewed in terms of Gaussian processes, with covariance matrices derived from the graph Laplacian. We derive hyperparameter learning with evidence maximization, and give an empirical study of various ways to parameterize the graph weights.\""
            },
            "slug": "Semi-supervised-learning-:-from-Gaussian-fields-to-Zhu-Lafferty",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning : from Gaussian fields to Gaussian processes"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "It is shown that the Gaussian random fields and harmonic energy minimizing function framework for semi-supervised learning can be viewed in terms of Gaussian processes, with covariance matrices derived from the graph Laplacian, to derive hyperparameter learning with evidence maximization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16960323"
                        ],
                        "name": "H. Scudder",
                        "slug": "H.-Scudder",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Scudder",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Scudder"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30807376,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17a620afc87f5266e3fd8b3f308c883cc2c2b7c7",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple taught pattern-recognition machine for detecting an unknown, fixed, randomly occurring pattern is derived using a Bayes' approach, and its probability of error is analyzed. It is shown that with probability one, the machine converges to the optimal detector (a matched filter) for the unknown pattern, that the asymptotic decision function statistics are Gaussian, and that for an important class of problems, the central-limit theorem can be invoked to calculate the approximate probability of error at any stage of convergence. An untaught adaptive pattern-recognition machine may be made from the taught machine by using its own output instead of a teacher, and the asymptotic probability of error of this device is derived. It is shown that it does not converge to a matched filter for the unknown pattern, but that in any practical case it performs almost as well. Finally, the results of an experimental simulation of both machines are presented as curves of the relative frequency of error vs. time, and are compared with the values calculated by theory."
            },
            "slug": "Probability-of-error-of-some-adaptive-machines-Scudder",
            "title": {
                "fragments": [],
                "text": "Probability of error of some adaptive pattern-recognition machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that with probability one, the machine converges to the optimal detector for the unknown pattern, that the asymptotic decision function statistics are Gaussian, and that the central-limit theorem can be invoked to calculate the approximate probability of error at any stage of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3286457"
                        ],
                        "name": "P. Derbeko",
                        "slug": "P.-Derbeko",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Derbeko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Derbeko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387872181"
                        ],
                        "name": "Ran El-Yaniv",
                        "slug": "Ran-El-Yaniv",
                        "structuredName": {
                            "firstName": "Ran",
                            "lastName": "El-Yaniv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ran El-Yaniv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766683"
                        ],
                        "name": "R. Meir",
                        "slug": "R.-Meir",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Meir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Meir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6202796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d462806cfb9fd1129c13eeabad2c5e379a42176",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with transductive learning. Although transduction appears to be an easier task than induction, there have not been many provably useful algorithms and bounds for transduction. We present explicit error bounds for transduction and derive a general technique for devising bounds within this setting. The technique is applied to derive error bounds for compression schemes such as (transductive) SVMs and for transduction algorithms based on clustering."
            },
            "slug": "Error-Bounds-for-Transductive-Learning-via-and-Derbeko-El-Yaniv",
            "title": {
                "fragments": [],
                "text": "Error Bounds for Transductive Learning via Compression and Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The technique is applied to derive error bounds for compression schemes such as (transductive) SVMs and for transduction algorithms based on clustering, which derive a general technique for devising bounds within this setting."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739260"
                        ],
                        "name": "A. Strehl",
                        "slug": "A.-Strehl",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Strehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Strehl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34724702"
                        ],
                        "name": "Joydeep Ghosh",
                        "slug": "Joydeep-Ghosh",
                        "structuredName": {
                            "firstName": "Joydeep",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joydeep Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2769532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e20ad680666a4e02a1b0a99d0b6c974f7390bc64",
            "isKey": false,
            "numCitedBy": 812,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering of web documents enables (semi-)automated categorization, and facilitates certain types of search. Any clustering method has to embed the documents in a suitable similarity space. While several clustering methods and the associated similarity measures have been proposed in the past, there is no systematic comparative study of the impact of similarity metrics on cluster quality, possibly because the popular cost criteria do not readily translate across qualitatively different metrics. We observe that in domains such as YAHOO that provide a categorization by human experts, a useful criteria for comparisons across similarity metrics is indeed available. We then compare four popular similarity measures (Euclidean, cosine, Pearson correlation and extended Jaccard) in conjunction with several clustering techniques (random, self-organizing feature map, hyper-graph partitioning, generalized kmeans, weighted graph partitioning), on high dimensionai sparse data representing web documents. Performance is measured against a human-imposed classification into news categories and industry categories. We conduct a number of experiments and use t-tests to assure statistical significance of results. Cosine and extended Jaccard similarities emerge as the best measures to capture human categorization behavior, while Euclidean performs poorest. Also, weighted graph partitioning approaches are clearly superior to all others."
            },
            "slug": "Impact-of-Similarity-Measures-on-Web-page-Strehl-Ghosh",
            "title": {
                "fragments": [],
                "text": "Impact of Similarity Measures on Web-page Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Comparing four popular similarity measures in conjunction with several clustering techniques, cosine and extended Jaccard similarities emerge as the best measures to capture human categorization behavior, while Euclidean performs poorest."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49967661"
                        ],
                        "name": "B. Rost",
                        "slug": "B.-Rost",
                        "structuredName": {
                            "firstName": "Burkhard",
                            "lastName": "Rost",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rost"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144882390"
                        ],
                        "name": "C. Sander",
                        "slug": "C.-Sander",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Sander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35392780,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6e5950cea430c2e12624083e2c33bd59b2042a3f",
            "isKey": false,
            "numCitedBy": 2961,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have trained a two-layered feed-forward neural network on a non-redundant data base of 130 protein chains to predict the secondary structure of water-soluble proteins. A new key aspect is the use of evolutionary information in the form of multiple sequence alignments that are used as input in place of single sequences. The inclusion of protein family information in this form increases the prediction accuracy by six to eight percentage points. A combination of three levels of networks results in an overall three-state accuracy of 70.8% for globular proteins (sustained performance). If four membrane protein chains are included in the evaluation, the overall accuracy drops to 70.2%. The prediction is well balanced between alpha-helix, beta-strand and loop: 65% of the observed strand residues are predicted correctly. The accuracy in predicting the content of three secondary structure types is comparable to that of circular dichroism spectroscopy. The performance accuracy is verified by a sevenfold cross-validation test, and an additional test on 26 recently solved proteins. Of particular practical importance is the definition of a position-specific reliability index. For half of the residues predicted with a high level of reliability the overall accuracy increases to better than 82%. A further strength of the method is the more realistic prediction of segment length. The protein family prediction method is available for testing by academic researchers via an electronic mail server."
            },
            "slug": "Prediction-of-protein-secondary-structure-at-better-Rost-Sander",
            "title": {
                "fragments": [],
                "text": "Prediction of protein secondary structure at better than 70% accuracy."
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A two-layered feed-forward neural network is trained on a non-redundant data base to predict the secondary structure of water-soluble proteins with a new key aspect is the use of evolutionary information in the form of multiple sequence alignments that are used as input in place of single sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770467"
                        ],
                        "name": "A. Agrawala",
                        "slug": "A.-Agrawala",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Agrawala",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2140,
                                "start": 34
                            }
                        ],
                        "text": ", Scudder [1965], Fralick [1967], Agrawala [1970]). An unsatisfactory aspect of self-learning is that the effect of the wrapper depends on the supervised method used inside it. If self-learning is used with empirical risk minimization and 1-0-loss, the unlabeled data will have no effect on the solution at all. If instead a margin maximizing method is used, as a result the decision boundary is pushed away from the unlabeled points (cf. chapter 6). In other cases it seems to be unclear what the self-learning is really doing, and which assumption it corresponds to. Closely related to semi-supervised learning is the concept of transductive inference, or transductive inference transduction, pioneered by Vapnik [Vapnik and Chervonenkis, 1974, Vapnik and Sterin, 1977]. In contrast to inductive inference, no general decision rule is inferred, but only the labels of the unlabeled (or test) points are predicted. An early instance of transduction (albeit without explicitly considering it as a concept) was already proposed by Hartley and Rao [1968]. They suggested a combinatorial optimization on the labels of the test points in order to maximize the likelihood of their model. It seems that semi-supervised learning really took off in the 1970s when the problem of estimating the Fisher linear discriminant rule with unlabeled data was considered [Hosmer mixture of Gaussians Jr., 1973, McLachlan, 1977, O\u2019Neill, 1978, McLachlan and Ganesalingam, 1982]. More precisely, the setting was in the case where each class-conditional density is Gaussian with equal covariance matrix. The likelihood of the model is then maximized using the labeled and unlabeled data with the help of an iterative algorithm such as the expectationmaximization (EM) algorithm [Dempster et al., 1977]. Instead of a mixture of Gaussians, the use of a mixture of multinomial distributions estimated with labeled and unlabeled data has been investigated in [Cooper and Freeman, 1970]. Later, this one component per class setting has been extended to several components per class [Shahshahani and Landgrebe, 1994] and further generalized by Miller and Uyar [1997]. Learning rates in a probably approximately correct (PAC) framework [Valiant, 1984] have been derived for the semi-supervised learning of a mixture of two Gaussians by theoretical analysis Ratsaby and Venkatesh [1995]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1053,
                                "start": 34
                            }
                        ],
                        "text": ", Scudder [1965], Fralick [1967], Agrawala [1970]). An unsatisfactory aspect of self-learning is that the effect of the wrapper depends on the supervised method used inside it. If self-learning is used with empirical risk minimization and 1-0-loss, the unlabeled data will have no effect on the solution at all. If instead a margin maximizing method is used, as a result the decision boundary is pushed away from the unlabeled points (cf. chapter 6). In other cases it seems to be unclear what the self-learning is really doing, and which assumption it corresponds to. Closely related to semi-supervised learning is the concept of transductive inference, or transductive inference transduction, pioneered by Vapnik [Vapnik and Chervonenkis, 1974, Vapnik and Sterin, 1977]. In contrast to inductive inference, no general decision rule is inferred, but only the labels of the unlabeled (or test) points are predicted. An early instance of transduction (albeit without explicitly considering it as a concept) was already proposed by Hartley and Rao [1968]. They suggested a combinatorial optimization on the labels of the test points in order to maximize the likelihood of their model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2358,
                                "start": 34
                            }
                        ],
                        "text": ", Scudder [1965], Fralick [1967], Agrawala [1970]). An unsatisfactory aspect of self-learning is that the effect of the wrapper depends on the supervised method used inside it. If self-learning is used with empirical risk minimization and 1-0-loss, the unlabeled data will have no effect on the solution at all. If instead a margin maximizing method is used, as a result the decision boundary is pushed away from the unlabeled points (cf. chapter 6). In other cases it seems to be unclear what the self-learning is really doing, and which assumption it corresponds to. Closely related to semi-supervised learning is the concept of transductive inference, or transductive inference transduction, pioneered by Vapnik [Vapnik and Chervonenkis, 1974, Vapnik and Sterin, 1977]. In contrast to inductive inference, no general decision rule is inferred, but only the labels of the unlabeled (or test) points are predicted. An early instance of transduction (albeit without explicitly considering it as a concept) was already proposed by Hartley and Rao [1968]. They suggested a combinatorial optimization on the labels of the test points in order to maximize the likelihood of their model. It seems that semi-supervised learning really took off in the 1970s when the problem of estimating the Fisher linear discriminant rule with unlabeled data was considered [Hosmer mixture of Gaussians Jr., 1973, McLachlan, 1977, O\u2019Neill, 1978, McLachlan and Ganesalingam, 1982]. More precisely, the setting was in the case where each class-conditional density is Gaussian with equal covariance matrix. The likelihood of the model is then maximized using the labeled and unlabeled data with the help of an iterative algorithm such as the expectationmaximization (EM) algorithm [Dempster et al., 1977]. Instead of a mixture of Gaussians, the use of a mixture of multinomial distributions estimated with labeled and unlabeled data has been investigated in [Cooper and Freeman, 1970]. Later, this one component per class setting has been extended to several components per class [Shahshahani and Landgrebe, 1994] and further generalized by Miller and Uyar [1997]. Learning rates in a probably approximately correct (PAC) framework [Valiant, 1984] have been derived for the semi-supervised learning of a mixture of two Gaussians by theoretical analysis Ratsaby and Venkatesh [1995]. In the case of an identifiable mixture, Castelli and Cover [1995] showed that with an infinite number of unlabeled points, the probability of error has an exponential convergence (w."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 34
                            }
                        ],
                        "text": ", Scudder [1965], Fralick [1967], Agrawala [1970])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206730306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "baf2c2c796470815a689ce04ce13ef6f10d61730",
            "isKey": true,
            "numCitedBy": 111,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian learning scheme is computationally infeasible for most of the unsupervised learning problems. This paper suggests a learning scheme, \"learning with a probabilistic teacher,\" which works with unclassified samples and is computationally feasible for many practical problems. In this scheme a sample is probabilistically assigned with a class with appropriate probabilities computed using all the information available: Then the sample is used in learning the parameter values given this assignment of the class. The convergence of the scheme is established and a comparison with the best linear estimator is presented."
            },
            "slug": "Learning-with-a-probabilistic-teacher-Agrawala",
            "title": {
                "fragments": [],
                "text": "Learning with a probabilistic teacher"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper suggests a learning scheme, \"learning with a probabilistic teacher,\" which works with unclassified samples and is computationally feasible for many practical problems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 296750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90929a6aa901ba958eb4960aeeb594c752e08369",
            "isKey": false,
            "numCitedBy": 2230,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation\u2014which is borne out in repeated experiments\u2014that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster."
            },
            "slug": "On-Discriminative-vs.-Generative-Classifiers:-A-of-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5813355"
                        ],
                        "name": "G. Sumara",
                        "slug": "G.-Sumara",
                        "structuredName": {
                            "firstName": "Grzegorz",
                            "lastName": "Sumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6520717"
                        ],
                        "name": "I. Sumara",
                        "slug": "I.-Sumara",
                        "structuredName": {
                            "firstName": "Izabela",
                            "lastName": "Sumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38788683"
                        ],
                        "name": "I. Rozenberg",
                        "slug": "I.-Rozenberg",
                        "structuredName": {
                            "firstName": "Izabela",
                            "lastName": "Rozenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rozenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6501510"
                        ],
                        "name": "M. Kurrer",
                        "slug": "M.-Kurrer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kurrer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kurrer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144593922"
                        ],
                        "name": "A. Akhmedov",
                        "slug": "A.-Akhmedov",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Akhmedov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Akhmedov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5810569"
                        ],
                        "name": "M. Hersberger",
                        "slug": "M.-Hersberger",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Hersberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hersberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4337767"
                        ],
                        "name": "U. Eriksson",
                        "slug": "U.-Eriksson",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "Eriksson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Eriksson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3071774"
                        ],
                        "name": "F. Eberli",
                        "slug": "F.-Eberli",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Eberli",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Eberli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145908189"
                        ],
                        "name": "B. Becher",
                        "slug": "B.-Becher",
                        "structuredName": {
                            "firstName": "Burkhard",
                            "lastName": "Becher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Becher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144952701"
                        ],
                        "name": "J. Bor\u00e9n",
                        "slug": "J.-Bor\u00e9n",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Bor\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bor\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3594093"
                        ],
                        "name": "M. Cybulsky",
                        "slug": "M.-Cybulsky",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Cybulsky",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cybulsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39377953"
                        ],
                        "name": "K. Moore",
                        "slug": "K.-Moore",
                        "structuredName": {
                            "firstName": "Kathryn",
                            "lastName": "Moore",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031543"
                        ],
                        "name": "M. Freeman",
                        "slug": "M.-Freeman",
                        "structuredName": {
                            "firstName": "Mason",
                            "lastName": "Freeman",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631369"
                        ],
                        "name": "E. Wagner",
                        "slug": "E.-Wagner",
                        "structuredName": {
                            "firstName": "Erwin",
                            "lastName": "Wagner",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wagner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6495962,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "16714f7b87faa80106c88b0674034da8dae5c7aa",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "clicking here. colleagues, clients, or customers by , you can order high-quality copies for your If you wish to distribute this article to others here. following the guidelines can be obtained by Permission to republish or repurpose articles or portions of articles ):  March 4, 2011  www.sciencemag.org (this infomation is current as of The following resources related to this article are available online at http://www.sciencemag.org/content/306/5701/1555.full.html version of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/content/suppl/2004/12/07/306.5701.1555.DC1.html can be found at: Supporting Online Material 243 article(s) on the ISI Web of Science cited by This article has been http://www.sciencemag.org/content/306/5701/1555.full.html#related-urls 71 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/cgi/collection/genetics Genetics subject collections: This article appears in the following"
            },
            "slug": "A-Probabilistic-Functional-Network-of-Yeast-Genes-Sumara-Sumara",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Functional Network of Yeast Genes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18751748,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "314441821d7243e13a8e1a61fd94b75fd9e1f863",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for learning the intersection of half spaces in n dimensions. Over nearly uniform distributions, it runs in polynomial time for up to O(logn/loglogn) half spaces or, more generally for any number of half spaces whose normal vectors lie in an O(log n/log log n) dimensional subspace. Over less restricted \"non-concentrated\" distributions it runs in polynomial time for a constant number of half spaces. This generalizes an earlier result of A. Blum and R. Kannan (1993). The algorithm is simple and is based on random sampling."
            },
            "slug": "A-random-sampling-based-algorithm-for-learning-the-Vempala",
            "title": {
                "fragments": [],
                "text": "A random sampling based algorithm for learning the intersection of half-spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "An algorithm for learning the intersection of half spaces in n dimensions that runs in polynomial time for up to O(logn/loglogn) half spaces or, more generally, for any number ofHalf spaces whose normal vectors lie in an O(log n/log log n) dimensional subspace."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 38th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 331378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bbc0c752570c46a772f2982728f9ad4191f25dd",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach."
            },
            "slug": "Cluster-Kernels-for-Semi-Supervised-Learning-Chapelle-Weston",
            "title": {
                "fragments": [],
                "text": "Cluster Kernels for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label is proposed by modifying the eigenspectrum of the kernel matrix."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714285"
                        ],
                        "name": "G. Yona",
                        "slug": "G.-Yona",
                        "structuredName": {
                            "firstName": "Golan",
                            "lastName": "Yona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Yona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32427977"
                        ],
                        "name": "M. Linial",
                        "slug": "M.-Linial",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Linial"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14895935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fed391f125bb7c6d93435e4c511933309b9f5c7f",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the space of all protein sequences in search of clusters of related proteins. Our aim is to automatically detect these sets, and thus obtain a classification of all protein sequences. Our analysis, which uses standard measures of sequence similarity as applied to an all\u2010vs.\u2010all comparison of SWISSPROT, gives a very conservative initial classification based on the highest scoring pairs. The many classes in this classification correspond to protein subfamilies. Subsequently we merge the subclasses using the weaker pairs in a two\u2010phase clustering algorithm. The algorithm makes use of transitivity to identify homologous proteins; however, transitivity is applied restrictively in an attempt to prevent unrelated proteins from clustering together. This process is repeated at varying levels of statistical significance. Consequently, a hierarchical organization of all proteins is obtained."
            },
            "slug": "ProtoMap:-Automatic-classification-of-protein-a-of-Yona-Linial",
            "title": {
                "fragments": [],
                "text": "ProtoMap: Automatic classification of protein sequences, a hierarchy of protein families, and local maps of the protein space"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A hierarchical organization of all proteins is obtained by using standard measures of sequence similarity as applied to an all\u2010vs."
            },
            "venue": {
                "fragments": [],
                "text": "Proteins"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2547442"
                        ],
                        "name": "Hiroto Saigo",
                        "slug": "Hiroto-Saigo",
                        "structuredName": {
                            "firstName": "Hiroto",
                            "lastName": "Saigo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroto Saigo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47719729"
                        ],
                        "name": "N. Ueda",
                        "slug": "N.-Ueda",
                        "structuredName": {
                            "firstName": "Nobuhisa",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145929645"
                        ],
                        "name": "T. Akutsu",
                        "slug": "T.-Akutsu",
                        "structuredName": {
                            "firstName": "Tatsuya",
                            "lastName": "Akutsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Akutsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1942527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3022fa48187e3099c59f2afa62068bd68befe9e9",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nRemote homology detection between protein sequences is a central problem in computational biology. Discriminative methods involving support vector machines (SVMs) are currently the most effective methods for the problem of superfamily recognition in the Structural Classification Of Proteins (SCOP) database. The performance of SVMs depends critically on the kernel function used to quantify the similarity between sequences.\n\n\nRESULTS\nWe propose new kernels for strings adapted to biological sequences, which we call local alignment kernels. These kernels measure the similarity between two sequences by summing up scores obtained from local alignments with gaps of the sequences. When tested in combination with SVM on their ability to recognize SCOP superfamilies on a benchmark dataset, the new kernels outperform state-of-the-art methods for remote homology detection.\n\n\nAVAILABILITY\nSoftware and data available upon request."
            },
            "slug": "Protein-homology-detection-using-string-alignment-Saigo-Vert",
            "title": {
                "fragments": [],
                "text": "Protein homology detection using string alignment kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "New kernels for strings adapted to biological sequences are proposed, which are called local alignment kernels, which measure the similarity between two sequences by summing up scores obtained from local alignments with gaps of the sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684296"
                        ],
                        "name": "C. Grimes",
                        "slug": "C.-Grimes",
                        "structuredName": {
                            "firstName": "Carrie",
                            "lastName": "Grimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Grimes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1810410,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "57a66ac4a4e0a00d2cdee8711ce0a18b49e9f7a2",
            "isKey": false,
            "numCitedBy": 1588,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space. The method, Hessian-based locally linear embedding, derives from a conceptual framework of local isometry in which the manifold M, viewed as a Riemannian submanifold of the ambient Euclidean space \u211dn, is locally isometric to an open, connected subset \u0398 of Euclidean space \u211dd. Because \u0398 does not have to be convex, this framework is able to handle a significantly wider class of situations than the original ISOMAP algorithm. The theoretical framework revolves around a quadratic form \u210b(f) = \u222bM\u2009\u2225Hf(m)\u2225\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{equation*}{\\mathrm{_{{\\mathit{F}}}^{2}}}\\end{equation*}\\end{document}dm defined on functions f : M \u21a6 \u211d. Here Hf denotes the Hessian of f, and \u210b(f) averages the Frobenius norm of the Hessian over M. To define the Hessian, we use orthogonal coordinates on the tangent planes of M. The key observation is that, if M truly is locally isometric to an open, connected subset of \u211dd, then \u210b(f) has a (d + 1)-dimensional null space consisting of the constant functions and a d-dimensional space of functions spanned by the original isometric coordinates. Hence, the isometric coordinates can be recovered up to a linear isometry. Our method may be viewed as a modification of locally linear embedding and our theoretical framework as a modification of the Laplacian eigenmaps framework, where we substitute a quadratic form based on the Hessian in place of one based on the Laplacian."
            },
            "slug": "Hessian-eigenmaps:-Locally-linear-embedding-for-Donoho-Grimes",
            "title": {
                "fragments": [],
                "text": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The Hessian-based locally linear embedding method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space is described, where the isometric coordinates can be recovered up to a linear isometry."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13980,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92579735"
                        ],
                        "name": "Peter Craven",
                        "slug": "Peter-Craven",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Craven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14094416,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b477dd12dd49e44a62c1a303501df5fb6706c7e9",
            "isKey": false,
            "numCitedBy": 3541,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "SummarySmoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.We consider the modelyi(ti)+\u03b5i,i=1, 2, ...,n,ti\u2208[0, 1], whereg\u2208W2(m)={f:f,f\u2032, ...,f(m\u22121) abs. cont.,f(m)\u2208\u21122[0,1]}, and the {\u03b5i} are random errors withE\u03b5i=0,E\u03b5i\u03b5j=\u03c32\u03b4ij. The error variance \u03c32 may be unknown. As an estimate ofg we take the solutiongn, \u03bb to the problem: Findf\u2208W2(m) to minimize\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 + \\lambda \\int\\limits_0^1 {(f^{(m)} (u))^2 du} }$$\n. The functiongn, \u03bb is a smoothing polynomial spline of degree 2m\u22121. The parameter \u03bb controls the tradeoff between the \u201croughness\u201d of the solution, as measured by\n$$\\int\\limits_0^1 {[f^{(m)} (u)]^2 du}$$\n, and the infidelity to the data as measured by\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 }$$\n, and so governs the average square errorR(\u03bb; g)=R(\u03bb) defined by\n$$R(\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g_{n,\\lambda } (t_j ) - g(t_j ))^2 }$$\n. We provide an estimate\n$$\\hat \\lambda$$\n, called the generalized cross-validation estimate, for the minimizer ofR(\u03bb). The estimate\n$$\\hat \\lambda$$\n is the minimizer ofV(\u03bb) defined by\n$$V(\\lambda ) = \\frac{1}{n}\\parallel (I - A(\\lambda ))y\\parallel ^2 /\\left[ {\\frac{1}{n}{\\text{Trace(}}I - A(\\lambda ))} \\right]^2$$\n, wherey=(y1, ...,yn)t andA(\u03bb) is then\u00d7n matrix satisfying(gn, \u03bb (t1), ...,gn, \u03bb (tn))t=A (\u03bb) y. We prove that there exist a sequence of minimizers\n$$\\tilde \\lambda = \\tilde \\lambda (n)$$\n ofEV(\u03bb), such that as the (regular) mesh{ti}i=1n becomes finer,\n$$\\mathop {\\lim }\\limits_{n \\to \\infty } ER(\\tilde \\lambda )/\\mathop {\\min }\\limits_\\lambda ER(\\lambda ) \\downarrow 1$$\n. A Monte Carlo experiment with several smoothg's was tried withm=2,n=50 and several values of \u03c32, and typical values of\n$$R(\\hat \\lambda )/\\mathop {\\min }\\limits_\\lambda R(\\lambda )$$\n were found to be in the range 1.01\u20131.4. The derivativeg\u2032 ofg can be estimated by\n$$g'_{n,\\hat \\lambda } (t)$$\n. In the Monte Carlo examples tried, the minimizer of\n$$R_D (\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g'_{n,\\lambda } (t_j ) - } g'(t_j ))$$\n tended to be close to the minimizer ofR(\u03bb), so that\n$$\\hat \\lambda$$\n was also a good value of the smoothing parameter for estimating the derivative."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Craven-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122353862,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science"
            ],
            "id": "c2c4aa2580e53ae163fa69d43c5ed9c21956cc08",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Estimation of the linear discriminant function L is considered in the case where there are n 1 and n 2 observations from the populations II1 and II2 and M unclassified observations. Estimates of L using all n 1 + n 2 + M observations are proposed and evaluated in terms of the expected error rate under the assumption that M is small relative to n 1 and n 2. By appropriately weighting the sample means of the unclassified observations, an estimate of L is given which dominates the usual estimate based on just the n 1 + n 2 classified observations."
            },
            "slug": "Estimating-the-Linear-Discriminant-Function-from-a-McLachlan",
            "title": {
                "fragments": [],
                "text": "Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658455"
                        ],
                        "name": "E. Gin\u00e9",
                        "slug": "E.-Gin\u00e9",
                        "structuredName": {
                            "firstName": "Eva",
                            "lastName": "Gin\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gin\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3355384"
                        ],
                        "name": "A. Guillou",
                        "slug": "A.-Guillou",
                        "structuredName": {
                            "firstName": "Armelle",
                            "lastName": "Guillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Guillou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6400313,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b4f11f3a85d9dfa900ab37a780ce6ea89f00c4d9",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Rates-of-strong-uniform-consistency-for-kernel-Gin\u00e9-Guillou",
            "title": {
                "fragments": [],
                "text": "Rates of strong uniform consistency for multivariate kernel density estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727076"
                        ],
                        "name": "H. Lodhi",
                        "slug": "H.-Lodhi",
                        "structuredName": {
                            "firstName": "Huma",
                            "lastName": "Lodhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lodhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 669209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f330f1f472f860212b980bb9be81eff884f7f0e1",
            "isKey": false,
            "numCitedBy": 1643,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results."
            },
            "slug": "Text-Classification-using-String-Kernels-Lodhi-Saunders",
            "title": {
                "fragments": [],
                "text": "Text Classification using String Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel kernel is introduced for comparing two text documents consisting of an inner product in the feature space consisting of all subsequences of length k, which can be efficiently evaluated by a dynamic programming technique."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": false,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50001876"
                        ],
                        "name": "J. Park",
                        "slug": "J.-Park",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476651"
                        ],
                        "name": "K. Karplus",
                        "slug": "K.-Karplus",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Karplus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karplus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50787569"
                        ],
                        "name": "C. Barrett",
                        "slug": "C.-Barrett",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Barrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144364614"
                        ],
                        "name": "R. Hughey",
                        "slug": "R.-Hughey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hughey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hughey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8126949"
                        ],
                        "name": "T. Hubbard",
                        "slug": "T.-Hubbard",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "J.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216149"
                        ],
                        "name": "C. Chothia",
                        "slug": "C.-Chothia",
                        "structuredName": {
                            "firstName": "Cyrus",
                            "lastName": "Chothia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chothia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8968874,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "940bc6874c837b32888b43bb9914c02da749b0a6",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The sequences of related proteins can diverge beyond the point where their relationship can be recognised by pairwise sequence comparisons. In attempts to overcome this limitation, methods have been developed that use as a query, not a single sequence, but sets of related sequences or a representation of the characteristics shared by related sequences. Here we describe an assessment of three of these methods: the SAM-T98 implementation of a hidden Markov model procedure; PSI-BLAST; and the intermediate sequence search (ISS) procedure. We determined the extent to which these procedures can detect evolutionary relationships between the members of the sequence database PDBD40-J. This database, derived from the structural classification of proteins (SCOP), contains the sequences of proteins of known structure whose sequence identities with each other are 40% or less. The evolutionary relationships that exist between those that have low sequence identities were found by the examination of their structural details and, in many cases, their functional features. For nine false positive predictions out of a possible 432,680, i.e. at a false positive rate of about 1/50,000, SAM-T98 found 35% of the true homologous relationships in PDBD40-J, whilst PSI-BLAST found 30% and ISS found 25%. Overall, this is about twice the number of PDBD40-J relations that can be detected by the pairwise comparison procedures FASTA (17%) and GAP-BLAST (15%). For distantly related sequences in PDBD40-J, those pairs whose sequence identity is less than 30%, SAM-T98 and PSI-BLAST detect three times the number of relationships found by the pairwise methods."
            },
            "slug": "Sequence-comparisons-using-multiple-sequences-three-Park-Karplus",
            "title": {
                "fragments": [],
                "text": "Sequence comparisons using multiple sequences detect three times as many remote homologues as pairwise methods."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The extent to which the SAM-T98 implementation of a hidden Markov model procedure; PSI-BLAST; and the intermediate sequence search (ISS) procedure can detect evolutionary relationships between the members of the sequence database PDBD40-J is determined."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50095217"
                        ],
                        "name": "Fabian H Sinz",
                        "slug": "Fabian-H-Sinz",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Sinz",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian H Sinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 156548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "007e186fd05b41f68b03d1ef0a5a65bebf1d6b83",
            "isKey": false,
            "numCitedBy": 468,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the first time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction.html ."
            },
            "slug": "Large-Scale-Transductive-SVMs-Collobert-Sinz",
            "title": {
                "fragments": [],
                "text": "Large Scale Transductive SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "It is shown how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem, and provides for the first time a highly scalable algorithm in the nonlinear case."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709847"
                        ],
                        "name": "E. Eskin",
                        "slug": "E.-Eskin",
                        "structuredName": {
                            "firstName": "Eleazar",
                            "lastName": "Eskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5112756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2aec4a2aa286a0093bf124482ed106f7e965ee8b",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem. These kernels measure sequence similarity based on shared occurrences of -length subsequences, counted with up to mismatches, and do not rely on any generative model for the positive training sequences. We compute the kernels efficiently using a mismatch tree data structure and report experiments on a benchmark SCOP dataset, where we show that the mismatch kernel used with an SVM classifier performs as well as the Fisher kernel, the most successful method for remote homology detection, while achieving considerable computational savings."
            },
            "slug": "Mismatch-String-Kernels-for-SVM-Protein-Leslie-Eskin",
            "title": {
                "fragments": [],
                "text": "Mismatch String Kernels for SVM Protein Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A class of string kernels, called mismatch kernels, are introduced for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem, and show that the mismatch kernel used with an SVM classifier performs as well as the Fisher kernel, the most successful method for remote homology detection."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14848918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173",
            "isKey": false,
            "numCitedBy": 12815,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging."
            },
            "slug": "Normalized-cuts-and-image-segmentation-Shi-Malik",
            "title": {
                "fragments": [],
                "text": "Normalized cuts and image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work treats image segmentation as a graph partitioning problem and proposes a novel global criterion, the normalized cut, for segmenting the graph, which measures both the total dissimilarity between the different groups as well as the total similarity within the groups."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32588087"
                        ],
                        "name": "R. Redner",
                        "slug": "R.-Redner",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Redner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Redner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145221576"
                        ],
                        "name": "H. Walker",
                        "slug": "H.-Walker",
                        "structuredName": {
                            "firstName": "Homer",
                            "lastName": "Walker",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2611600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54323bf565cea5d2aaee88a03ec9d1d3444a9bfd",
            "isKey": false,
            "numCitedBy": 2830,
            "numCiting": 158,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of estimating the parameters which determine a mixture density has been the subject of a large, diverse body of literature spanning nearly ninety years. During the last two decades, the method of maximum likelihood has become the most widely followed approach to this problem, thanks primarily to the advent of high speed electronic computers. Here, we first offer a brief survey of the literature directed toward this problem and review maximum-likelihood estimation for it. We then turn to the subject of ultimate interest, which is a particular iterative procedure for numerically approximating maximum-likelihood estimates for mixture density problems. This procedure, known as the EM algorithm, is a specialization to the mixture density context of a general algorithm of the same name used to approximate maximum-likelihood estimates for incomplete data problems. We discuss the formulation and theoretical and practical properties of the EM algorithm for mixture densities, focussing in particular on ..."
            },
            "slug": "Mixture-densities,-maximum-likelihood,-and-the-EM-Redner-Walker",
            "title": {
                "fragments": [],
                "text": "Mixture densities, maximum likelihood, and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work discusses the formulation and theoretical and practical properties of the EM algorithm, a specialization to the mixture density context of a general algorithm used to approximate maximum-likelihood estimates for incomplete data problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015507"
                        ],
                        "name": "Jean-Yves Audibert",
                        "slug": "Jean-Yves-Audibert",
                        "structuredName": {
                            "firstName": "Jean-Yves",
                            "lastName": "Audibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Yves Audibert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2789515,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7edde67273c2ad09458d73328628f3385d0df837",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In the machine learning community it is generally believed that graph Laplacians corresponding to a finite sample of data points converge to a continuous Laplace operator if the sample size increases. Even though this assertion serves as a justification for many Laplacian-based algorithms, so far only some aspects of this claim have been rigorously proved. In this paper we close this gap by establishing the strong pointwise consistency of a family of graph Laplacians with data-dependent weights to some weighted Laplace operator. Our investigation also includes the important case where the data lies on a submanifold of R d ."
            },
            "slug": "From-Graphs-to-Manifolds-Weak-and-Strong-Pointwise-Hein-Audibert",
            "title": {
                "fragments": [],
                "text": "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper establishes the strong pointwise consistency of a family of graph Laplacians with data-dependent weights to some weighted Laplace operator."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2923061"
                        ],
                        "name": "Zhenyue Zhang",
                        "slug": "Zhenyue-Zhang",
                        "structuredName": {
                            "firstName": "Zhenyue",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenyue Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145203884"
                        ],
                        "name": "H. Zha",
                        "slug": "H.-Zha",
                        "structuredName": {
                            "firstName": "Hongyuan",
                            "lastName": "Zha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120517672,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "21c9879f0b9adb692d8ddbebf7e8e22dbe20e2de",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized data points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approximation for the tangent space at each point, and those tangent spaces are then aligned to give the global coordinates of the data points with respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can be quite small in some cases. We illustrate our algorithm using curves and surfaces both in 2D/3D Euclidean spaces and higher dimensional Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements."
            },
            "slug": "Principal-manifolds-and-nonlinear-dimensionality-Zhang-Zha",
            "title": {
                "fragments": [],
                "text": "Principal manifolds and nonlinear dimensionality reduction via tangent space alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new algorithm for manifold learning and nonlinear dimensionality reduction is presented, based on a set of unorganized data points sampled with noise from a parameterized manifold, which is illustrated using curves and surfaces both in 2D/3D Euclidean spaces and higher dimensional Euclideans."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17702358,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac",
            "isKey": false,
            "numCitedBy": 1371,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs. The method can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set. The family of kernels generated generalizes the family of radial basis kernels. It can also be used to deene kernels in the form of joint Gibbs probability distributions. Kernels can be built from hidden Markov random elds, generalized regular expressions, pair-HMMs, or ANOVA de-compositions. Uses of the method lead to open problems involving the theory of innnitely divisible positive deenite functions. Fundamentals of this theory and the theory of reproducing kernel Hilbert spaces are reviewed and applied in establishing the validity of the method."
            },
            "slug": "Convolution-kernels-on-discrete-structures-Haussler",
            "title": {
                "fragments": [],
                "text": "Convolution kernels on discrete structures"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs is introduced, which can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2865389"
                        ],
                        "name": "A. Murzin",
                        "slug": "A.-Murzin",
                        "structuredName": {
                            "firstName": "Alexey",
                            "lastName": "Murzin",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Murzin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3329398"
                        ],
                        "name": "S. Brenner",
                        "slug": "S.-Brenner",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Brenner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8126949"
                        ],
                        "name": "T. Hubbard",
                        "slug": "T.-Hubbard",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "J.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216149"
                        ],
                        "name": "C. Chothia",
                        "slug": "C.-Chothia",
                        "structuredName": {
                            "firstName": "Cyrus",
                            "lastName": "Chothia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chothia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6869184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a",
            "isKey": false,
            "numCitedBy": 6454,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SCOP:-a-structural-classification-of-proteins-for-Murzin-Brenner",
            "title": {
                "fragments": [],
                "text": "SCOP: a structural classification of proteins database for the investigation of sequences and structures."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1762283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e68c54f39e87daf3a8bdc0ee005aece3c652d11",
            "isKey": false,
            "numCitedBy": 3960,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. Occam's razor is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling."
            },
            "slug": "Bayesian-Interpolation-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data by examining the posterior probability distribution of regularizing constants and noise levels."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30883040"
                        ],
                        "name": "S. Ganesalingam",
                        "slug": "S.-Ganesalingam",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Ganesalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ganesalingam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115980920,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "63f4c77d8900b1d5595091a6821c3f005e90cfb5",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of updating a discriminant function on the basis of data of unknown origin is studied. There are observations of known origin from each of the underlying populations, and subsequently there is available a limited number of unclassified observations assumed to have been drawn from a mixture of the underlying populations. A sample discriminant function can be formed initially from the classified data. The question of whether the subsequent updating of this discriminant function on the basis of the unclassified data produces a reduction in the error rate of sufficient magnitude to warrant the computational effort is considered by carrying out a series of Monte Carlo experiments. The simulation results are contrasted with available asymptotic results."
            },
            "slug": "Updating-a-discriminant-function-in-basis-of-data-McLachlan-Ganesalingam",
            "title": {
                "fragments": [],
                "text": "Updating a discriminant function in basis of unclassified data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3118640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e0d11533c411e3c0559762e7cfc6790c28ccf2b",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We address in this paper the question of how the knowledge of the marginal distribution P(x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations."
            },
            "slug": "Measure-Based-Regularization-Bousquet-Chapelle",
            "title": {
                "fragments": [],
                "text": "Measure Based Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes three theoretical methods for taking into account this distribution P(x) for regularization and provides links to existing graph-based semi-supervised learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399356737"
                        ],
                        "name": "A. Ben-Hur",
                        "slug": "A.-Ben-Hur",
                        "structuredName": {
                            "firstName": "Asa",
                            "lastName": "Ben-Hur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ben-Hur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730241"
                        ],
                        "name": "D. Brutlag",
                        "slug": "D.-Brutlag",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Brutlag",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Brutlag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9160976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "630b25443a303a0be455ceda550123c96f602a5b",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nRemote homology detection is the problem of detecting homology in cases of low sequence similarity. It is a hard computational problem with no approach that works well in all cases.\n\n\nRESULTS\nWe present a method for detecting remote homology that is based on the presence of discrete sequence motifs. The motif content of a pair of sequences is used to define a similarity that is used as a kernel for a Support Vector Machine (SVM) classifier. We test the method on two remote homology detection tasks: prediction of a previously unseen SCOP family and prediction of an enzyme class given other enzymes that have a similar function on other substrates. We find that it performs significantly better than an SVM method that uses BLAST or Smith-Waterman similarity scores as features."
            },
            "slug": "Remote-homology-detection:-a-motif-based-approach-Ben-Hur-Brutlag",
            "title": {
                "fragments": [],
                "text": "Remote homology detection: a motif based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A method for detecting remote homology that is based on the presence of discrete sequence motifs that performs significantly better than an SVM method that uses BLAST or Smith-Waterman similarity scores as features."
            },
            "venue": {
                "fragments": [],
                "text": "ISMB"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8606662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9e51a86d106e73e60650f2c4784c5271e8c9735",
            "isKey": false,
            "numCitedBy": 509,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels, based on the heat equation, called diffusion kernels, and show that these can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "slug": "Diffusion-kernels-on-graphs-and-other-discrete-Kondor",
            "title": {
                "fragments": [],
                "text": "Diffusion kernels on graphs and other discrete structures"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper focuses on generating kernels on graphs, for which a special class of exponential kernels, based on the heat equation, are proposed, called diffusion kernels, and shows that these can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134211067"
                        ],
                        "name": "J. Rocchio",
                        "slug": "J.-Rocchio",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rocchio",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rocchio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61859400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4083ad1066cfa2ff0d65866ef4b011399d6873d1",
            "isKey": false,
            "numCitedBy": 3242,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-feedback-in-information-retrieval-Rocchio",
            "title": {
                "fragments": [],
                "text": "Relevance feedback in information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87339519"
                        ],
                        "name": "M. Kanehisa",
                        "slug": "M.-Kanehisa",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Kanehisa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kanehisa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723007"
                        ],
                        "name": "S. Goto",
                        "slug": "S.-Goto",
                        "structuredName": {
                            "firstName": "Susumu",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35004671"
                        ],
                        "name": "S. Kawashima",
                        "slug": "S.-Kawashima",
                        "structuredName": {
                            "firstName": "Shuichi",
                            "lastName": "Kawashima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kawashima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50998912"
                        ],
                        "name": "Y. Okuno",
                        "slug": "Y.-Okuno",
                        "structuredName": {
                            "firstName": "Yasushi",
                            "lastName": "Okuno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Okuno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148114"
                        ],
                        "name": "M. Hattori",
                        "slug": "M.-Hattori",
                        "structuredName": {
                            "firstName": "Masahiro",
                            "lastName": "Hattori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hattori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2133503,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4667ad4302254f67ef8c831ede8e4cd8d0709715",
            "isKey": false,
            "numCitedBy": 3790,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A grand challenge in the post-genomic era is a complete computer representation of the cell and the organism, which will enable computational prediction of higher-level complexity of cellular processes and organism behavior from genomic information. Toward this end we have been developing a knowledge-based approach for network prediction, which is to predict, given a complete set of genes in the genome, the protein interaction networks that are responsible for various cellular processes. KEGG at http://www.genome.ad.jp/kegg/ is the reference knowledge base that integrates current knowledge on molecular interaction networks such as pathways and complexes (PATHWAY database), information about genes and proteins generated by genome projects (GENES/SSDB/KO databases) and information about biochemical compounds and reactions (COMPOUND/GLYCAN/REACTION databases). These three types of database actually represent three graph objects, called the protein network, the gene universe and the chemical universe. New efforts are being made to abstract knowledge, both computationally and manually, about ortholog clusters in the KO (KEGG Orthology) database, and to collect and analyze carbohydrate structures in the GLYCAN database."
            },
            "slug": "The-KEGG-resource-for-deciphering-the-genome-Kanehisa-Goto",
            "title": {
                "fragments": [],
                "text": "The KEGG resource for deciphering the genome"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A knowledge-based approach for network prediction is developed, which is to predict, given a complete set of genes in the genome, the protein interaction networks that are responsible for various cellular processes."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043610"
                        ],
                        "name": "E. Gurewitz",
                        "slug": "E.-Gurewitz",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Gurewitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gurewitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153840193"
                        ],
                        "name": "G. Fox",
                        "slug": "G.-Fox",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Fox",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 2
                            }
                        ],
                        "text": ", Scudder [1965], Fralick [1967], Agrawala [1970])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36450624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ee14dd35886c44c87d66f8490528fa58c19fc25",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A deterministic annealing approach is suggested to search for the optimal vector quantizer given a set of training data. The problem is reformulated within a probabilistic framework. No prior knowledge is assumed on the source density, and the principle of maximum entropy is used to obtain the association probabilities at a given average distortion. The corresponding Lagrange multiplier is inversely related to the 'temperature' and is used to control the annealing process. In this process, as the temperature is lowered, the system undergoes a sequence of phase transitions when existing clusters split naturally, without use of heuristics. The resulting codebook is independent of the codebook used to initialize the iterations. >"
            },
            "slug": "Vector-quantization-by-deterministic-annealing-Rose-Gurewitz",
            "title": {
                "fragments": [],
                "text": "Vector quantization by deterministic annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A deterministic annealing approach is suggested to search for the optimal vector quantizer given a set of training data and the resulting codebook is independent of the codebook used to initialize the iterations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120764023,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "59da587c46f695a0b7867b68a22d832ca92999f3",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The construction of a suitable rule of allocation in the two-population discrimination problem is considered in the case where there are initially available from the populations II1, II2, n 1, n 2 observations and M unclassified observations. An iterative reclassification procedure based on the n 1 + n 3 + M observations is proposed and found asymptotically optimal when M \u2192 \u221e and n 1 and n 2 are moderately large. The case of finite M is evaluated by a Monte Carlo experiment which suggests that the proposed procedure, after only one iteration, gives a rule with smaller average risk than the usual rule based on just the n 1 + n 2 classified observations."
            },
            "slug": "Iterative-Reclassification-Procedure-for-An-Optimal-McLachlan",
            "title": {
                "fragments": [],
                "text": "Iterative Reclassification Procedure for Constructing An Asymptotically Optimal Rule of Allocation in Discriminant-Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118947079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0b74dd2397001588673891771de6c221fb91a894",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis discusses the general problem of learning a function on a manifold given by data points. The space of functions on a Riemannian manifold has a family of smoothness functionals and a canonical basis associated to the Laplace-Beltrami operator. Moreover, the Laplace-Beltrami operator can be reconstructed with certain convergence guarantees when the manifold is only known through the sampled data points. This allows the techniques of regularization and Fourier analysis to be applied to functions defined on data. A convergence result is proved for the case when data is sampled from a compact submanifold of R\u2227k . Several applications are considered."
            },
            "slug": "Problems-of-learning-on-manifolds-Niyogi-Belkin",
            "title": {
                "fragments": [],
                "text": "Problems of learning on manifolds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613576"
                        ],
                        "name": "Peter Sollich",
                        "slug": "Peter-Sollich",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sollich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Sollich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9266225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50060458d8f46ef751b58fad0f23edeac263f468",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "I describe a framework for interpreting Support Vector Machines (SVMs) as maximum a posteriori (MAP) solutions to inference problems with Gaussian Process priors. This can provide intuitive guidelines for choosing a 'good' SVM kernel. It can also assign (by evidence maximization) optimal values to parameters such as the noise level C which cannot be determined unambiguously from properties of the MAP solution alone (such as cross-validation error). I illustrate this using a simple approximate expression for the SVM evidence. Once C has been determined, error bars on SVM predictions can also be obtained."
            },
            "slug": "Probabilistic-Methods-for-Support-Vector-Machines-Sollich",
            "title": {
                "fragments": [],
                "text": "Probabilistic Methods for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A framework for interpreting Support Vector Machines (SVMs) as maximum a posteriori (MAP) solutions to inference problems with Gaussian Process priors is described, which can provide intuitive guidelines for choosing a 'good' SVM kernel."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144972322"
                        ],
                        "name": "J. Anderson",
                        "slug": "J.-Anderson",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Anderson",
                            "middleNames": [
                                "A.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122044830,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "b6c49d52173c77ad64726364658c8e9e69040dc0",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A general method for estimating the proportions of a finite compound distribution is suggested, based on direct estimation of the likelihood ratio. The only distributional assumption is that the log likelihood ratio is linear in the observations and hence the method is robust and applicable to a number of families of distributions. Sample points are required from each component distribution and from the compound. In particular, the method handles continuous and discrete data with equal facility. Only a moderate number of parameters need estimation so that problems in quite high dimensions can be solved. The method can be used to update logistic discriminant functions using data points whose parent populations are unknown."
            },
            "slug": "Multivariate-logistic-compounds-Anderson",
            "title": {
                "fragments": [],
                "text": "Multivariate logistic compounds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806591"
                        ],
                        "name": "G. Celeux",
                        "slug": "G.-Celeux",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Celeux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Celeux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7663024"
                        ],
                        "name": "G. Govaert",
                        "slug": "G.-Govaert",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Govaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Govaert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121694251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bce728b23956b5fdaa8d70d01ff40ee1f007082",
            "isKey": false,
            "numCitedBy": 794,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Classification-EM-algorithm-for-clustering-and-Celeux-Govaert",
            "title": {
                "fragments": [],
                "text": "A Classification EM algorithm for clustering and two stochastic versions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613576"
                        ],
                        "name": "Peter Sollich",
                        "slug": "Peter-Sollich",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sollich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Sollich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14204055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd1ccd5f8a75a133a15b273431158f37c97b9f9d",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs) can be interpreted as maximum a posteriori solutions to inference problems with Gaussian process priors and appropriate likelihood functions. Focusing on the case of classification, the author shows first that such an interpretation gives a clear intuitive meaning to SVM kernels, as covariance functions of GP priors; this can be used to guide the choice of kernel. Next, a probabilistic interpretation allows Bayesian methods to be used for SVMs. Using a local approximation of the posterior around its maximum (the standard SVM solution), he discusses how the evidence for a given kernel and noise parameter can be estimated, and how approximate error bars for the classification of test points can be calculated."
            },
            "slug": "Probabilistic-interpretations-and-Bayesian-methods-Sollich",
            "title": {
                "fragments": [],
                "text": "Probabilistic interpretations and Bayesian methods for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The author shows that a probabilistic interpretation of support vector machines as maximum a posteriori solutions to inference problems with Gaussian process priors and appropriate likelihood functions gives a clear intuitive meaning to SVM kernels, as covariance functions of GP priors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50631189"
                        ],
                        "name": "A. Vazquez",
                        "slug": "A.-Vazquez",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Vazquez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vazquez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769960"
                        ],
                        "name": "A. Flammini",
                        "slug": "A.-Flammini",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Flammini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Flammini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123952"
                        ],
                        "name": "A. Maritan",
                        "slug": "A.-Maritan",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Maritan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Maritan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690690"
                        ],
                        "name": "Alessandro Vespignani",
                        "slug": "Alessandro-Vespignani",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vespignani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Vespignani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2577381,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b47e59a9e6f182a97fc6d93708bbbc624289de93",
            "isKey": false,
            "numCitedBy": 658,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Determining protein function is one of the most challenging problems of the post-genomic era. The availability of entire genome sequences and of high-throughput capabilities to determine gene coexpression patterns has shifted the research focus from the study of single proteins or small complexes to that of the entire proteome. In this context, the search for reliable methods for assigning protein function is of primary importance. There are various approaches available for deducing the function of proteins of unknown function using information derived from sequence similarity or clustering patterns of co-regulated genes, phylogenetic profiles, protein-protein interactions (refs. 5\u20138 and Samanta, M.P. and Liang, S., unpublished data), and protein complexes. Here we propose the assignment of proteins to functional classes on the basis of their network of physical interactions as determined by minimizing the number of protein interactions among different functional categories. Function assignment is proteome-wide and is determined by the global connectivity pattern of the protein network. The approach results in multiple functional assignments, a consequence of the existence of multiple equivalent solutions. We apply the method to analyze the yeast Saccharomyces cerevisiae protein-protein interaction network. The robustness of the approach is tested in a system containing a high percentage of unclassified proteins and also in cases of deletion and insertion of specific protein interactions."
            },
            "slug": "Global-protein-function-prediction-from-interaction-Vazquez-Flammini",
            "title": {
                "fragments": [],
                "text": "Global protein function prediction from protein-protein interaction networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The assignment of proteins to functional classes on the basis of their network of physical interactions as determined by minimizing the number of protein interactions among different functional categories is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Biotechnology"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144243543"
                        ],
                        "name": "S. Rosenberg",
                        "slug": "S.-Rosenberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rosenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118250915,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "32a00effe852d527067648d82f35b58c1ecf6de8",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this chapter we will generalize the Laplacian on Euclidean space to an operator on differential forms on a Riemannian manifold. By a Riemannian manifold, we roughly mean a manifold equipped with a method for measuring lengths of tangent vectors, and hence of curves. Throughout this text, we will concentrate on studying the heat flow associated to these Laplacians. The main result of this chapter, the Hodge theorem, states that the long time behavior of the heat flow is controlled by the topology of the manifold. In \u00a71.1, the basic examples of heat flow on the one dimensional manifolds S 1 and R are studied. The heat flow on the circle already contains the basic features of heat flow on a compact manifold, although the circle is too simple topologically and geometrically to really reveal the information contained in the heat flow. In contrast, heat flow on R is more difficult to study, which indicates why we will restrict attention to compact manifolds. In \u00a71.2, we introduce the notion of a Riemannian metric on a manifold, define the spaces of L 2 functions and forms on a manifold with a Riemannian metric, and introduce the Laplacian associated to the metric. The Hodge theorem is proved in \u00a71.3 by heat equation methods. The kernel of the Laplacian on forms is isomorphic to the de Rham cohomology groups, and hence is a topological invariant. The de Rham cohomology groups are discussed in \u00a71.4, and the isomorphism between the kernel of the Laplacian and de Rham cohomology is shown in \u00a71.5."
            },
            "slug": "The-Laplacian-on-a-Riemannian-Manifold:-The-on-a-Rosenberg",
            "title": {
                "fragments": [],
                "text": "The Laplacian on a Riemannian Manifold: The Laplacian on a Riemannian Manifold"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 18
                            }
                        ],
                        "text": ", Scudder [1965], Fralick [1967], Agrawala [1970])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120199221,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7d61ed7f458aba6c563235dabbfe37e4c100e89f",
            "isKey": false,
            "numCitedBy": 754,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a survey article concerning recent advances in certain areas of statistical theory, written for a mathematical audience with no background in statistics. The topics are chosen to illustrate a special point: how the advent of the high-speed computer has affected the development of statistical theory. The topics discussed include nonparametric methods, the jackknife, the bootstrap, cross-validation, error-rate estimation in discriminant analysis, robust estimation, the influence function, censored data, the EM algorithm, and Cox\u2019s likelihood function. The exposition is mainly by example, with only a little offered in the way of theoretical development."
            },
            "slug": "Computers-and-the-Theory-of-Statistics:-Thinking-Efron",
            "title": {
                "fragments": [],
                "text": "Computers and the Theory of Statistics: Thinking the Unthinkable"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50264152"
                        ],
                        "name": "N. E. Day",
                        "slug": "N.-E.-Day",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Day",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. E. Day"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119479077,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fa8c415fb5c4d25a3a910d3c7764a1714093b07a",
            "isKey": false,
            "numCitedBy": 816,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The problem of estimating the components of a mixture of two normal distributions, multivariate or otherwise, with common but unknown covariance matrices is examined. The maximum likelihood equations are shown to be not unduly laborious to solve and the sampling properties of the resulting estimates are investigated, mainly by simulation. Moment estimators, minimum x2 and Bayes estimators are discussed but they appear greatly inferior to maximum likelihood except in the univariate case, the inferiority lying either in the sampling properties of the estimates or in the complexity of the computation. The wider problems obtained by allowing the components in the mixture to have different covariance matrices, or by having more than two components in the mixture, are briefly discussed, as is the relevance of this problem to cluster analysis."
            },
            "slug": "Estimating-the-components-of-a-mixture-of-normal-Day",
            "title": {
                "fragments": [],
                "text": "Estimating the components of a mixture of normal distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3248397"
                        ],
                        "name": "Jan Ihmels",
                        "slug": "Jan-Ihmels",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Ihmels",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Ihmels"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153572606"
                        ],
                        "name": "G. Friedlander",
                        "slug": "G.-Friedlander",
                        "structuredName": {
                            "firstName": "Gilgi",
                            "lastName": "Friedlander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Friedlander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145671573"
                        ],
                        "name": "S. Bergmann",
                        "slug": "S.-Bergmann",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Bergmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bergmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4175280"
                        ],
                        "name": "O. Sarig",
                        "slug": "O.-Sarig",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Sarig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Sarig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38410776"
                        ],
                        "name": "Yaniv Ziv",
                        "slug": "Yaniv-Ziv",
                        "structuredName": {
                            "firstName": "Yaniv",
                            "lastName": "Ziv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaniv Ziv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860762"
                        ],
                        "name": "N. Barkai",
                        "slug": "N.-Barkai",
                        "structuredName": {
                            "firstName": "Naama",
                            "lastName": "Barkai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Barkai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3088185,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "5e7362fdc706297561ab32aa2ce0113d9d0435ee",
            "isKey": false,
            "numCitedBy": 753,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard clustering methods can classify genes successfully when applied to relatively small data sets, but have limited use in the analysis of large-scale expression data, mainly owing to their assignment of a gene to a single cluster. Here we propose an alternative method for the global analysis of genome-wide expression data. Our approach assigns genes to context-dependent and potentially overlapping 'transcription modules', thus overcoming the main limitations of traditional clustering methods. We use our method to elucidate regulatory properties of cellular pathways and to characterize cis-regulatory elements. By applying our algorithm systematically to all of the available expression data on Saccharomyces cerevisiae, we identify a comprehensive set of overlapping transcriptional modules. Our results provide functional predictions for numerous genes, identify relations between modules and present a global view on the transcriptional network."
            },
            "slug": "Revealing-modular-organization-in-the-yeast-network-Ihmels-Friedlander",
            "title": {
                "fragments": [],
                "text": "Revealing modular organization in the yeast transcriptional network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach assigns genes to context-dependent and potentially overlapping 'transcription modules', thus overcoming the main limitations of traditional clustering methods, and uses the method to elucidate regulatory properties of cellular pathways and to characterize cis-regulatory elements."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Genetics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18764978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c02dfd94b11933093c797c362e2f8f6a3b9b8012",
            "isKey": false,
            "numCitedBy": 8411,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite many empirical successes of spectral clustering methods\u2014 algorithms that cluster points using eigenvectors of matrices derived from the data\u2014there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems."
            },
            "slug": "On-Spectral-Clustering:-Analysis-and-an-algorithm-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Spectral Clustering: Analysis and an algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A simple spectral clustering algorithm that can be implemented using a few lines of Matlab is presented, and tools from matrix perturbation theory are used to analyze the algorithm, and give conditions under which it can be expected to do well."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 67
                            }
                        ],
                        "text": "Learning rates in a probably approximately correct (PAC) framework [Valiant, 1984] have been derived for the semi-supervised learning of a mixture of two Gaussians by theoretical analysis Ratsaby and Venkatesh [1995]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4190,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1894794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffa38ca7a97b42f3e28d983172aa907317b9aade",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this note we show that the kernel PCA algorithm of Sch\u00f6lkopf, Smola, and M\u00fcller (Neural Computation, 10, 1299\u20131319.) can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on \u2016x \u2212 y\u2016. This leads to a metric MDS algorithm where the desired configuration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function. The question of kernel choice is also discussed."
            },
            "slug": "On-a-Connection-between-Kernel-PCA-and-Metric-Williams",
            "title": {
                "fragments": [],
                "text": "On a Connection between Kernel PCA and Metric Multidimensional Scaling"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The kernel PCA algorithm of Sch\u00f6lkopf, Smola, and M\u00fcller can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on \u2016x \u2212 y\u2016."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10032299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cbdddc71258cd515eab354c639a76d6b8801429",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Model selection is an important ingredient of many machine learning algorithms, in particular when the sample size in small, in order to strike the right trade-off between overfitting and underfitting. Previous classical results for linear regression are based on an asymptotic analysis. We present a new penalization method for performing model selection for regression that is appropriate even for small samples. Our penalization is based on an accurate estimator of the ratio of the expected training error and the expected generalization error, in terms of the expected eigenvalues of the input covariance matrix."
            },
            "slug": "Model-Selection-for-Small-Sample-Regression-Chapelle-Vapnik",
            "title": {
                "fragments": [],
                "text": "Model Selection for Small Sample Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a new penalization method for performing model selection for regression that is appropriate even for small samples, based on an accurate estimator of the ratio of the expected training error and the expected generalization error, in terms of theexpected eigenvalues of the input covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2006750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "127a1612de2194245377f9cefb049c8c34cacf9f",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant."
            },
            "slug": "Covariance-Kernels-from-Bayesian-Generative-Models-Seeger",
            "title": {
                "fragments": [],
                "text": "Covariance Kernels from Bayesian Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An implementation of the framework of mutual information kernels for learning covariance kernels from unlabeled task data using Bayesian techniques is described which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeling data is abundant."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709706"
                        ],
                        "name": "Minghua Deng",
                        "slug": "Minghua-Deng",
                        "structuredName": {
                            "firstName": "Minghua",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghua Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358506"
                        ],
                        "name": "Ting Chen",
                        "slug": "Ting-Chen",
                        "structuredName": {
                            "firstName": "Ting",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ting Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34318607"
                        ],
                        "name": "Fengzhu Sun",
                        "slug": "Fengzhu-Sun",
                        "structuredName": {
                            "firstName": "Fengzhu",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengzhu Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 130115,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "335aba58c9bb90d46e6c2089c44cb8003cb7266d",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an integrated probabilistic model to combine protein physical interactions, genetic interactions, highly correlated gene expression network, protein complex data, and domain structures of individual proteins to predict protein functions. The model is an extension of our previous model for protein function prediction based on Markovian random field theory. The model is flexible in that other protein pairwise relationship information and features of individual proteins can be easily incorporated. Two features distinguish the integrated approach from other available methods for protein function prediction. One is that the integrated approach uses all available sources of information with different weights for different sources of data. It is a global approach that takes the whole network into consideration. The second feature is that the posterior probability that a protein has the function of interest is assigned. The posterior probability indicates how confident we are about assigning the function to the protein. We apply our integrated approach to predict functions of yeast proteins based upon MIPS protein function classifications and upon the interaction networks based on MIPS physical and genetic interactions, gene expression profiles, Tandem Affinity Purification (TAP) protein complex data, and protein domain information. We study the sensitivity and specificity of the integrated approach using different sources of information by the leave-one-out approach. In contrast to using MIPS physical interactions only, the integrated approach combining all of the information increases the sensitivity from 57% to 87% when the specificity is set at 57%-an increase of 30%. It should also be noted that enlarging the interaction network greatly increases the number of proteins whose functions can be predicted."
            },
            "slug": "An-integrated-probabilistic-model-for-functional-of-Deng-Chen",
            "title": {
                "fragments": [],
                "text": "An integrated probabilistic model for functional prediction of proteins"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An integrated probabilistic model to combine protein physical interactions, genetic interactions, highly correlated gene expression network, protein complex data, and domain structures of individual proteins to predict protein functions is developed."
            },
            "venue": {
                "fragments": [],
                "text": "RECOMB '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2720935"
                        ],
                        "name": "Jihun Ham",
                        "slug": "Jihun-Ham",
                        "structuredName": {
                            "firstName": "Jihun",
                            "lastName": "Ham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jihun Ham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675444"
                        ],
                        "name": "Daniel D. Lee",
                        "slug": "Daniel-D.-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel D. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9082905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94f7f1ccd7e25c4d5f997d66365b00231478e987",
            "isKey": false,
            "numCitedBy": 588,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We interpret several well-known algorithms for dimensionality reduction of manifolds as kernel methods. Isomap, graph Laplacian eigenmap, and locally linear embedding (LLE) all utilize local neighborhood information to construct a global embedding of the manifold. We show how all three algorithms can be described as kernel PCA on specially constructed Gram matrices, and illustrate the similarities and differences between the algorithms with representative examples."
            },
            "slug": "A-kernel-view-of-the-dimensionality-reduction-of-Ham-Lee",
            "title": {
                "fragments": [],
                "text": "A kernel view of the dimensionality reduction of manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Isomap, graph Laplacian eigenmap, and locally linear embedding all utilize local neighborhood information to construct a global embedding of the manifold and it is shown how all three algorithms can be described as kernel PCA on specially constructed Gram matrices."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119188393"
                        ],
                        "name": "Jun Sun",
                        "slug": "Jun-Sun",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145942106"
                        ],
                        "name": "Lin Xiao",
                        "slug": "Lin-Xiao",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796350"
                        ],
                        "name": "P. Diaconis",
                        "slug": "P.-Diaconis",
                        "structuredName": {
                            "firstName": "Persi",
                            "lastName": "Diaconis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diaconis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6249579,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fe74880300301fbe81232683203de993a0d7ad4f",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a Markov process on a connected graph, with edges labeled with transition rates between the adjacent vertices. The distribution of the Markov process converges to the uniform distribution at a rate determined by the second smallest eigenvalue $\\lambda_2$ of the Laplacian of the weighted graph. In this paper we consider the problem of assigning transition rates to the edges so as to maximize $\\lambda_2$ subject to a linear constraint on the rates. This is the problem of finding the fastest mixing Markov process (FMMP) on the graph. We show that the FMMP problem is a convex optimization problem, which can in turn be expressed as a semidefinite program, and therefore effectively solved numerically. We formulate a dual of the FMMP problem and show that it has a natural geometric interpretation as a maximum variance unfolding (MVU) problem, , the problem of choosing a set of points to be as far apart as possible, measured by their variance, while respecting local distance constraints. This MVU problem is closely related to a problem recently proposed by Weinberger and Saul as a method for \u201cunfolding\u201d high-dimensional data that lies on a low-dimensional manifold. The duality between the FMMP and MVU problems sheds light on both problems, and allows us to characterize and, in some cases, find optimal solutions."
            },
            "slug": "The-Fastest-Mixing-Markov-Process-on-a-Graph-and-a-Sun-Boyd",
            "title": {
                "fragments": [],
                "text": "The Fastest Mixing Markov Process on a Graph and a Connection to a Maximum Variance Unfolding Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A dual of the FMMP problem is formulated and it is shown that it has a natural geometric interpretation as a maximum variance unfolding (MVU) problem, the problem of choosing a set of points to be as far apart as possible, measured by their variance, while respecting local distance constraints."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043610"
                        ],
                        "name": "E. Gurewitz",
                        "slug": "E.-Gurewitz",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Gurewitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gurewitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153840193"
                        ],
                        "name": "G. Fox",
                        "slug": "G.-Fox",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Fox",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36027870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7984546a154eae54375fcda9bc9bcde08c096d0",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-deterministic-annealing-approach-to-clustering-Rose-Gurewitz",
            "title": {
                "fragments": [],
                "text": "A deterministic annealing approach to clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145913312"
                        ],
                        "name": "P. Doyle",
                        "slug": "P.-Doyle",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Doyle",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doyle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34296841"
                        ],
                        "name": "J. Snell",
                        "slug": "J.-Snell",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Snell",
                            "middleNames": [
                                "Laurie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Snell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119671461,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6d67a36f5fae72da77cdfa4c69c92b34ce27a9f4",
            "isKey": false,
            "numCitedBy": 1583,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability theory, like much of mathematics, is indebted to physics as a source of problems and intuition for solving these problems. Unfortunately, the level of abstraction of current mathematics often makes it difficult for anyone but an expert to appreciate this fact. In this work we will look at the interplay of physics and mathematics in terms of an example where the mathematics involved is at the college level. The example is the relation between elementary electric network theory and random walks. Central to the work will be Polya\u2019s beautiful theorem that a random walker on an infinite street network in d-dimensional space is bound to return to the starting point when d = 2, but has a positive probability of escaping to infinity without returning to the starting point when d \u2265 3. Our goal will be to interpret this theorem as a statement about electric networks, and then to prove the theorem using techniques from classical electrical theory. The techniques referred to go back to Lord Rayleigh, who introduced them in connection with an investigation of musical instruments. The analog of Polya\u2019s theorem in this connection is that wind instruments are possible in our three-dimensional world, but are not possible in Flatland (Abbott [1]). The connection between random walks and electric networks has been recognized for some time (see Kakutani [12], Kemeny, Snell, and"
            },
            "slug": "Random-walks-and-electric-networks-Doyle-Snell",
            "title": {
                "fragments": [],
                "text": "Random walks and electric networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The goal will be to interpret Polya\u2019s beautiful theorem that a random walker on an infinite street network in d-dimensional space is bound to return to the starting point when d = 2, but has a positive probability of escaping to infinity without returning to the Starting Point when d \u2265 3, and to prove the theorem using techniques from classical electrical theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1487550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944cba683d10d8c1a902e05cd68e32a9f47b372e",
            "isKey": false,
            "numCitedBy": 2536,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%."
            },
            "slug": "Unsupervised-Word-Sense-Disambiguation-Rivaling-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444268"
                        ],
                        "name": "R. Lordo",
                        "slug": "R.-Lordo",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Lordo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lordo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35612632,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "99dbf84594820af16b394c781688d941237fdc77",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Weisberg (1985). Also, very little recent literature (after 1984) is covered (with the exception of Sec. 7.3, which covers radial basis functions). Following Cook and Weisberg (1999, p. 432), the most important idea from the recent literature is that MLR is the study of the conditional distribution of the response variable given the predictors, and this distribution can be visualized with a plot of the fitted values versus the response variable. Texts that do not discuss this plot may be obsolete."
            },
            "slug": "Nonparametric-and-Semiparametric-Models-Lordo",
            "title": {
                "fragments": [],
                "text": "Nonparametric and Semiparametric Models"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Following Cook and Weisberg (1999, p. 432), the most important idea from the recent literature is that MLR is the study of the conditional distribution of the response variable given the predictors, and this distribution can be visualized with a plot of the fitted values versus the responseVariable."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066339"
                        ],
                        "name": "H. Hishigaki",
                        "slug": "H.-Hishigaki",
                        "structuredName": {
                            "firstName": "Haretsugu",
                            "lastName": "Hishigaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hishigaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145542952"
                        ],
                        "name": "K. Nakai",
                        "slug": "K.-Nakai",
                        "structuredName": {
                            "firstName": "Kenta",
                            "lastName": "Nakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33074634"
                        ],
                        "name": "T. Ono",
                        "slug": "T.-Ono",
                        "structuredName": {
                            "firstName": "Toshihide",
                            "lastName": "Ono",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ono"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3675601"
                        ],
                        "name": "A. Tanigami",
                        "slug": "A.-Tanigami",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Tanigami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tanigami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47969863"
                        ],
                        "name": "T. Takagi",
                        "slug": "T.-Takagi",
                        "structuredName": {
                            "firstName": "Toshihisa",
                            "lastName": "Takagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Takagi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 20768270,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "379cbb12fbf5d2bf50a0594ac06737223aa2be4f",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Functional prediction of open reading frames coded in the genome is one of the most important tasks in yeast genomics. Among a number of large\u2010scale experiments for assigning certain functional classes to proteins, experiments determining protein\u2013protein interaction are especially important because interacting proteins usually have the same function. Thus, it seems possible to predict the function of a protein when the function of its interacting partner is known. However, in vitro experiments often suffer from artifacts and a protein can often have multiple binding partners with different functions. We developed an objective prediction method that can systematically include the information of indirect interaction. Our method can predict the subcellular localization, the cellular role and the biochemical function of yeast proteins with accuracies of 72.7%, 63.6% and 52.7%, respectively. The prediction accuracy rises for proteins with more than three binding partners and thus we present the open prediction results for 16 such proteins. Copyright \u00a9 2001 John Wiley & Sons, Ltd."
            },
            "slug": "Assessment-of-prediction-accuracy-of-protein-from-Hishigaki-Nakai",
            "title": {
                "fragments": [],
                "text": "Assessment of prediction accuracy of protein function from protein\u2013protein interaction data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An objective prediction method that can systematically include the information of indirect interaction is developed that can predict the subcellular localization, the cellular role and the biochemical function of yeast proteins with accuracies of 72.7%, 63.6% and 52.7%."
            },
            "venue": {
                "fragments": [],
                "text": "Yeast"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 411526,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "50a42ed2f81b9fe150883a6c89194c88a9647106",
            "isKey": false,
            "numCitedBy": 42032,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples."
            },
            "slug": "A-new-look-at-the-statistical-model-identification-Akaike",
            "title": {
                "fragments": [],
                "text": "A new look at the statistical model identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2427083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49",
            "isKey": false,
            "numCitedBy": 8601,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning."
            },
            "slug": "Text-Categorization-with-Support-Vector-Machines:-Joachims",
            "title": {
                "fragments": [],
                "text": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper explores the use of Support Vector Machines for learning text classifiers from examples and analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717379"
                        ],
                        "name": "D. Hochbaum",
                        "slug": "D.-Hochbaum",
                        "structuredName": {
                            "firstName": "Dorit",
                            "lastName": "Hochbaum",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hochbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747127"
                        ],
                        "name": "D. Shmoys",
                        "slug": "D.-Shmoys",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shmoys",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shmoys"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17379599,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f276c00bac7594107c291947f560b7b48b1439d7",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a 2-approximation algorithm for the k-center problem with triangle inequality. This result is \u201cbest possible\u201d since for any \u03b4 < 2 the existence of \u03b4-approximation algorithm would imply that P = NP. It should be noted that no \u03b4-approximation algorithm, for any constant \u03b4, has been reported to date. Linear programming duality theory provides interesting insight to the problem and enables us to derive, in O|E| log |E| time, a solution with value no more than twice the k-center optimal value. \n \nA by-product of the analysis is an O|E| algorithm that identifies a dominating set in G2, the square of a graph G, the size of which is no larger than the size of the minimum dominating set in the graph G. The key combinatorial object used is called a strong stable set, and we prove the NP-completeness of the corresponding decision problem."
            },
            "slug": "A-Best-Possible-Heuristic-for-the-k-Center-Problem-Hochbaum-Shmoys",
            "title": {
                "fragments": [],
                "text": "A Best Possible Heuristic for the k-Center Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A 2-approximation algorithm for the k-center problem with triangle inequality is presented, the key combinatorial object used is called a strong stable set, and the NP-completeness of the corresponding decision problem is proved."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8632802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf527ca11d7d81a15ff5b5603374a4e9d53b55b6",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, \u201cExpectation Propagation,\u201d unifies and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. \nLoopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction\u2014propagating richer belief states which incorporate correlations between variables. \nThis framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "A-family-of-algorithms-for-approximate-Bayesian-Minka",
            "title": {
                "fragments": [],
                "text": "A family of algorithms for approximate Bayesian inference"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible, and is found to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14159881,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "20ce95262aa2781c2c3127ca77f18afece3c8f69",
            "isKey": false,
            "numCitedBy": 2627,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a systematic account of the subject area, concentrating on the most recent advances in the field. While the focus is on practical considerations, both theoretical and practical issues are explored. Among the advances covered are: regularized discriminant analysis and bootstrap-based assessment of the performance of a sample-based discriminant rule and extensions of discriminant analysis motivated by problems in statistical image analysis. Includes over 1,200 references in the bibliography."
            },
            "slug": "Discriminant-Analysis-and-Statistical-Pattern-McLachlan",
            "title": {
                "fragments": [],
                "text": "Discriminant Analysis and Statistical Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2316535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39664b871e5b90aa0f82d89469a230d9ecd02498",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The Vicinal Risk Minimization principle establishes a bridge between generative models and methods derived from the Structural Risk Minimization Principle such as Support Vector Machines or Statistical Regularization. We explain how VRM provides a framework which integrates a number of existing algorithms, such as Parzen windows, Support Vector Machines, Ridge Regression, Constrained Logistic Classifiers and Tangent-Prop. We then show how the approach implies new algorithms for solving problems usually associated with generative models. New algorithms are described for dealing with pattern recognition problems with very different pattern distributions and dealing with unlabeled data. Preliminary empirical results are presented."
            },
            "slug": "Vicinal-Risk-Minimization-Chapelle-Weston",
            "title": {
                "fragments": [],
                "text": "Vicinal Risk Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how VRM provides a framework which integrates a number of existing algorithms, such as Parzen windows, Support Vector Machines, Ridge Regression, Constrained Logistic Classifiers and Tangent-Prop, and implies new algorithms for solving problems usually associated with generative models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34727111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1938e0ae87955edc4eb930df8d5e839245e604a7",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper applies fast sparse multidimensional scaling (MDS) to a large graph of music similarity, with 267K vertices that represent artists, albums, and tracks; and 3.22M edges that represent similarity between those entities. Once vertices are assigned locations in a Euclidean space, the locations can be used to browse music and to generate playlists. MDS on very large sparse graphs can be effectively performed by a family of algorithms called Rectangular Dijsktra (RD) MDS algorithms. These RD algorithms operate on a dense rectangular slice of the distance matrix, created by calling Dijsktra a constant number of times. Two RD algorithms are compared: Landmark MDS, which uses the Nystrom approximation to perform MDS; and a new algorithm called Fast Sparse Embedding, which uses FastMap. These algorithms compare favorably to Laplacian Eigenmaps, both in terms of speed and embedding quality."
            },
            "slug": "Fast-Embedding-of-Sparse-Similarity-Graphs-Platt",
            "title": {
                "fragments": [],
                "text": "Fast Embedding of Sparse Similarity Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper applies fast sparse multidimensional scaling (MDS) to a large graph of music similarity, with 267K vertices that represent artists, albums, and tracks; and 3.22M edges that represent similarity between those entities."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42793,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3329398"
                        ],
                        "name": "S. Brenner",
                        "slug": "S.-Brenner",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Brenner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145013593"
                        ],
                        "name": "P. Koehl",
                        "slug": "P.-Koehl",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Koehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Koehl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162720"
                        ],
                        "name": "M. Levitt",
                        "slug": "M.-Levitt",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Levitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levitt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10761520,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "c424cd03fb9eb8e4fcfd9e2894a0fd3d2ee32174",
            "isKey": false,
            "numCitedBy": 510,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The ASTRAL compendium provides several databases and tools to aid in the analysis of protein structures, particularly through the use of their sequences. The SPACI scores included in the system summarize the overall characteristics of a protein structure. A structural alignments database indicates residue equivalencies in superimposed protein domain structures. The PDB sequence-map files provide a linkage between the amino acid sequence of the molecule studied (SEQRES records in a database entry) and the sequence of the atoms experimentally observed in the structure (ATOM records). These maps are combined with information in the SCOPdatabase to provide sequences of protein domains. Selected subsets of the domain database, with varying degrees of similarity measured in several different ways, are also available. ASTRALmay be accessed at http://astral.stanford.edu/"
            },
            "slug": "The-ASTRAL-compendium-for-protein-structure-and-Brenner-Koehl",
            "title": {
                "fragments": [],
                "text": "The ASTRAL compendium for protein structure and sequence analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The ASTRAL compendium provides several databases and tools to aid in the analysis of protein structures, particularly through the use of their sequences, and summarizes the overall characteristics of a protein structure."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116668597"
                        ],
                        "name": "Donghui Wu",
                        "slug": "Donghui-Wu",
                        "structuredName": {
                            "firstName": "Donghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donghui Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14895712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29fa9b903dbd8d19e39b0d7fb06efc6a1907dfdb",
            "isKey": false,
            "numCitedBy": 1429,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the use of support vector machines (SVM's) in classifying e-mail as spam or nonspam by comparing it to three other classification algorithms: Ripper, Rocchio, and boosting decision trees. These four algorithms were tested on two different data sets: one data set where the number of features were constrained to the 1000 best features and another data set where the dimensionality was over 7000. SVM's performed best when using binary features. For both data sets, boosting trees and SVM's had acceptable test performance in terms of accuracy and speed. However, SVM's had significantly less training time."
            },
            "slug": "Support-vector-machines-for-spam-categorization-Drucker-Wu",
            "title": {
                "fragments": [],
                "text": "Support vector machines for spam categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The use of support vector machines in classifying e-mail as spam or nonspam is studied by comparing it to three other classification algorithms: Ripper, Rocchio, and boosting decision trees, which found SVM's performed best when using binary features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 47328136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ee87290fa827f1217b8fa2bccb3485da1a300e",
            "isKey": false,
            "numCitedBy": 15183,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2266267"
                        ],
                        "name": "S. Mendelson",
                        "slug": "S.-Mendelson",
                        "structuredName": {
                            "firstName": "Shahar",
                            "lastName": "Mendelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mendelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 463216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "009f35c0e453f2435efd8d8ef8086b76b294967a",
            "isKey": false,
            "numCitedBy": 2177,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines."
            },
            "slug": "Rademacher-and-Gaussian-Complexities:-Risk-Bounds-Bartlett-Mendelson",
            "title": {
                "fragments": [],
                "text": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This work investigates the use of certain data-dependent estimates of the complexity of a function class called Rademacher and Gaussian complexities and proves general risk bounds in terms of these complexities in a decision theoretic setting."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915042"
                        ],
                        "name": "P. Stolorz",
                        "slug": "P.-Stolorz",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Stolorz",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Stolorz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082896"
                        ],
                        "name": "J. Utans",
                        "slug": "J.-Utans",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Utans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Utans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8147965,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "56ab8eebda89b251418ed39b794706c9652b0067",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that there are strong relationships between approaches to optmization and learning based on statistical physics or mixtures of experts. In particular, the EM algorithm can be interpreted as converging either to a local maximum of the mixtures model or to a saddle point solution to the statistical physics system. An advantage of the statistical physics approach is that it naturally gives rise to a heuristic continuation method, deterministic annealing, for finding good solutions."
            },
            "slug": "Statistical-Physics,-Mixtures-of-Distributions,-and-Yuille-Stolorz",
            "title": {
                "fragments": [],
                "text": "Statistical Physics, Mixtures of Distributions, and the EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "There are strong relationships between approaches to optmization and learning based on statistical physics or mixtures of experts, and the EM algorithm can be interpreted as converging either to a local maximum of the mixtures model or to a saddle point solution to the statistical physics system."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112427179"
                        ],
                        "name": "Jie Cheng",
                        "slug": "Jie-Cheng",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145931269"
                        ],
                        "name": "D. Bell",
                        "slug": "D.-Bell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bell",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1500379841"
                        ],
                        "name": "Weiru Liu",
                        "slug": "Weiru-Liu",
                        "structuredName": {
                            "firstName": "Weiru",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiru Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17756251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bca86c8d46fc36b3fbef513c961f2f47ed7a6a76",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides algorithms that use an information-theoretic analysis to learn Bayesian network structures from data. Based on our three-phase learning framework, we develop efficient algorithms that can effectively learn Bayesian networks, requiring only polynomial numbers of conditional independence (CI) tests in typical cases. We provide precise conditions that specify when these algorithms are guaranteed to be correct as well as empirical evidence (from real world applications and simulation tests) that demonstrates that these systems work efficiently and reliably in practice. \uf6d9 2002 Elsevier Science B.V. All rights reserved."
            },
            "slug": "Learning-belief-networks-from-data:-an-information-Cheng-Bell",
            "title": {
                "fragments": [],
                "text": "Learning belief networks from data: an information theory based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Algorithms that use an information-theoretic analysis to learn Bayesian network structures from data, requiring only polynomial numbers of conditional independence tests in typical cases are provided."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3113164"
                        ],
                        "name": "P. Spellman",
                        "slug": "P.-Spellman",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Spellman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Spellman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018956"
                        ],
                        "name": "G. Sherlock",
                        "slug": "G.-Sherlock",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "Sherlock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sherlock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107448603"
                        ],
                        "name": "M. Q. Zhang",
                        "slug": "M.-Q.-Zhang",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Zhang",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Q. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145415724"
                        ],
                        "name": "V. Iyer",
                        "slug": "V.-Iyer",
                        "structuredName": {
                            "firstName": "Vishwanath",
                            "lastName": "Iyer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49364698"
                        ],
                        "name": "K. Anders",
                        "slug": "K.-Anders",
                        "structuredName": {
                            "firstName": "Kirk",
                            "lastName": "Anders",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Anders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240772"
                        ],
                        "name": "M. Eisen",
                        "slug": "M.-Eisen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Eisen",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Eisen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32037613"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Brown",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149626"
                        ],
                        "name": "D. Botstein",
                        "slug": "D.-Botstein",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Botstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Botstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118785"
                        ],
                        "name": "B. Futcher",
                        "slug": "B.-Futcher",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Futcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Futcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5839507,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "956d43b30629e6be5e8f2c4edb4cea2404598eff",
            "isKey": false,
            "numCitedBy": 5247,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": "We sought to create a comprehensive catalog of yeast genes whose transcript levels vary periodically within the cell cycle. To this end, we used DNA microarrays and samples from yeast cultures synchronized by three independent methods: alpha factor arrest, elutriation, and arrest of a cdc15 temperature-sensitive mutant. Using periodicity and correlation algorithms, we identified 800 genes that meet an objective minimum criterion for cell cycle regulation. In separate experiments, designed to examine the effects of inducing either the G1 cyclin Cln3p or the B-type cyclin Clb2p, we found that the mRNA levels of more than half of these 800 genes respond to one or both of these cyclins. Furthermore, we analyzed our set of cell cycle-regulated genes for known and new promoter elements and show that several known elements (or variations thereof) contain information predictive of cell cycle regulation. A full description and complete data sets are available at http://cellcycle-www.stanford.edu"
            },
            "slug": "Comprehensive-identification-of-cell-genes-of-the-Spellman-Sherlock",
            "title": {
                "fragments": [],
                "text": "Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization."
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A comprehensive catalog of yeast genes whose transcript levels vary periodically within the cell cycle is created, and it is found that the mRNA levels of more than half of these 800 genes respond to one or both of these cyclins."
            },
            "venue": {
                "fragments": [],
                "text": "Molecular biology of the cell"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121352283"
                        ],
                        "name": "Van Rijsbergen",
                        "slug": "Van-Rijsbergen",
                        "structuredName": {
                            "firstName": "Van",
                            "lastName": "Rijsbergen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Van Rijsbergen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62560433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ea50374077a506b86dce4796c683abcd98e18d7",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides a foundation for a practical way of improving the effectiveness of an automatic retrieval system. Its main concern is with the weighting of index terms as a device for increasing retrieval effectiveness. Previously index terms have been assumed to be independent for the good reason that then a very simple weighting scheme can be used. In reality index terms are most unlikely to be independent. This paper explores one way of removing the independence assumption. Instead the extent of the dependence between index terms is measured and used to construct a non\u2010linear weighting function. In a practical situation the values of some of the parameters of such a function must be estimated from small samples of documents. So a number of estimation rules are discussed and one in particular is recommended. Finally the feasibility of the computations required for a non\u2010linear weighting scheme is examined."
            },
            "slug": "A-theoretical-basis-for-the-use-of-co-occurence-in-Rijsbergen",
            "title": {
                "fragments": [],
                "text": "A theoretical basis for the use of co-occurence data in information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper provides a foundation for a practical way of improving the effectiveness of an automatic retrieval system by measuring the extent of the dependence between index terms and using it to construct a non\u2010linear weighting function."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144908550"
                        ],
                        "name": "Jianhua Lin",
                        "slug": "Jianhua-Lin",
                        "structuredName": {
                            "firstName": "Jianhua",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianhua Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12121632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b586837ac372141eedafd124c57335cbc893bed",
            "isKey": false,
            "numCitedBy": 3505,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness. >"
            },
            "slug": "Divergence-measures-based-on-the-Shannon-entropy-Lin",
            "title": {
                "fragments": [],
                "text": "Divergence measures based on the Shannon entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced, which do not require the condition of absolute continuity to be satisfied by the probability distributions involved and are established in terms of bounds."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81080659"
                        ],
                        "name": "M. Mohri",
                        "slug": "M.-Mohri",
                        "structuredName": {
                            "firstName": "Mehryar",
                            "lastName": "Mohri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mohri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3012473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "024d218fdfb8395f7bbc55f394f4489eee91cebf",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that can be used for analysis of variable-length sequences or more generally weighted automata, in applications such as computational biology or speech recognition. We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm. We also describe several general families of positive definite symmetric rational kernels. These general kernels can be combined with Support Vector Machines to form efficient and powerful techniques for spoken-dialog classification: highly complex kernels become easy to design and implement and lead to substantial improvements in the classification accuracy. We also show that the string kernels considered in applications to computational biology are all specific instances of rational kernels."
            },
            "slug": "Rational-Kernels-Cortes-Haffner",
            "title": {
                "fragments": [],
                "text": "Rational Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A general family of kernels based on weighted transducers or rational relations, rational kernels, that can be used for analysis of variable-length sequences or more generally weighted automata, in applications such as computational biology or speech recognition are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767284"
                        ],
                        "name": "M. Anjos",
                        "slug": "M.-Anjos",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Anjos",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anjos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1391148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b396518283e4b412a2c0bc97d57c9beed0cd2013",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that many of the optimization problems which arise in applications are \u201chard\u201d, which usually means that they are NP-hard. Hence much research has been devoted to finding \u201cgood\u201d relaxations for these hard problems. Usually a \u201cgood\u201d relaxation is one which can be solved (either exactly or within a prescribed numerical tolerance) in polynomial-time. Nesterov and Nemirovskii showed that by this criterion, many convex optimization problems are good relaxations. This thesis presents new convex relaxations for two such hard problems, namely the Maximum-Cut (Max-Cut) problem and the VLSI (Very Large Scale Integration of electronic circuits) layout problem. We derive and study the properties of two new strengthened semidefinite programming relaxations for the Max-Cut problem. Our theoretical results hold for every instance of Max-Cut; in particular, we make no assumptions about the edge weights. The first relaxation provides a strengthening of the well-known GoemansWilliamson relaxation, and the second relaxation is a further tightening of the first. We prove that the tighter relaxation automatically enforces the well-known triangle inequalities, and in fact is stronger than the simple addition of these inequalities to the Goemans-Williamson relaxation. We further prove that the tighter relaxation fully characterizes some low dimensional faces of the cut polytope via the rank of its feasible matrices. We also address some practical issues arising in the solution of these relaxations and present numerical results showing the remarkably good bounds computed by the tighter relaxation. For the VLSI layout problem, we derive a new relaxation by extending the \u201ctar-"
            },
            "slug": "New-Convex-Relaxations-for-the-Maximum-Cut-and-VLSI-Anjos",
            "title": {
                "fragments": [],
                "text": "New Convex Relaxations for the Maximum Cut and VLSI Layout Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis presents new convex relaxations for two such hard problems, namely the Maximum-Cut (Max-Cut) problem and the VLSI (Very Large Scale Integration of electronic circuits) layout problem and derive and study the properties of two new strengthened semidefinite programming relaxations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37805393"
                        ],
                        "name": "S. Lafon",
                        "slug": "S.-Lafon",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Lafon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lafon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107734495"
                        ],
                        "name": "A. B. Lee",
                        "slug": "A.-B.-Lee",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Lee",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. B. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34207023"
                        ],
                        "name": "M. Maggioni",
                        "slug": "M.-Maggioni",
                        "structuredName": {
                            "firstName": "Mauro",
                            "lastName": "Maggioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maggioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786884"
                        ],
                        "name": "B. Nadler",
                        "slug": "B.-Nadler",
                        "structuredName": {
                            "firstName": "Boaz",
                            "lastName": "Nadler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Nadler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49818480"
                        ],
                        "name": "F. Warner",
                        "slug": "F.-Warner",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Warner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Warner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698824"
                        ],
                        "name": "S. Zucker",
                        "slug": "S.-Zucker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Zucker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15926341,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "01b24de15cf337c55b9866c4b534596ca3d93abe",
            "isKey": false,
            "numCitedBy": 1368,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a framework for structural multiscale geometric organization of graphs and subsets of R(n). We use diffusion semigroups to generate multiscale geometries in order to organize and represent complex structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices, which describe local transitions, lead to macroscopic descriptions at different scales. The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration. We provide a unified view of ideas from data analysis, machine learning, and numerical analysis."
            },
            "slug": "Geometric-diffusions-as-a-tool-for-harmonic-and-of-Coifman-Lafon",
            "title": {
                "fragments": [],
                "text": "Geometric diffusions as a tool for harmonic analysis and structure definition of data: diffusion maps."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8919068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c871e8935afccc52c83e8166117be9b1420c01cb",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In supervising learning it is commonly believed that penalizing complex functions help one avoid ``overfitting'' functions to data, and therefore improves generalization. It is also commonly believed that cross-validation is an effective way to choose amongst algorithms for fitting functions to data. In a recent paper, Schaffer (1993) presents experimental evidence disputing these claims. The current paper consists of a formal analysis of these contentions of Schaffer's. It proves that his contentions are valid, although some of his experiments must be interpreted with caution."
            },
            "slug": "On-Overfitting-Avoidance-as-Bias-Wolpert",
            "title": {
                "fragments": [],
                "text": "On Overfitting Avoidance as Bias"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A formal analysis of contentions of Schaffer (1993) proves that his contentions are valid, although some of his experiments must be interpreted with caution."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991343"
                        ],
                        "name": "E. Segal",
                        "slug": "E.-Segal",
                        "structuredName": {
                            "firstName": "Eran",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Segal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108874645"
                        ],
                        "name": "Haidong Wang",
                        "slug": "Haidong-Wang",
                        "structuredName": {
                            "firstName": "Haidong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haidong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6317633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0051252657f1b7095ff6081988a8683dcf602860",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an approach for identifying 'pathways' from gene expression and protein interaction data. Our approach is based on the assumption that many pathways exhibit two properties: their genes exhibit a similar gene expression profile, and the protein products of the genes often interact. Our approach is based on a unified probabilistic model, which is learned from the data using the EM algorithm. We present results on two Saccharomyces cerevisiae gene expression data sets, combined with a binary protein interaction data set. Our results show that our approach is much more successful than other approaches at discovering both coherent functional groups and entire protein complexes."
            },
            "slug": "Discovering-molecular-pathways-from-protein-and-Segal-Wang",
            "title": {
                "fragments": [],
                "text": "Discovering molecular pathways from protein interaction and gene expression data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach is based on a unified probabilistic model, which is learned from the data using the EM algorithm, and shows that it is much more successful than other approaches at discovering both coherent functional groups and entire protein complexes."
            },
            "venue": {
                "fragments": [],
                "text": "ISMB"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405428555"
                        ],
                        "name": "Terence J. O'Neill",
                        "slug": "Terence-J.-O'Neill",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "O'Neill",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terence J. O'Neill"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123184012,
            "fieldsOfStudy": [
                "Environmental Science",
                "Mathematics"
            ],
            "id": "4b82bce6a021aea889a1e4f734277ffa73b1595b",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Fisher's linear discriminant rule may be estimated by maximum likelihood estimation using unclassified observations. It is shown that the ratio of the relevant information contained in unclassified observations to that in classified observations varies from approximately one-fifth to two-thirds for the statistically interesting range of separation of the populations. Thus, more information may be obtained from large numbers of inexpensive unclassified observations than from a small classified sample. Also, all available unclassified and classified data should be used for estimating Fisher's linear discriminant rule."
            },
            "slug": "Normal-Discrimination-with-Unclassified-O'Neill",
            "title": {
                "fragments": [],
                "text": "Normal Discrimination with Unclassified Observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805699"
                        ],
                        "name": "S. Altschul",
                        "slug": "S.-Altschul",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Altschul",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Altschul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818825"
                        ],
                        "name": "W. Gish",
                        "slug": "W.-Gish",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "Gish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119333843"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "Webb",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145910"
                        ],
                        "name": "E. Myers",
                        "slug": "E.-Myers",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Myers",
                            "middleNames": [
                                "Wimberly"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816655"
                        ],
                        "name": "D. Lipman",
                        "slug": "D.-Lipman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lipman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lipman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14441902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45d7345cf9b160fbe0169fc396ff0f60f4f8644a",
            "isKey": false,
            "numCitedBy": 78940,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Basic-local-alignment-search-tool.-Altschul-Gish",
            "title": {
                "fragments": [],
                "text": "Basic local alignment search tool."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1636783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "298a09325dce98155779f9640ccae8fa5ddca62d",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine-LearningApplicationsofAlgorithmicRandomnessVolodyaovk,AlexGammerman,CraigSaundersComputerLearningResearchCentreandDepartmentofScienceRoyalHollowa,UniversitofLondon,Egham,SurreyTW200EX,Englandfvovk,alex,craigg@dcs.rhbnc.ac.ukAbstractMostmachinelearningalgorithmssharethefollowingdrawback:theyonlyoutputbarepredictionsbutnotthecon denceinthosepredictions.Inthe1960salgorithmicinfor-mationtheorysupplieduniversalmeasuresofcon dencebuttheseare,unfortunately,non-computable.Inthispap erwecombinetheideasofalgorithmicinformationtheorywiththetheoryofSupp ortVectormachinestoobtainpracticableapproximationsuni-versalmeasuresofcon dence.Weshowthatinsomestandardproblemsofpatternrecog-nitionourapproximationsworkell.1INTRODUCTIONTwoimp ortantdi erencesofmostmo dernmetho dsmachinelearning(suchasstatisticaltheory,seeVapnik[21],1998,orPACtheory)fromclassicalstatisticalmetho dsarethat:\u000fmachinelearningmetho dspro ducebarepredic-tions,withoutestimatingcon denceinthosepre-dictions(unlike,eg,predictionoffutureobser-vationsintraditionalstatistics(Guttman[5],1970));\u000fmanymachinelearningmetho dsaredesignedtowork(andtheirp erformanceisanalysed)un-derthegeneraliidassumption(unlikeclas-sicalparametricstatistics)andtheyareabletodealwithextremelyhigh-dimensionalhyp othesisspaces;cfVapnik[21](1998).Inthispap erwewillfurtherdeveloptheapproachofGammermanetal[4](1998)andSaunders[17Figure1:Ifthetrainingsetonlycontainsclear2sand7s,weouldliktoattachmucloercon dencethemiddleimagethantorightandleftones(1999),wherethegoalistoobtaincon dencesforpredictionsunderthegeneraliidassumptioninhigh-dimensionalsituations.Figure1demonstratesthede-sirabilityofcon dences.Themaincontributionthispap erisemb eddingtheapproachesofGammermanetal[4](1998)andSaunderset[17(1999)intoagen-eralschemebasedonthenotionofalgorithmicran-domness.Aswillb ecomeclearlater,theproblemofassigningcon dencestopredictionsiscloselyconnectedtheproblemofde ningrandomsequences.ThelatterproblemwassolvedbyKolmogorov[8](1965),whobasedhisde nitionontheexistenceUniver-salTuringMachine(thoughitb ecameclearthatKol-mogorov'sde nitiondo essolvetheproblemofde ningrandomsequencesonlyafterMartin-L\u007fof 'spap er[15],1966);Kolmogorov'sde nitionmovedthenotionofrandomnessfromthegreyareasurroundingprobabil-itytheoryandstatisticstomathematicalcomputersci-ence.Kolmogorovb elievedhisnotionofrandomnesstob easuitablebasisforapplicationsofprobability.Unfor-tunately,fateideaasdi erentfromKol-mogorov's1933axioms(Kolmogorov[7],1933),which"
            },
            "slug": "Machine-Learning-Applications-of-Algorithmic-Vovk-Gammerman",
            "title": {
                "fragments": [],
                "text": "Machine-Learning Applications of Algorithmic Randomness"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805699"
                        ],
                        "name": "S. Altschul",
                        "slug": "S.-Altschul",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Altschul",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Altschul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34806045"
                        ],
                        "name": "T. Madden",
                        "slug": "T.-Madden",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Madden",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Madden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783217"
                        ],
                        "name": "A. Sch\u00e4ffer",
                        "slug": "A.-Sch\u00e4ffer",
                        "structuredName": {
                            "firstName": "Alejandro",
                            "lastName": "Sch\u00e4ffer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sch\u00e4ffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123275539"
                        ],
                        "name": "J. Zhang",
                        "slug": "J.-Zhang",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3026199"
                        ],
                        "name": "Z. Zhang",
                        "slug": "Z.-Zhang",
                        "structuredName": {
                            "firstName": "Z",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145011589"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "Webb",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816655"
                        ],
                        "name": "D. Lipman",
                        "slug": "D.-Lipman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lipman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lipman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221657079,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf",
            "isKey": false,
            "numCitedBy": 41506,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily."
            },
            "slug": "Gapped-BLAST-and-PSI-BLAST:-a-new-generation-of-Altschul-Madden",
            "title": {
                "fragments": [],
                "text": "Gapped BLAST and PSI-BLAST: a new generation of protein database search programs."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic acids research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682471"
                        ],
                        "name": "Adam R. Klivans",
                        "slug": "Adam-R.-Klivans",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Klivans",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam R. Klivans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410077373"
                        ],
                        "name": "R. O'Donnell",
                        "slug": "R.-O'Donnell",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "O'Donnell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O'Donnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729835"
                        ],
                        "name": "R. Servedio",
                        "slug": "R.-Servedio",
                        "structuredName": {
                            "firstName": "Rocco",
                            "lastName": "Servedio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Servedio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1664758,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8b9553cfa595fe25e5c823145efaf1a7cc98ed57",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We give the first polynomial time algorithm to learn any function of a constant number of halfspaces under the uniform distribution to within any constant error parameter. We also give the first quasipolynomial time algorithm for learning any function of a polylog number of polynomial-weight halfspaces under any distribution. As special cases of these results we obtain algorithms for learning intersections and thresholds of halfspaces. Our uniform distribution learning algorithms involve a novel non-geometric approach to learning halfspaces; we use Fourier techniques together with a careful analysis of the noise sensitivity of functions of halfspaces. Our algorithms for learning under any distribution use techniques from real approximation theory to construct low degree polynomial threshold functions."
            },
            "slug": "Learning-intersections-and-thresholds-of-halfspaces-Klivans-O'Donnell",
            "title": {
                "fragments": [],
                "text": "Learning intersections and thresholds of halfspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work gives the first polynomial time algorithm to learn any function of a constant number of halfspaces under the uniform distribution to within any constant error parameter."
            },
            "venue": {
                "fragments": [],
                "text": "The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2671293"
                        ],
                        "name": "M. Diekhans",
                        "slug": "M.-Diekhans",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Diekhans",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Diekhans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2048632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e2dd064daaac3603581ec65b580b7b5385e2c2b",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for detecting remote protein homologies is introduced and shown to perform well in classifying protein domains by SCOP superfamily. The method is a variant of support vector machines using a new kernel function. The kernel function is derived from a generative statistical model for a protein family, in this case a hidden Markov model. This general approach of combining generative models like HMMs with discriminative methods such as support vector machines may have applications in other areas of biosequence analysis as well."
            },
            "slug": "A-Discriminative-Framework-for-Detecting-Remote-Jaakkola-Diekhans",
            "title": {
                "fragments": [],
                "text": "A Discriminative Framework for Detecting Remote Protein Homologies"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new method for detecting remote protein homologies is introduced and shown to perform well in classifying protein domains by SCOP superfamily using a new kernel function derived from a generative statistical model for a protein family, in this case a hidden Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Biol."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724065"
                        ],
                        "name": "D. M. Chickering",
                        "slug": "D.-M.-Chickering",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chickering",
                            "middleNames": [
                                "Maxwell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Chickering"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50004012"
                        ],
                        "name": "Christopher Meek",
                        "slug": "Christopher-Meek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Meek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Meek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745567"
                        ],
                        "name": "Robert Rounthwaite",
                        "slug": "Robert-Rounthwaite",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Rounthwaite",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Rounthwaite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772349"
                        ],
                        "name": "C. Kadie",
                        "slug": "C.-Kadie",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Kadie",
                            "middleNames": [
                                "Myers"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kadie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11581349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30afca3a4056bc54deadc1c5794048436d1c9eb4",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a graphical model for probabilistic relationships--an alternative to the Bayesian network--called a dependency network. The graph of a dependency network, unlike a Bayesian network, is potentially cyclic. The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents. We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships."
            },
            "slug": "Dependency-Networks-for-Inference,-Collaborative-Heckerman-Chickering",
            "title": {
                "fragments": [],
                "text": "Dependency Networks for Inference, Collaborative Filtering, and Data Visualization"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work describes a graphical model for probabilistic relationships--an alternative to the Bayesian network--called a dependency network and identifies several basic properties of this representation and describes a computationally efficient procedure for learning the graph and probability components from data."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848824"
                        ],
                        "name": "Karen Sp\u00e4rck Jones",
                        "slug": "Karen-Sp\u00e4rck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sp\u00e4rck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sp\u00e4rck Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2996187,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4f09e6ec1b7d4390d23881852fd7240994abeb58",
            "isKey": false,
            "numCitedBy": 3208,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently\u2010occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure."
            },
            "slug": "A-statistical-interpretation-of-term-specificity-in-Jones",
            "title": {
                "fragments": [],
                "text": "A statistical interpretation of term specificity and its application in retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Documentation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5321968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1579f0d7efeec5ca91d8c16ce17cfa103bfd32f7",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Measuring the properties of a large, unstructured network can be difficult: one may not have full knowledge of the network topology, and detailed global measurements may be infeasible. A valuable approach to such problems is to take measurements from selected locations within the network and then aggregate them to infer large-scale properties. One sees this notion applied in settings that range from Internet topology discovery tools to remote software agents that estimate the download times of popular Web pages. Some of the most basic questions about this type of approach, however, are largely unresolved at an analytical level. How reliable are the results? How much does the choice of measurement locations affect the aggregate information one infers about the network? We describe algorithms that yield provable guarantees for a particular problem of this type: detecting a network failure. Suppose we want to detect events of the following form: an adversary destroys up to k nodes or edges, after which two subsets of the nodes, each at least an /spl epsi/ fraction of the network, are disconnected from one another. We call such an event an (/spl epsi/,k) partition. One method for detecting such events would be to place \"agents\" at a set D of nodes, and record a fault whenever two of them become separated from each other. To be a good detection set, D should become disconnected whenever there is an (/spl epsi/,k)-partition; in this way, it \"witnesses\" all such events. We show that every graph has a detection set of size polynomial in k and /spl epsi//sup -1/, and independent of the size of the graph itself. Moreover, random sampling provides an effective way to construct such a set. Our analysis establishes a connection between graph separators and the notion of VC-dimension, using techniques based on matchings and disjoint paths."
            },
            "slug": "Detecting-a-network-failure-Kleinberg",
            "title": {
                "fragments": [],
                "text": "Detecting a network failure"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work describes algorithms that yield provable guarantees for a particular problem of this type: detecting a network failure, and establishes a connection between graph separators and the notion of VC-dimension, using techniques based on matchings and disjoint paths."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 41st Annual Symposium on Foundations of Computer Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689609"
                        ],
                        "name": "N. Nisan",
                        "slug": "N.-Nisan",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Nisan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nisan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8753721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6072c6734cc6911bdf4b260f16a2e6253e64897b",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Boolean functions in AC/sup O/ are studied using the harmonic analysis of the cube. The main result is that an AC/sup O/ Boolean function has almost all of its power spectrum on the low-order coefficients. This result implies the following properties of functions in AC/sup O/: functions in AC/sup O/ have low average sensitivity; they can be approximated well be a real polynomial of low degree; they cannot be pseudorandom function generators and their correlation with any polylog-wide independent probability distribution is small. An O(n/sup polylog(/ /sup sup)/ /sup (n)/)-time algorithm for learning functions in AC/sup O/ is obtained. The algorithm observed the behavior of an AC/sup O/ function on O(n/sup polylog/ /sup (n)/) randomly chosen inputs and derives a good approximation for the Fourier transform of the function. This allows it to predict with high probability the value of the function on other randomly chosen inputs.<<ETX>>"
            },
            "slug": "Constant-depth-circuits,-Fourier-transform,-and-Linial-Mansour",
            "title": {
                "fragments": [],
                "text": "Constant depth circuits, Fourier transform, and learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An O(n/sup polylog(/ /sup sup)/ /sup (n)/)-time algorithm for learning functions in AC/sup O/ is obtained and derives a good approximation for the Fourier transform of the function."
            },
            "venue": {
                "fragments": [],
                "text": "30th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46345617"
                        ],
                        "name": "D. W. Scott",
                        "slug": "D.-W.-Scott",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Scott",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. W. Scott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30821484"
                        ],
                        "name": "E. Nadaraya",
                        "slug": "E.-Nadaraya",
                        "structuredName": {
                            "firstName": "Elizbar",
                            "lastName": "Nadaraya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nadaraya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32741853"
                        ],
                        "name": "S. Kotz",
                        "slug": "S.-Kotz",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Kotz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kotz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121635988,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9c33d0c1e6a617e74ac77894888ccd2589fd3bd8",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Asymptotic Properties of Certain Measures of Deviation for Kernel-Type Non Parametric Estimators of Probability Densities.- 1. Integrated Mean Square Error of Nonparametric Kernel-Type Probability Density Estimators.- 2. The Mean Square Error of Nonparametric Kernel-Type Density Estimators.- 2. Strongly Consistent in Functional Metrics Estimators of Probability Density.- 1. Strong Consistency of Kernel-Type Density Estimators in the Norm of the Space C.- 2. Convergence in the L2 Norm of Kernel-Type Density Estimators.- 3. Convergence in Variation of Kernel-Type Density Estimators and its Application to a Nonparametric Estimator of Bayesian Risk in a Classification Problem.- 3. Limiting Distributions of Deviations of Kernel-Type Density Estimators.- 1. Limiting Distribution of Maximal Deviation of Kernel-Type Estimators.- 2. Limiting Distribution of Quadratic Deviation of Two Nonparametric Kernel-Type Density Estimators.- 3. The Asymptotic Power of the Un1n2-Test in the Case of' singular' Close Alternatives.- 4. Testing for Symmetry of a Distribution.- 5. Independence of Tests Based on Kernel-Type Density Estimators.- 4. Nonparametric Estimation of the Regression Curve and Components of a Convolution.- 1. Some Asymptotic Properties of Nonparametric Estimators of Regression Curves.- 2. Strong Consistency of Regression Curve Estimators in the Norm of the Space C(a, b).- 3. Limiting Distribution of the Maximal Deviation of Estimators of Regression Curves.- 4. Limiting Distribution of Quadratic Deviation of Estimators of Regression Curves.- 5. Nonparametric Estimators of Components of a Convolution (S.N. Bernstein's Problem).- 5. Projection Type Nonparametric Estimation of Probability Density.- 1. Consistency of Projection-Type Probability Density Estimator in the Norms of Spaces C and L2.- 2. Limiting Distribution of the Squared Norm of a Projection-Type Density Estimator.- Addendum Limiting Distribution of Quadratic Deviation for a Wide Class of Probability Density Estimators.- 1. Limiting Distribution of Un.- 2. Kernel Density Estimators / Rosenblatt-Parzen Estimators.- 3. Projection Estimators of Probability Density / Chentsov Estimators.- 4. Histogram.- 5. Deviation of Kernel Estimators in the Sence of the Hellinger Distance.- References.- Author Index."
            },
            "slug": "Nonparametric-Estimation-of-Probability-Densities-Scott-Nadaraya",
            "title": {
                "fragments": [],
                "text": "Nonparametric Estimation of Probability Densities and Regression Curves"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717932"
                        ],
                        "name": "G. Lebanon",
                        "slug": "G.-Lebanon",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Lebanon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lebanon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15411921,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "121bea110dcf80750e295d4cf1ba5e8c83b24c77",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a solution to the problem of estimating a Riemannian metric associated with a given differentiable manifold. The metric learning problem is based on minimizing the relative volume of a given set of points. We derive the details for a family of metrics on the multinomial simplex. The resulting metric has applications in text classification and bears some similarity to TFIDF representation of text documents."
            },
            "slug": "Learning-Riemannian-Metrics-Lebanon",
            "title": {
                "fragments": [],
                "text": "Learning Riemannian Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A solution to the problem of estimating a Riemannian metric associated with a given differentiable manifold and derive the details for a family of metrics on the multinomial simplex has applications in text classification and bears some similarity to TFIDF representation of text documents."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1411416797"
                        ],
                        "name": "Bernhard Sch\u00c3\u00b6lkopf",
                        "slug": "Bernhard-Sch\u00c3\u00b6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00c3\u00b6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Sch\u00c3\u00b6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63820100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bc9fe21de471db0b7ec0ca28f7e0e73302a5d39",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, A Formal Framework, Sample Complexity Results, Algorithmic Results, Related Models and Discussion"
            },
            "slug": "An-Augmented-PAC-Model-for-Semi-Supervised-Learning-Chapelle-Sch\u00c3\u00b6lkopf",
            "title": {
                "fragments": [],
                "text": "An Augmented PAC Model for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction, A Formal Framework, Sample Complexity Results, Algorithmic Results, Related Models and Discussion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694388"
                        ],
                        "name": "M. Goemans",
                        "slug": "M.-Goemans",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Goemans",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17221714,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1dce2df7f9f3fe46ba6114c796a1c1846fb6f842",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the use of semidefinite programming for combinatorial optimization problems. The main topics covered include (i) the Lov\u00e1sz theta function and its applications to stable sets, perfect graphs, and coding theory, (ii) the automatic generation of strong valid inequalities, (iii) the maximum cut problem and related problems, and (iv) the embedding of finite metric spaces and its relationship to the sparsest cut problem."
            },
            "slug": "Semidefinite-programming-in-combinatorial-Goemans",
            "title": {
                "fragments": [],
                "text": "Semidefinite programming in combinatorial optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The main topics covered include the Lov\u00e1sz theta function and its applications to stable sets, perfect graphs, and coding theory, the automatic generation of strong valid inequalities, and the embedding of finite metric spaces."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347035"
                        ],
                        "name": "E. George",
                        "slug": "E.-George",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "George",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. George"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120901749,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "f9f15d0c475479c5afa6e53691cd8e0b16752ffc",
            "isKey": false,
            "numCitedBy": 598,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new criterion is proposed for the evaluation of variable selection procedures in multiple regression. This criterion, which we call the risk inflation, is based on an adjustment to the risk. Essentially, the risk inflation is the maximum increase in risk due to selecting rather than knowing the \u00abcorrect\u00bb predictors. A new variable selection procedure is obtained which, in the case of orthogonal predictors, substantially improves on AIC, C p and BIC and is close to optimal. In contrast to AIC, C p and BIC which use dimensionality penalties of 2, 2 and log n, respectively, this new procedure uses a penalty 2 log p, where p is the number of available predictors. For the case of nonorthogonal predictors, bounds for the optimal penalty are obtained"
            },
            "slug": "The-risk-inflation-criterion-for-multiple-Foster-George",
            "title": {
                "fragments": [],
                "text": "The risk inflation criterion for multiple regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1838914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "937c3e93a15cc416af330f9fcbcf447f7ad77e1e",
            "isKey": false,
            "numCitedBy": 694,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Hardness-of-Approximate-Reasoning-Roth",
            "title": {
                "fragments": [],
                "text": "On the Hardness of Approximate Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6884486,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "28667c276ba78ab1d855064d5456d50d9932b775",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads in to a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions for neural network models and the use of Gaussian processes in classification problems."
            },
            "slug": "Prediction-with-Gaussian-Processes:-From-Linear-to-Williams",
            "title": {
                "fragments": [],
                "text": "Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes, starting from Bayesian linear regression, and showing how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on prior over parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5842708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094fc15bc058b0d62a661a1460885a9490bdb1bd",
            "isKey": false,
            "numCitedBy": 1533,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : A probabilistic analysis of the Rocchio relevance feedback algorithm, one of the most popular learning methods from information retrieval, is presented in a text categorization framework. The analysis results in a probabilistic version of the Rocchio classifier and offers an explanation for the TFIDF word weighting heuristic. The Rocchio classifier, its probabilistic variant and a standard naive Bayes classifier are compared on three text categorization tasks. The results suggest that the probabilistic algorithms are preferable to the heuristic Rocchio classifier."
            },
            "slug": "A-Probabilistic-Analysis-of-the-Rocchio-Algorithm-Joachims",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A Probabilistic analysis of the Rocchio relevance feedback algorithm, one of the most popular learning methods from information retrieval, is presented in a text categorization framework and suggests that the probabilistic algorithms are preferable to the heuristic Rocchio classifier."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40383812"
                        ],
                        "name": "Ying-li Tian",
                        "slug": "Ying-li-Tian",
                        "structuredName": {
                            "firstName": "Ying-li",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying-li Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737918"
                        ],
                        "name": "J. Cohn",
                        "slug": "J.-Cohn",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Cohn",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cohn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16251989,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e3f2391513693647e0ea87bfa86cd89e468f51d0",
            "isKey": false,
            "numCitedBy": 2691,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis."
            },
            "slug": "Comprehensive-database-for-facial-expression-Kanade-Tian",
            "title": {
                "fragments": [],
                "text": "Comprehensive database for facial expression analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The problem space for facial expression analysis is described, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39453972"
                        ],
                        "name": "Vin de Silva",
                        "slug": "Vin-de-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "Silva",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vin de Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2049761,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "df83034e88557e1e2c7f9d268d90b19762312847",
            "isKey": false,
            "numCitedBy": 890,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps."
            },
            "slug": "Global-Versus-Local-Methods-in-Nonlinear-Reduction-Silva-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "Global Versus Local Methods in Nonlinear Dimensionality Reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122051864"
                        ],
                        "name": "S. Boucheron",
                        "slug": "S.-Boucheron",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Boucheron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Boucheron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 749141,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "d364ee88edbb0a7298cea72b80ee9151a9967f78",
            "isKey": false,
            "numCitedBy": 533,
            "numCiting": 299,
            "paperAbstract": {
                "fragments": [],
                "text": "The last few years have witnessed important new developments in the theory and practice of pattern classification. We intend to survey some of the main new ideas that have lead to these important recent developments. Resume. Durant ces dernieres annees, la theorie et la pratique de la reconnaissance des formes ont \u00b4e marquees par des developpements originaux. Ce survol presente certaines des principales idees novatrices qui ont conduitces developpements importants."
            },
            "slug": "Theory-of-classification-:-a-survey-of-some-recent-Boucheron-Bousquet",
            "title": {
                "fragments": [],
                "text": "Theory of classification : a survey of some recent advances"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18218,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141737"
                        ],
                        "name": "P. Uetz",
                        "slug": "P.-Uetz",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Uetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Uetz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152895452"
                        ],
                        "name": "L. Giot",
                        "slug": "L.-Giot",
                        "structuredName": {
                            "firstName": "Loic",
                            "lastName": "Giot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Giot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8960625"
                        ],
                        "name": "G. Cagney",
                        "slug": "G.-Cagney",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Cagney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cagney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48782514"
                        ],
                        "name": "T. Mansfield",
                        "slug": "T.-Mansfield",
                        "structuredName": {
                            "firstName": "Traci",
                            "lastName": "Mansfield",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Mansfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771795"
                        ],
                        "name": "R. Judson",
                        "slug": "R.-Judson",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Judson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Judson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144612262"
                        ],
                        "name": "James R. Knight",
                        "slug": "James-R.-Knight",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Knight",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3843456"
                        ],
                        "name": "D. Lockshon",
                        "slug": "D.-Lockshon",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lockshon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lockshon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053080529"
                        ],
                        "name": "Vaibhav A. Narayan",
                        "slug": "Vaibhav-A.-Narayan",
                        "structuredName": {
                            "firstName": "Vaibhav",
                            "lastName": "Narayan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vaibhav A. Narayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50522060"
                        ],
                        "name": "Maithreyan Srinivasan",
                        "slug": "Maithreyan-Srinivasan",
                        "structuredName": {
                            "firstName": "Maithreyan",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maithreyan Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48683675"
                        ],
                        "name": "P. Pochart",
                        "slug": "P.-Pochart",
                        "structuredName": {
                            "firstName": "Pascale",
                            "lastName": "Pochart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pochart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403371282"
                        ],
                        "name": "Alia Qureshi-Emili",
                        "slug": "Alia-Qureshi-Emili",
                        "structuredName": {
                            "firstName": "Alia",
                            "lastName": "Qureshi-Emili",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alia Qureshi-Emili"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155504702"
                        ],
                        "name": "Ying Li",
                        "slug": "Ying-Li",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141537"
                        ],
                        "name": "B. Godwin",
                        "slug": "B.-Godwin",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Godwin",
                            "middleNames": [
                                "Christopher"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Godwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48218406"
                        ],
                        "name": "D. Conover",
                        "slug": "D.-Conover",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "Conover",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Conover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153369746"
                        ],
                        "name": "T. Kalbfleisch",
                        "slug": "T.-Kalbfleisch",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Kalbfleisch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kalbfleisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6577271"
                        ],
                        "name": "G. Vijayadamodar",
                        "slug": "G.-Vijayadamodar",
                        "structuredName": {
                            "firstName": "Govindan",
                            "lastName": "Vijayadamodar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Vijayadamodar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6590166"
                        ],
                        "name": "Meijia Yang",
                        "slug": "Meijia-Yang",
                        "structuredName": {
                            "firstName": "Meijia",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Meijia Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145960136"
                        ],
                        "name": "M. Johnston",
                        "slug": "M.-Johnston",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Johnston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145280573"
                        ],
                        "name": "S. Fields",
                        "slug": "S.-Fields",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Fields",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fields"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47511243"
                        ],
                        "name": "J. Rothberg",
                        "slug": "J.-Rothberg",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Rothberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rothberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4352495,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3416a9e618f42ae68d7788775b68becaa46f6999",
            "isKey": false,
            "numCitedBy": 4835,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Two large-scale yeast two-hybrid screens were undertaken to identify protein\u2013protein interactions between full-length open reading frames predicted from the Saccharomyces cerevisiae genome sequence. In one approach, we constructed a protein array of about 6,000 yeast transformants, with each transformant expressing one of the open reading frames as a fusion to an activation domain. This array was screened by a simple and automated procedure for 192 yeast proteins, with positive responses identified by their positions in the array. In a second approach, we pooled cells expressing one of about 6,000 activation domain fusions to generate a library. We used a high-throughput screening procedure to screen nearly all of the 6,000 predicted yeast proteins, expressed as Gal4 DNA-binding domain fusion proteins, against the library, and characterized positives by sequence analysis. These approaches resulted in the detection of 957 putative interactions involving 1,004 S. cerevisiae proteins. These data reveal interactions that place functionally unclassified proteins in a biological context, interactions between proteins involved in the same biological function, and interactions that link biological functions together into larger cellular processes. The results of these screens are shown here."
            },
            "slug": "A-comprehensive-analysis-of-protein\u2013protein-in-Uetz-Giot",
            "title": {
                "fragments": [],
                "text": "A comprehensive analysis of protein\u2013protein interactions in Saccharomyces cerevisiae"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Examination of large-scale yeast two-hybrid screens reveals interactions that place functionally unclassified proteins in a biological context, interactions between proteins involved in the same biological function, and interactions that link biological functions together into larger cellular processes."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 793899,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbc78c2669eb595c988ab69a6dd4cbabc2421043",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "New functionals for parameter (model) selection of Support Vector Machines are introduced based on the concepts of the span of support vectors and rescaling of the feature space. It is shown that using these functionals, one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter."
            },
            "slug": "Model-Selection-for-Support-Vector-Machines-Chapelle-Vapnik",
            "title": {
                "fragments": [],
                "text": "Model Selection for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "New functionals for parameter (model) selection of Support Vector Machines are introduced based on the concepts of the span of support vectors and rescaling of the feature space and it is shown that using these functionals one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057287718"
                        ],
                        "name": "M. Porter",
                        "slug": "M.-Porter",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Porter",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Porter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6093716,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a651bb7cc7fc68ece0cc66ab921486d163373385",
            "isKey": false,
            "numCitedBy": 6533,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length."
            },
            "slug": "An-algorithm-for-suffix-stripping-Porter",
            "title": {
                "fragments": [],
                "text": "An algorithm for suffix stripping"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL and performs slightly better than a much more elaborate system with which it has been compared."
            },
            "venue": {
                "fragments": [],
                "text": "Program"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312432"
                        ],
                        "name": "T. N. Lal",
                        "slug": "T.-N.-Lal",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lal",
                            "middleNames": [
                                "Navin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. N. Lal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2345823"
                        ],
                        "name": "M. Tangermann",
                        "slug": "M.-Tangermann",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tangermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tangermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1812411"
                        ],
                        "name": "T. Hinterberger",
                        "slug": "T.-Hinterberger",
                        "structuredName": {
                            "firstName": "Thilo",
                            "lastName": "Hinterberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hinterberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2239056"
                        ],
                        "name": "M. Bogdan",
                        "slug": "M.-Bogdan",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Bogdan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bogdan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145198983"
                        ],
                        "name": "N. Birbaumer",
                        "slug": "N.-Birbaumer",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Birbaumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Birbaumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12194873,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "3c32b60e2797be3351f6bc72f0ffcd9eb4439a6b",
            "isKey": false,
            "numCitedBy": 493,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Designing a brain computer interface (BCI) system one can choose from a variety of features that may be useful for classifying brain activity during a mental task. For the special case of classifying electroencephalogram (EEG) signals we propose the usage of the state of the art feature selection algorithms Recursive Feature Elimination and Zero-Norm Optimization which are based on the training of support vector machines (SVM) . These algorithms can provide more accurate solutions than standard filter methods for feature selection . We adapt the methods for the purpose of selecting EEG channels. For a motor imagery paradigm we show that the number of used channels can be reduced significantly without increasing the classification error. The resulting best channels agree well with the expected underlying cortical activity patterns during the mental tasks. Furthermore we show how time dependent task specific information can be visualized."
            },
            "slug": "Support-vector-channel-selection-in-BCI-Lal-Tangermann",
            "title": {
                "fragments": [],
                "text": "Support vector channel selection in BCI"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Recursive Feature Elimination and Zero-Norm Optimization which are based on the training of support vector machines (SVM) can provide more accurate solutions than standard filter methods for feature selection for EEG channels."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Biomedical Engineering"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 189781595,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "02cb8ef325adcc6f29e0b1759920527836ea8b2b",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how to choose the smoothing parameter when a smoothing periodic spline of degree 2m\u22121 is used to reconstruct a smooth periodic curve from noisy ordinate data. The noise is assumed \u201cwhite\u201d, and the true curve is assumed to be in the Sobolev spaceW2(2m) of periodic functions with absolutely continuousv-th derivative,v=0, 1, ..., 2m\u22121 and square integrable 2m-th derivative. The criteria is minimum expected square error, averaged over the data points. The dependency of the optimum smoothing parameter on the sample size, the noise variance, and the smoothness of the true curve is found explicitly."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144882893"
                        ],
                        "name": "M. Sandler",
                        "slug": "M.-Sandler",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sandler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sandler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158559"
                        ],
                        "name": "Aleksandrs Slivkins",
                        "slug": "Aleksandrs-Slivkins",
                        "structuredName": {
                            "firstName": "Aleksandrs",
                            "lastName": "Slivkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleksandrs Slivkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207057109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6830ca120cd9f274f0575ed7e4891671608547d9",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a model for monitoring the connectivity of a network subject to node or edge failures. In particular, we are concerned with detecting (\u03b5, <i>k</i>)-<i>failures</i>: events in which an adversary deletes up to network elements (nodes or edges), after which there are two sets of nodes <i>A</i> and <i>B</i>, each at least an \u03b5 fraction of the network, that are disconnected from one another. We say that a set <i>D</i> of nodes is an (\u03b5 <i>k</i>)-detection set if, for any (\u03b5 <i>k</i>)-failure of the network, some two nodes in <i>D</i> are no longer able to communicate; in this way, <i>D</i> \"witnesses\" any such failure. Recent results show that for any graph <i>G</i>, there is an is (\u03b5 <i>k</i>)-detection set of size bounded by a polynomial in <i>k</i> and \u03b5, independent of the size of <i>G</i>.In this paper, we expose some relationships between bounds on detection sets and the edge-connectivity \u03bb and node-connectivity \u03ba of the underlying graph. Specifically, we show that detection set bounds can be made considerably stronger when parameterized by these connectivity values. We show that for an adversary that can delete \u03ba\u03bb edges, there is always a detection set of size <i>O</i>((\u03ba/\u03b5) log (1/\u03b5)) which can be found by random sampling. Moreover, an (\u03b5, &lambda)-detection set of minimum size (which is at most 1/\u03b5\u03b5) can be computed in polynomial time. A crucial point is that these bounds are independent not just of the size of <i>G</i> but also of the value of \u03bb.Extending these bounds to node failures is much more challenging. The most technically difficult result of this paper is that a random sample of <i>O</i>((\u03ba/\u03b5) log (1/\u03b5)) nodes is a detection set for adversaries that can delete a number of nodes up to \u03ba, the node-connectivity.For the case of edge-failures we use VC-dimension techniques and the cactus representation of all minimum edge-cuts of a graph; for node failures, we develop a novel approach for working with the much more complex set of all minimum node-cuts of a graph."
            },
            "slug": "Network-failure-detection-and-graph-connectivity-Kleinberg-Sandler",
            "title": {
                "fragments": [],
                "text": "Network failure detection and graph connectivity"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that detection set bounds can be made considerably stronger when parameterized by these connectivity values, and for an adversary that can delete \u03ba\u03bb edges, there is always a detection set of size <i>O</i>((\u03ba/\u03b5) log (1/\u03b5)) which can be found by random sampling."
            },
            "venue": {
                "fragments": [],
                "text": "SODA '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2266267"
                        ],
                        "name": "S. Mendelson",
                        "slug": "S.-Mendelson",
                        "structuredName": {
                            "firstName": "Shahar",
                            "lastName": "Mendelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mendelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28913430"
                        ],
                        "name": "P. Philips",
                        "slug": "P.-Philips",
                        "structuredName": {
                            "firstName": "Petra",
                            "lastName": "Philips",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Philips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10751914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f80da77df739fe6c346be37835d557c4eefe3ea",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been recently shown that sharp generalization bounds can be obtained when the function class from which the algorithm choo-ses its hypotheses is \u201csmall\u201d in the sense that the Rademacher averages of this function class are small [8,9]. Seemingly based on different arguments, generalization bounds were obtained in the compression scheme [7], luckiness [13], and algorithmic luckiness [6] frameworks in which the \u201csize\u201d of the function class is not specified a priori."
            },
            "slug": "Random-Subclass-Bounds-Mendelson-Philips",
            "title": {
                "fragments": [],
                "text": "Random Subclass Bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Generalization bounds were obtained in the compression scheme, luckiness, and algorithmic luckiness frameworks in which the \u201csize\u201d of the function class is not specified a priori."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880237"
                        ],
                        "name": "S. Dasgupta",
                        "slug": "S.-Dasgupta",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Dasgupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dasgupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1625660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0b4b5ce93d6b7d4644df0c5c6dad39810b6cd67",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Performance-guarantees-for-hierarchical-clustering-Dasgupta-Long",
            "title": {
                "fragments": [],
                "text": "Performance guarantees for hierarchical clustering"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679622"
                        ],
                        "name": "L. Lamport",
                        "slug": "L.-Lamport",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Lamport",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamport"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123139727,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "871cd36d5d3c265af8ca2b81e8245fd9b0fce433",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Yet, the structure of mathematical proofs has not changed in 300 years. The proofs in Newton's Przncipia differ in style from those of a modern textbook only by being written in Latin. Proofs are still written like essays, in a stilted form of ordinary prose. Formulas written in prose, like (1), are hard to understand and hard to get right. Proofs written in prose are also hard to understand and hard to get right. Anecdotal evidence suggests that as many as a third of all papers published in mathematical journals contain mistakes not just minor errors, but incorrect theorems and proofs. Statement (2) is easier to read than statement (1) for two reasons: variables are given names, and formulas are written in a more structured fashion. The benefits of using names is obvious. The benefit of structure is less obvious; we are so used to formulas like xn + yn = zn that we tend to take their structure for granted, and to think they are easy to read just because they are short. Although the brevity of the formula helps, it is primarily its structure that makes it easier to understand than a prose version. The expression"
            },
            "slug": "How-to-Write-a-Proof-Lamport",
            "title": {
                "fragments": [],
                "text": "How to Write a Proof"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The structure of mathematical proofs has not changed in 300 years, and as many as a third of all papers published in mathematical journals contain mistakes not just minor errors, but incorrect theorems and proofs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39416713"
                        ],
                        "name": "M. Degroot",
                        "slug": "M.-Degroot",
                        "structuredName": {
                            "firstName": "Morris",
                            "lastName": "Degroot",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Degroot"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119884967,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81c8a5823de21d98ea395081cbfe647bfb456cd6",
            "isKey": false,
            "numCitedBy": 4235,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword.Preface.PART ONE. SURVEY OF PROBABILITY THEORY.Chapter 1. Introduction.Chapter 2. Experiments, Sample Spaces, and Probability.2.1 Experiments and Sample Spaces.2.2 Set Theory.2.3 Events and Probability.2.4 Conditional Probability.2.5 Binomial Coefficients.Exercises.Chapter 3. Random Variables, Random Vectors, and Distributions Functions.3.1 Random Variables and Their Distributions.3.2 Multivariate Distributions.3.3 Sums and Integrals.3.4 Marginal Distributions and Independence.3.5 Vectors and Matrices.3.6 Expectations, Moments, and Characteristic Functions.3.7 Transformations of Random Variables.3.8 Conditional Distributions.Exercises.Chapter 4. Some Special Univariate Distributions.4.1 Introduction.4.2 The Bernoulli Distributions.4.3 The Binomial Distribution.4.4 The Poisson Distribution.4.5 The Negative Binomial Distribution.4.6 The Hypergeometric Distribution.4.7 The Normal Distribution.4.8 The Gamma Distribution.4.9 The Beta Distribution.4.10 The Uniform Distribution.4.11 The Pareto Distribution.4.12 The t Distribution.4.13 The F Distribution.Exercises.Chapter 5. Some Special Multivariate Distributions.5.1 Introduction.5.2 The Multinomial Distribution.5.3 The Dirichlet Distribution.5.4 The Multivariate Normal Distribution.5.5 The Wishart Distribution.5.6 The Multivariate t Distribution.5.7 The Bilateral Bivariate Pareto Distribution.Exercises.PART TWO. SUBJECTIVE PROBABILITY AND UTILITY.Chapter 6. Subjective Probability.6.1 Introduction.6.2 Relative Likelihood.6.3 The Auxiliary Experiment.6.4 Construction of the Probability Distribution.6.5 Verification of the Properties of a Probability Distribution.6.6 Conditional Likelihoods.Exercises.Chapter 7. Utility.7.1 Preferences Among Rewards.7.2 Preferences Among Probability Distributions.7.3 The Definitions of a Utility Function.7.4 Some Properties of Utility Functions.7.5 The Utility of Monetary Rewards.7.6 Convex and Concave Utility Functions.7.7 The Anxiomatic Development of Utility.7.8 Construction of the Utility Function.7.9 Verification of the Properties of a Utility Function.7.10 Extension of the Properties of a Utility Function to the Class ?E.Exercises.PART THREE. STATISTICAL DECISION PROBLEMS.Chapter 8. Decision Problems.8.1 Elements of a Decision Problem.8.2 Bayes Risk and Bayes Decisions.8.3 Nonnegative Loss Functions.8.4 Concavity of the Bayes Risk.8.5 Randomization and Mixed Decisions.8.6 Convex Sets.8.7 Decision Problems in Which ~2 and D Are Finite.8.8 Decision Problems with Observations.8.9 Construction of Bayes Decision Functions.8.10 The Cost of Observation.8.11 Statistical Decision Problems in Which Both ? and D contains Two Points.8.12 Computation of the Posterior Distribution When the Observations Are Made in More Than One Stage.Exercises.Chapter 9. Conjugate Prior Distributions.9.1 Sufficient Statistics.9.2 Conjugate Families of Distributions.9.3 Construction of the Conjugate Family.9.4 Conjugate Families for Samples from Various Standard Distributions.9.5 Conjugate Families for Samples from a Normal Distribution.9.6 Sampling from a Normal Distribution with Unknown Mean and Unknown Precision.9.7 Sampling from a Uniform Distribution.9.8 A Conjugate Family for Multinomial Observations.9.9 Conjugate Families for Samples from a Multivariate Normal Distribution.9.10 Multivariate Normal Distributions with Unknown Mean Vector and Unknown Precision matrix.9.11 The Marginal Distribution of the Mean Vector.9.12 The Distribution of a Correlation.9.13 Precision Matrices Having an Unknown Factor.Exercises.Chapter 10. Limiting Posterior Distributions.10.1 Improper Prior Distributions.10.2 Improper Prior Distributions for Samples from a Normal Distribution.10.3 Improper Prior Distributions for Samples from a Multivariate Normal Distribution.10.4 Precise Measurement.10.5 Convergence of Posterior Distributions.10.6 Supercontinuity.10.7 Solutions of the Likelihood Equation.10.8 Convergence of Supercontinuous Functions.10.9 Limiting Properties of the Likelihood Function.10.10 Normal Approximation to the Posterior Distribution.10.11 Approximation for Vector Parameters.10.12 Posterior Ratios.Exercises.Chapter 11. Estimation, Testing Hypotheses, and linear Statistical Models.11.1 Estimation.11.2 Quadratic Loss.11.3 Loss Proportional to the Absolute Value of the Error.11.4 Estimation of a Vector.11.5 Problems of Testing Hypotheses.11.6 Testing a Simple Hypothesis About the Mean of a Normal Distribution.11.7 Testing Hypotheses about the Mean of a Normal Distribution.11.8 Deciding Whether a Parameter Is Smaller or larger Than a Specific Value.11.9 Deciding Whether the Mean of a Normal Distribution Is Smaller or larger Than a Specific Value.11.10 Linear Models.11.11 Testing Hypotheses in Linear Models.11.12 Investigating the Hypothesis That Certain Regression Coefficients Vanish.11.13 One-Way Analysis of Variance.Exercises.PART FOUR. SEQUENTIAL DECISIONS.Chapter 12. Sequential Sampling.12.1 Gains from Sequential Sampling.12.2 Sequential Decision Procedures.12.3 The Risk of a Sequential Decision Procedure.12.4 Backward Induction.12.5 Optimal Bounded Sequential Decision procedures.12.6 Illustrative Examples.12.7 Unbounded Sequential Decision Procedures.12.8 Regular Sequential Decision Procedures.12.9 Existence of an Optimal Procedure.12.10 Approximating an Optimal Procedure by Bounded Procedures.12.11 Regions for Continuing or Terminating Sampling.12.12 The Functional Equation.12.13 Approximations and Bounds for the Bayes Risk.12.14 The Sequential Probability-ratio Test.12.15 Characteristics of Sequential Probability-ratio Tests.12.16 Approximating the Expected Number of Observations.Exercises.Chapter 13. Optimal Stopping.13.1 Introduction.13.2 The Statistician's Reward.13.3 Choice of the Utility Function.13.4 Sampling Without Recall.13.5 Further Problems of Sampling with Recall and Sampling without Recall.13.6 Sampling without Recall from a Normal Distribution with Unknown Mean.13.7 Sampling with Recall from a Normal Distribution with Unknown Mean.13.8 Existence of Optimal Stopping Rules.13.9 Existence of Optimal Stopping Rules for Problems of Sampling with Recall and Sampling without Recall.13.10 Martingales.13.11 Stopping Rules for Martingales.13.12 Uniformly Integrable Sequences of Random Variables.13.13 Martingales Formed from Sums and Products of Random Variables.13.14 Regular Supermartingales.13.15 Supermartingales and General Problems of Optimal Stopping.13.16 Markov Processes.13.17 Stationary Stopping Rules for Markov Processes.13.18 Entrance-fee Problems.13.19 The Functional Equation for a Markov Process.Exercises.Chapter 14. Sequential Choice of Experiments.14.1 Introduction.14.2 Markovian Decision Processes with a Finite Number of Stages.14.3 Markovian Decision Processes with an Infinite Number of Stages.14.4 Some Betting Problems.14.5 Two-armed-bandit Problems.14.6 Two-armed-bandit Problems When the Value of One Parameter Is Known.14.7 Two-armed-bandit Problems When the Parameters Are Dependent.14.8 Inventory Problems.14.9 Inventory Problems with an Infinite Number of Stages.14.10 Control Problems.14.11 Optimal Control When the Process Cannot Be Observed without Error.14.12 Multidimensional Control Problems.14.13 Control Problems with Actuation Errors.14.14 Search Problems.14.15 Search Problems with Equal Costs.14.16 Uncertainty Functions and Statistical Decision Problems.14.17 Sufficient Experiments.14.18 Examples of Sufficient Experiments.Exercises.References.Supplementary Bibliography.Name Index.Subject Index."
            },
            "slug": "Optimal-Statistical-Decisions-Degroot",
            "title": {
                "fragments": [],
                "text": "Optimal Statistical Decisions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836534"
                        ],
                        "name": "L. Hagen",
                        "slug": "L.-Hagen",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Hagen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hagen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730778"
                        ],
                        "name": "A. Kahng",
                        "slug": "A.-Kahng",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Kahng",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kahng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17757903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36278bf6919c6dced7d16dc0c02d725e1ed178f8",
            "isKey": false,
            "numCitedBy": 1153,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Partitioning of circuit netlists in VLSI design is considered. It is shown that the second smallest eigenvalue of a matrix derived from the netlist gives a provably good approximation of the optimal ratio cut partition cost. It is also demonstrated that fast Lanczos-type methods for the sparse symmetric eigenvalue problem are a robust basis for computing heuristic ratio cuts based on the eigenvector of this second eigenvalue. Effective clustering methods are an immediate by-product of the second eigenvector computation and are very successful on the difficult input classes proposed in the CAD literature. The intersection graph representation of the circuit netlist is considered, as a basis for partitioning, a heuristic based on spectral ratio cut partitioning of the netlist intersection graph is proposed. The partitioning heuristics were tested on industry benchmark suites, and the results were good in terms of both solution quality and runtime. Several types of algorithmic speedups and directions for future work are discussed. >"
            },
            "slug": "New-spectral-methods-for-ratio-cut-partitioning-and-Hagen-Kahng",
            "title": {
                "fragments": [],
                "text": "New spectral methods for ratio cut partitioning and clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "It is shown that the second smallest eigenvalue of a matrix derived from the netlist gives a provably good approximation of the optimal ratio cut partition cost."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Comput. Aided Des. Integr. Circuits Syst."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805342"
                        ],
                        "name": "R. Blahut",
                        "slug": "R.-Blahut",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Blahut",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Blahut"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18060654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90507f43e958c59ccea41aafd06ab2728749429c",
            "isKey": false,
            "numCitedBy": 1259,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "By defining mutual information as a maximum over an appropriate space, channel capacities can be defined as double maxima and rate-distortion functions as double minima. This approach yields valuable new insights regarding the computation of channel capacities and rate-distortion functions. In particular, it suggests a simple algorithm for computing channel capacity that consists of a mapping from the set of channel input probability vectors into itself such that the sequence of probability vectors generated by successive applications of the mapping converges to the vector that achieves the capacity of the given channel. Analogous algorithms then are provided for computing rate-distortion functions and constrained channel capacities. The algorithms apply both to discrete and to continuous alphabet channels or sources. In addition, a formalization of the theory of channel capacity in the presence of constraints is included. Among the examples is the calculation of close upper and lower bounds to the rate-distortion function of a binary symmetric Markov source."
            },
            "slug": "Computation-of-channel-capacity-and-rate-distortion-Blahut",
            "title": {
                "fragments": [],
                "text": "Computation of channel capacity and rate-distortion functions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple algorithm for computing channel capacity is suggested that consists of a mapping from the set of channel input probability vectors into itself such that the sequence of probability vectors generated by successive applications of the mapping converges to the vector that achieves the capacity of the given channel."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f607f03272e4d62708f5b2441355f9e005cb452",
            "isKey": false,
            "numCitedBy": 38725,
            "numCiting": 276,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics."
            },
            "slug": "Convex-Optimization-Boyd-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Convex Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive introduction to the subject of convex optimization shows in detail how such problems can be solved numerically with great efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145615522"
                        ],
                        "name": "G. D. Murray",
                        "slug": "G.-D.-Murray",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Murray",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. D. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742419"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115627924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5138d649e77448c56319cc8f1b347bdc42afb29d",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of estimating density functions using data from different distributions, and a mixture of them, is considered. Maximum likelihood and Bayesian parametric techniques are summarized and various approaches using distribution\u2010free kernel methods are expounded. A comparative study is made using the halibut data of Hosmer (1973) and the problem of incomplete data is briefly discussed."
            },
            "slug": "Estimation-Problems-with-Data-from-a-Mixture-Murray-Titterington",
            "title": {
                "fragments": [],
                "text": "Estimation Problems with Data from a Mixture"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The problem of estimating density functions using data from different distributions, and a mixture of them, is considered and maximum likelihood and Bayesian parametric techniques are summarized and various approaches using distribution\u2010free kernel methods are expounded."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32800624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44e915a220ce74badf755aae870fa0b69ee2b82a",
            "isKey": false,
            "numCitedBy": 2252,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents."
            },
            "slug": "Naive-(Bayes)-at-Forty:-The-Independence-Assumption-Lewis",
            "title": {
                "fragments": [],
                "text": "Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval, and some of the variations used for text retrieval and classification are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5118862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72a7e7bc1911b6a327c4614553bfcde98194d4ef",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new algorithm suitable for matching discrete objects such as strings and trees in linear time, thus obviating dynamic programming with quadratic time complexity. Furthermore, prediction cost in many cases can be reduced to linear cost in the length of the sequence to be classified, regardless of the number of support vectors. This improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner."
            },
            "slug": "Fast-Kernels-for-String-and-Tree-Matching-Vishwanathan-Smola",
            "title": {
                "fragments": [],
                "text": "Fast Kernels for String and Tree Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A new algorithm suitable for matching discrete objects such as strings and trees in linear time is presented, thus obviating dynamic programming with quadratic time complexity and improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14427019,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "90e2534ce964836613eb3d3e9661b478b9005689",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We point out that a number of standard sample complexity bounds (VC-dimension, PAC-Bayes, and others) are all related to the number of bits required to communicate the labels given the unlabeled data for a natural communication game. Motivated by this observation, we give a general sample complexity bound based on this game that allows us to unify these different bounds in one common framework."
            },
            "slug": "PAC-MDL-Bounds-Blum-Langford",
            "title": {
                "fragments": [],
                "text": "PAC-MDL Bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A general sample complexity bound is given based on this game that allows us to unify these different bounds in one common framework."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7326173,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "60de4b6068407defa3c88f5feeb8b74d8e55fe9c",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators."
            },
            "slug": "Kernels-and-Regularization-on-Graphs-Smola-Kondor",
            "title": {
                "fragments": [],
                "text": "Kernels and Regularization on Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators and can be found as a special case of the reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125105198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e003f0a280275de163269d32046950ad37aa37f0",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction: Linear Methods using Kernel function, Applying Linear Methods to Structured Objects, Conditional Symmetric Independence Kernels, Pair Hidden Markov Models, Conditionally Symmetrically Independent PHMMs, Conclusion"
            },
            "slug": "Dynamic-Alignment-Kernels-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Dynamic Alignment Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction: Linear Methods using Kernel function, Applying Linear Methods to Structured Objects, Conditional Symmetric Independence Kernels, Pair Hidden Markov Models, Conditionally Symmetrically Independent PHMMs, Conclusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776520"
                        ],
                        "name": "Benno Schwikowski",
                        "slug": "Benno-Schwikowski",
                        "structuredName": {
                            "firstName": "Benno",
                            "lastName": "Schwikowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benno Schwikowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141737"
                        ],
                        "name": "P. Uetz",
                        "slug": "P.-Uetz",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Uetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Uetz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145280573"
                        ],
                        "name": "S. Fields",
                        "slug": "S.-Fields",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Fields",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fields"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3009359,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "85308a6a42a53dac5776a5709f00eb14d381dcd5",
            "isKey": false,
            "numCitedBy": 1340,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A global analysis of 2,709 published interactions between proteins of the yeast Saccharomyces cerevisiae has been performed, enabling the establishment of a single large network of 2,358 interactions among 1,548 proteins. Proteins of known function and cellular location tend to cluster together, with 63% of the interactions occurring between proteins with a common functional assignment and 76% occurring between proteins found in the same subcellular compartment. Possible functions can be assigned to a protein based on the known functions of its interacting partners. This approach correctly predicts a functional category for 72% of the 1,393 characterized proteins with at least one partner of known function, and has been applied to predict functions for 364 previously uncharacterized proteins."
            },
            "slug": "A-network-of-protein\u2013protein-interactions-in-yeast-Schwikowski-Uetz",
            "title": {
                "fragments": [],
                "text": "A network of protein\u2013protein interactions in yeast"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This approach correctly predicts a functional category for 72% of the 1,393 characterized proteins with at least one partner of known function, and has been applied to predict functions for 364 previously uncharacterized proteins."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Biotechnology"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991343"
                        ],
                        "name": "E. Segal",
                        "slug": "E.-Segal",
                        "structuredName": {
                            "firstName": "Eran",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Segal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34770441"
                        ],
                        "name": "M. Shapira",
                        "slug": "M.-Shapira",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Shapira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shapira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144416712"
                        ],
                        "name": "A. Regev",
                        "slug": "A.-Regev",
                        "structuredName": {
                            "firstName": "Aviv",
                            "lastName": "Regev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Regev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397424343"
                        ],
                        "name": "D. Pe\u2019er",
                        "slug": "D.-Pe\u2019er",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Pe\u2019er",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pe\u2019er"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149626"
                        ],
                        "name": "D. Botstein",
                        "slug": "D.-Botstein",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Botstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Botstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6146032,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8bf6c7eef879565d2416a730e26c24536bf25a34",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of a cell's activity is organized as a network of interacting modules: sets of genes coregulated to respond to different conditions. We present a probabilistic method for identifying regulatory modules from gene expression data. Our procedure identifies modules of coregulated genes, their regulators and the conditions under which regulation occurs, generating testable hypotheses in the form 'regulator X regulates module Y under conditions W'. We applied the method to a Saccharomyces cerevisiae expression data set, showing its ability to identify functionally coherent modules and their correct regulators. We present microarray experiments supporting three novel predictions, suggesting regulatory roles for previously uncharacterized proteins."
            },
            "slug": "Module-networks:-identifying-regulatory-modules-and-Segal-Shapira",
            "title": {
                "fragments": [],
                "text": "Module networks: identifying regulatory modules and their condition-specific regulators from gene expression data"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The procedure identifies modules of coregulated genes, their regulators and the conditions under which regulation occurs, generating testable hypotheses in the form 'regulator X regulates module Y under conditions W'."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Genetics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742419"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59699866,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "15cf26b46bb92cafa4f78c13de30c1bc7328ebe5",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A Bayesian approach is made to the problem of using individuals of unconfirmed categories to provide information supplementary to a basic data bank of categorized observations. The exact analysis is briefly presented, followed by suggestions for more practicable approximate procedures, which are applied to examples involving medical and simulated data. The general conclusion is that the discriminatory performance of the data bank can be usefully improved by making use of uncategorized observations."
            },
            "slug": "Updating-a-Diagnostic-System-using-Unconfirmed-Titterington",
            "title": {
                "fragments": [],
                "text": "Updating a Diagnostic System using Unconfirmed Cases"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The general conclusion is that the discriminatory performance of the data bank can be usefully improved by making use of uncategorized observations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30883040"
                        ],
                        "name": "S. Ganesalingam",
                        "slug": "S.-Ganesalingam",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Ganesalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ganesalingam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120781994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ae7e5e395cb316f2aac15b743bc91e95ea7a2f89",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "An investigation is undertaken of the performance of the linear discriminant function estimated from a mixture of two multivariate normal populations with a common covariance matrix when the total number of observations available is small. It is concluded from a series of simulation experiments that although the individual estimates of the discriminant function coefficients so obtained may not be very reliable the resulting discriminant function still provides adequate separation between the populations."
            },
            "slug": "Small-sample-results-for-a-linear-discriminant-from-Ganesalingam-McLachlan",
            "title": {
                "fragments": [],
                "text": "Small sample results for a linear discriminant function estimated from a mixture of normal populations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696575"
                        ],
                        "name": "J. Bentley",
                        "slug": "J.-Bentley",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Bentley",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bentley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236311"
                        ],
                        "name": "R. Finkel",
                        "slug": "R.-Finkel",
                        "structuredName": {
                            "firstName": "Raphael",
                            "lastName": "Finkel",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Finkel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10811510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab3c73f1b2140231b98944c720100b356d91b28",
            "isKey": false,
            "numCitedBy": 2965,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional to logN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods."
            },
            "slug": "An-Algorithm-for-Finding-Best-Matches-in-Expected-Friedman-Bentley",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Finding Best Matches in Logarithmic Expected Time"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": false,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865983"
                        ],
                        "name": "T. Allen",
                        "slug": "T.-Allen",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Allen",
                            "middleNames": [
                                "Karl",
                                "van."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686063"
                        ],
                        "name": "R. Greiner",
                        "slug": "R.-Greiner",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Greiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Greiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12724557,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "d9410c55149272b1f0b714666ab06f290454ea65",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "\u009b\u009d\u009cr\u009e`\u009fM C\u00a1 \u00a2n\u009f \u00a3mq*tXuwz^\u0080ZzC\u0082\u00a4v\u0081\u00a5Cq\u00a6\u0093Cq snq z^\u0093Mq*z^\u0084 \u008b\u00a7\u0089 \u0308vwu\u0081\u007f^\u0084 v\u0081\u007fCuwq\u0087{ |\u00a9t a>\u00ab t\u00ac\u008bXqg\u0089 \u0308\u0080\u00adt z\\\u00ae \u0091nq \u0090b\u0080bq |azCq vr\u0080bz \u0304\u0088X{X\u0090b\u0088Xqg\u0089 tyv\u0081u\u0081t`\u0093Mq \u00b0\u00b1{ 23\u0091\\q \u00b0 v\u008c \u0301~q*q z \u0089 \u0308\u0080bxysC\u0090b\u0080b\u0084 \u0080ov\u008c\u008b\u03bct z^\u0093\u03bc\u0082X{ \u0304{M\u0093MzCqg\u0089w\u0089y{X|r\u00b6^v vw{\u00b7v\u0081\u00a5Cq vwu\u008at \u0080bzC\u0080Zz^\u0082,\u0093Ct v\u0081tC \u0327\u00a713q\u03bc\u0093Cq*\u0089\u0081\u0084 u\u0081\u0080Z\u0091nq#v\u0081\u00a5Cqou\u0081q*\u0089w\u007fC\u0090Zv\u0081\u0089\u00bb{ | t z5q xysC\u0080buw\u0080\u00ad\u0084 tX\u00908\u0084 {`x s\\t u\u0081\u0080b\u0089w{Xz1\u20444{ | vw\u00a5Cu\u0081q q#\u0089 \u0308v\u0081tXz^\u0093CtXu\u0081\u0093 x {M\u0093Mq*\u0090o\u0089wq \u0090bq*\u0084 vw\u0080b{Xz-\u0084 u\u0081\u0080ov\u0081q u\u0081\u0080bt3\u204441\u20442 \u0088J\u0080b\u00bfX \u0327b\u0085Et\"\u00c0 \u0080bzC\u0080Z\u00b0 x<\u007fCx\u00c1prq*\u0089\u0081\u0084 u\u0081\u0080ZsCvw\u0080b{Xz\u00c2\u00a3Oq zC\u0082Xvw\u00a5\u00c2\u0084 uw\u0080Zvwq*uw\u0080b{Xz a \u00c0\u00c3p\u00c4\u00a3f\u00ae \u0085 \u008e \u00c5\u00c6t \u0080b\u00c5Xq`\u00c7 \u0089$\u00c8\u00c9zC|\u00ca{Xu\u0081xyt vw\u0080b{Xz\u00cb}~u\u0081\u0080ov\u0081q u\u0081\u0080Z{`z a \u008e \u00c8w}.\u00ae$tXz^\u0093\u00b7t }~uw{J\u0089w\u0089 \u0308\u00b0\u00cd\u00cc tX\u0090Z\u0080\u00ad\u0093Ct vw\u0080b{Xz$\u0084 u\u0081\u0080ov\u0081q u\u0081\u0080Z{`z a\u00ca\u00ce \u00cc\u00c4\u00aem1\u20442\u00cft sCs^\u0090Z\u0080bq*\u0093\u00d0v\u0081{ vw\u00a5C\u0080\u00ad\u0089$sCu\u0081{X\u0091C\u0090bq x& \u0327G\u00d1r\u007fCu<uwqg\u0089 \u0308\u007fC\u0090Zv\u0081\u0089$\u0089w\u007fC\u0082`\u0082Xq*\u0089 \u0308v\u00d0vw\u00a5^t v<\u008e \u00c8w} t z^\u0093 \u00ce \u00cc3\u20444tXuwq \u0091n{ v\u0081\u00a5\u00cb\u0082`{J{M\u0093\u00d2\u0084 uw\u0080Zvwq*uw\u0080\u00adt\u00d3|\u00ca{Xuyt\u00ac\u0088X{`\u0080b\u0093M\u0080bzC\u0082 {\u00c6\u0088Xq uw\u00b6Cvwvw\u0080bzC\u0082^\u0085 \u0091C\u007fMvy\u00c0\u00c3pr\u00a3\u00d4\u0093M{ \u0304q*\u00898z^{ v \u0301 {Xu\u0081\u00c5\u00d5 \u0301 q \u0090b\u0090a\u0080bz vw\u00a5C\u0080\u00ad\u0089 \u0084 {`zJvwq \u00d6 \u0304vg \u0327"
            },
            "slug": "Model-Selection-Criteria-for-Learning-Belief-Nets:-Allen-Greiner",
            "title": {
                "fragments": [],
                "text": "Model Selection Criteria for Learning Belief Nets: An Empirical Comparison"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544559"
                        ],
                        "name": "C. V. Mering",
                        "slug": "C.-V.-Mering",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Mering",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. V. Mering"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47370303"
                        ],
                        "name": "R. Krause",
                        "slug": "R.-Krause",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2536413"
                        ],
                        "name": "B. Snel",
                        "slug": "B.-Snel",
                        "structuredName": {
                            "firstName": "Berend",
                            "lastName": "Snel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Snel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35293180"
                        ],
                        "name": "M. Cornell",
                        "slug": "M.-Cornell",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cornell",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cornell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2409818"
                        ],
                        "name": "S. Oliver",
                        "slug": "S.-Oliver",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Oliver",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Oliver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145280573"
                        ],
                        "name": "S. Fields",
                        "slug": "S.-Fields",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Fields",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fields"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3534315"
                        ],
                        "name": "P. Bork",
                        "slug": "P.-Bork",
                        "structuredName": {
                            "firstName": "Peer",
                            "lastName": "Bork",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4419762,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8c48222c90bbeff0bcddad3cea75e4581167a190",
            "isKey": false,
            "numCitedBy": 2379,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Comprehensive protein\u2013protein interaction maps promise to reveal many aspects of the complex regulatory network underlying cellular function. Recently, large-scale approaches have predicted many new protein interactions in yeast. To measure their accuracy and potential as well as to identify biases, strengths and weaknesses, we compare the methods with each other and with a reference set of previously reported protein interactions."
            },
            "slug": "Comparative-assessment-of-large-scale-data-sets-of-Mering-Krause",
            "title": {
                "fragments": [],
                "text": "Comparative assessment of large-scale data sets of protein\u2013protein interactions"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "Comprehensive protein\u2013protein interaction maps promise to reveal many aspects of the complex regulatory network underlying cellular function and are compared with each other and with a reference set of previously reported protein interactions."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100694753"
                        ],
                        "name": "R. Strichartz",
                        "slug": "R.-Strichartz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Strichartz",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Strichartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116926047,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5838073d64cee225489f1a95a19f084065658798",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Way of Analysis gives a thorough account of real analysis in one or several variables, from the construction of the real number system to an introduction of the Lebesgue integral. The text provides proofs of all main results, as well as motivations, examples, applications, exercises, and formal chapter summaries. Additionally, there are three chapters on application of analysis, ordinary differential equations, Fourier series, and curves and surfaces to show how the techniques of analysis are used in concrete settings."
            },
            "slug": "The-way-of-analysis-Strichartz",
            "title": {
                "fragments": [],
                "text": "The way of analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735221"
                        ],
                        "name": "N. Ueda",
                        "slug": "N.-Ueda",
                        "structuredName": {
                            "firstName": "Naonori",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763517"
                        ],
                        "name": "R. Nakano",
                        "slug": "R.-Nakano",
                        "structuredName": {
                            "firstName": "Ryohei",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nakano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 93554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad1244aaf43c469bf6143f540fa4a177a49de7e2",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a deterministic annealing variant of the EM algorithm for maximum likelihood parameter estimation problems. In our approach, the EM process is reformulated as the problem of minimizing the thermodynamic free energy by using the principle of maximum entropy and statistical mechanics analogy. Unlike simulated annealing approaches, this minimization is deterministically performed. Moreover, the derived algorithm, unlike the conventional EM algorithm, can obtain better estimates free of the initial parameter values."
            },
            "slug": "Deterministic-Annealing-Variant-of-the-EM-Algorithm-Ueda-Nakano",
            "title": {
                "fragments": [],
                "text": "Deterministic Annealing Variant of the EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This work presents a deterministic annealing variant of the EM algorithm for maximum likelihood parameter estimation problems, reformulated as the problem of minimizing the thermodynamic free energy by using the principle of maximum entropy and statistical mechanics analogy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792144"
                        ],
                        "name": "J. Sturm",
                        "slug": "J.-Sturm",
                        "structuredName": {
                            "firstName": "Jos",
                            "lastName": "Sturm",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sturm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909395"
                        ],
                        "name": "Gerardo A. Guerra",
                        "slug": "Gerardo-A.-Guerra",
                        "structuredName": {
                            "firstName": "Gerardo",
                            "lastName": "Guerra",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerardo A. Guerra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17004369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2ad732c6bc4916c63c9c80eb5cdeb0066bd68b1",
            "isKey": false,
            "numCitedBy": 7068,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "SeDuMi is an add-on for MATLAB, which lets you solve optimization problems with linear, quadratic and semidefiniteness constraints. It is possible to have complex valued data and variables in SeDuMi. Moreover, large scale optimization problems are solved efficiently, by exploiting sparsity. This paper describes how to work with this toolbox."
            },
            "slug": "A-Matlab-toolbox-for-optimization-over-symmetric-Sturm-Guerra",
            "title": {
                "fragments": [],
                "text": "A Matlab toolbox for optimization over symmetric cones"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper describes how to work with SeDuMi, an add-on for MATLAB, which lets you solve optimization problems with linear, quadratic and semidefiniteness constraints by exploiting sparsity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46753437"
                        ],
                        "name": "U. Vazirani",
                        "slug": "U.-Vazirani",
                        "structuredName": {
                            "firstName": "Umesh",
                            "lastName": "Vazirani",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Vazirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44944785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97e14147f2e61456bba016f720488410393f9e48",
            "isKey": false,
            "numCitedBy": 1786,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata by experimentation appendix - some tools for probabilistic analysis."
            },
            "slug": "An-Introduction-to-Computational-Learning-Theory-Kearns-Vazirani",
            "title": {
                "fragments": [],
                "text": "An Introduction to Computational Learning Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143622829"
                        ],
                        "name": "C. Schaffer",
                        "slug": "C.-Schaffer",
                        "structuredName": {
                            "firstName": "Cullen",
                            "lastName": "Schaffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schaffer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46432355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9887e9a7e97cb55b344c1e391d4d04ec1f129b19",
            "isKey": false,
            "numCitedBy": 424,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Conservation-Law-for-Generalization-Performance-Schaffer",
            "title": {
                "fragments": [],
                "text": "A Conservation Law for Generalization Performance"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117118"
                        ],
                        "name": "S. Sperlich",
                        "slug": "S.-Sperlich",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Sperlich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sperlich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69932841"
                        ],
                        "name": "J. Zelinka",
                        "slug": "J.-Zelinka",
                        "structuredName": {
                            "firstName": "Ji\u00e9r\u00ed",
                            "lastName": "Zelinka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelinka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61047344,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "87083d0fd9fb1af9c7171b26c1653f835b2bb9a3",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In Chapter 8 we discussed additive models (AM) of the form \n \n$$ E(Y|X) = c + \\sum\\limits_{\\alpha = 1}^d {g_\\alpha (x_\\alpha )} . $$ \n \n(1) \n \nNote that we put EY = c and E(g \u03b1 (X \u03b1 ) = 0 for identification."
            },
            "slug": "Generalized-Additive-Models-Sperlich-Zelinka",
            "title": {
                "fragments": [],
                "text": "Generalized Additive Models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In Chapter 8 the authors discussed additive models (AM) of the form E(Y|X) = c + \\sum\\limits_{alpha = 1}^d {g_alpha (x_\\alpha )} ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 174
                            }
                        ],
                        "text": "The likelihood of the model is then maximized using the labeled and unlabeled data with the help of an iterative algorithm such as the expectationmaximization (EM) algorithm [Dempster et al., 1977]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500274"
                        ],
                        "name": "D. Gottlieb",
                        "slug": "D.-Gottlieb",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gottlieb",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gottlieb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347536"
                        ],
                        "name": "S. Orszag",
                        "slug": "S.-Orszag",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Orszag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Orszag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48897539"
                        ],
                        "name": "P. J. Huber",
                        "slug": "P.-J.-Huber",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Huber",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. J. Huber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2175970"
                        ],
                        "name": "F. Roberts",
                        "slug": "F.-Roberts",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Roberts",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Roberts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 63602833,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ab91f8b2372ec432d8f93c86b545cd9729446291",
            "isKey": false,
            "numCitedBy": 1583,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral Methods in Fluid DynamicsNumerical Methods for Partial Differential EquationsNumerical Analysis of Partial Differential EquationsNumerical analysis of spectral methods : theory and applicationsSpectral Methods And Their ApplicationsA Brief Introduction to Numerical AnalysisA First Course in the Numerical Analysis of Differential Equations South Asian EditionConvergence of Spectral Methods for Hyperbolic Initial-boundary Value SystemsReview of Some Approximation Operators for the Numerical Analysis of Spectral MethodsSpectral Methods in MATLABA Modified Spectral Method in Phase SpaceThe Birth of Numerical AnalysisSpectral Methods for Non-Standard Eigenvalue ProblemsPartial Differential EquationsNumerical Analysis of Spectral MethodsNumerical Analysis of Partial Differential Equations Using Maple and MATLABSpectral MethodsSpectral Methods for NonStandard Eigenvalue ProblemsAn Introduction to the Numerical Analysis of Spectral MethodsSpectral Methods in Time for Parabolic ProblemsSpectral Methods in Chemistry and PhysicsA First Course in the Numerical Analysis of Differential Equations South Asian EditionSummary of Research in Applied Mathematics, Numerical Analysis and Computer Science at the Institute for Computer Applications in Science and EngineeringNumerical AnalysisSpectral Methods for Compressible Flow ProblemsA First Course in the Numerical Analysis of Differential EquationsSummary of Research in Applied Mathematics, Numerical Analysis, and Computer SciencesA Theoretical Introduction to Numerical AnalysisNumerical AnalysisRiemann-Hilbert Problems, Their Numerical Solution, and the Computation of Nonlinear Special FunctionsSpectral MethodsSpectral Methods for Uncertainty QuantificationSpectral Methods and Their ApplicationsNumerical Analysis of Spectral Methods: Theory and ApplicatonsSpectral Methods for Incompressible Viscous FlowAdvances in Numerical Analysis: Nonlinear partial differential equations and dynamical systemsSpectral Methods Using Multivariate Polynomials on the Unit BallA First Course in the Numerical Analysis of Differential EquationsFundamentals of Engineering Numerical AnalysisSpectral Methods for Time-Dependent Problems"
            },
            "slug": "CBMS-NSF-REGIONAL-CONFERENCE-SERIES-IN-APPLIED-Gottlieb-Orszag",
            "title": {
                "fragments": [],
                "text": "CBMS-NSF REGIONAL CONFERENCE SERIES IN APPLIED MATHEMATICS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7883,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4043,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417095"
                        ],
                        "name": "D. Spielman",
                        "slug": "D.-Spielman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Spielman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spielman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461956"
                        ],
                        "name": "S. Teng",
                        "slug": "S.-Teng",
                        "structuredName": {
                            "firstName": "Shang-Hua",
                            "lastName": "Teng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2323676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "407b1ad9d0dbcef919a0c4624b65323cafcb5f11",
            "isKey": false,
            "numCitedBy": 871,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We present algorithms for solving symmetric, diagonally-dominant linear systems to accuracy \u03b5 in time linear in their number of non-zeros and log (\u03baf (A) \u03b5), where \u03baf (A) is the condition number of the matrix defining the linear system. Our algorithm applies the preconditioned Chebyshev iteration with preconditioners designed using nearly-linear time algorithms for graph sparsification and graph partitioning."
            },
            "slug": "Nearly-linear-time-algorithms-for-graph-graph-and-Spielman-Teng",
            "title": {
                "fragments": [],
                "text": "Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper presents algorithms for solving symmetric, diagonally-dominant linear systems to accuracy \u03b5 in time linear in their number of non-zeros and log (\u03baf (A) \u03b5), where \u03b5 is the condition number of the matrix defining the linear system."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": false,
            "numCitedBy": 5073,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152240986"
                        ],
                        "name": "Tong Ihn Lee",
                        "slug": "Tong-Ihn-Lee",
                        "structuredName": {
                            "firstName": "Tong Ihn",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Ihn Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33210773"
                        ],
                        "name": "Nicola J. Rinaldi",
                        "slug": "Nicola-J.-Rinaldi",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Rinaldi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicola J. Rinaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50477806"
                        ],
                        "name": "F. Robert",
                        "slug": "F.-Robert",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Robert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Robert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393323"
                        ],
                        "name": "D. Odom",
                        "slug": "D.-Odom",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Odom",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Odom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397939637"
                        ],
                        "name": "Z. Bar-Joseph",
                        "slug": "Z.-Bar-Joseph",
                        "structuredName": {
                            "firstName": "Ziv",
                            "lastName": "Bar-Joseph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Bar-Joseph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40238354"
                        ],
                        "name": "G. Gerber",
                        "slug": "G.-Gerber",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gerber",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gerber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3607792"
                        ],
                        "name": "N. Hannett",
                        "slug": "N.-Hannett",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Hannett",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hannett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46811849"
                        ],
                        "name": "C. T. Harbison",
                        "slug": "C.-T.-Harbison",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Harbison",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. T. Harbison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071764753"
                        ],
                        "name": "C. Thompson",
                        "slug": "C.-Thompson",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Thompson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34775673"
                        ],
                        "name": "I. Simon",
                        "slug": "I.-Simon",
                        "structuredName": {
                            "firstName": "Itamar",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2927729"
                        ],
                        "name": "J. Zeitlinger",
                        "slug": "J.-Zeitlinger",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Zeitlinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zeitlinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40458090"
                        ],
                        "name": "E. Jennings",
                        "slug": "E.-Jennings",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Jennings",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jennings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47800019"
                        ],
                        "name": "H. L. Murray",
                        "slug": "H.-L.-Murray",
                        "structuredName": {
                            "firstName": "Heather",
                            "lastName": "Murray",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. L. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717100"
                        ],
                        "name": "D. B. Gordon",
                        "slug": "D.-B.-Gordon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gordon",
                            "middleNames": [
                                "Benjamin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142877618"
                        ],
                        "name": "B. Ren",
                        "slug": "B.-Ren",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1827605"
                        ],
                        "name": "J. Wyrick",
                        "slug": "J.-Wyrick",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wyrick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wyrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5627284"
                        ],
                        "name": "J. Tagne",
                        "slug": "J.-Tagne",
                        "structuredName": {
                            "firstName": "Jean-Bosco",
                            "lastName": "Tagne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tagne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799985"
                        ],
                        "name": "T. Volkert",
                        "slug": "T.-Volkert",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Volkert",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Volkert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776130"
                        ],
                        "name": "E. Fraenkel",
                        "slug": "E.-Fraenkel",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Fraenkel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fraenkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253803"
                        ],
                        "name": "D. Gifford",
                        "slug": "D.-Gifford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gifford",
                            "middleNames": [
                                "Kenneth"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gifford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144787676"
                        ],
                        "name": "R. Young",
                        "slug": "R.-Young",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Young",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4841222,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "674b70b8f5c1115ed6c79b1325fc2cbb09ab019c",
            "isKey": false,
            "numCitedBy": 3148,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "We have determined how most of the transcriptional regulators encoded in the eukaryote Saccharomyces cerevisiaeassociate with genes across the genome in living cells. Just as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. We use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. Our results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators."
            },
            "slug": "Transcriptional-Regulatory-Networks-in-cerevisiae-Lee-Rinaldi",
            "title": {
                "fragments": [],
                "text": "Transcriptional Regulatory Networks in Saccharomyces cerevisiae"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work determines how most of the transcriptional regulators encoded in the eukaryote Saccharomyces cerevisiae associate with genes across the genome in living cells, and identifies network motifs, the simplest units of network architecture, and demonstrates that an automated process can use motifs to assemble a transcriptional regulatory network structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53480826,
            "fieldsOfStudy": [
                "Mathematics",
                "Economics"
            ],
            "id": "395bc88e0c22c646acd1d95d69ccca9c03e4113d",
            "isKey": false,
            "numCitedBy": 4664,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the consequences and detection of model misspecification when using maximum likelihood techniques for estimation and inference. The quasi-maximum likelihood estimator (QMLE) converges to a well defined limit, and may or may not be consistent for particular parameters of interest. Standard tests (Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in the presence of misspecification, but more general statistics are given which allow inferences to be drawn robustly. The properties of the QMLE and the information matrix are exploited to yield several useful tests for model misspecification."
            },
            "slug": "Maximum-Likelihood-Estimation-of-Misspecified-White",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Estimation of Misspecified Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145248839"
                        ],
                        "name": "J. Heinonen",
                        "slug": "J.-Heinonen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Heinonen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heinonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101153280"
                        ],
                        "name": "T. Kilpel\u00e4inen",
                        "slug": "T.-Kilpel\u00e4inen",
                        "structuredName": {
                            "firstName": "Tero",
                            "lastName": "Kilpel\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kilpel\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69059890"
                        ],
                        "name": "O. Martio",
                        "slug": "O.-Martio",
                        "structuredName": {
                            "firstName": "Olli",
                            "lastName": "Martio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Martio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118291185,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a221de8f12c9ce1f9efd4af7c1ef4949c7192d68",
            "isKey": false,
            "numCitedBy": 1830,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction. 1: Weighted Sobolev spaces. 2: Capacity. 3: Supersolutions and the obstacle problem. 4: Refined Sobolev spaces. 5: Variational integrals. 6: A-harmonic functions. 7: A superharmonic functions. 8: Balayage. 9: Perron's method, barriers, and resolutivity. 10: Polar sets. 11: A-harmonic measure. 12: Fine topology. 13: Harmonic morphisms. 14: Quasiregular mappings. 15: Ap-weights and Jacobians of quasiconformal mappings. 16: Axiomatic nonlinear potential theory. Appendix I: The existence of solutions. Appendix II: The John-Nirenberg lemma. Bibliography. List of symbols. Index"
            },
            "slug": "Nonlinear-Potential-Theory-of-Degenerate-Elliptic-Heinonen-Kilpel\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Nonlinear Potential Theory of Degenerate Elliptic Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116158963"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Sheila",
                            "lastName": "Nayar",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58758670,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "77afac8f4d7f47c8b34371d8f8355cefbea1d4f6",
            "isKey": false,
            "numCitedBy": 2004,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Columbia Object Image Library COIL is a database of color images of objects The objects were placed on a motorized turntable against a black background The turntable was rotated through degrees to vary object pose with respect to a xed color camera Images of the objects were taken at pose intervals of degrees This corresponds to poses per object The images were size normalized COIL is available online via ftp"
            },
            "slug": "Columbia-Object-Image-Library-(COIL100)-Nayar",
            "title": {
                "fragments": [],
                "text": "Columbia Object Image Library (COIL100)"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Columbia Object Image Library COIL is a database of color images of objects that were placed on a motorized turntable against a black background and rotated through degrees to vary object pose with respect to a xed color camera."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120741100,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On demontre un theoreme fondamental qui donne une borne inferieure pour la longueur de code et donc, pour les erreurs de prediction. On definit les notions \u00abd'information a priori\u00bb et \u00abd'information utile\u00bb dans les donnees"
            },
            "slug": "Stochastic-Complexity-and-Modeling-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity and Modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2371236"
                        ],
                        "name": "B. Abboud",
                        "slug": "B.-Abboud",
                        "structuredName": {
                            "firstName": "Bouchra",
                            "lastName": "Abboud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Abboud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742818"
                        ],
                        "name": "F. Davoine",
                        "slug": "F.-Davoine",
                        "structuredName": {
                            "firstName": "Franck",
                            "lastName": "Davoine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Davoine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112620962"
                        ],
                        "name": "M. Dang",
                        "slug": "M.-Dang",
                        "structuredName": {
                            "firstName": "M\u00f4",
                            "lastName": "Dang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 178905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d2c7253faf69d8f632beea0099d1bef2475a0a4",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Facial expression interpretation, recognition and analysis is a key issue in visual communication and man to machine interaction. In this paper, we present a technique for extracting appearance parameters from a natural image or video sequence, which allow reproduction of natural looking expressive synthetic faces. This technique was used to perform face synthesis and tracking in video sequences as well as facial expression recognition and control."
            },
            "slug": "Expressive-face-recognition-and-synthesis-Abboud-Davoine",
            "title": {
                "fragments": [],
                "text": "Expressive face recognition and synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This technique was used to perform face synthesis and tracking in video sequences as well as facial expression recognition and control."
            },
            "venue": {
                "fragments": [],
                "text": "2003 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765199"
                        ],
                        "name": "Y. Saad",
                        "slug": "Y.-Saad",
                        "structuredName": {
                            "firstName": "Yousef",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Saad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26325536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd4f2ffcda1f9907936daf79ed67356b6694479a",
            "isKey": false,
            "numCitedBy": 12801,
            "numCiting": 221,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Background in linear algebra 2. Discretization of partial differential equations 3. Sparse matrices 4. Basic iterative methods 5. Projection methods 6. Krylov subspace methods Part I 7. Krylov subspace methods Part II 8. Methods related to the normal equations 9. Preconditioned iterations 10. Preconditioning techniques 11. Parallel implementations 12. Parallel preconditioners 13. Multigrid methods 14. Domain decomposition methods Bibliography Index."
            },
            "slug": "Iterative-methods-for-sparse-linear-systems-Saad",
            "title": {
                "fragments": [],
                "text": "Iterative methods for sparse linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter discusses methods related to the normal equations of linear algebra, and some of the techniques used in this chapter were derived from previous chapters of this book."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2655181"
                        ],
                        "name": "C. Mallows",
                        "slug": "C.-Mallows",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Mallows",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mallows"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30342955,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f603e9476b6613e4f36c5848902a17b3af19e1cb",
            "isKey": false,
            "numCitedBy": 1070,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the interpretation of Cp -plots and show how they can be calibrated in several ways. We comment on the practice of using the display as a basis for formal selection of a subset-regression model, and extend the range of application of the device to encompass arbitrary linear estimates of the regression coefficients, for example Ridge estimates."
            },
            "slug": "Some-Comments-on-Cp-Mallows",
            "title": {
                "fragments": [],
                "text": "Some Comments on Cp"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The practice of using the display as a basis for formal selection of a subset-regression model is commented on, and the range of application of the device is extended to encompass arbitrary linear estimates of the regression coefficients."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580190"
                        ],
                        "name": "U. Makov",
                        "slug": "U.-Makov",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Makov",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Makov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124992180,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures."
            },
            "slug": "Statistical-analysis-of-finite-mixture-Titterington-Smith",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of finite mixture distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This course discusses Mathematical Aspects of Mixtures, Sequential Problems and Procedures, and Applications of Finite Mixture Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48627980"
                        ],
                        "name": "R. Shibata",
                        "slug": "R.-Shibata",
                        "structuredName": {
                            "firstName": "Ritei",
                            "lastName": "Shibata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shibata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122369511,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "242e18e7c5bbcd1bfc63c89d9e086a7340ab4177",
            "isKey": false,
            "numCitedBy": 543,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY An asymptotically optimal selection of regression variables is proposed. The key assumption is that the number of control variables is infinite or increases with the sample size. It is also shown that Mallows's Qp, Akaike's FPE -and AIC methods are all asymptotically equivalent to this method."
            },
            "slug": "An-optimal-selection-of-regression-variables-Shibata",
            "title": {
                "fragments": [],
                "text": "An optimal selection of regression variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4231116"
                        ],
                        "name": "F. Chung",
                        "slug": "F.-Chung",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Chung",
                            "middleNames": [
                                "R.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Chung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60624922,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95d6ff6279fa0f92df6fae0e6bd4c259acfc8f09",
            "isKey": false,
            "numCitedBy": 4221,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index."
            },
            "slug": "Spectral-Graph-Theory-Chung",
            "title": {
                "fragments": [],
                "text": "Spectral Graph Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122051864"
                        ],
                        "name": "S. Boucheron",
                        "slug": "S.-Boucheron",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Boucheron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Boucheron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226094"
                        ],
                        "name": "P. Massart",
                        "slug": "P.-Massart",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Massart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Massart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121984080,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d1b54db9c96d120d74c90d4298b8598726dc6c0a",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new general concentration-of-measure inequality and illustrate its power by applications in random combinatorics. The results find direct applications in some problems of learning theory."
            },
            "slug": "A-sharp-concentration-inequality-with-application-Boucheron-Lugosi",
            "title": {
                "fragments": [],
                "text": "A sharp concentration inequality with application"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30821484"
                        ],
                        "name": "E. Nadaraya",
                        "slug": "E.-Nadaraya",
                        "structuredName": {
                            "firstName": "Elizbar",
                            "lastName": "Nadaraya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nadaraya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120067924,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "05175204318c3c01e3301fd864553071039605d2",
            "isKey": false,
            "numCitedBy": 3287,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly."
            },
            "slug": "On-Estimating-Regression-Nadaraya",
            "title": {
                "fragments": [],
                "text": "On Estimating Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69405045"
                        ],
                        "name": "H. J. Larson",
                        "slug": "H.-J.-Larson",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Larson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J. Larson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2206997"
                        ],
                        "name": "B. Shubert",
                        "slug": "B.-Shubert",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Shubert",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shubert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62155240,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9c52faa909778fad942dcd2f603e446374808d31",
            "isKey": false,
            "numCitedBy": 4617,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An electromagnetic pulse counter having successively operable, contact-operating armatures. The armatures are movable to a rest position, an intermediate position and an active position between the main pole and the secondary pole of a magnetic circuit."
            },
            "slug": "Random-variables-and-stochastic-processes-Larson-Shubert",
            "title": {
                "fragments": [],
                "text": "Random variables and stochastic processes"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "An electromagnetic pulse counter having successively operable, contact-operating armatures that are movable to a rest position, an intermediate position and an active position between the main pole and the secondary pole of a magnetic circuit."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144578381"
                        ],
                        "name": "E. M. Wright",
                        "slug": "E.-M.-Wright",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Wright",
                            "middleNames": [
                                "Maitland"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. M. Wright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 64832941,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1729f731482a628177a0fb81050966514c385e5e",
            "isKey": false,
            "numCitedBy": 2372,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The description for this book, Adaptive Control Processes: A Guided Tour, will be forthcoming."
            },
            "slug": "Adaptive-Control-Processes:-A-Guided-Tour-Wright-Bellman",
            "title": {
                "fragments": [],
                "text": "Adaptive Control Processes: A Guided Tour"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The description for this book, Adaptive Control Processes: A Guided Tour, will be forthcoming."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150947925"
                        ],
                        "name": "W. N. Wapnik",
                        "slug": "W.-N.-Wapnik",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Wapnik",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. N. Wapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150946676"
                        ],
                        "name": "A. J. Tscherwonenkis",
                        "slug": "A.-J.-Tscherwonenkis",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tscherwonenkis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Tscherwonenkis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 246218030,
            "fieldsOfStudy": [],
            "id": "406bb8942eca3b2eafe141847b76ce56d68ef5e8",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theorie-der-Zeichenerkennung-Wapnik-Tscherwonenkis",
            "title": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221248668,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79b5a846e62b66e6446b8cccf10efe64387019c6",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Kernel-methods-for-exploratory-data-analysis:-a-on-Bie-Cristianini",
            "title": {
                "fragments": [],
                "text": "Kernel methods for exploratory data analysis: a demonstration on text data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "225ca57add3b3fb12ef01cc97c4683350dc93fe4",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143628300"
                        ],
                        "name": "H. Hartley",
                        "slug": "H.-Hartley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Hartley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hartley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144288528"
                        ],
                        "name": "J. Rao",
                        "slug": "J.-Rao",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rao",
                            "middleNames": [
                                "N.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126256187,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ba4a874e78be4dc8fd658de91f8eaf9d39776226",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Classification-and-Estimation-in-Analysis-of-Hartley-Rao",
            "title": {
                "fragments": [],
                "text": "Classification and Estimation in Analysis of Variance Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216687"
                        ],
                        "name": "D. Hosmer",
                        "slug": "D.-Hosmer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hosmer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hosmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124275211,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "97096bd37f85b741268cdd526025c96dbf36bc76",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Comparison-of-Iterative-Maximum-Likelihood-of-the-Hosmer",
            "title": {
                "fragments": [],
                "text": "A Comparison of Iterative Maximum Likelihood Estimates of the Parameters of a Mixture of Two Normal Distributions Under Three Different Types of Sample"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48566708"
                        ],
                        "name": "G. S. Watson",
                        "slug": "G.-S.-Watson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Watson",
                            "middleNames": [
                                "Stuart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. S. Watson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124218927,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "14821ac1bf09890a857fca2a6c324e8c85f2c0d0",
            "isKey": false,
            "numCitedBy": 2957,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Smooth-regression-analysis-Watson",
            "title": {
                "fragments": [],
                "text": "Smooth regression analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143639508"
                        ],
                        "name": "A. Tikhonov",
                        "slug": "A.-Tikhonov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tikhonov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tikhonov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102992139"
                        ],
                        "name": "Vasiliy Yakovlevich Arsenin",
                        "slug": "Vasiliy-Yakovlevich-Arsenin",
                        "structuredName": {
                            "firstName": "Vasiliy",
                            "lastName": "Arsenin",
                            "middleNames": [
                                "Yakovlevich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasiliy Yakovlevich Arsenin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122072756,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bc14819e745cd7af37efd09ea29773dc0065119e",
            "isKey": false,
            "numCitedBy": 7884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solutions-of-ill-posed-problems-Tikhonov-Arsenin",
            "title": {
                "fragments": [],
                "text": "Solutions of ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121260138,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "eba5ff9c61e59151bf1cce02ae0b4c87afd39572",
            "isKey": false,
            "numCitedBy": 786,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-Rates-of-Convergence-for-Nonparametric-Stone",
            "title": {
                "fragments": [],
                "text": "Optimal Rates of Convergence for Nonparametric Estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30883040"
                        ],
                        "name": "S. Ganesalingam",
                        "slug": "S.-Ganesalingam",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Ganesalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ganesalingam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121947252,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "51cf5a18f2a08aa49d67183a00e2aecb822357e2",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-efficiency-of-a-linear-discriminant-function-on-Ganesalingam-McLachlan",
            "title": {
                "fragments": [],
                "text": "The efficiency of a linear discriminant function based on unclassified initial samples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144249485"
                        ],
                        "name": "L. Goldstein",
                        "slug": "L.-Goldstein",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Goldstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goldstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47362632"
                        ],
                        "name": "K. Messer",
                        "slug": "K.-Messer",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Messer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Messer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122154571,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1512e0e842aabaa9a8515c214805b8bcb0131524",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-Plug-in-Estimators-for-Nonparametric-Goldstein-Messer",
            "title": {
                "fragments": [],
                "text": "Optimal Plug-in Estimators for Nonparametric Functional Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121001834,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8b53361184ff985530d0e3f747b47b3427a65a3c",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machines-that-Learn-from-Hints-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Machines that Learn from Hints"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2408638"
                        ],
                        "name": "R. Hardt",
                        "slug": "R.-Hardt",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hardt",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hardt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465068"
                        ],
                        "name": "F. Lin",
                        "slug": "F.-Lin",
                        "structuredName": {
                            "firstName": "Fanghua",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120677986,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0ff252cd0b4c1f434f13995fe4712493b3495185",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mappings-minimizing-the-Lp-norm-of-the-gradient-Hardt-Lin",
            "title": {
                "fragments": [],
                "text": "Mappings minimizing the Lp norm of the gradient"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34286280"
                        ],
                        "name": "R. Berk",
                        "slug": "R.-Berk",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Berk",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Berk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120760010,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "39fb6eb8926c29e2a1559f724fdc1389c5b19b08",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Limiting-Behavior-of-Posterior-Distributions-when-Berk",
            "title": {
                "fragments": [],
                "text": "Limiting Behavior of Posterior Distributions when the Model is Incorrect"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405176333"
                        ],
                        "name": "Golub Gene H. Et.Al",
                        "slug": "Golub-Gene-H.-Et.Al",
                        "structuredName": {
                            "firstName": "Golub",
                            "lastName": "Et.Al",
                            "middleNames": [
                                "Gene",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Golub Gene H. Et.Al"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203043402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c5526b67530484023f29fbfa6b39cb5d841b4df",
            "isKey": false,
            "numCitedBy": 915,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-Computations,-3rd-Edition-Et.Al",
            "title": {
                "fragments": [],
                "text": "Matrix Computations, 3rd Edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3009361"
                        ],
                        "name": "M. Yamasaki",
                        "slug": "M.-Yamasaki",
                        "structuredName": {
                            "firstName": "Maretsugu",
                            "lastName": "Yamasaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yamasaki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 117997747,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2960dfa13e7f966e1cc1856896eb2c380dea3bbf",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Ideal-boundary-limit-of-discrete-Dirichlet-Yamasaki",
            "title": {
                "fragments": [],
                "text": "Ideal boundary limit of discrete Dirichlet functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145409928"
                        ],
                        "name": "J. Hammersley",
                        "slug": "J.-Hammersley",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hammersley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hammersley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145578421"
                        ],
                        "name": "P. Clifford",
                        "slug": "P.-Clifford",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clifford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Clifford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118635048,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ec75e3ca906681bd900218a348a4a35dfed3d6fd",
            "isKey": false,
            "numCitedBy": 946,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Markov-fields-on-finite-graphs-and-lattices-Hammersley-Clifford",
            "title": {
                "fragments": [],
                "text": "Markov fields on finite graphs and lattices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5327685"
                        ],
                        "name": "N. Blackstone",
                        "slug": "N.-Blackstone",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Blackstone",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Blackstone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 84278872,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9f8daebab419c42b9d315ed4172b04f281823c7a",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Essential-Cell-Biology:-An-Introduction-to-the-of-,-Blackstone",
            "title": {
                "fragments": [],
                "text": "Essential Cell Biology: An Introduction to the Molecular Biology of the Cell.Bruce Alberts , Dennis Bray , Alexander Johnson , Julian Lewis , Martin Raff , Keith Roberts , Peter Walter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9397819"
                        ],
                        "name": "J. Sethian",
                        "slug": "J.-Sethian",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Sethian",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sethian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60873075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfdb7324a90b5bc11b5c8b39bff6cfa498587c86",
            "isKey": false,
            "numCitedBy": 4128,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Level-Set-Methods-and-Fast-Marching-Methods-Sethian",
            "title": {
                "fragments": [],
                "text": "Level Set Methods and Fast Marching Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59746611,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "78ecaabe915ba7df950671d36f92678192802df4",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-of-Dependences-Based-on-Empirical-Data:-Vapnik",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences Based on Empirical Data: Springer Series in Statistics (Springer Series in Statistics)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207082617,
            "fieldsOfStudy": [],
            "id": "21c9879f0b9adb692d8ddbebf7e8e22dbe20e2de",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5080176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e4a5edf46e65c87072c85e7362e63210849c69a",
            "isKey": false,
            "numCitedBy": 613,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models provide a powerful framework for probabilistic modelling and reasoning. Although theory behind learning and inference is well understood, most practical applications require approximation to known algorithms. We review learning of thin junction trees\u2013a class of graphical models that permits efficient inference. We discuss particular cases in clique graphs where exact inference is possible in polynomial time and some special cases where good approximation guarantees can be given. We also point out the drawbacks of learning with approximate inference. Finally, a practical application of probabilistic generative model for learning visual attributes from images is discussed."
            },
            "slug": "Learning-with-Graphical-Models-Sarawagi",
            "title": {
                "fragments": [],
                "text": "Learning with Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Learning of thin junction trees is reviewed\u2013a class of graphical models that permits efficient inference and particular cases in clique graphs where exact inference is possible in polynomial time and some special cases where good approximation guarantees can be given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1991940"
                        ],
                        "name": "J. Proakis",
                        "slug": "J.-Proakis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Proakis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Proakis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2072334,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7eeb730524a8e980f91c923b8e1d026b17883e38",
            "isKey": false,
            "numCitedBy": 7907,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability,-random-variables-and-stochastic-Proakis",
            "title": {
                "fragments": [],
                "text": "Probability, random variables and stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11951646,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "ed4f7492dc5fa988dccccdf14388c96ebd0d9451",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 242,
            "paperAbstract": {
                "fragments": [],
                "text": "The last few years have witnessed important new developments in the theory and practice of pattern classification. We intend to survey some of the main new ideas that have lead to these important recent developments. Resume. Durant ces dernieres annees, la theorie et la pratique de la reconnaissance des formes ont \u00b4e marquees par des developpements originaux. Ce survol presente certaines des principales idees novatrices qui ont conduitces developpements importants."
            },
            "slug": "THEORY-OF-CLASSIFICATION:-A-SURVEY-OF-RECENT-Bousquet",
            "title": {
                "fragments": [],
                "text": "THEORY OF CLASSIFICATION: A SURVEY OF RECENT ADVANCES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157031"
                        ],
                        "name": "Karsten A. Verbeurgt",
                        "slug": "Karsten-A.-Verbeurgt",
                        "structuredName": {
                            "firstName": "Karsten",
                            "lastName": "Verbeurgt",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karsten A. Verbeurgt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34010731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50780f102ee7c18e8b889a6afc025beca5be4f10",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-DNF-under-the-uniform-distribution-in-time-Verbeurgt",
            "title": {
                "fragments": [],
                "text": "Learning DNF under the uniform distribution in quasi-polynomial time"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889875"
                        ],
                        "name": "R. Horn",
                        "slug": "R.-Horn",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Horn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144610701"
                        ],
                        "name": "Charles R. Johnson",
                        "slug": "Charles-R.-Johnson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Johnson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43188859,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7c3b564bbdc8e7e3242257189ab7702d3e095115",
            "isKey": false,
            "numCitedBy": 28661,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints."
            },
            "slug": "Matrix-analysis-Horn-Johnson",
            "title": {
                "fragments": [],
                "text": "Matrix analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications."
            },
            "venue": {
                "fragments": [],
                "text": "Statistical Inference for Engineers and Data Scientists"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052516042"
                        ],
                        "name": "G. Schwarz",
                        "slug": "G.-Schwarz",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Schwarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schwarz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123722079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37e44d1de8003d8394d158ec6afd1ff0e87e595b",
            "isKey": false,
            "numCitedBy": 39569,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-the-Dimension-of-a-Model-Schwarz",
            "title": {
                "fragments": [],
                "text": "Estimating the Dimension of a Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765150"
                        ],
                        "name": "B. Krishnapuram",
                        "slug": "B.-Krishnapuram",
                        "structuredName": {
                            "firstName": "Balaji",
                            "lastName": "Krishnapuram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Krishnapuram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152725286"
                        ],
                        "name": "David Williams",
                        "slug": "David-Williams",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145644275"
                        ],
                        "name": "Y. Xue",
                        "slug": "Y.-Xue",
                        "structuredName": {
                            "firstName": "Ya",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2659620"
                        ],
                        "name": "A. Hartemink",
                        "slug": "A.-Hartemink",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hartemink",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hartemink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145006560"
                        ],
                        "name": "L. Carin",
                        "slug": "L.-Carin",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Carin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Carin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34659351"
                        ],
                        "name": "M\u00e1rio A. T. Figueiredo",
                        "slug": "M\u00e1rio-A.-T.-Figueiredo",
                        "structuredName": {
                            "firstName": "M\u00e1rio",
                            "lastName": "Figueiredo",
                            "middleNames": [
                                "A.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rio A. T. Figueiredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1997402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79f40b4c767c8ae1619e8c8624fe6e272943ec01",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A graph-based prior is proposed for parametric semi-supervised classification. The prior utilizes both labelled and unlabelled data; it also integrates features from multiple views of a given sample (e.g., multiple sensors), thus implementing a Bayesian form of co-training. An EM algorithm for training the classifier automatically adjusts the tradeoff between the contributions of: (a) the labelled data; (b) the unlabelled data; and (c) the co-training information. Active label query selection is performed using a mutual information based criterion that explicitly uses the unlabelled data and the co-training information. Encouraging results are presented on public benchmarks and on measured data from single and multiple sensors."
            },
            "slug": "Semi-Supervised-Classification-Krishnapuram-Williams",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A graph-based prior is proposed for parametric semi-supervised classification that utilizes both labelled and unlabelled data and integrates features from multiple views of a given sample, thus implementing a Bayesian form of co-training."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Database Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122051864"
                        ],
                        "name": "S. Boucheron",
                        "slug": "S.-Boucheron",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Boucheron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Boucheron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226094"
                        ],
                        "name": "P. Massart",
                        "slug": "P.-Massart",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Massart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Massart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14014604,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "09c6891c7e49a93f67e1b74ede1b7597a0b733bc",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new general concentration-of-measure inequality and illustrate its power by applications in random combinatorics. The results find direct applications in some problems of learning theory."
            },
            "slug": "A-sharp-concentration-inequality-with-applications-Boucheron-Lugosi",
            "title": {
                "fragments": [],
                "text": "A sharp concentration inequality with applications"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new general concentration-of-measure inequality is presented and its power is illustrated by applications in random combinatorics and some problems of learning theory."
            },
            "venue": {
                "fragments": [],
                "text": "Random Struct. Algorithms"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40632403"
                        ],
                        "name": "Sugato Basu",
                        "slug": "Sugato-Basu",
                        "structuredName": {
                            "firstName": "Sugato",
                            "lastName": "Basu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sugato Basu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144205696"
                        ],
                        "name": "A. Banerjee",
                        "slug": "A.-Banerjee",
                        "structuredName": {
                            "firstName": "Arindam",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12453655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4f3a10d96e0b6d134e7e347e1727b7438d4006f",
            "isKey": false,
            "numCitedBy": 906,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semi-supervised-Clustering-by-Seeding-Basu-Banerjee",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Clustering by Seeding"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057372949"
                        ],
                        "name": "M. Bernstein",
                        "slug": "M.-Bernstein",
                        "structuredName": {
                            "firstName": "Mira",
                            "lastName": "Bernstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39453972"
                        ],
                        "name": "Vin de Silva",
                        "slug": "Vin-de-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "Silva",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vin de Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10751942,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2dbc325d7014114242e63c93709ec820d8caf7b0",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "0 Introduction In [1] Tenenbaum, de Silva and Langford consider the problem of non-linear dimensionality reduction: discovering intrinsically low-dimensional structures embedded in high-dimensional data sets. They describe an algorithm, called Isomap, and demonstrate its successful application to several real and synthetic data sets. In this paper, we discuss some of the theoretical claims for Isomap made in [1]. In particular, we give a full proof of the asymptotic convergence theorem referred to in that paper. Isomap deals with finite data sets of points in R n which are assumed to lie on a smooth submanifold M d of low dimension d < n. The algorithm attempts to recover M given only the data points. A crucial stage in the algorithm involves estimating the unknown geodesic distance in M between data points in terms of the graph distance with respect to some graph G constructed on the data points. We show that the two distance metrics approximate each other arbitrarily closely, as the density of data points tends to infinity. Main Theorem A * The authors are listed alphabetically."
            },
            "slug": "Graph-approximations-to-geodesics-on-embedded-Bernstein-Silva",
            "title": {
                "fragments": [],
                "text": "Graph approximations to geodesics on embedded manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A full proof of the asymptotic convergence theorem of Isomap is given, showing that the two distance metrics approximate each other arbitrarily closely, as the density of data points tends to infinity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42041158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6fff8b8ea77f157913986e7af53951d9fc1128e",
            "isKey": false,
            "numCitedBy": 2170,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n3), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m < n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m2n). We report experiments on the USPS and abalone data sets and show that we can set m \u226a n without any significant decrease in the accuracy of the solution."
            },
            "slug": "Using-the-Nystr\u00f6m-Method-to-Speed-Up-Kernel-Williams-Seeger",
            "title": {
                "fragments": [],
                "text": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems) and the computational complexity of a predictor using this approximation is O(m2n)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61480753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edb62d05f8eeaa7e1921c6c25c544935a2b6b131",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Pattern-Recognition-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic interpretation and Bayesian methods for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings 1999 International Conference on Artificial Neural Networks,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The reuters-21578 data set"
            },
            "venue": {
                "fragments": [],
                "text": "The reuters-21578 data set"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic interpretation and Bayesian methods for support vector machines The Institution of Electrical Engineers"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Artificial Neural Networks, ICANN'99"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interior-point polynomial methods in convex programming: Theory and Chapelle, Sch\u00f6lkopf & Zien: Semi-Supervised Learning"
            },
            "venue": {
                "fragments": [],
                "text": "08 19:34 REFERENCES applications. SIAM, 13"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The KEGG resources for deciphering genome"
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Res"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Logistic regression for partial labels"
            },
            "venue": {
                "fragments": [],
                "text": "9th Information Processing and Management of Uncertainty in Knowledge-based Systems \u2013 IPMU'02"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Models. Oxford Statistical Sciences"
            },
            "venue": {
                "fragments": [],
                "text": "Graphical Models. Oxford Statistical Sciences"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. of the Conference on Uncertainty in Geometric Computations"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the Conference on Uncertainty in Geometric Computations"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regression and regularization on large graphs"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventeenth Annual Conference on Learning Theory"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Bayesian numerical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Some Bayesian numerical analysis"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the asymptotic improvement in the outcome of supervised learning provided Chapelle, Sch\u00f6lkopf & Zien: Semi-Supervised Learning 2006/03/08 19:34 REFERENCES 467 by additional nonsupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On estimating regression. Theory of Probability and its Applications"
            },
            "venue": {
                "fragments": [],
                "text": "On estimating regression. Theory of Probability and its Applications"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matrix Computations. Johns Hopkins"
            },
            "venue": {
                "fragments": [],
                "text": "Matrix Computations. Johns Hopkins"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discussion on the paper by Professor Dempster, Professor Laird and Dr. Rubin"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society, Series B,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On structural risk minimization or overall risk in a problem of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Automation and Remote Control"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning The ASTRAL compendium for sequence and structure analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Research"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning on manifolds"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems (NIPS)"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The necessary and sufficient conditions for consistency in the empirical risk minimization method"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition and Image Analysis"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning using prior probabilities and EM"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI-01 Workshop on Text Learning: Beyond Supervision"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with labeled and unlabeled data. Technical report, Institute for ANC, Edinburgh, UK, 2000b. See www.kyb.tuebingen.mpg.de/bs/people/seeger"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spectral Graph Theory. Number 92 in Regional Conference Series in Mathematics"
            },
            "venue": {
                "fragments": [],
                "text": "Spectral Graph Theory. Number 92 in Regional Conference Series in Mathematics"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic alignment kernels Advances in Large Margin Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic alignment kernels Advances in Large Margin Classifiers"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cluster kernels for semi-supervised protein classification"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 17"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hattori. The KEGG resources for deciphering genome"
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Res.,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semidefinite programming for combinatorial optimization. Habilitationsschrift ZIB-Report ZR00-34"
            },
            "venue": {
                "fragments": [],
                "text": "TU Berlin, Konrad-Zuse-Zentrum Berlin,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Logistic regression for partial labels. In 9th Information Processing and Management of Uncertainty in Knowledge-based Systems \u2013 IPMU\u201902"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1935
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature selection and transduction for prediction of molecular bioactivity for drug"
            },
            "venue": {
                "fragments": [],
                "text": "design. Bioinformatics,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applications of model selection techniques to polynomial approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Applications of model selection techniques to polynomial approximation"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning bayesian network classifiers for facial expression recognition using both labeled and unlabeled data"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of tight generalization bounds"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings International Conference on Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised gaussian processes"
            },
            "venue": {
                "fragments": [],
                "text": "Yahoo! Research"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised support vector machines for unlabeled data classification. Optimization Methods and Software"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised support vector machines for unlabeled data classification. Optimization Methods and Software"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities . Theory of Probability and its Applications"
            },
            "venue": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities . Theory of Probability and its Applications"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with labeled and unlabeled data Institute for ANC"
            },
            "venue": {
                "fragments": [],
                "text": "Learning with labeled and unlabeled data Institute for ANC"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "When does Isomap recover the natural parameterization of families of articulated images"
            },
            "venue": {
                "fragments": [],
                "text": "When does Isomap recover the natural parameterization of families of articulated images"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modern Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Modern Information Retrieval"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical predictor information"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of the Institute of Statistical Mathematics"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relevance feedback in information retrieval. In The SMART Retrieval System:Experiments in Automatic Document Processing, chapter 14, pages 313\u2013323"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using SeDuMi 1.02, a matlab toolbox for optimization over symmetric cones. Optimization Methods and Software"
            },
            "venue": {
                "fragments": [],
                "text": "Using SeDuMi 1.02, a matlab toolbox for optimization over symmetric cones. Optimization Methods and Software"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The ASTRAL compendium for sequence and structure analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Research,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning and model search. In Proceedings of the ICML-2003 workshop The Continuun from Labeled to Unlabeled Data in Machine Learning and Data Mining, pages 111\u2013112"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature selection in statistical learning of text categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning: Proceedings of the Fourteenth International Conference"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning and model search"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the ICML-2003 workshop The Continuun from Labeled to Unlabeled Data in Machine Learning and Data Mining"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung. Akademie Verlag"
            },
            "venue": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung. Akademie Verlag"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of tight generalization bounds"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings International Conference on Machine Learning (ICML-05)"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rational kernels. Neural Information Processing Systems 15"
            },
            "venue": {
                "fragments": [],
                "text": "Rational kernels. Neural Information Processing Systems 15"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sch \u00f6lkopf. Cluster kernels for semi-supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems (NIPS)"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An optimal selection of regression"
            },
            "venue": {
                "fragments": [],
                "text": "variables. Biometrika,"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning 2006/03/08"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Diffusion maps and geometric harmonics"
            },
            "venue": {
                "fragments": [],
                "text": "Diffusion maps and geometric harmonics"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Structural risk minimization over datadependent hierarchies"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Bayesian numerical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Statistics"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eighteenth Annual Conference on Uncertainty in Artificial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Eighteenth Annual Conference on Uncertainty in Artificial Intelligence"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discussion on the paper by Professor Dempster, Professor Laird and Dr"
            },
            "venue": {
                "fragments": [],
                "text": "Rubin. Journal of the Royal Statistical Society, Series B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Density-sensitive metrics and kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Advances in Machine Learning"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The reuters-21578 data"
            },
            "venue": {
                "fragments": [],
                "text": "set. http://www.daviddlewis.com/resources/ testcollections/reuters21578/,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of VC-method with classical methods for model selection"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings World Congress on Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Density-sensitive metrics and kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Advances in Machine Learning"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The reuters-21578 data"
            },
            "venue": {
                "fragments": [],
                "text": "set. http://www.daviddlewis.com/resources/ testcollections/reuters21578/,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combinig active learning and semi-supervised learning using gaussian fields and harmonic functions"
            },
            "venue": {
                "fragments": [],
                "text": "ICML-2003 Workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A generalized Gaussian mixture classifier with learning based on both labelled and unlabelled data"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1996 Conference on Information Science and Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov random field model and segmentation of brain"
            },
            "venue": {
                "fragments": [],
                "text": "MR images. IEEE Transactions on Medical Imaging,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition [in Russian"
            },
            "venue": {
                "fragments": [],
                "text": "Nauka,"
            },
            "year": 1974
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 477,
        "totalPages": 48
    },
    "page_url": "https://www.semanticscholar.org/paper/Semi-Supervised-Learning-Chapelle-Schlkopf/5ee8a371fc5adc5469435020a52fb815f3b57a71?sort=total-citations"
}