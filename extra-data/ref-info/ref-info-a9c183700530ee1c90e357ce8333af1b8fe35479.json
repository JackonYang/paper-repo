{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078154"
                        ],
                        "name": "Holger Caesar",
                        "slug": "Holger-Caesar",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Caesar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Caesar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823362"
                        ],
                        "name": "J. Uijlings",
                        "slug": "J.-Uijlings",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Uijlings",
                            "middleNames": [
                                "R.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uijlings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 57
                            }
                        ],
                        "text": "Many of these works use statically-defined cost matrices [7, 12, 35, 44, 45] or introduce additional parameter learning steps [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "[7, 35, 44, 45] or median frequency re-weighting [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 40
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1849209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79257758f03f9ff115db06ff6b753a4f21d17603",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic segmentation is the task of assigning a class-label to each pixel in an image. We propose a region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems: (1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification. Our method outperforms the state-of-the-art on the popular SIFT Flow [18] dataset in both the fully and weakly supervised setting by a considerably margin (+6% and +10%, respectively)."
            },
            "slug": "Joint-Calibration-for-Semantic-Segmentation-Caesar-Uijlings",
            "title": {
                "fragments": [],
                "text": "Joint Calibration for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2604251"
                        ],
                        "name": "Guosheng Lin",
                        "slug": "Guosheng-Lin",
                        "structuredName": {
                            "firstName": "Guosheng",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guosheng Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388378062"
                        ],
                        "name": "Anton van dan Hengel",
                        "slug": "Anton-van-dan-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anton van dan Hengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14554538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cef5476f9da50c1a8fefdcb7114863966f61d67",
            "isKey": false,
            "numCitedBy": 797,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information, specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78:0 on the challenging PASCAL VOC 2012 dataset."
            },
            "slug": "Efficient-Piecewise-Training-of-Deep-Structured-for-Lin-Shen",
            "title": {
                "fragments": [],
                "text": "Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work shows how to improve semantic segmentation through the use of contextual information, specifically, ' patch-patch' context between image regions, and 'patch-background' context, and formulate Conditional Random Fields with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110562225"
                        ],
                        "name": "Jia Xu",
                        "slug": "Jia-Xu",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068227"
                        ],
                        "name": "A. Schwing",
                        "slug": "A.-Schwing",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schwing",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schwing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 57
                            }
                        ],
                        "text": "Many of these works use statically-defined cost matrices [7, 12, 35, 44, 45] or introduce additional parameter learning steps [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "[7, 35, 44, 45] or median frequency re-weighting [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 40
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8499745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1f74c935b999cd0a4faf35af8b3d6f507876a83",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the promising performance of conventional fully supervised algorithms, semantic segmentation has remained an important, yet challenging task. Due to the limited availability of complete annotations, it is of great interest to design solutions for semantic segmentation that take into account weakly labeled data, which is readily available at a much larger scale. Contrasting the common theme to develop a different algorithm for each type of weak annotation, in this work, we propose a unified approach that incorporates various forms of weak supervision - image level tags, bounding boxes, and partial labels - to produce a pixel-wise labeling. We conduct a rigorous evaluation on the challenging Siftflow dataset for various weakly labeled settings, and show that our approach outperforms the state-of-the-art by 12% on per-class accuracy, while maintaining comparable per-pixel accuracy."
            },
            "slug": "Learning-to-segment-under-various-forms-of-weak-Xu-Schwing",
            "title": {
                "fragments": [],
                "text": "Learning to segment under various forms of weak supervision"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a unified approach that incorporates various forms of weak supervision - image level tags, bounding boxes, and partial labels - to produce a pixel-wise labeling on the challenging Siftflow dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150607621"
                        ],
                        "name": "Chen Huang",
                        "slug": "Chen-Huang",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111166188"
                        ],
                        "name": "Yining Li",
                        "slug": "Yining-Li",
                        "structuredName": {
                            "firstName": "Yining",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yining Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717179"
                        ],
                        "name": "Chen Change Loy",
                        "slug": "Chen-Change-Loy",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Loy",
                            "middleNames": [
                                "Change"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Change Loy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "The work in [23] tackles class imbalance via enforcing inter-cluster and inter-class margins, obtained by employing quintuplet instance sampling with a triple-header hinge loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10726895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c88dbaa5d8f4c915e286be5e38b5599038220493",
            "isKey": false,
            "numCitedBy": 520,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional neural network (CNN) typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain both intercluster and inter-class margins. This tighter constraint effectively reduces the class imbalance inherent in the local data neighborhood. We show that the margins can be easily deployed in standard deep learning framework through quintuplet instance sampling and the associated triple-header hinge loss. The representation learned by our approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high-and low-level vision classification tasks that exhibit imbalanced class distribution."
            },
            "slug": "Learning-Deep-Representation-for-Imbalanced-Huang-Li",
            "title": {
                "fragments": [],
                "text": "Learning Deep Representation for Imbalanced Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The representation learned by this approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high- and low-level vision classification tasks that exhibit imbalanced class distribution."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3429309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "isKey": false,
            "numCitedBy": 9408,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or \u2018atrous\u00a0convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous\u00a0spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online."
            },
            "slug": "DeepLab:-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work addresses the task of semantic image segmentation with Deep Learning and proposes atrous\u00a0spatial pyramid pooling (ASPP), which is proposed to robustly segment objects at multiple scales, and improves the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143877313"
                        ],
                        "name": "Zifeng Wu",
                        "slug": "Zifeng-Wu",
                        "structuredName": {
                            "firstName": "Zifeng",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zifeng Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "We have experimentally validated the effectiveness of our new loss function and showed consistently improved results on standard benchmark datasets for semantic segmentation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Again, the majority of categories benefits from our approach, confirming its efficacy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "It follows from Propositions 3 and 2 that \u03b1\u2217 = m\u22121/q\u2016`y\u0302y \u2212 \u03bb\u2217\u2016q is a root of \u03b7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "We ran a total of 200k training iterations and fixed parameters p = 1.3 and m to account for 25% of valid pixels per crop for our LMP."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8690944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42e583e2f1dca03c7607974c99f3ff66c846d2c9",
            "isKey": true,
            "numCitedBy": 108,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for high-performance semantic image segmentation (or semantic pixel labelling) based on very deep residual networks, which achieves the state-of-the-art performance. A few design factors are carefully considered to this end. \nWe make the following contributions. (i) First, we evaluate different variations of a fully convolutional residual network so as to find the best configuration, including the number of layers, the resolution of feature maps, and the size of field-of-view. Our experiments show that further enlarging the field-of-view and increasing the resolution of feature maps are typically beneficial, which however inevitably leads to a higher demand for GPU memories. To walk around the limitation, we propose a new method to simulate a high resolution network with a low resolution network, which can be applied during training and/or testing. (ii) Second, we propose an online bootstrapping method for training. We demonstrate that online bootstrapping is critically important for achieving good accuracy. (iii) Third we apply the traditional dropout to some of the residual blocks, which further improves the performance. (iv) Finally, our method achieves the currently best mean intersection-over-union 78.3\\% on the PASCAL VOC 2012 dataset, as well as on the recent dataset Cityscapes."
            },
            "slug": "High-performance-Semantic-Segmentation-Using-Very-Wu-Shen",
            "title": {
                "fragments": [],
                "text": "High-performance Semantic Segmentation Using Very Deep Fully Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A method for high-performance semantic image segmentation based on very deep residual networks, which achieves the state-of-the-art performance and demonstrates that online bootstrapping is critically important for achieving good accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 59
                            }
                        ],
                        "text": "Extending CNNs to directly cast dense, semantic label maps [2, 34], including more contextual information [9, 16, 33, 46] or refining results with graphical models [31, 47], have led to impressive results in many realworld applications and on standard benchmark datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1629541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "317aee7fc081f2b137a85c4f20129007fd8e717e",
            "isKey": false,
            "numCitedBy": 15652,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image."
            },
            "slug": "Fully-Convolutional-Networks-for-Semantic-Shelhamer-Long",
            "title": {
                "fragments": [],
                "text": "Fully Convolutional Networks for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that convolutional networks by themselves, trained end- to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152973423"
                        ],
                        "name": "S. Khan",
                        "slug": "S.-Khan",
                        "structuredName": {
                            "firstName": "Salman",
                            "lastName": "Khan",
                            "middleNames": [
                                "Hameed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145684318"
                        ],
                        "name": "Munawar Hayat",
                        "slug": "Munawar-Hayat",
                        "structuredName": {
                            "firstName": "Munawar",
                            "lastName": "Hayat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Munawar Hayat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698675"
                        ],
                        "name": "Bennamoun",
                        "slug": "Bennamoun",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Bennamoun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bennamoun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470423"
                        ],
                        "name": "Ferdous Sohel",
                        "slug": "Ferdous-Sohel",
                        "structuredName": {
                            "firstName": "Ferdous",
                            "lastName": "Sohel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ferdous Sohel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444665"
                        ],
                        "name": "R. Togneri",
                        "slug": "R.-Togneri",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Togneri",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Togneri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 492338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60b8cd6cddb08e27e0e4158eed5fad3680f177e",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes are abundant, making them an overrepresented majority, and data of other classes are scarce, making them an underrepresented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this paper, we propose a cost-sensitive (CoSen) deep neural network, which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class-dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multiclass problems without any modification. Moreover, as opposed to data-level approaches, we do not alter the original data distribution, which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification data sets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and CoSen classifiers demonstrate the superior performance of our proposed method."
            },
            "slug": "Cost-Sensitive-Learning-of-Deep-Feature-From-Data-Khan-Hayat",
            "title": {
                "fragments": [],
                "text": "Cost-Sensitive Learning of Deep Feature Representations From Imbalanced Data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a cost-sensitive (CoSen) deep neural network, which can automatically learn robust feature representations for both the majority and minority classes, and shows that the proposed approach significantly outperforms the baseline algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks and Learning Systems"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3294630"
                        ],
                        "name": "Aayush Bansal",
                        "slug": "Aayush-Bansal",
                        "structuredName": {
                            "firstName": "Aayush",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aayush Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39717886"
                        ],
                        "name": "Xinlei Chen",
                        "slug": "Xinlei-Chen",
                        "structuredName": {
                            "firstName": "Xinlei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinlei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14743438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d49979366cc8a5578e99a3a310500067899c5a87",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore architectures for general pixel-level prediction problems, from low-level edge detection to mid-level surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fully-convolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that (1) stratified sampling allows us to add diversity during batch updates and (2) sampled multi-scale features allow us to explore more nonlinear predictors (multiple fully-connected layers followed by ReLU) that improve overall accuracy. Finally, our objective is to show how a architecture can get performance better than (or comparable to) the architectures designed for a particular task. Interestingly, our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context, surface normal estimation on NYUDv2 dataset, and edge detection on BSDS without contextual post-processing."
            },
            "slug": "PixelNet:-Towards-a-General-Pixel-level-Bansal-Chen",
            "title": {
                "fragments": [],
                "text": "PixelNet: Towards a General Pixel-level Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work explores architectures for general pixel-level prediction problems, from low-level edge detection to mid-level surface normal estimation to high-level semantic segmentation, and shows how a architecture can get performance better than (or comparable to) the architectures designed for a particular task."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41187410"
                        ],
                        "name": "Wei Shen",
                        "slug": "Wei-Shen",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443233"
                        ],
                        "name": "Xinggang Wang",
                        "slug": "Xinggang-Wang",
                        "structuredName": {
                            "firstName": "Xinggang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinggang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152539822"
                        ],
                        "name": "Yan Wang",
                        "slug": "Yan-Wang",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50316590"
                        ],
                        "name": "Zhijiang Zhang",
                        "slug": "Zhijiang-Zhang",
                        "structuredName": {
                            "firstName": "Zhijiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhijiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3130125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cc46a96000db7b92493cc03126a987006641b28",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Contour detection serves as the basis of a variety of computer vision tasks such as image segmentation and object recognition. The mainstream works to address this problem focus on designing engineered gradient features. In this work, we show that contour detection accuracy can be improved by instead making the use of the deep features learned from convolutional neural networks (CNNs). While rather than using the networks as a blackbox feature extractor, we customize the training strategy by partitioning contour (positive) data into subclasses and fitting each subclass by different model parameters. A new loss function, named positive-sharing loss, in which each subclass shares the loss for the whole positive class, is proposed to learn the parameters. Compared to the sofmax loss function, the proposed one, introduces an extra regularizer to emphasizes the losses for the positive and negative classes, which facilitates to explore more discriminative features. Our experimental results demonstrate that learned deep features can achieve top performance on Berkeley Segmentation Dataset and Benchmark (BSDS500) and obtain competitive cross dataset generalization result on the NYUD dataset."
            },
            "slug": "DeepContour:-A-deep-convolutional-feature-learned-Shen-Wang",
            "title": {
                "fragments": [],
                "text": "DeepContour: A deep convolutional feature learned by positive-sharing loss for contour detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work shows that contour detection accuracy can be improved by instead making the use of the deep features learned from convolutional neural networks (CNNs), while rather than using the networks as a blackbox feature extractor, it customize the training strategy by partitioning contour (positive) data into subclasses and fitting each subclass by different model parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898210"
                        ],
                        "name": "Golnaz Ghiasi",
                        "slug": "Golnaz-Ghiasi",
                        "structuredName": {
                            "firstName": "Golnaz",
                            "lastName": "Ghiasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Golnaz Ghiasi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6989586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "752fc36f9813ebff3837cb12f790820d2f851c14",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture, akin to a Laplacian pyramid, that uses skip connections from higher resolution feature maps to successively refine segment boundaries reconstructed from lower resolution maps. This approach yields state-of-the-art semantic segmentation results on PASCAL without resorting to more complex CRF or detection driven architectures."
            },
            "slug": "Laplacian-Reconstruction-and-Refinement-for-Ghiasi-Fowlkes",
            "title": {
                "fragments": [],
                "text": "Laplacian Reconstruction and Refinement for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A multi-resolution reconstruction architecture, akin to a Laplacian pyramid, that uses skip connections from higher resolution feature maps to successively refine segment boundaries reconstructed from lower resolution maps is described."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807197"
                        ],
                        "name": "F. Yu",
                        "slug": "F.-Yu",
                        "structuredName": {
                            "firstName": "Fisher",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145231047"
                        ],
                        "name": "V. Koltun",
                        "slug": "V.-Koltun",
                        "structuredName": {
                            "firstName": "Vladlen",
                            "lastName": "Koltun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 96
                            }
                        ],
                        "text": "In particular, we are using ResNet-101 [21] in a fully-convolutional way with atrous extensions [22, 46] for the base layers before adding DeepLab\u2019s atrous spatial pyramid pooling (ASPP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 106
                            }
                        ],
                        "text": "Extending CNNs to directly cast dense, semantic label maps [2, 34], including more contextual information [9, 16, 33, 46] or refining results with graphical models [31, 47], have led to impressive results in many realworld applications and on standard benchmark datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17127188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
            "isKey": false,
            "numCitedBy": 5426,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
            },
            "slug": "Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun",
            "title": {
                "fragments": [],
                "text": "Multi-Scale Context Aggregation by Dilated Convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work develops a new convolutional network module that is specifically designed for dense prediction, and shows that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10875471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2173f1b8172c9b5bed4650c81b69988717bd6df6",
            "isKey": false,
            "numCitedBy": 890,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at this https URL ."
            },
            "slug": "ParseNet:-Looking-Wider-to-See-Better-Liu-Rabinovich",
            "title": {
                "fragments": [],
                "text": "ParseNet: Looking Wider to See Better"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work presents a technique for adding global context to deep convolutional networks for semantic segmentation, and achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "After fixing p = 1.3, we conducted additional experiments with m selected in a way to correspond to selecting at least 10%, 25% or 50% of non-ignore per-crop pixels, obtaining 74.09%\u00b1 0.22, 74.64\u00b1 0.10 and 73.44\u00b1 0.21 on the validation data, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": true,
            "numCitedBy": 19780,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841796"
                        ],
                        "name": "Marius Cordts",
                        "slug": "Marius-Cordts",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Cordts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Cordts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187309"
                        ],
                        "name": "Mohamed Omran",
                        "slug": "Mohamed-Omran",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Omran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed Omran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39940699"
                        ],
                        "name": "Sebastian Ramos",
                        "slug": "Sebastian-Ramos",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Ramos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Ramos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3393153"
                        ],
                        "name": "Timo Rehfeld",
                        "slug": "Timo-Rehfeld",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Rehfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo Rehfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798000"
                        ],
                        "name": "Rodrigo Benenson",
                        "slug": "Rodrigo-Benenson",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Benenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rodrigo Benenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145582788"
                        ],
                        "name": "Uwe Franke",
                        "slug": "Uwe-Franke",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe Franke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 65
                            }
                        ],
                        "text": "In particular, we have performed an extensive parameter sweep on Cityscapes, assessing the performance development for different settings of our hyper-parameters p and m (see Equ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "We have evaluated our novel loss max-pooling (LMP) approach on the Cityscapes [10] and the extended Pascal VOC [14] semantic image segmentation datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 26
                            }
                        ],
                        "text": "In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 71
                            }
                        ],
                        "text": "Images in this dataset are considerably smaller than the ones from the Cityscapes dataset so we increased the minibatch size to 4 (with crop sizes of 321\u00d7321), using the (extended)\ntraining set with 10.582 images [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 68
                            }
                        ],
                        "text": "5 we assess the performance of our contributions on the challenging Cityscapes and Pascal VOC segmentation benchmarks before we conclude in Sect."
                    },
                    "intents": []
                }
            ],
            "corpusId": 502946,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "c8c494ee5488fe20e0aa01bddf3fc4632086d654",
            "isKey": true,
            "numCitedBy": 6029,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."
            },
            "slug": "The-Cityscapes-Dataset-for-Semantic-Urban-Scene-Cordts-Omran",
            "title": {
                "fragments": [],
                "text": "The Cityscapes Dataset for Semantic Urban Scene Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling, and exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850973"
                        ],
                        "name": "Mohammadreza Mostajabi",
                        "slug": "Mohammadreza-Mostajabi",
                        "structuredName": {
                            "firstName": "Mohammadreza",
                            "lastName": "Mostajabi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammadreza Mostajabi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376506"
                        ],
                        "name": "Payman Yadollahpour",
                        "slug": "Payman-Yadollahpour",
                        "structuredName": {
                            "firstName": "Payman",
                            "lastName": "Yadollahpour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Payman Yadollahpour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490189"
                        ],
                        "name": "Gregory Shakhnarovich",
                        "slug": "Gregory-Shakhnarovich",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Shakhnarovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Shakhnarovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 57
                            }
                        ],
                        "text": "Many of these works use statically-defined cost matrices [7, 12, 35, 44, 45] or introduce additional parameter learning steps [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "[7, 35, 44, 45] or median frequency re-weighting [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 40
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215824593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31622dee41d13f90a099025425dcfb88d0970e60",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by \u201czooming out\u201d from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves 69.6% average accuracy on the PASCAL VOC 2012 test set."
            },
            "slug": "Feedforward-semantic-segmentation-with-zoom-out-Mostajabi-Yadollahpour",
            "title": {
                "fragments": [],
                "text": "Feedforward semantic segmentation with zoom-out features"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work introduces a purely feed-forward architecture for semantic segmentation that exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40474289"
                        ],
                        "name": "Shuai Zheng",
                        "slug": "Shuai-Zheng",
                        "structuredName": {
                            "firstName": "Shuai",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuai Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078751"
                        ],
                        "name": "Sadeep Jayasumana",
                        "slug": "Sadeep-Jayasumana",
                        "structuredName": {
                            "firstName": "Sadeep",
                            "lastName": "Jayasumana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sadeep Jayasumana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403031665"
                        ],
                        "name": "B. Romera-Paredes",
                        "slug": "B.-Romera-Paredes",
                        "structuredName": {
                            "firstName": "Bernardino",
                            "lastName": "Romera-Paredes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Romera-Paredes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143729959"
                        ],
                        "name": "Vibhav Vineet",
                        "slug": "Vibhav-Vineet",
                        "structuredName": {
                            "firstName": "Vibhav",
                            "lastName": "Vineet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vibhav Vineet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118650"
                        ],
                        "name": "Zhizhong Su",
                        "slug": "Zhizhong-Su",
                        "structuredName": {
                            "firstName": "Zhizhong",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhizhong Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40359161"
                        ],
                        "name": "Dalong Du",
                        "slug": "Dalong-Du",
                        "structuredName": {
                            "firstName": "Dalong",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dalong Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48908475"
                        ],
                        "name": "Chang Huang",
                        "slug": "Chang-Huang",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 164
                            }
                        ],
                        "text": "Extending CNNs to directly cast dense, semantic label maps [2, 34], including more contextual information [9, 16, 33, 46] or refining results with graphical models [31, 47], have led to impressive results in many realworld applications and on standard benchmark datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1318262,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
            "isKey": false,
            "numCitedBy": 2224,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark."
            },
            "slug": "Conditional-Random-Fields-as-Recurrent-Neural-Zheng-Jayasumana",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields as Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling is introduced, and top results are obtained on the challenging Pascal VOC 2012 segmentation benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2442177"
                        ],
                        "name": "Vijay Badrinarayanan",
                        "slug": "Vijay-Badrinarayanan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Badrinarayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Badrinarayanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47645184"
                        ],
                        "name": "Alex Kendall",
                        "slug": "Alex-Kendall",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Kendall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Kendall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 59
                            }
                        ],
                        "text": "Extending CNNs to directly cast dense, semantic label maps [2, 34], including more contextual information [9, 16, 33, 46] or refining results with graphical models [31, 47], have led to impressive results in many realworld applications and on standard benchmark datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60814714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0c065cd43aa7280e766b5dcbcc7e26abce59330",
            "isKey": false,
            "numCitedBy": 8375,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/."
            },
            "slug": "SegNet:-A-Deep-Convolutional-Encoder-Decoder-for-Badrinarayanan-Kendall",
            "title": {
                "fragments": [],
                "text": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures, including FCN and DeconvNet."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 199
                            }
                        ],
                        "text": "In separate though related research fields, focus was put on directly optimizing the target measures like Area under curve (AUC), Intersection over Union (IoU or Jaccard Index) or Average class (AC) [1, 4, 36, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10402359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8483da1bc3302c34b437baf8e329390da8f2c9bf",
            "isKey": false,
            "numCitedBy": 366,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Sliding window classifiers are among the most successful and widely applied techniques for object localization. However, training is typically done in a way that is not specific to the localization task. First a binary classifier is trained using a sample of positive and negative examples, and this classifier is subsequently applied to multiple regions within test images. We propose instead to treat object localization in a principled way by posing it as a problem of predicting structured data: we model the problem not as binary classification, but as the prediction of the bounding box of objects located in images. The use of a joint-kernelframework allows us to formulate the training procedure as a generalization of an SVM, which can be solved efficiently. We further improve computational efficiency by using a branch-and-bound strategy for localization during both training and testing. Experimental evaluation on the PASCAL VOC and TU Darmstadt datasets show that the structured training procedure improves performance over binary training as well as the best previously published scores."
            },
            "slug": "Learning-to-Localize-Objects-with-Structured-Output-Blaschko-Lampert",
            "title": {
                "fragments": [],
                "text": "Learning to Localize Objects with Structured Output Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes to treat object localization in a principled way by posing it as a problem of predicting structured data: it model the problem not as binary classification, but as the prediction of the bounding box of objects located in images."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110562225"
                        ],
                        "name": "Jia Xu",
                        "slug": "Jia-Xu",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068227"
                        ],
                        "name": "A. Schwing",
                        "slug": "A.-Schwing",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schwing",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schwing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 57
                            }
                        ],
                        "text": "Many of these works use statically-defined cost matrices [7, 12, 35, 44, 45] or introduce additional parameter learning steps [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "[7, 35, 44, 45] or median frequency re-weighting [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 40
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "866ace76c906edc8d0e4ba9152204563c525f1c3",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We tackle the problem of weakly labeled semantic segmentation, where the only source of annotation are image tags encoding which classes are present in the scene. This is an extremely difficult problem as no pixel-wise labelings are available, not even at training time. In this paper, we show that this problem can be formalized as an instance of learning in a latent structured prediction framework, where the graphical model encodes the presence and absence of a class as well as the assignments of semantic labels to superpixels. As a consequence, we are able to leverage standard algorithms with good theoretical properties. We demonstrate the effectiveness of our approach using the challenging SIFT-flow dataset and show average per-class accuracy improvements of 7% over the state-of-the-art."
            },
            "slug": "Tell-Me-What-You-See-and-I-Will-Show-You-Where-It-Xu-Schwing",
            "title": {
                "fragments": [],
                "text": "Tell Me What You See and I Will Show You Where It Is"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The problem of weakly labeled semantic segmentation can be formalized as an instance of learning in a latent structured prediction framework, where the graphical model encodes the presence and absence of a class as well as the assignments of semantic labels to superpixels."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "A similar idea termed online hard example mining [41] was found to be effective for object detection, where high-loss bounding boxes retained after a non-maximum-suppression step were preferably updated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2843566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63333669bcf694aba2e1928f6060ab1d6a5161fe",
            "isKey": false,
            "numCitedBy": 1571,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been - detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively."
            },
            "slug": "Training-Region-Based-Object-Detectors-with-Online-Shrivastava-Gupta",
            "title": {
                "fragments": [],
                "text": "Training Region-Based Object Detectors with Online Hard Example Mining"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use that leads to consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Images are captured at high resolution (2.048 \u00d7 1.024) and are divided into training, validation and test sets holding 2.975, 500 and 1.525 images, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95326,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2049889"
                        ],
                        "name": "P. Kontschieder",
                        "slug": "P.-Kontschieder",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Kontschieder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kontschieder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 171
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1384803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f801120b2aee9e4bcfa444be375a4cf01713bd16",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both state of-the-art forest models and the conventional pair wise CRF."
            },
            "slug": "GeoF:-Geodesic-Forests-for-Learning-Coupled-Kontschieder-Kohli",
            "title": {
                "fragments": [],
                "text": "GeoF: Geodesic Forests for Learning Coupled Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on, which outperforms both state of theart forest models and the conventional pair wise CRF."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "We additionally assess the quality of our novel LMP on the Pascal VOC 2012 segmentation benchmark dataset [13], comprising 20 object classes and a background class."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207252270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "616b246e332573af1f4859aa91440280774c183a",
            "isKey": false,
            "numCitedBy": 3769,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\u00a0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\u00a0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\u20132012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community\u2019s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "slug": "The-Pascal-Visual-Object-Classes-Challenge:-A-Everingham-Eslami",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes Challenge: A Retrospective"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A review of the Pascal Visual Object Classes challenge from 2008-2012 and an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "39]. Related Works. Many semantic segmentation works follow a relatively simple cost-sensitive approach via an inverse frequency rebalancing scheme, e.g. [7,35,44,45] or median frequency re-weighting [12]. Other approaches construct best-practice heuristics by e.g. restricting the number of pixels to be updated during backpropagation: The work in [3] suggests increasing the minibatch size while decrea"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " layers with \ufb01xed, uniform weights and therefore performing bilinear upsampling) before using standard softmax loss for all baseline methods BASE, BASE+ and for the inverse median frequency weighting [12] while we use our proposed loss max-pooling layer in LMP. Both our approaches, BASE+ and LMP are using the complimentary sampling strategy for minibatch compilation as described in the previous sectio"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ng, some of them following ideas previously applied in shallow learning methods like random forests [27,28] or support vector machines [38,42]. Many of these works use statically-de\ufb01ned cost matrices [7,12,35,44,45] or introduce additional parameter learning steps [26]. Due to the spatial arrangement and strong correlations of classes between adjacent pixels, costsensitive learning techniques are preferred over "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "m. 73.32 [43] FCRN-101 &amp; Online BS &amp; Augm. 74.80 [43] FCRN-152 &amp; Online BS &amp; Augm. 74.72 Our approaches - Resnet-101 BASE Augm. &amp; ASPP 75.74 0:05 BASE+ Augm. &amp; ASPP 75.42 0:04 [12] Inverse median freq. &amp; Augm. &amp; ASPP 74.93 0:03 Proposed loss only &amp; Augm. &amp; ASPP 76.01 0:01 LMP Augm. &amp; ASPP 76.29 0:02 Table 4: ResNet based results on Pascal VOC 2012 segmentati"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "RN-101 &amp; Augm. 71.16 [43] FCRN-152 &amp; Augm. 71.51 [43] FCRN-152 &amp; Online BS &amp; Augm. 74.64 Our approaches - Resnet-101 BASE Augm. &amp; ASPP 72.55 0:04 BASE+ Augm. &amp; ASPP 73.63 0:04 [12] Inverse median freq. &amp; Augm. &amp; ASPP 69.81 0:08 Proposed loss only &amp; Augm. &amp; ASPP 74.17 0:03 LMP Augm. &amp; ASPP 75.06 0:09 Table 2: ResNet-based results (in [%]) on validation set of"
                    },
                    "intents": []
                }
            ],
            "corpusId": 102496818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67711d42b77a13a04822ae00620660cef3abf8c4",
            "isKey": true,
            "numCitedBy": 1973,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks."
            },
            "slug": "Predicting-Depth,-Surface-Normals-and-Semantic-with-Eigen-Fergus",
            "title": {
                "fragments": [],
                "text": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper addresses three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling using a multiscale convolutional network that is able to adapt easily to each task using only small modifications."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054472270"
                        ],
                        "name": "Faruk Ahmed",
                        "slug": "Faruk-Ahmed",
                        "structuredName": {
                            "firstName": "Faruk",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faruk Ahmed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725299"
                        ],
                        "name": "Daniel Tarlow",
                        "slug": "Daniel-Tarlow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Tarlow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Tarlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 199
                            }
                        ],
                        "text": "In separate though related research fields, focus was put on directly optimizing the target measures like Area under curve (AUC), Intersection over Union (IoU or Jaccard Index) or Average class (AC) [1, 4, 36, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 541863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a596dd5bc68b73c1579300e283eb44d7c6b8ed4",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the Intersection-over-Union (IoU) measure that is used widely in evaluating image segmentation systems. Currently, there are two dominant approaches: the first approximates the Expected-IoU (EIoU) score as Expected-Intersection-over-Expected-Union (EIoEU), and the second approach is to compute exact EIoU but only over a small set of high-quality candidate solutions. We begin by asking which approach we should favor for two typical image segmentation tasks. Studying this question leads to two new methods that draw ideas from both existing approaches. Our new methods use the EIoEU approximation paired with high quality candidate solutions. Experimentally we show that our new approaches lead to improved performance on both image segmentation tasks."
            },
            "slug": "Optimizing-Expected-Intersection-Over-Union-with-Ahmed-Tarlow",
            "title": {
                "fragments": [],
                "text": "Optimizing Expected Intersection-Over-Union with Candidate-Constrained CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This work studies the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the Intersection-over-Union (IoU) measure and develops two new methods that draw ideas from both existing approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 71
                            }
                        ],
                        "text": "Datasets following such an approach are ImageNet [11], Caltech 101/256 [15, 17] or CIFAR 10/100 [29], where training, validation and test sets are roughly balanced w."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": true,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "For all experiments, we are using a network architecture similar to the one of DeepLabV2 [9], implemented within Caffe [25] using cuDNN for performance improve-\nment and NCCL3 for multi-GPU support."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11690,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "582 images [20]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6683607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82fae97673a353271b1d4c001afda1af6ef6dc23",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the challenging problem of localizing and classifying category-specific object contours in real world images. For this purpose, we present a simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours. We also provide a principled way of combining information from different part detectors and across categories. In order to study the problem and evaluate quantitatively our approach, we present a dataset of semantic exterior boundaries on more than 20, 000 object instances belonging to 20 categories, using the images from the VOC2011 PASCAL challenge [7]."
            },
            "slug": "Semantic-contours-from-inverse-detectors-Hariharan-Arbel\u00e1ez",
            "title": {
                "fragments": [],
                "text": "Semantic contours from inverse detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours is presented and a principled way of combining information from different part detectors and across categories is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 113
                            }
                        ],
                        "text": "For all experiments, we are using a network architecture similar to the one of DeepLabV2 [9], implemented within Caffe [25] using cuDNN for performance improve-\nment and NCCL3 for multi-GPU support."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "For all experiments, we are using a network architecture similar to the one of DeepLabV2 [9], implemented within Caffe [25] using cuDNN for performance improve-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13756,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413711"
                        ],
                        "name": "M. Ranjbar",
                        "slug": "M.-Ranjbar",
                        "structuredName": {
                            "firstName": "Mani",
                            "lastName": "Ranjbar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ranjbar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 199
                            }
                        ],
                        "text": "In separate though related research fields, focus was put on directly optimizing the target measures like Area under curve (AUC), Intersection over Union (IoU or Jaccard Index) or Average class (AC) [1, 4, 36, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10585394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1858573a93399fbb785b9fcc1e8fb38f70af6932",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we develop an algorithm for structured prediction that optimizes against complex performance measures, those which are a function of false positive and false negative counts. The approach can be directly applied to performance measures such as F\u03b2 score (natural language processing), intersection over union (image segmentation), Precision/Recall at k (search engines) and ROC area (binary classifiers). We attack this optimization problem by approximating the loss function with a piecewise linear function and relaxing the obtained QP problem to a LP which we solve with an off-the-shelf LP solver. We present experiments on object class-specific segmentation and show significant improvement over baseline approaches that either use simple loss functions or simple compatibility functions on VOC 2009."
            },
            "slug": "Optimizing-Complex-Loss-Functions-in-Structured-Ranjbar-Mori",
            "title": {
                "fragments": [],
                "text": "Optimizing Complex Loss Functions in Structured Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "An algorithm for structured prediction that optimizes against complex performance measures, those which are a function of false positive and false negative counts, which is directly applied to performance measures such as F\u03b2 score, intersection over union, Precision/Recall at k, and ROC area."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539424"
                        ],
                        "name": "N. Chawla",
                        "slug": "N.-Chawla",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Chawla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chawla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25887296"
                        ],
                        "name": "L. Hall",
                        "slug": "L.-Hall",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Hall",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757085"
                        ],
                        "name": "W. Kegelmeyer",
                        "slug": "W.-Kegelmeyer",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Kegelmeyer",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kegelmeyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 27
                            }
                        ],
                        "text": "SMOTE and derived variants [6, 8, 19, 24] on ways to avoid over-fitting)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1554582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cb44f06586f609a29d9b496cc752ec01475dffe",
            "isKey": false,
            "numCitedBy": 14426,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \"normal\" examples with only a small percentage of \"abnormal\" or \"interesting\" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy."
            },
            "slug": "SMOTE:-Synthetic-Minority-Over-sampling-Technique-Chawla-Bowyer",
            "title": {
                "fragments": [],
                "text": "SMOTE: Synthetic Minority Over-sampling Technique"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A combination of the method of oversampling the minority (abnormal) class and under-sampling the majority class can achieve better classifier performance (in ROC space) and a combination of these methods and the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166392154"
                        ],
                        "name": "Yuchun Tang",
                        "slug": "Yuchun-Tang",
                        "structuredName": {
                            "firstName": "Yuchun",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuchun Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108082363"
                        ],
                        "name": "Yanqing Zhang",
                        "slug": "Yanqing-Zhang",
                        "structuredName": {
                            "firstName": "Yanqing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanqing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539424"
                        ],
                        "name": "N. Chawla",
                        "slug": "N.-Chawla",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Chawla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chawla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701631"
                        ],
                        "name": "S. Krasser",
                        "slug": "S.-Krasser",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Krasser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Krasser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 207
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14272714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22e136921a1ef0fd58071dc92a11eea986997789",
            "isKey": true,
            "numCitedBy": 773,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional classification algorithms can be limited in their performance on highly unbalanced data sets. A popular stream of work for countering the problem of class imbalance has been the application of a sundry of sampling strategies. In this paper, we focus on designing modifications to support vector machines (SVMs) to appropriately tackle the problem of class imbalance. We incorporate different ldquorebalancerdquo heuristics in SVM modeling, including cost-sensitive learning, and over- and undersampling. These SVM-based strategies are compared with various state-of-the-art approaches on a variety of data sets by using various metrics, including G-mean, area under the receiver operating characteristic curve, F-measure, and area under the precision/recall curve. We show that we are able to surpass or match the previously known best algorithms on each data set. In particular, of the four SVM variations considered in this paper, the novel granular SVMs-repetitive undersampling algorithm (GSVM-RU) is the best in terms of both effectiveness and efficiency. GSVM-RU is effective, as it can minimize the negative effect of information loss while maximizing the positive effect of data cleaning in the undersampling process. GSVM-RU is efficient by extracting much less support vectors and, hence, greatly speeding up SVM prediction."
            },
            "slug": "SVMs-Modeling-for-Highly-Imbalanced-Classification-Tang-Zhang",
            "title": {
                "fragments": [],
                "text": "SVMs Modeling for Highly Imbalanced Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Of the four SVM variations considered in this paper, the novel granular SVMs-repetitive undersampling algorithm (GSVM-RU) is the best in terms of both effectiveness and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 40
                            }
                        ],
                        "text": "Datasets following such an approach are ImageNet [11], Caltech 101/256 [15, 17] or CIFAR 10/100 [29], where training, validation and test sets are roughly balanced w.r.t. the instances per class."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Datasets following such an approach are ImageNet [11], Caltech 101/256 [15, 17] or CIFAR 10/100 [29], where training, validation and test sets are roughly balanced w."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": true,
            "numCitedBy": 27407,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 71
                            }
                        ],
                        "text": "Datasets following such an approach are ImageNet [11], Caltech 101/256 [15, 17] or CIFAR 10/100 [29], where training, validation and test sets are roughly balanced w."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6953475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "812355cec91fa30bb50e9e992a3549af39e4f6eb",
            "isKey": true,
            "numCitedBy": 2365,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "slug": "One-shot-learning-of-object-categories-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "One-shot learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693373"
                        ],
                        "name": "Bhavani Raskutti",
                        "slug": "Bhavani-Raskutti",
                        "structuredName": {
                            "firstName": "Bhavani",
                            "lastName": "Raskutti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bhavani Raskutti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97484233"
                        ],
                        "name": "A. Kowalczyk",
                        "slug": "A.-Kowalczyk",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Kowalczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kowalczyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 207
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30487374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "486edf39b88ad34db11628887a9e7b799ece43b9",
            "isKey": true,
            "numCitedBy": 333,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many practical applications where learning from single class examples is either, the only possible solution, or has a distinct performance advantage. The first case occurs when obtaining examples of a second class is difficult, e.g., classifying sites of \"interest\" based on web accesses. The second situation is exemplified by the gene knock-out experiments for understanding Aryl Hydrocarbon Receptor signalling pathway that provided the data for the second task of the KDD 2002 Cup, where minority one-class SVMs significantly outperform models learnt using examples from both classes.This paper explores the limits of supervised learning of a two class discrimination from data with heavily unbalanced class proportions. We focus on the case of supervised learning with support vector machines. We consider the impact of both sampling and weighting imbalance compensation techniques and then extend the balancing to extreme situations when one of the classes is ignored completely and the learning is accomplished using examples from a single class.Our investigation with the data for KDD 2002 Cup as well as text benchmarks such as Reuters Newswire shows that there is a consistent pattern of performance differences between one and two-class learning for all SVMs investigated, and these patterns persist even with aggressive dimensionality reduction through automated feature selection. Using insight gained from the above analysis, we generate synthetic data showing similar pattern of performance."
            },
            "slug": "Extreme-re-balancing-for-SVMs:-a-case-study-Raskutti-Kowalczyk",
            "title": {
                "fragments": [],
                "text": "Extreme re-balancing for SVMs: a case study"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "There is a consistent pattern of performance differences between one and two-class learning for all SVMs investigated, and these patterns persist even with aggressive dimensionality reduction through automated feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "SKDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 199
                            }
                        ],
                        "text": "In separate though related research fields, focus was put on directly optimizing the target measures like Area under curve (AUC), Intersection over Union (IoU or Jaccard Index) or Average class (AC) [1, 4, 36, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10223057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca736f2c509631b7226957dbab2188f842468c8f",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "A probabilistic model allows us to reason about the world and make statistically optimal decisions using Bayesian decision theory. However, in practice the intractability of the decision problem forces us to adopt simplistic loss functions such as the 0/1 loss or Hamming loss and as result we make poor decisions through MAP estimates or through low-order marginal statistics. In this work we investigate optimal decision making for more realistic loss functions. Specifically we consider the popular intersection-over-union (IoU) score used in image segmentation benchmarks and show that it results in a hard combinatorial decision problem. To make this problem tractable we propose a statistical approximation to the objective function, as well as an approximate algorithm based on parametric linear programming. We apply the algorithm on three benchmark datasets and obtain improved intersection-over-union scores compared to maximum-posterior-marginal decisions. Our work points out the difficulties of using realistic loss functions with probabilistic computer vision models."
            },
            "slug": "Optimal-Decisions-from-Probabilistic-Models:-The-Nowozin",
            "title": {
                "fragments": [],
                "text": "Optimal Decisions from Probabilistic Models: The Intersection-over-Union Case"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work considers the popular intersection-over-union (IoU) score used in image segmentation benchmarks and shows that it results in a hard combinatorial decision problem, and proposes a statistical approximation to the objective function, as well as an approximate algorithm based on parametric linear programming."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112487994"
                        ],
                        "name": "Hui Han",
                        "slug": "Hui-Han",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108497307"
                        ],
                        "name": "Wenyuan Wang",
                        "slug": "Wenyuan-Wang",
                        "structuredName": {
                            "firstName": "Wenyuan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyuan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3029382"
                        ],
                        "name": "Binghuan Mao",
                        "slug": "Binghuan-Mao",
                        "structuredName": {
                            "firstName": "Binghuan",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binghuan Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 27
                            }
                        ],
                        "text": "SMOTE and derived variants [6, 8, 19, 24] on ways to avoid over-fitting)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12126950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b618f88ebaab51c4d38182e773419478abe44cf8",
            "isKey": false,
            "numCitedBy": 2096,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, mining with imbalanced data sets receives more and more attentions in both theoretical and practical aspects. This paper introduces the importance of imbalanced data sets and their broad application domains in data mining, and then summarizes the evaluation metrics and the existing methods to evaluate and solve the imbalance problem. Synthetic minority over-sampling technique (SMOTE) is one of the over-sampling methods addressing this problem. Based on SMOTE method, this paper presents two new minority over-sampling methods, borderline-SMOTE1 and borderline-SMOTE2, in which only the minority examples near the borderline are over-sampled. For the minority class, experiments show that our approaches achieve better TP rate and F-value than SMOTE and random over-sampling methods."
            },
            "slug": "Borderline-SMOTE:-A-New-Over-Sampling-Method-in-Han-Wang",
            "title": {
                "fragments": [],
                "text": "Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two new minority over-sampling methods are presented, borderline- SMOTE1 and borderline-SMOTE2, in which only the minority examples near the borderline are over- Sampling, which achieve better TP rate and F-value than SMOTE and random over-Sampling methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICIC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822319"
                        ],
                        "name": "C. Bunkhumpornpat",
                        "slug": "C.-Bunkhumpornpat",
                        "structuredName": {
                            "firstName": "Chumphol",
                            "lastName": "Bunkhumpornpat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bunkhumpornpat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289998"
                        ],
                        "name": "K. Sinapiromsaran",
                        "slug": "K.-Sinapiromsaran",
                        "structuredName": {
                            "firstName": "Krung",
                            "lastName": "Sinapiromsaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sinapiromsaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715152"
                        ],
                        "name": "C. Lursinsap",
                        "slug": "C.-Lursinsap",
                        "structuredName": {
                            "firstName": "Chidchanok",
                            "lastName": "Lursinsap",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lursinsap"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9477920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b012256861a72e6d046f454f8d867647c51ced0e",
            "isKey": false,
            "numCitedBy": 558,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The class imbalanced problem occurs in various disciplines when one of target classes has a tiny number of instances comparing to other classes. A typical classifier normally ignores or neglects to detect a minority class due to the small number of class instances. SMOTE is one of over-sampling techniques that remedies this situation. It generates minority instances within the overlapping regions. However, SMOTE randomly synthesizes the minority instances along a line joining a minority instance and its selected nearest neighbours, ignoring nearby majority instances. Our technique called Safe-Level-SMOTE carefully samples minority instances along the same line with different weight degree, called safe level. The safe level computes by using nearest neighbour minority instances. By synthesizing the minority instances more around larger safe level, we achieve a better accuracy performance than SMOTE and Borderline-SMOTE."
            },
            "slug": "Safe-Level-SMOTE:-Safe-Level-Synthetic-Minority-for-Bunkhumpornpat-Sinapiromsaran",
            "title": {
                "fragments": [],
                "text": "Safe-Level-SMOTE: Safe-Level-Synthetic Minority Over-Sampling TEchnique for Handling the Class Imbalanced Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The technique called Safe-Level-SMOTE carefully samples minority instances along the same line with different weight degree, called safe level, and achieves a better accuracy performance than SMOTE and Borderline- SMOTE."
            },
            "venue": {
                "fragments": [],
                "text": "PAKDD"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2248077"
                        ],
                        "name": "P. Jeatrakul",
                        "slug": "P.-Jeatrakul",
                        "structuredName": {
                            "firstName": "Piyasak",
                            "lastName": "Jeatrakul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jeatrakul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145313762"
                        ],
                        "name": "K. Wong",
                        "slug": "K.-Wong",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Wong",
                            "middleNames": [
                                "Kok",
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1866074"
                        ],
                        "name": "L. Fung",
                        "slug": "L.-Fung",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Fung",
                            "middleNames": [
                                "Chun",
                                "Che"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 27
                            }
                        ],
                        "text": "SMOTE and derived variants [6, 8, 19, 24] on ways to avoid over-fitting)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40971286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "449ef2aab34ef411b48c489a3a396837d01a93b7",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In classification, when the distribution of the training data among classes is uneven, the learning algorithm is generally dominated by the feature of the majority classes. The features in the minority classes are normally difficult to be fully recognized. In this paper, a method is proposed to enhance the classification accuracy for the minority classes. The proposed method combines Synthetic Minority Over-sampling Technique (SMOTE) and Complementary Neural Network (CMTNN) to handle the problem of classifying imbalanced data. In order to demonstrate that the proposed technique can assist classification of imbalanced data, several classification algorithms have been used. They are Artificial Neural Network (ANN), k-Nearest Neighbor (k-NN) and Support Vector Machine (SVM). The benchmark data sets with various ratios between the minority class and the majority class are obtained from the University of California Irvine (UCI) machine learning repository. The results show that the proposed combination techniques can improve the performance for the class imbalance problem."
            },
            "slug": "Classification-of-Imbalanced-Data-by-Combining-the-Jeatrakul-Wong",
            "title": {
                "fragments": [],
                "text": "Classification of Imbalanced Data by Combining the Complementary Neural Network and SMOTE Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The proposed method combines Synthetic Minority Over-sampling Technique (SMOTE) and Complementary Neural Network (CMTNN) to handle the problem of classifying imbalanced data and shows that the proposed combination techniques can improve the performance for the class imbalance problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICONIP"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725285"
                        ],
                        "name": "T. Khoshgoftaar",
                        "slug": "T.-Khoshgoftaar",
                        "structuredName": {
                            "firstName": "Taghi",
                            "lastName": "Khoshgoftaar",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Khoshgoftaar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41181613"
                        ],
                        "name": "M. Golawala",
                        "slug": "M.-Golawala",
                        "structuredName": {
                            "firstName": "Moiz",
                            "lastName": "Golawala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Golawala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328057"
                        ],
                        "name": "J. V. Hulse",
                        "slug": "J.-V.-Hulse",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Hulse",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Hulse"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 171
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29812549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5799b07761d21c51350f0f837d35b30e4c78abe3",
            "isKey": true,
            "numCitedBy": 174,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a comprehensive suite of experiments that analyze the performance of the random forest (RF) learner implemented in Weka. RF is a relatively new learner, and to the best of our knowledge, only preliminary experimentation on the construction of random forest classifiers in the context of imbalanced data has been reported in previous work. Therefore, the contribution of this study is to provide an extensive empirical evaluation of RF learners built from imbalanced data. What should be the recommended default number of trees in the ensemble? What should the recommended value be for the number of attributes? How does the RF learner perform on imbalanced data when compared with other commonly-used learners? We address these and other related issues in this work."
            },
            "slug": "An-Empirical-Study-of-Learning-from-Imbalanced-Data-Khoshgoftaar-Golawala",
            "title": {
                "fragments": [],
                "text": "An Empirical Study of Learning from Imbalanced Data Using Random Forest"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A comprehensive suite of experiments that analyze the performance of the random forest (RF) learner implemented in Weka are discussed, providing an extensive empirical evaluation of RF learners built from imbalanced data."
            },
            "venue": {
                "fragments": [],
                "text": "19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "image and therefore constitutes the basis for high-level image understanding. Recent works have contributed to the progress in this research \ufb01eld by building upon convolutional neural networks (CNNs) [30] and enriching them with task-speci\ufb01c functionalities. Extending CNNs to directly cast dense, semantic label maps [2,34], including more contextual information [9,16,33,46] or re\ufb01ning results with gra"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35262,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "Recent works have contributed to the progress in this research field by building upon convolutional neural networks (CNNs) [30] and enriching them with task-specific functionalities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": false,
            "numCitedBy": 3516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722716"
                        ],
                        "name": "G. Folino",
                        "slug": "G.-Folino",
                        "structuredName": {
                            "firstName": "Gianluigi",
                            "lastName": "Folino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Folino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788535"
                        ],
                        "name": "C. Pizzuti",
                        "slug": "C.-Pizzuti",
                        "structuredName": {
                            "firstName": "Clara",
                            "lastName": "Pizzuti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pizzuti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2743420"
                        ],
                        "name": "G. Spezzano",
                        "slug": "G.-Spezzano",
                        "structuredName": {
                            "firstName": "Giandomenico",
                            "lastName": "Spezzano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Spezzano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1825915,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "df5d6edc8a5c888d94e5a3fd2a1e451800236259",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive boosting ensemble algorithm for classifying homogeneous distributed data streams is presented. The method builds an ensemble of classifiers by using Genetic Programming (GP) to inductively generate decision trees, each trained on different parts of the distributed training set. The approach adopts a co-evolutionary platform to support a cooperative model of GP. A change detection strategy, based on self-similarity of the ensemble behavior, and measured by its fractal dimension, permits to capture time- evolving trends and patterns in the stream, and to reveal changes in evolving data streams. The approach tracks online ensemble accuracy deviation over time and decides to recompute the ensemble if the deviation has exceeded a pre- specified threshold. This allows the maintenance of an accurate and up-to-date ensemble of classifiers for continuous flows of data with concept drifts. Experimental results on a real life data set show the validity of the approach."
            },
            "slug": "An-Adaptive-Distributed-Ensemble-Approach-to-Mine-Folino-Pizzuti",
            "title": {
                "fragments": [],
                "text": "An Adaptive Distributed Ensemble Approach to Mine Concept-Drifting Data Streams"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "An adaptive boosting ensemble algorithm for classifying homogeneous distributed data streams is presented and experimental results on a real life data set show the validity of the approach."
            },
            "venue": {
                "fragments": [],
                "text": "19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Indeed, the Slater\u2019s condition applies [5] (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 37925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f607f03272e4d62708f5b2441355f9e005cb452",
            "isKey": false,
            "numCitedBy": 38723,
            "numCiting": 276,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics."
            },
            "slug": "Convex-Optimization-Boyd-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Convex Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive introduction to the subject of convex optimization shows in detail how such problems can be solved numerically with great efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059358552"
                        ],
                        "name": "P. Cochat",
                        "slug": "P.-Cochat",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Cochat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cochat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13267685"
                        ],
                        "name": "L. Vaucoret",
                        "slug": "L.-Vaucoret",
                        "structuredName": {
                            "firstName": "L",
                            "lastName": "Vaucoret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vaucoret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097644863"
                        ],
                        "name": "J. Sarles",
                        "slug": "J.-Sarles",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Sarles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sarles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 164
                            }
                        ],
                        "text": "Extending CNNs to directly cast dense, semantic label maps [2, 34], including more contextual information [9, 16, 33, 46] or refining results with graphical models [31, 47], have led to impressive results in many realworld applications and on standard benchmark datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11759366,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "10d85561e4aafc516d10064f30dff05b41f70afe",
            "isKey": false,
            "numCitedBy": 57731,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "disasters. Plenum, 2001. 11. Haley R, Thomas L, Hom J. Is there a Gulf War Syndrome? Searching for syndromes by factor analysis of symptoms. JAMA 1997;277:215\u201322. 12. Fukuda K, Nisenbaum R, Stewart G, et al. Chronic multi-symptom illness affecting Air Force veterans of the Gulf War. JAMA 1998;280:981\u20138. 13. Ismail K, Everitt B, Blatchley N, et al. Is there a Gulf War Syndrome? Lancet 1999;353:179\u201382. 14. Shapiro S, Lasarev M, McCauley L. Factor analysis of Gulf War illness: what does it add to our understanding of possible health effects of deployment. Am J Epidemiol 2002;156:578\u201385. 15. Doebbeling B, Clarke W, Watson D, et al. Is there a Persian Gulf War Syndrome? Evidence from a large population-based survey of veterans and nondeployed controls. Am J Med 2000;108:695\u2013704. 16. Knoke J, Smith T, Gray G, et al. Factor analysis of self reported symptoms: Does it identify a Gulf War Syndrome? Am J Epidemiol 2000;152:379\u201388. 17. Kang H, Mahan C, Lee K, et al. Evidence for a deployment-related Gulf War syndrome by factor analysis. Arch Environ Health 2002;57:61\u20138."
            },
            "slug": "Et-al-Cochat-Vaucoret",
            "title": {
                "fragments": [],
                "text": "[Et al]."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A large population-based survey of veterans and nondeployed controls found evidence of a deployment-related Gulf War syndrome by factor analysis in Air Force veterans and controls."
            },
            "venue": {
                "fragments": [],
                "text": "Archives de pediatrie : organe officiel de la Societe francaise de pediatrie"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217430"
                        ],
                        "name": "M. Holschneider",
                        "slug": "M.-Holschneider",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Holschneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Holschneider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398530308"
                        ],
                        "name": "R. Kronland-Martinet",
                        "slug": "R.-Kronland-Martinet",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kronland-Martinet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kronland-Martinet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145515955"
                        ],
                        "name": "J. Morlet",
                        "slug": "J.-Morlet",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Morlet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Morlet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830702"
                        ],
                        "name": "P. Tchamitchian",
                        "slug": "P.-Tchamitchian",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Tchamitchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tchamitchian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 96
                            }
                        ],
                        "text": "In particular, we are using ResNet-101 [21] in a fully-convolutional way with atrous extensions [22, 46] for the base layers before adding DeepLab\u2019s atrous spatial pyramid pooling (ASPP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60719890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef3ed634a28eee78a7e31fc525cf7083e6689345",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform. We shall briefly describe this transformation in the following. For more details, we refer to the literature [1]."
            },
            "slug": "A-real-time-algorithm-for-signal-analysis-with-the-Holschneider-Kronland-Martinet",
            "title": {
                "fragments": [],
                "text": "A real-time algorithm for signal analysis with the help of the wavelet transform"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169667054"
                        ],
                        "name": "\u7d39\u4ecbRyan Dunch",
                        "slug": "\u7d39\u4ecbRyan-Dunch",
                        "structuredName": {
                            "firstName": "\u7d39\u4ecbRyan",
                            "lastName": "Dunch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u7d39\u4ecbRyan Dunch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169671793"
                        ],
                        "name": "Fuzhou Protestants",
                        "slug": "Fuzhou-Protestants",
                        "structuredName": {
                            "firstName": "Fuzhou",
                            "lastName": "Protestants",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuzhou Protestants"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 104
                            }
                        ],
                        "text": "We can again obtain a considerable relative improvement over BASE+ as well as comparable baselines from [9, 43]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 131
                            }
                        ],
                        "text": "4 (this time averaged after training iterations 180k, 190k and 200k), and list results from comparable state-of-the-art approaches [9, 43] next to ours."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "2, demonstrating improved results over BASE, BASE+ and related approaches like DeepLabV2 [9] (even when using CRF) or [43] with deeper ResNet and on-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [43], an approach coined online bootstrapping is introduced, where pixel losses are sorted and only the k highest loss positions are updated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 240283508,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "032db195efd97fe2bcd20c4ad04628c70ff4e79c",
            "isKey": true,
            "numCitedBy": 34875,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "tobacco and products and use of tanning beds. Friends the biggest on predisposition to harmful habits."
            },
            "slug": "and-a-at-Dunch-Protestants",
            "title": {
                "fragments": [],
                "text": "and a at"
            },
            "venue": {
                "fragments": [],
                "text": "The William Makepeace Thackeray Library"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 207
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extreme re-balancing for 2134  SVMs: a case study"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Sigkdd Explorations Newsletter, 6(1):60\u201369"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 207
                            }
                        ],
                        "text": "Such methods were recently investigated [7, 35, 44, 45] for deep learning, some of them following ideas previously applied in shallow learning methods like random forests [27, 28] or support vector machines [38, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extreme re-balancing for 7090  SVMs: a case study"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Sigkdd Explorations Newsletter, 6(1):60\u201369"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854385"
                        ],
                        "name": "\u00c7aglar G\u00fcl\u00e7ehre",
                        "slug": "\u00c7aglar-G\u00fcl\u00e7ehre",
                        "structuredName": {
                            "firstName": "\u00c7aglar",
                            "lastName": "G\u00fcl\u00e7ehre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c7aglar G\u00fcl\u00e7ehre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "The work in [18] is proposing a nonlinear activation function computing the Lp norm of projections from the lower layers, allowing to interpret max-, average- and root-mean-squared-pooling operators as special cases of their activation function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29485400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c902b2076a948ad994a542bb9c93e1a6513fc40",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learned-norm-pooling-for-deep-neural-networks-G\u00fcl\u00e7ehre-Cho",
            "title": {
                "fragments": [],
                "text": "Learned-norm pooling for deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "In this paper we focused on a particular family of weighting functions, enabling us to control the pixel selectivity and the extent of the supported pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic contours from inverse detectors. In (ICCV)"
            },
            "venue": {
                "fragments": [],
                "text": "Semantic contours from inverse detectors. In (ICCV)"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Finally, refer to [39] for more in-depth analyses and correctness proofs for our approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Loss maxpooling for semantic image segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "arXiv preprint arXiv:1704.02966"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conditional random fields as recurrent neural networks. In (ICCV)"
            },
            "venue": {
                "fragments": [],
                "text": "Conditional random fields as recurrent neural networks. In (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 57
                            }
                        ],
                        "text": "Many of these works use statically-defined cost matrices [7, 12, 35, 44, 45] or introduce additional parameter learning steps [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "[7, 35, 44, 45] or median frequency re-weighting [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 247
                            }
                        ],
                        "text": "Finally, we apply upscaling (via deconvolution layers with fixed, uniform weights and therefore performing bilinear upsampling) before using standard softmax loss for all baseline methods BASE, BASE+ and for the inverse median frequency weighting [12] while we use our proposed loss max-pooling layer in LMP."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Predicting depth"
            },
            "venue": {
                "fragments": [],
                "text": "surface normals and semantic labels with a common multi-scale convolutional architecture. In (ICCV), pages 2650\u20132658"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Loss maxpooling for semantic image segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "In (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cifar-10 (canadian institute for advanced research)"
            },
            "venue": {
                "fragments": [],
                "text": "Cifar-10 (canadian institute for advanced research)"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 24,
            "methodology": 28,
            "result": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 57,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Loss-Max-Pooling-for-Semantic-Image-Segmentation-Bul\u00f2-Neuhold/a9c183700530ee1c90e357ce8333af1b8fe35479?sort=total-citations"
}