{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144985567"
                        ],
                        "name": "M. M\u00e4chler",
                        "slug": "M.-M\u00e4chler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "M\u00e4chler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00e4chler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107683368"
                        ],
                        "name": "R. Martin",
                        "slug": "R.-Martin",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Martin",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992435"
                        ],
                        "name": "J. Schimert",
                        "slug": "J.-Schimert",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Schimert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schimert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103205634"
                        ],
                        "name": "M. Csoppenszky",
                        "slug": "M.-Csoppenszky",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Csoppenszky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Csoppenszky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159381"
                        ],
                        "name": "Jenq-Neng Hwang",
                        "slug": "Jenq-Neng-Hwang",
                        "structuredName": {
                            "firstName": "Jenq-Neng",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenq-Neng Hwang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33853908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f17213659ec04c49c773c01ee0bf80536031e2b",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Two types of learning networks for nonparametric regression problems are studied and compared: one is the parametric two-layer perceptron type neural network, which is well known in artificial neural network (ANN) literature; the other is the semiparametric projection pursuit network (PPN), which has emerged in recent years in the statistical estimation literature. From an algorithmic viewpoint, both the PPN and the ANN parametrically form projections of the data in directions determined from interconnection weights. However, unlike an ANN which uses a fixed set of nonlinear nodal functions to perform an explicit parametric estimate of a nonparametric model, the PPN nonparametrically estimates the nonlinear functions using a one-dimensional data smoother. From experimental simulations, ANNs and PPNs perform comparably in predicting independent test data but PPN training is much faster than that of an ANN.<<ETX>>"
            },
            "slug": "Projection-pursuit-learning-networks-for-regression-M\u00e4chler-Martin",
            "title": {
                "fragments": [],
                "text": "Projection pursuit learning networks for regression"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "From experimental simulations, ANNs and PPNs perform comparably in predicting independent test data but PPN training is much faster than that of an ANN."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings of the 2nd International IEEE Conference on Tools for Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103188660"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "A.R.",
                            "lastName": "Barron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 97
                            }
                        ],
                        "text": "The study of neural networks in recent years has involved increasingly sophisticated mathematics (cf. Barron and Barron 1988; Barron 1989; Baum and Haussler 1989; Haussler 1989b; White 1989, 1990; Amari 1990; Amari et al. 1990; Azencott 1990; Baum 1990a), often directly connected with the statistical-inference issues discussed in the previous sections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51276807,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "72d761afbe35634213849419ff63fad5bc9fabeb",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Convergence properties of empirically estimated neural networks are examined. In this theory, an appropriate size feedforward network is automatically determined from the data. The networks studied include two- and three-layer networks with an increasing number of simple sigmoidal nodes, multiple-layer polynomial networks, and networks with certain fixed structures but an increasing complexity in each unit. Each of these classes of networks is dense in the space of continuous functions on compact subsets of d-dimensional Euclidean space, with respect to the topology of uniform convergence. It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases. Bounds on the rate of convergence are given in terms of an index of the approximation capability of the class of networks.<<ETX>>"
            },
            "slug": "Statistical-properties-of-artificial-neural-Barron",
            "title": {
                "fragments": [],
                "text": "Statistical properties of artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 28th IEEE Conference on Decision and Control,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14333248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9552ac39a57daacf3d75865a268935b5a0df9bbb",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-and-principal-component-analysis:-Baldi-Hornik",
            "title": {
                "fragments": [],
                "text": "Neural networks and principal component analysis: Learning from examples without local minima"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 79
                            }
                        ],
                        "text": "This tradeoff is, in fact, observed in experiments reported by several authors (see, for example, Chauvin 1990; Morgan and Bourlard 1990), as well as in our experiments with artificial data (Section 3."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 18821787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f3175b3930d0c71495a52a7bccb3889e5f33520",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We have done an empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance. Two experiments are reported. In one, we use simulated data sets with well-controlled parameters, such as the signal-to-noise ratio of continuous-valued data. In the second, we train the network on vector-quantized mel cepstra from real speech samples. In each case, we use back-propagation to train the feedforward net to discriminate in a multiple class pattern classification problem. We report the results of these studies, and show the application of cross-validation techniques to prevent overfitting."
            },
            "slug": "Generalization-and-Parameter-Estimation-in-Netws:-Morgan-Bourlard",
            "title": {
                "fragments": [],
                "text": "Generalization and Parameter Estimation in Feedforward Netws: Some Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance and the application of cross-validation techniques to prevent overfitting is done."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987652"
                        ],
                        "name": "E. Karnin",
                        "slug": "E.-Karnin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Karnin",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Karnin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1101832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8e8b54f87f43c4a6f85695712dff55e0edec760",
            "isKey": false,
            "numCitedBy": 663,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The sensitivity of the global error (cost) function to the inclusion/exclusion of each synapse in the artificial neural network is estimated. Introduced are shadow arrays which keep track of the incremental changes to the synaptic weights during a single pass of back-propagating learning. The synapses are then ordered by decreasing sensitivity numbers so that the network can be efficiently pruned by discarding the last items of the sorted list. Unlike previous approaches, this simple procedure does not require a modification of the cost function, does not interfere with the learning process, and demands a negligible computational overhead."
            },
            "slug": "A-simple-procedure-for-pruning-back-propagation-Karnin",
            "title": {
                "fragments": [],
                "text": "A simple procedure for pruning back-propagation trained neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Shadow arrays are introduced which keep track of the incremental changes to the synaptic weights during a single pass of back-propagating learning and are ordered by decreasing sensitivity numbers so that the network can be efficiently pruned by discarding the last items of the sorted list."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10184407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1c6dd9c58b30ebe02e2340a9132607183819071",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Judd (1988) and Blum and Rivest (1988) have recently proved that the loading problem for neural networks is NP complete. This makes it very unlikely that any algorithm like backpropagation which varies weights on a network of fixed size and topology will be able to learn in polynomial time. However, Valiant has recently proposed a learning protocol (Valiant 1984), which allows one to sensibly consider generalization by learning algorithms with the freedom to add neurons and synapses, as well as simply adjusting weights. Within this context, standard circuit complexity arguments show that learning algorithms with such freedom can solve in polynomial time any learning problem that can be solved in polynomial time by any algorithm whatever. In this sense, neural nets are universal learners, capable of learning any learnable class of concepts."
            },
            "slug": "A-Proposal-for-More-Powerful-Learning-Algorithms-Baum",
            "title": {
                "fragments": [],
                "text": "A Proposal for More Powerful Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Neural nets are universal learners, capable of learning any learnable class of concepts, and standard circuit complexity arguments show that learning algorithms with such freedom can solve in polynomial time any learning problem that can be solved in poynomial time by any algorithm whatever."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 36
                            }
                        ],
                        "text": "It is by now well-known (see, e.g., White 1990) that a feedforward neural network (with some mild conditions on Ely I x] and network structure, and some optimistic assumptions about minimizing 3.3) can be made consistent by suitably letting the network size grow with the size of the training set,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205119351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77fdd39ab366b65a617015a72fe8dc9d0b394d64",
            "isKey": true,
            "numCitedBy": 716,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-nonparametric-regression:-Multilayer-White",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20330,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 2
                            }
                        ],
                        "text": "\" Poggio and Girosi (1990) have shown how splines and related estimators can be computed with multilayer networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6635519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4abd4e51705e74f1739bd3a1e47ac10e45f6468b",
            "isKey": false,
            "numCitedBy": 1171,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage."
            },
            "slug": "Regularization-Algorithms-for-Learning-That-Are-to-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": false,
            "numCitedBy": 6386,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15012839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b10440620da8a43a1b97e3da4b1ff13746306475",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.<<ETX>>"
            },
            "slug": "Consistent-inference-of-probabilities-in-layered-Tishby-Levin",
            "title": {
                "fragments": [],
                "text": "Consistent inference of probabilities in layered networks: predictions and generalizations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework and the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118425960,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8b7d60f49e7a5368920457c885c813a912c34997",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts of universal data compression lead to minimum description-length criteria for parsimonious statistical model selection for the estimation of functions. In this paper we define general complexity regularization criteria and establish bounds on the statistical risk of the estimated functions. These bounds establish consistency, yield rates of convergence, and demonstrate the near asymptotic optimality of the model selection criterion in both parametric and nonparametric cases. A fundamental role is played by an index of resolvability that quantifies the tradeoff between complexity and accuracy of candidate models. Applications are given to polynomial regression and artificial neural networks."
            },
            "slug": "Complexity-Regularization-with-Application-to-Barron",
            "title": {
                "fragments": [],
                "text": "Complexity Regularization with Application to Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper defines general complexity regularization criteria and establishes bounds on the statistical risk of the estimated functions and establishes consistency, yield rates of convergence, and the near asymptotic optimality of the model selection criterion in both parametric and nonparametric cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": false,
            "numCitedBy": 3816,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39206632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1528918ae0c9f70ba6da030cb8dbc72f71bc198b",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of current speech recognition systems is far below that of humans. Neural nets offer the potential of providing massive parallelism, adaptation, and new algorithmic approaches to problems in speech recognition. Initial studies have demonstrated that multilayer networks with time delays can provide excellent discrimination between small sets of pre-segmented difficult-to-discriminate words, consonants, and vowels. Performance for these small vocabularies has often exceeded that of more conventional approaches. Physiological front ends have provided improved recognition accuracy in noise and a cochlea filter-bank that could be used in these front ends has been implemented using micro-power analog VLSI techniques. Techniques have been developed to scale networks up in size to handle larger vocabularies, to reduce training time, and to train nets with recurrent connections. Multilayer perceptron classifiers are being integrated into conventional continuous-speech recognizers. Neural net architectures have been developed to perform the computations required by vector quantizers, static pattern classifiers, and the Viterbi decoding algorithm. Further work is necessary for large-vocabulary continuous-speech problems, to develop training algorithms that progressively build internal word models, and to develop compact VLSI neural net hardware."
            },
            "slug": "Review-of-Neural-Networks-for-Speech-Recognition-Lippmann",
            "title": {
                "fragments": [],
                "text": "Review of Neural Networks for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Further work is necessary for large-vocabulary continuous-speech problems, to develop training algorithms that progressively build internal word models, and to develop compact VLSI neural net hardware."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743637"
                        ],
                        "name": "J. W. Silverstein",
                        "slug": "J.-W.-Silverstein",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Silverstein",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Silverstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52412616"
                        ],
                        "name": "Stephen A. Ritz",
                        "slug": "Stephen-A.-Ritz",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Ritz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen A. Ritz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109214855"
                        ],
                        "name": "Randall S. Jones",
                        "slug": "Randall-S.-Jones",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Jones",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randall S. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17526697,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "00c6914dab0fb75c0fe5c8d8ad57d726223b7d9b",
            "isKey": false,
            "numCitedBy": 937,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": "A previously proposed model for memory based on neurophysiolo gical considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to \"categorical perception.\" Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy."
            },
            "slug": "Distinctive-features,-categorical-perception,-and-a-Anderson-Silverstein",
            "title": {
                "fragments": [],
                "text": "Distinctive features, categorical perception, and probability learning: some applications of a neural model"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy and applies the model to \"categorical perception\" and probability learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065228513"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213649"
                        ],
                        "name": "C. Wellekens",
                        "slug": "C.-Wellekens",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wellekens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wellekens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14700006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee50abb5aff3e5c43a38f24396b9552d593a9ae0",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The statistical use of a particular classic form of a connectionist system, the multilayer perceptron (MLP), is described in the context of the recognition of continuous speech. A discriminant hidden Markov model (HMM) is defined, and it is shown how a particular MLP with contextual and extra feedback input units can be considered as a general form of such a Markov model. A link between these discriminant HMMs, trained along the Viterbi algorithm, and any other approach based on least mean square minimization of an error function (LMSE) is established. It is shown theoretically and experimentally that the outputs of the MLP (when trained along the LMSE or the entropy criterion) approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities. Results of a series of speech recognition experiments are reported. The possibility of embedding MLP into HMM is described. Relations with other recurrent networks are also explained. >"
            },
            "slug": "Links-Between-Markov-Models-and-Multilayer-Bourlard-Wellekens",
            "title": {
                "fragments": [],
                "text": "Links Between Markov Models and Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown theoretically and experimentally that the outputs of the MLP approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145230652"
                        ],
                        "name": "D. B. Schwartz",
                        "slug": "D.-B.-Schwartz",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2326064"
                        ],
                        "name": "V. Samalam",
                        "slug": "V.-Samalam",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Samalam",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Samalam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33847826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1bc210dc095c6021bf5b8657f63060c2d9107d8",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Exhaustive exploration of an ensemble of networks is used to model learning and generalization in layered neural networks. A simple Boolean learning problem involving networks with binary weights is numerically solved to obtain the entropy Sm and the average generalization ability Gm as a function of the size m of the training set. Learning curves Gm vs m are shown to depend solely on the distribution of generalization abilities over the ensemble of networks. Such distribution is determined prior to learning, and provides a novel theoretical tool for the prediction of network performance on a specific task."
            },
            "slug": "Exhaustive-Learning-Schwartz-Samalam",
            "title": {
                "fragments": [],
                "text": "Exhaustive Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple Boolean learning problem involving networks with binary weights is numerically solved to obtain the entropy Sm and the average generalization ability Gm as a function of the size m of the training set."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17763150,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "8b3c195b087ec39dcc66adb3ade24e35ff63f23f",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The learning dynamics of the back-propagation algorithm are investigated when complexity constraints are added to the standard Least Mean Square (LMS) cost function. It is shown that loss of generalization performance due to overtraining can be avoided when using such complexity constraints. Furthermore, \"energy,\" hidden representations and weight distributions are observed and compared during learning. An attempt is made at explaining the results in terms of linear and non-linear effects in relation to the gradient descent learning algorithm."
            },
            "slug": "Dynamic-Behavior-of-Constained-Back-Propagation-Chauvin",
            "title": {
                "fragments": [],
                "text": "Dynamic Behavior of Constained Back-Propagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The learning dynamics of the back-propagation algorithm are investigated when complexity constraints are added to the standard Least Mean Square (LMS) cost function and it is shown that loss of generalization performance due to overtraining can be avoided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44550777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "803f7c4ee35f3c7e8dc5c32f76c9f32701062198",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We first review in pedagogical fashion previous results which gave lower and upper bounds on the number of examples needed for training feedforward neural networks when valid generalization is desired. Experimental tests of generalization versus number of examples are then presented for random target networks and examples drawn from a uniform distribution. The experimental results are roughly consistent with the following heuristic: if a database of M examples is loaded onto a W weight net (for M\u226bW), one expects to make a fraction \u025b=W/M errors in classifying future examples drawn from the same distribution. This is consistent with our previous bounds, but if reliable strengthens them in that: (1) the bounds had large numerical constants and log factors, all of which are set equal one in the heuristic, (2) previous lower bounds on number of examples needed were valid only in a distribution independent context, whereas the experiments were conducted for a uniform distribution, and (3) the previous lower bound was valid for nets with one hidden layer only. These experiments also seem to indicate that networks with two hidden layers have Vapnik-Chervonenkis dimension roughly equal to their total number of weights."
            },
            "slug": "When-Are-k-Nearest-Neighbor-and-Back-Propagation-of-Baum",
            "title": {
                "fragments": [],
                "text": "When Are k-Nearest Neighbor and Back Propagation Accurate for Feasible Sized Sets of Examples?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental tests of generalization versus number of examples are presented for random target networks and examples drawn from a uniform distribution, and seem to indicate that networks with two hidden layers have Vapnik-Chervonenkis dimension roughly equal to their total number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "EURASIP Workshop"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121696597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09526c3ff288be85e51508f4270c920899389181",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A time\u2010delay neural network (TDNN) approach is presented to speech recognition that is characterized by two important properties: (1) Using multilayer arrangements of simple computing units, a TDNN can represent arbitrary nonlinear classification decision surfaces that are learned automatically using error back propagation. (2) The time\u2010delay arrangement enables the network to discover acoustic\u2010phonetic features and the temporal relationships between them independent of position in time and, hence, not blurred by temporal shifts in the input. The TDNNs are compared with the currently most popular technique in speech recognition, hidden Markov models (HMM). Extensive performance evaluation shows that the TDNN recognizes voiced stops extracted from varying phonetic contexts at an error rate four times lower (1.5% vs 6.3%) than the best of our HMMs. To perform this task, the TDNN \u201cinvented\u201d well\u2010known acoustic\u2010phonetic features (e.g., F2 rise, F2 fall, vowel onset) as useful abstractions. It also developed a..."
            },
            "slug": "Speech-recognition-using-time\u2010delay-neural-networks-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Speech recognition using time\u2010delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A time\u2010delay neural network approach to speech recognition that \u201cinvented\u201d well\u2010known acoustic\u2010phonetic features as useful abstractions and recognizes voiced stops extracted from varying phonetic contexts at an error rate four times lower than the best of the authors' HMMs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 97
                            }
                        ],
                        "text": "The study of neural networks in recent years has involved increasingly sophisticated mathematics (cf. Barron and Barron 1988; Barron 1989; Baum and Haussler 1989; Haussler 1989b; White 1989, 1990; Amari 1990; Amari et al. 1990; Azencott 1990; Baum 1990a), often directly connected with the statistical-inference issues discussed in the previous sections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14921581,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fedfc9fbcfe46d50b81078560bce724678f90176",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7830,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144144572"
                        ],
                        "name": "G. Martin",
                        "slug": "G.-Martin",
                        "structuredName": {
                            "firstName": "Gale",
                            "lastName": "Martin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145945979"
                        ],
                        "name": "J. Pittman",
                        "slug": "J.-Pittman",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Pittman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pittman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21326085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70927dc841596f3e3dba07b472ccc565b944ddf0",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on results of training backpropagation nets with samples of hand-printed digits scanned off of bank checks and hand-printed letters interactively entered into a computer through a stylus digitizer. Generalization results are reported as a function of training set size and network capacity. Given a large training set, and a net with sufficient capacity to achieve high performance on the training set, nets typically achieved error rates of 4-5% at a 0% reject rate and 1-2% at a 10% reject rate. The topology and capacity of the system, as measured by the number of connections in the net, have surprisingly little effect on generalization. For those developing hand-printed character recognition systems, these results suggest that a large and representative training sample may be the single, most important factor in achieving high recognition accuracy. Benefits of reducing the number of net connections, other than improving generalization, are discussed."
            },
            "slug": "Recognizing-Hand-Printed-Letters-and-Digits-Using-Martin-Pittman",
            "title": {
                "fragments": [],
                "text": "Recognizing Hand-Printed Letters and Digits Using Backpropagation Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results suggest that a large and representative training sample may be the single, most important factor in achieving high recognition accuracy in hand-printed character recognition systems, and benefits of reducing the number of net connections are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 490,
                                "start": 473
                            }
                        ],
                        "text": "Parzen windows and nearestneighbor rules (see, e.g., Duda and Hart 1973; Hardle 1990), regularization methods (see, e.g., Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 19851, recursive partitioning methods such as \"CART,\" which stands for \"Classification and Regression Trees\" (Breiman et al. 1984) ), as well as feedforward neural networks (Rumelhart et al. 1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used to construct consistent nonparametric estimators."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 118
                            }
                        ],
                        "text": "Similar remarks apply to likelihood-based (instead of least-squaresbased ) approaches, such as the Boltzmann Machine (Ackley et al. 1985; Hinton and Sejnowski 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 56
                            }
                        ],
                        "text": "1977) is related to factor analysis; Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986) compute (approximate) maximum-likelihood density estimators; and backpropagation networks realize an algorithm for nonparametric least-squares regression."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 32
                            }
                        ],
                        "text": "1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used to construct consistent nonparametric estimators."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "\u2026and Regression Trees\" (Breiman et al. 1984) ), as well as feedforward neural networks (Rumelhart et al. 1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used to construct consistent nonparametric estimators."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Boltzmann Machine implements a Monte Carlo computational algorithm for increasing likelihood."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 116
                            }
                        ],
                        "text": "Similar remarks apply to likelihood-based (instead of least-squaresbased) approaches, such as the Boltzmann Machine (Ackley et al. 1985; Hinton and Sejnowski 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": true,
            "numCitedBy": 3395,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17352,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142501319,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0296aff580e2f0b1637bbbd6db3cc9b43b5c99fc",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This work is a theoretical investigation of some consequences of the hypothesis that transmission efficacies of synapses in the Central Nervous System (CNS) undergo modification on a short time-scale. Short-term synaptic plasticity appears to be an almost necessary condition for the existence of activity states in the CNS which are stable for about 1 sec., the time-scale of psychological processes. It gives rise to joint \u201cactivity-and-connectivity\u201d dynamics. This dynamics selects and stabilizes particular high-order statistical relationships in the timing of neuronal firing; at the same time, it selects and stabilizes particular connectivity patterns. In analogy to statistical mechanics, these stable states, the attractors of the dynamics, can be viewed as the minima of a hamiltonian, or cost function. It is found that these low-cost states, termed synaptic patterns, are topologically organized. Two important properties of synaptic patterns are demonstrated: (i) synaptic patterns can be \u201cmemorized\u201d and later \u201cretrieved\u201d, and (ii) synaptic patterns have a tendency to assemble into compound patterns according to simple topological rules. A model of position-invariant and size-invariant pattern recognition based on these two properties is briefly described. It is suggested that the scheme of a synaptic pattern may be more adapted than the classical cell-assembly notion for explaining cognitive abilities such as generalization and categorization, which pertain to the notion of invariance."
            },
            "slug": "Statistical-Coding-and-Short-Term-Synaptic-A-Scheme-Malsburg-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Statistical Coding and Short-Term Synaptic Plasticity: A Scheme for Knowledge Representation in the Brain"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is suggested that the scheme of a synaptic pattern may be more adapted than the classical cell-assembly notion for explaining cognitive abilities such as generalization and categorization, which pertain to the notion of invariance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1958776"
                        ],
                        "name": "R. Azencott",
                        "slug": "R.-Azencott",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Azencott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Azencott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60261333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3db66c3ea6f38c427d427f1146d78509f6729acb",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The Boltzmann machines are stochastic networks of formal neurons linked by a quadratic energy function. Hinton, Sejnowski and Ackley who introduced them as pattern classifiers that learn, have proposed a learning algorithm for the asynchronous machine. Here we study the synchronous machine where all neurons are simultaneously updated, we compute its equilibrium energy, and propose a synchronous learning algorithm based on delayed average coactivity of pairs of connected neurons. We generalize the Boltzmann machine paradigm to much wider types of interactions and energies allowing multiple interactions of arbitrary order. We propose a learning algorithm for these generalized machines using the theory of Gibbs fields and parameter estimation for such fields. We give quasi-convergence results for all these algorithms, within the framework of stochastic algorithms theory. The links between generalized Boltzmann machines and Markov field models sketched here provide the groundwork for designing generalized Boltzmann machines capable of performing efficient low level vision tasks. These Boltzmann vision modules are described in a forthcoming paper."
            },
            "slug": "Synchronous-Boltzmann-Machines-and-Gibbs-Fields:-Azencott",
            "title": {
                "fragments": [],
                "text": "Synchronous Boltzmann Machines and Gibbs Fields: Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The synchronous machine where all neurons are simultaneously updated, the equilibrium energy is computed, and a synchronous learning algorithm based on delayed average coactivity of pairs of connected neurons is proposed, within the framework of stochastic algorithms theory."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061358113"
                        ],
                        "name": "Edward Rosenfeld",
                        "slug": "Edward-Rosenfeld",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8160958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d0e5a21512d2aea34026f83b1ff86ea30b8c0d6",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "ion for Knowledge Acquisition\u201d by T. Bylander and B. Chadrasekaran. Chandrasekaran\u2019s papers are usually illuminating, and this one does not fail: He and Bylander re-examine such traditional beliefs as knowledge should be uniformly represented and controlled and the knowledge base should be separated from the inference engine. The final 10 papers in volume 1 discuss generalized learning and ruleinduction techniques. They are interesting and informative, particularly \u201cGeneralization and Noise\u201d by Y. Kodratoff and M. Manango, which discusses symbolic and numeric rule induction. Most rule-induction techniques focus on the use of examples and numeric analysis such as repertory grids. Kodratoff\u2019s and Manango\u2019s exploration of how the two complement each other is refreshing. Because of their technical nature and the amount of work it would take to put their content to use, most of the papers in this section of the volume are more appropriate for a specialized or research-oriented group. For those just getting involved in knowledge-based\u2013systems development, Knowledge Acquisition Tools for Expert Systems is the more useful volume. In addition to discussing the tools themselves, most of the papers contain details of the knowledgeacquisition techniques that are automated, thus providing much of the same information which is available in the first volume. As an added benefit, they also often discuss the underlying architectures for solving domain-specific problems. For instance, the details of the medical diagnostic architecture laid out in \u201cDesign for Acquisition: Principles of Knowledge System Design to Facilitate Knowledge Acquisition\u201d by T. R. Gruber and P. R. Cohen are almost as useful as the discussion of how to build a knowledge-acquisition system. Volume 2 is particularly germane given the rise in commercial interest about automated knowledge acquisition following this year\u2019s introduction of Neuron Data\u2019s NEXTRATM product and last year\u2019s introduction of Test Bench by Texas Instruments. Test Bench is actually discussed in \u201cA Mixed-Initiative Workbench for Knowledge Acquisition\u201d by G. S. Kahn, E. H. Breaux, P. De Klerk, and R. L. Joseph. This volume provides the background necessary to evaluate knowledge-acquisition tools such as NEXTRA, Test Bench, and AutoIntelligence (IntelligenceWare). The vendors of knowledge-based\u2013systems development tools, for example, Inference, IntelliCorp, Aion, AI Corp., and IBM, would do well to pay heed to these books because they point the way to removing the knowledge bottleneck from knowledge-based\u2013systems development. Overall, the papers in both volumes are comprehensive and well integrated, a sometimes difficult state to achieve when compiling a collection of papers resulting from a small conference. The collection is comparable to Anna Hart\u2019s Knowledge Acquisition for Expert Systems (McGraw-Hill, 1986), but it is broader in scope and not as structured. The arrangement of the papers is marred only by an overly brief index. Few readers can be expected to read a collection from beginning to end, and a better index would facilitate more enlightened use. Less important\u2014but nevertheless distracting\u2014is the large number of typographical errors in both volumes. In conclusion, the set is recommended for both the commercial and research knowledge-based\u2013systems practitioner. Reading the volumes in reverse order might be more useful to the commercial developer given the extra information available in volume 2. Neurocomputing: Foundations of Research"
            },
            "slug": "Neurocomputing:-Foundations-of-Research-Anderson-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Neurocomputing: Foundations of Research"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The set is recommended for both the commercial and research knowledge-based\u2013systems practitioner and provides the background necessary to evaluate knowledge-acquisition tools such as NEXTRA, Test Bench, and AutoIntelligence (IntelligenceWare)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45670932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59fbb0b127de1ea9be27d5f8c1e30d0cf394a503",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how to formulate matching and combinatorial problems of vision and neural network theory by generalizing elastic and deformable templates models to include binary matching elements. Techniques from statistical physics, which can be interpreted as computing marginal probability distributions, are then used to analyze these models and are shown to (1) relate them to existing theories and (2) give insight into the relations between, and relative effectivenesses of, existing theories. In particular we exploit the power of statistical techniques to put global constraints on the set of allowable states of the binary matching elements. The binary elements can then be removed analytically before minimization. This is demonstrated to be preferable to existing methods of imposing such constraints by adding bias terms in the energy functions. We give applications to winner-take-all networks, correspondence for stereo and long-range motion, the traveling salesman problem, deformable template matching, learning, content addressable memories, and models of brain development. The biological plausibility of these networks is briefly discussed."
            },
            "slug": "Generalized-Deformable-Models,-Statistical-Physics,-Yuille",
            "title": {
                "fragments": [],
                "text": "Generalized Deformable Models, Statistical Physics, and Matching Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Techniques from statistical physics are used to exploit the power of statistical techniques to put global constraints on the set of allowable states of the binary matching elements and be preferable to existing methods of imposing such constraints by adding bias terms in the energy functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143736719"
                        ],
                        "name": "E. Hartman",
                        "slug": "E.-Hartman",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Hartman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hartman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1952587"
                        ],
                        "name": "J. Keeler",
                        "slug": "J.-Keeler",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Keeler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Keeler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39221643"
                        ],
                        "name": "J. Kowalski",
                        "slug": "J.-Kowalski",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Kowalski",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kowalski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44931577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68ec8a1e9aea1916e2280489729bab74d5bf6631",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network with a single layer of hidden units of gaussian type is proved to be a universal approximator for real-valued maps defined on convex, compact sets of Rn."
            },
            "slug": "Layered-Neural-Networks-with-Gaussian-Hidden-Units-Hartman-Keeler",
            "title": {
                "fragments": [],
                "text": "Layered Neural Networks with Gaussian Hidden Units as Universal Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A neural network with a single layer of hidden units of gaussian type is proved to be a universal approximator for real-valued maps defined on convex, compact sets of Rn."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2991937,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e62f7643f616aaad65ffd47155a53bfa325e455d",
            "isKey": false,
            "numCitedBy": 1356,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A summary of brain theory is given so far as it is contained within the framework of Localization Theory. Diffculties of this \"conventional theory\" are traced back to a specific deficiency: there is no way to express relations between active cells (as for instance their representing parts of the same object). A new theory is proposed to cure this deficiency. It introduces a new kind of dynamical control, termed synaptic modulation, according to which synapses switch between a conducting and a non- conducting state. The dynamics of this variable is controlled on a fast time scale by correlations in the temporal fine structure of cellular signals. Furthermore, conventional synaptic plasticity is replaced by a refined version. Synaptic modulation and plasticity form the basis for short-term and long-term memory, respectively. Signal correlations, shaped by the variable network, express structure and relationships within objects. In particular, the figure-ground problem may be solved in this way. Synaptic modulation introduces flexibility into cerebral networks which is necessary to solve the invariance problem. Since momentarily useless connections are deactivated, interference between different memory traces can be reduced, and memory capacity increased, in comparison with conventional associative memory."
            },
            "slug": "The-Correlation-Theory-of-Brain-Function-Malsburg",
            "title": {
                "fragments": [],
                "text": "The Correlation Theory of Brain Function"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A summary of brain theory is given so far as it is contained within the framework of Localization Theory, according to which synaptic modulation introduces flexibility into cerebral networks which is necessary to solve the invariance problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2015030"
                        ],
                        "name": "W. H\u00e4rdle",
                        "slug": "W.-H\u00e4rdle",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "H\u00e4rdle",
                            "middleNames": [
                                "Karl"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. H\u00e4rdle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143971171"
                        ],
                        "name": "P. Hall",
                        "slug": "P.-Hall",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145956495"
                        ],
                        "name": "J. Marron",
                        "slug": "J.-Marron",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Marron",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Marron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1917077,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3ff67239fe6a4d63f97b4ae7459f6c09a7fb737d",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We address the problem of smoothing parameter selection for nonparametric curve estimators in the specific context of kernel regression estimation. Call the \u201coptimal bandwidth\u201d the minimizer of the average squared error. We consider several automatically selected bandwidths that approximate the optimum. How far are the automatically selected bandwidths from the optimum? The answer is studied theoretically and through simulations. The theoretical results include a central limit theorem that quantifies the convergence rate and gives the differences asymptotic distribution. The convergence rate turns out to be excruciatingly slow. This is not too disappointing, because this rate is of the same order as the convergence rate of the difference between the minimizers of the average squared error and the mean average squared error. In some simulations by John Rice, the selectors considered here performed quite differently from each other. We anticipated that these differences would be reflected in differ..."
            },
            "slug": "How-Far-are-Automatically-Chosen-Regression-from-H\u00e4rdle-Hall",
            "title": {
                "fragments": [],
                "text": "How Far are Automatically Chosen Regression Smoothing Parameters from their Optimum"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62710001,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9b486c647916df9f8be0f8d4fc5c94c493bfaa80",
            "isKey": false,
            "numCitedBy": 1904,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light."
            },
            "slug": "PRINCIPLES-OF-NEURODYNAMICS.-PERCEPTRONS-AND-THE-OF-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons are reviewed, and some of the notation to be used in later sections are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 11
                            }
                        ],
                        "text": "For a precise technical definition of Vapnikzervonenkis dimension (and some generalizations), as well as for demonstrations of its utility in establishing uniform convergence results, the reader is referred to Vapnik (1982), Pollard (1984), Dudley (1987), and Haussler (1989a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 8
                            }
                        ],
                        "text": "thereby suggesting efficient parallel, and possibly even ultrafast, analog, implementations. Examples of networks based upon projection pursuit can be found in Intrator (1990) and Maechler ef ul. (1990). Modern nonparametric statistical methods, and hence many recent neural models, are important tools for wide-ranging applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 640,
                                "start": 7
                            }
                        ],
                        "text": "The cross-validated smoothing parameter is the minimizer of A( A), which we will denote by A*. The cross-validated estimator is thenf(x;N, A', DN) . Cross-validation is computation-intensive. In the worst case, we need to form N estimators at each value of A, to generate A(A), and then to find a global minimum with respect to A. Actually, the computation can often be very much reduced by introducing closely related (sometimes even better) assessment functions, or by taking advantage of special structure in the particular function f ( x ; N, A, D N ) at hand. The reader is referred to Wahba (1984,1985) and OSullivan and Wahba (1985) for various generalizations of cross-validation, as well as for support of the method in the way of analytic arguments and experiments with numerous applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 26
                            }
                        ],
                        "text": "the derivative at the knots. When m = 2 the polynomials are cubic, the first two derivatives are continuous at the knots, and the curve appears globally \u201csmooth.\u201d Poggio and Girosi (1990) have shown how splines and related estimators can be computed with multilayer networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 4
                            }
                        ],
                        "text": "ments with handwritten numerals that better results could be achieved by stopping the gradient descent well short of convergence; see, for example, Chauvin (1990) and Morgan and Bourlard (1990) who report on similar findings."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 11
                            }
                        ],
                        "text": "For a precise technical definition of Vapnikzervonenkis dimension (and some generalizations), as well as for demonstrations of its utility in establishing uniform convergence results, the reader is referred to Vapnik (1982), Pollard (1984), Dudley (1987), and Haussler (1989a). Putting aside the details, the important point here is that the definition can be used constructiuely to measure the size of a class of functions, such as the one defined in 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1175,
                                "start": 7
                            }
                        ],
                        "text": "The cross-validated smoothing parameter is the minimizer of A( A), which we will denote by A*. The cross-validated estimator is thenf(x;N, A', DN) . Cross-validation is computation-intensive. In the worst case, we need to form N estimators at each value of A, to generate A(A), and then to find a global minimum with respect to A. Actually, the computation can often be very much reduced by introducing closely related (sometimes even better) assessment functions, or by taking advantage of special structure in the particular function f ( x ; N, A, D N ) at hand. The reader is referred to Wahba (1984,1985) and OSullivan and Wahba (1985) for various generalizations of cross-validation, as well as for support of the method in the way of analytic arguments and experiments with numerous applications. In fact, there is now a large statistical literature on cross-validation and related methods (Scott and Terrell 1987; Hardle et al. 1988; Marron 1988; Faraway and Jhun 1990; Hardle 1990 are some recent examples), and there have been several papers in the neural network literature as well - see White (1988,19901, Hansen and Salamon (19901, and Morgan and Bourlard (1990). Computational issues aside, the resulting estimator, f ( x ; N, A*, DN) , is often strikingly effective, although some \"pathological\" behaviors have been pointed out (see, tor example, Schuster and Gregory 1981)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2878,
                                "start": 10
                            }
                        ],
                        "text": "gular cells, and estimates the class-probabilities { P ( y = k) : k = 1, . . . , m } within each cell. Criteria are defined that promote cells in which the estimated class probabilities are well-peaked around a single class, and at the same time discourage partitions into large numbers of cells, relative to N. CART provides a family of recursive partitioning algorithms for approximately optimizing a combination of these competing criteria. The GM problem solved by CART concerned the casting of certain engine-block components. A new technology known as lost-foam casting promises to alleviate the high scrap rate associated with conventional casting methods. A Styrofoam \u201dmodel\u201d of the desired part is made, and then surrounded by packed sand. Molten metal is poured onto the Styrofoam, which vaporizes and escapes through the sand. The metal then solidifies into a replica of the styrofoam model. Many \u201cprocess variables\u201d enter into the procedure, involving the settings of various temperatures, pressures, and other parameters, as well as the detailed composition of the various materials, such as sand. Engineers identified 80 such variables that were expected to be of particular importance, and data were collected to study the relationship between these variables and the likelihood of success of the lost-foam casting procedure. (These variables are proprietary.) Straightforward data analysis on a training set of 470 examples revealed no good \u201cfirst-order\u201d predictors of success of casts (a binary variable) among the 80 process variables. Figure 1 (from Lorenzen 1988) shows a histogram comparison for that variable that was judged to have the most visually disparate histograms among the 80 variables: the left histogram is from a population of scrapped casts, and the right is from a population of accepted casts. Evidently, this variable has no important prediction power in isolation from other variables. Other data analyses indicated similarly that no obvious low-order multiple relations could reliably predict success versus failure. Nevertheless, the CART procedure identified achievable regions in the space of process variables that reduced the scrap rate in this production facility by over 75%. As might be expected, this success was achieved by a useful mix of the nonparametric algorithm, which in principal is fully automatic, and the statistician\u2019s need to bring to bear the realities and limitations of the production process. In this regard, several important modifications were made to the standard CART algorithm. Nevertheless, the result is a striking affirmation of the potential utility of nonparametric methods. There have been many success stories for nonparametric methods. An intriguing application of CART to medical diagnosis is reported in Goldman et al. (19821, and further examples with CART can be found in Breiman et al. (1984). The recent statistics and neural network literatures contain examples of the application of other nonparametric methods as well."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31220579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1339348aeef592802288d9d929a085cb3ae61c4b",
            "isKey": true,
            "numCitedBy": 451,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions."
            },
            "slug": "A-Theory-of-Adaptive-Pattern-Classifiers-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Adaptive Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions, and there is an important tradeoff between speed and accuracy of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955900"
                        ],
                        "name": "C. Hwang",
                        "slug": "C.-Hwang",
                        "structuredName": {
                            "firstName": "Chii-Ruey",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hwang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 41
                            }
                        ],
                        "text": "4 is then to cover FM with \"small balls\" (see, for example, Grenander 1981; Geman and Hwang 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026e.g., Duda and Hart 1973; Hardle 1990), regularization methods (see, e.g., Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 19851, recursive partitioning methods such as \"CART,\" which stands for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 55
                            }
                        ],
                        "text": ", Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 1985), recursive partitioning methods such as \"CART,\" which stands for \"Classification and Regression Trees\" (Breiman et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14650349,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2d00d92e9ff17ce7fe991e700a1ce8ced639c2c8",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum likelihood estimation often fails when the parameter takes values in an infinite dimensional space. For example, the maximum likelihood method cannot be applied to the completely nonparametric estimation of a density function from an iid sample; the maximum of the likelihood is not attained by any density. In this example, as in many other examples, the parameter space (positive functions with area one) is too big. But the likelihood method can often be salvaged if we first maximize over a constrained subspace of the parameter space and then relax the constraint as the sample size grows. This is Grenander's \"method of sieves.\" Application of the method sometimes leads to new estimators for familiar problems, or to a new motivation for an already well-studied technique. We will establish some general consistency results for the method, and then we will focus on three applications."
            },
            "slug": "Nonparametric-Maximum-Likelihood-Estimation-by-the-Geman-Hwang",
            "title": {
                "fragments": [],
                "text": "Nonparametric Maximum Likelihood Estimation by the Method of Sieves"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37115712"
                        ],
                        "name": "M. L. Rossen",
                        "slug": "M.-L.-Rossen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Rossen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L. Rossen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3197398"
                        ],
                        "name": "S. R. Viscuso",
                        "slug": "S.-R.-Viscuso",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Viscuso",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. R. Viscuso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2196304"
                        ],
                        "name": "M. Sereno",
                        "slug": "M.-Sereno",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Sereno",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sereno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 3
                            }
                        ],
                        "text": "Neural Computation 4,l-58 (1992) @ 1992 Massachusetts Institute of Technology"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60258009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7316b3baddf4836478520bc2041ce79e5b2aeb68",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Simple network models for object motion, perception of spoken syllables, and learning of simple arithmetic are discussed. Many of the same data representation techniques can be used for all three of these highly dissimilar tasks."
            },
            "slug": "Experiments-with-Representation-in-Neural-Networks:-Anderson-Rossen",
            "title": {
                "fragments": [],
                "text": "Experiments with Representation in Neural Networks: Object Motion, Speech, and Arithmetic"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Simple network models for object motion, perception of spoken syllables, and learning of simple arithmetic are discussed and many of the same data representation techniques can be used for all three of these highly dissimilar tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34906458"
                        ],
                        "name": "P. Carnevali",
                        "slug": "P.-Carnevali",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Carnevali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Carnevali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067126"
                        ],
                        "name": "S. Patarnello",
                        "slug": "S.-Patarnello",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Patarnello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Patarnello"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120641834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38edbfcb199759b58aa15dab7c896256ff008dcd",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The learning and generalization capabilities exhibited by a recently introduced model of self-organizing Boolean networks are explained and interpreted by studying the thermodynamics of the training process of that model. The thermodynamical analysis shows that learning and generalization occur as a direct consequence of the second law of thermodynamics. The complexity of solving a problem, for a given architecture, can be precisely defined in terms of entropy changes and is related to the amount of specialization present in that architecture. We speculate our conclusions to be valid, to some extent, not only for our model, but also for more general and complex systems, including biological learning systems."
            },
            "slug": "Exhaustive-Thermodynamical-Analysis-of-Boolean-Carnevali-Patarnello",
            "title": {
                "fragments": [],
                "text": "Exhaustive Thermodynamical Analysis of Boolean Learning Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The learning and generalization capabilities exhibited by a recently introduced model of self-organizing Boolean networks are explained and interpreted by studying the thermodynamics of the training process of that model and are speculated to be valid, to some extent, not only for that model, but also for more general and complex systems, including biological learning systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46345617"
                        ],
                        "name": "D. W. Scott",
                        "slug": "D.-W.-Scott",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Scott",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. W. Scott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69499807"
                        ],
                        "name": "G. Terrell",
                        "slug": "G.-Terrell",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Terrell",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Terrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123681190,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a28229d6a68b271cf3b5f9bafc053960b3cd1612",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Nonparametric density estimation requires the specification of smoothing parameters. The demands of statistical objectivity make it highly desirable to base the choice on properties of the data set. In this article we introduce some biased cross-validation criteria for selection of smoothing parameters for kernel and histogram density estimators, closely related to one investigated in Scott and Factor (1981). These criteria are obtained by estimating L 2 norms of derivatives of the unknown density and provide slightly biased estimates of the average squared L 2 error or mean integrated squared error. These criteria are roughly the analog of Wahba's (1981) generalized cross-validation procedure for orthogonal series density estimators. We present the relationship of the biased cross-validation procedure to the least squares cross-validation procedure, which provides unbiased estimates of the mean integrated squared error. Both methods are shown to be based on U statistics. We compare the two metho..."
            },
            "slug": "Biased-and-Unbiased-Cross-Validation-in-Density-Scott-Terrell",
            "title": {
                "fragments": [],
                "text": "Biased and Unbiased Cross-Validation in Density Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221967737,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9430105f125277f3b32380ec85c965fc796dd580",
            "isKey": false,
            "numCitedBy": 1808,
            "numCiting": 274,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A set of hypotheses is formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts consciously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models. Higher-level analyses of these connectionist models reveal subtle relations to symbolic models. Parallel connectionist memory and linguistic processes are hypothesized to give rise to processes that are describable at a higher level as sequential rule application. At the lower level, computation has the character of massively parallel satisfaction of soft numerical constraints; at the higher level, this can lead to competence characterizable by hard rules. Performance will typically deviate from this competence since behavior is achieved not by interpreting hard rules but by satisfying soft constraints. The result is a picture in which traditional and connectionist theoretical constructs collaborate intimately to provide an understanding of cognition."
            },
            "slug": "On-the-proper-treatment-of-connectionism-Smolensky",
            "title": {
                "fragments": [],
                "text": "On the proper treatment of connectionism"
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral and Brain Sciences"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145230652"
                        ],
                        "name": "D. B. Schwartz",
                        "slug": "D.-B.-Schwartz",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726209"
                        ],
                        "name": "B. Wittner",
                        "slug": "B.-Wittner",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Wittner",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wittner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7508740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33fdc91c520b54e097f5e09fae1cfc94793fbfcf",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Since an tiquity, man has dreamed of building a de vice that would \"learn from examples\" 1 \"form generalizations\", and \"discover t he rules\" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be \"curve fit\" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n \" regularized\", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he \"human \" solut ions stable against perturbations. Fortuna tely, the re are \u00a9 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of \"program ming\" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem (\"c l umps\") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that \"automat ic learn ing will always succeed, given t he right preprocessor,\" but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the \"programming\" or \"architecture\" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be \"programmed\" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a \"7\" or a \"Q\" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could \"discover t he rules\" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of \"test cases\" where t here was an alternativeth at is, where the \"correct\" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, \"moment um term s\" , \"weight decay te rms\" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing"
            },
            "slug": "Large-Automatic-Learning,-Rule-Extraction,-and-Denker-Schwartz",
            "title": {
                "fragments": [],
                "text": "Large Automatic Learning, Rule Extraction, and Generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061439286"
                        ],
                        "name": "Edward Collins",
                        "slug": "Edward-Collins",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49870691"
                        ],
                        "name": "Sushmito Ghosh",
                        "slug": "Sushmito-Ghosh",
                        "structuredName": {
                            "firstName": "Sushmito",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sushmito Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957320"
                        ],
                        "name": "C. Scofield",
                        "slug": "C.-Scofield",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Scofield",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Scofield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 268
                            }
                        ],
                        "text": "On the other hand, very large training sets are available, and it makes good sense to try less parametric methods, such as the backpropagation algorithm , the nearest-neighbor algorithm, or the \" Multiple-Neural-Network Learning System \" advocated for this problem by Collins et al. (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "A much-advertised neural network example is the evaluation of loan applications (cf. Collins et al. 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14840004,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "c91d34aa6dd5e866e8fbb13eb2892fb4f6e258ff",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A multiple neural network learning system (MNNLS) was used to replicate the decisions made by mortgage insurance-underwriters. The MNNLS was trained on previous underwriter judgements and learned to mimic their underwriting skills. The system reached a high degree of agreement with human underwriters when testing on previously unseen examples. Disagreements were examined using case studies, a single feature distribution analysis and a quality analysis. These studies indicate that human underwriters in many cases disagree with one another and are inconsistent in the use of their underwriting guidelines. It was found that when the MNNLS system and the underwriter disagree, the system's classifications are more consistent with the guidelines than the underwriter's judgement.<<ETX>>"
            },
            "slug": "An-application-of-a-multiple-neural-network-system-Collins-Ghosh",
            "title": {
                "fragments": [],
                "text": "An application of a multiple neural network learning system to emulation of mortgage underwriting judgements"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It was found that when the MNNLS system and the underwriters disagree, the system's classifications are more consistent with the guidelines than the underwriter's judgement."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 97
                            }
                        ],
                        "text": "The study of neural networks in recent years has involved increasingly sophisticated mathematics (cf. Barron and Barron 1988; Barron 1989; Baum and Haussler 1989; Haussler 1989b; White 1989, 1990; Amari 1990; Amari et al. 1990; Azencott 1990; Baum 1990a), often directly connected with the statistical-inference issues discussed in the previous sections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28610806,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "dd8c37b359e6bfe186683c4874ab964487639e4d",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dualistic-geometry-of-the-manifold-of-higher-order-Amari",
            "title": {
                "fragments": [],
                "text": "Dualistic geometry of the manifold of higher-order neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058739668"
                        ],
                        "name": "F. Crick",
                        "slug": "F.-Crick",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Crick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Crick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5892527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66752d19577b2007d4b5e3a20c6ddeb8f1d1e600",
            "isKey": false,
            "numCitedBy": 646,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain. Unfortunately most of these neural nets are unrealistic in important respects."
            },
            "slug": "The-recent-excitement-about-neural-networks-Crick",
            "title": {
                "fragments": [],
                "text": "The recent excitement about neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain, but most of these neural nets are unrealistic in important respects."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari ef al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36671080,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a35203c70c6ed6a95faacd4f1c71a51692af37fb",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The probably approximately correct (PAC) model of learning from examples is generalized. The problem of learning functions from a set X into a set Y is considered, assuming only that the examples are generated by independent draws according to an unknown probability measure on X*Y. The learner's goal is to find a function in a given hypothesis space of functions from X into Y that on average give Y values that are close to those observed in random examples. The discrepancy is measured by a bounded real-valued loss function. The average loss is called the error of the hypothesis. A theorem on the uniform convergence of empirical error estimates to true error rates is given for certain hypothesis spaces, and it is shown how this implies learnability. A generalized notion of VC dimension that applies to classes of real-valued functions and a notion of capacity for classes of functions that map into a bounded metric space are given. These measures are used to bound the rate of convergence of empirical error estimates to true error rates, giving bounds on the sample size needed for learning using hypotheses in these classes. As an application, a distribution-independent uniform convergence result for certain classes of functions computed by feedforward neural nets is obtained. Distribution-specific uniform convergence results for classes of functions that are uniformly continuous on average are also obtained.<<ETX>>"
            },
            "slug": "Generalizing-the-PAC-model:-sample-size-bounds-from-Haussler",
            "title": {
                "fragments": [],
                "text": "Generalizing the PAC model: sample size bounds from metric dimension-based uniform convergence results"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The probably approximately correct (PAC) model of learning from examples is generalized, and a distribution-independent uniform convergence result for certain classes of functions computed by feedforward neural nets is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "30th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51078293"
                        ],
                        "name": "E. F. Schuster",
                        "slug": "E.-F.-Schuster",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Schuster",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. F. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054076135"
                        ],
                        "name": "G. Gregory",
                        "slug": "G.-Gregory",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Gregory",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gregory"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117466966,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5df3f8cd3b140627206218cf7d1549a30d0afc99",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "One criterion proposed in the literature for selecting the smoothing parameter(s) in RosenblattParzen nonparametric constant kernel estimators of a probability density function is a leave-out-one-at-a-time nonparametric maximum likelihood method. Empirical work with this estimator in the univariate case showed that it worked quite well for short tailed distributions. However, it drastically oversmoothed for long tailed distributions. In this paper it is shown that this nonparametric maximum likelihood method will not select consistent estimates of the density for long tailed distributions such as the double exponential and Cauchy distributions. A remedy which was found for estimating long tailed distributions was to apply the nonparametric maximum likelihood procedure to a variable kernel class of estimators. This paper considers one data set, which is a pseudo-random sample of size 100 from a Cauchy distribution, to illustrate the problem with the leave-out-one-at-a-time nonparametric maximum likelihood method and to illustrate a remedy to this problem via a variable kernel class of estimators."
            },
            "slug": "On-the-Nonconsistency-of-Maximum-Likelihood-Density-Schuster-Gregory",
            "title": {
                "fragments": [],
                "text": "On the Nonconsistency of Maximum Likelihood Nonparametric Density Estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 122
                            }
                        ],
                        "text": "Parzen windows and nearestneighbor rules (see, e.g., Duda and Hart 1973; Hardle 1990), regularization methods (see, e.g., Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 19851, recursive partitioning\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118571164,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "09e6ca99220c47e5622d4272083cf0e3c4017eac",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Constrained-Regularization-for-Ill-Posed-Linear-in-Wahba",
            "title": {
                "fragments": [],
                "text": "Constrained Regularization for Ill Posed Linear Operator Equations, with Applications in Meteorology and Medicine."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14649340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5caeae74c2638db84ce6793e5cea60d908a8b7a0",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the context of Valiant's protocol for learning, the perceptron algorithm is shown to learn an arbitrary half-space in time O(n2/\u220a3) if D, the probability distribution of examples, is taken uniform over the unit sphere Sn. Here \u220a is the accuracy parameter. This is surprisingly fast, as standard approaches involve solution of a linear programming problem involving (n/\u220a) constraints in n dimensions. A modification of Valiant's distribution-independent protocol for learning is proposed in which the distribution and the function to be learned may be chosen by adversaries, however these adversaries may not communicate. It is argued that this definition is more reasonable and applicable to real world learning than Valiant's. Under this definition, the perceptron algorithm is shown to be a distribution-independent learning algorithm. In an appendix we show that, for uniform distributions, some classes of infinite V-C dimension including convex sets and a class of nested differences of convex sets are learnable."
            },
            "slug": "The-Perceptron-Algorithm-is-Fast-for-Nonmalicious-Baum",
            "title": {
                "fragments": [],
                "text": "The Perceptron Algorithm is Fast for Nonmalicious Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modification of Valiant's distribution-independent protocol for learning is proposed in which the distribution and the function to be learned may be chosen by adversaries, however these adversaries may not communicate."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3358277"
                        ],
                        "name": "E. Veklerov",
                        "slug": "E.-Veklerov",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Veklerov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Veklerov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145264121"
                        ],
                        "name": "J. Llacer",
                        "slug": "J.-Llacer",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Llacer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Llacer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 30637447,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "7aef08d0685023f62b3370cddc749eb827005a12",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "It is known that when the maximum likelihood estimator (MLE) algorithm passes a certain point, it produces images that begin to deteriorate. We propose a quantitative criterion with a simple probabilistic interpretation that allows the user to stop the algorithm just before this effect begins. The MLE algorithm searches for the image that has the maximum probability to generate the projection data. The underlying assumption of the algorithm is a Poisson distribution of the data. Therefore, the best image, according to the MLE algorithm, is the one that results in projection means which are as close to the data as possible. It is shown that this goal conflicts with the assumption that the data are Poisson-distributed. We test a statistical hypothesis whereby the projection data could have been generated by the image produced after each iteration. The acceptance or rejection of the hypothesis is based on a parameter that decreases as the images improve and increases as they deteriorate. We show that the best MLE images, which pass the test, result in somewhat lower noise in regions of high activity than the filtered back-projection results and much improved images in low activity regions. The applicability of the proposed stopping rule to other iterative schemes is discussed."
            },
            "slug": "Stopping-Rule-for-the-MLE-Algorithm-Based-on-Veklerov-Llacer",
            "title": {
                "fragments": [],
                "text": "Stopping Rule for the MLE Algorithm Based on Statistical Hypothesis Testing"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work proposes a quantitative criterion with a simple probabilistic interpretation that allows the user to stop the MLE algorithm just before this effect begins, and test a statistical hypothesis whereby the projection data could have been generated by the image produced after each iteration."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Medical Imaging"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145956495"
                        ],
                        "name": "J. Marron",
                        "slug": "J.-Marron",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Marron",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Marron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15860483,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f77932679eecbd03fe5fa3a71cc30657a2350ff",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a survey of recent developments in smoothing parameter selection for curve estimation. The first goal of this paper is to provide an introduction to the methods available, with discussion at both a practical and also a nontechnical level, including comparison of methods. The second goal is to provide access to the literature, especially on smoothing parameter selection, but also on curve estimation in general. The two main settings considered here are nonparametric regression and probability density estimation, although the points made apply to other settings as well. These points also apply to many different estimators, although the focus is on kernel estimators, because they are the most easily understood and motivated, and have been at the heart of the development in the field."
            },
            "slug": "Automatic-smoothing-parameter-selection:-A-survey-Marron",
            "title": {
                "fragments": [],
                "text": "Automatic smoothing parameter selection: A survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2223248"
                        ],
                        "name": "J. Faraway",
                        "slug": "J.-Faraway",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Faraway",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Faraway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726532"
                        ],
                        "name": "M. Jhun",
                        "slug": "M.-Jhun",
                        "structuredName": {
                            "firstName": "Myoungshic",
                            "lastName": "Jhun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jhun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122176410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6aea5b8d82b98fb46480446a0dc4bc615e70eb1",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A bootstrap-based choice of bandwidth for kernel density estimation is introduced. The method works by estimating the integrated mean squared error (IMSE) for any given bandwidth and then minimizing over all bandwidths. A straightforward application of the bootstrap method to estimate the IMSE fails because it does not capture the bias component. A smoothed bootstrap method based on an initial density estimate is described that solves this problem. It is possible to construct pointwise and simultaneous confidence intervals for the density. The simulation study compares cross-validation and the bootstrap method over a wide range of densities\u2014a long-tailed, a short-tailed, an asymmetric, and a bimodal, among others. The bootstrap method uniformly outperforms cross-validation. The accuracy of the constructed confidence bands improves as the sample size increases."
            },
            "slug": "Bootstrap-choice-of-bandwidth-for-density-Faraway-Jhun",
            "title": {
                "fragments": [],
                "text": "Bootstrap choice of bandwidth for density estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A smoothed bootstrap method based on an initial density estimate is described that solves the problem of how to construct pointwise and simultaneous confidence intervals for the density and uniformly outperforms cross-validation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 138
                            }
                        ],
                        "text": "Similar remarks apply to likelihood-based (instead of least-squaresbased ) approaches, such as the Boltzmann Machine (Ackley et al. 1985; Hinton and Sejnowski 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 158
                            }
                        ],
                        "text": "\u2026and Regression Trees\" (Breiman et al. 1984) ), as well as feedforward neural networks (Rumelhart et al. 1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used to construct consistent nonparametric estimators."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17651092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87953825b0bea2a5d52bfccf09d2518295c5053",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units. This skeletonization technique can be used to simplify networks by eliminating units that convey redundant information; to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away, thereby constraining generalization; and to understand the behavior of networks in terms of minimal \"rules.\""
            },
            "slug": "Skeletonization:-A-Technique-for-Trimming-the-Fat-a-Mozer-Smolensky",
            "title": {
                "fragments": [],
                "text": "Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862374"
                        ],
                        "name": "U. Grenander",
                        "slug": "U.-Grenander",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Grenander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Grenander"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 148
                            }
                        ],
                        "text": "There is often a tradeoff between the bias and variance contributions to the estimation error, which makes for a kind of \" uncertainty principle \" (Grenander 1951 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 145
                            }
                        ],
                        "text": "There is often a tradeoff between the bias and variance contributions to the estimation error, which makes for a kind of \"uncertainty principle\" (Grenander 1951)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122878699,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1e27bb201e54c09df99b4cf9e66f2fb6966ce09e",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Two main cases of empirical determination of the spectrum of a stat ionary stochastic process will be treated in this paper. To a given frequency, which all the t ime will be chosen as zero, there corresponds a discrete line in the spectrum. One wants to determine its mean amplitude when a realization of the process has been observed. Among all possible ways of estimating the mean amplitude there is one which gives maximum precision. If the rest of the spectrum is known a p r i o r i this estimate can be explicitly constructed. I t is shown tha t this construction is related to the problem of prediction. For the construction of the asymptotical ly best estimate the knowledge of the rest of the spectrum is not necessary, at least not for purely non-deterministic processes. The other case arises when the spectrum is absolutely continuous and one wants to estimate the spectral intensity or, what is equivalent, the covariance function. A class of estimates is given and studied in relation to periodogram analysis and to an estimate proposed by BARTLETT [2]. A principle of uncerta in ty is stated. In passing, some simple properties of linear processes are studied."
            },
            "slug": "On-empirical-spectral-analysis-of-stochastic-Grenander",
            "title": {
                "fragments": [],
                "text": "On empirical spectral analysis of stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684300"
                        ],
                        "name": "W. Stuetzle",
                        "slug": "W.-Stuetzle",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Stuetzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stuetzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 149
                            }
                        ],
                        "text": "\u2026regularization methods (see, e.g., Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 19851, recursive partitioning methods such as \"CART,\" which stands for \"Classification and Regression Trees\"\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 114
                            }
                        ],
                        "text": ", Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 1985), recursive partitioning methods such as \"CART,\" which stands for \"Classification and Regression Trees\" (Breiman et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14183758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "589b8659007e1124f765a5d1bd940b2bf4d79054",
            "isKey": false,
            "numCitedBy": 2178,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation."
            },
            "slug": "Projection-Pursuit-Regression-Friedman-Stuetzle",
            "title": {
                "fragments": [],
                "text": "Projection Pursuit Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38663378"
                        ],
                        "name": "J. Fodor",
                        "slug": "J.-Fodor",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Fodor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fodor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194015"
                        ],
                        "name": "Z. Pylyshyn",
                        "slug": "Z.-Pylyshyn",
                        "structuredName": {
                            "firstName": "Zenon",
                            "lastName": "Pylyshyn",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Pylyshyn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 94
                            }
                        ],
                        "text": "The debate about their adequacy as models of cognition is probably more intense now than ever (Fodor and Pylyshyn 1988; Smolensky 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29043627,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology"
            ],
            "id": "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7",
            "isKey": false,
            "numCitedBy": 3540,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionism-and-cognitive-architecture:-A-Fodor-Pylyshyn",
            "title": {
                "fragments": [],
                "text": "Connectionism and cognitive architecture: A critical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 195
                            }
                        ],
                        "text": "These arise by first restricting f via a \" smoothing criterion \" such as for some fixed integer m 2 1 and fixed A. (Partial and mixed partial derivatives enter when x + x E R'; see, for example, Wahba 1979.)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 118660255,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "807c3722f9284fc0e6fede61b6303ee4f0bbda1d",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the use of \"thin plate\" smoothing splines for smoothing noisy d dimensional data. The model is \n \n$$z_i = u(t_i ) + \\varepsilon _i ,i = 1,2,...,n,$$ \n \nwhere u is a real valued function on a closed, bounded subset \u03a9 of Euclidean d-space and the ei are random variables satisfying Eei=0, Eeiej=\u03c32, i=j, =0, i\u2260j, tie\u03a9. The zi are observed. It is desired to estimate u, given zl, ..., zn. u is only assumed to be \"smooth\", more precisely we assume that u is in the Sobolev space Hm(\u03a9) of functions with partial derivatives up to order m in L2(\u03a9), with m>d/2. u is estimated by un,m,\u03bb, the restriction to \u03a9 of \u0169n,m,\u03bb, where \u0169n,m,\u03bb is the solution to: Find \u0169 (in an appropriate space of functions on Rd) to minimize \n \n$$\\frac{1}{n}\\sum\\limits_{i = 1}^n {(\\tilde u(t_i ) - z_i ) + \\lambda _i } ,\\sum\\limits_{1,...,i_m = 1_R d}^d {(\\frac{{\\partial ^m \\tilde u}}{{\\partial x_{i_1 } \\partial x_{i_2 } ...\\partial x_{i_m } }})^2 dx_1 ,dx_2 ,...,dx_d }$$"
            },
            "slug": "Convergence-rates-of-\"thin-plate\"-smoothing-splines-Wahba",
            "title": {
                "fragments": [],
                "text": "Convergence rates of \"thin plate\" smoothing splines wihen the data are noisy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42681933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af6759ecd0f6c8ba1eb7030894eff4c91a55778d",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm that is widely used for adaptive equalization in current modems is the bootstrap or decision-directed version of the Widrow-Hoff rule. We show that this algorithm can be viewed as an unsupervised clustering algorithm in which the data points are transformed so that they form two clusters that are as tight as possible. The standard algorithm performs gradient ascent in a crude model of the log likelihood of generating the transformed data points from two gaussian distributions with fixed centers. Better convergence is achieved by using the exact gradient of the log likelihood."
            },
            "slug": "The-Bootstrap-Widrow-Hoff-Rule-as-a-Algorithm-Hinton-Nowlan",
            "title": {
                "fragments": [],
                "text": "The Bootstrap Widrow-Hoff Rule as a Cluster-Formation Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work shows that the bootstrap or decision-directed version of the Widrow-Hoff rule can be viewed as an unsupervised clustering algorithm in which the data points are transformed so that they form two clusters that are as tight as possible."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 145
                            }
                        ],
                        "text": "\u2026methods such as \"CART,\" which stands for \"Classification and Regression Trees\" (Breiman et al. 1984) ), as well as feedforward neural networks (Rumelhart et al. 1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 38
                            }
                        ],
                        "text": "For example, in feedforward networks (Rumelhart et al. 1986a,b), one usually forms the sum of observed squared errors, and f is chosen to make this sum as small as possible."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 246
                            }
                        ],
                        "text": "The most extensively studied neural network in recent years is probably the backpropagation network, that is, a multilayer feedforward network with the associated error-backpropagation algorithm for minimizing the observed sum of squared errors (Rumelhart et al. 1986a,b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862374"
                        ],
                        "name": "U. Grenander",
                        "slug": "U.-Grenander",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Grenander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Grenander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40755847"
                        ],
                        "name": "M. Piccioni",
                        "slug": "M.-Piccioni",
                        "structuredName": {
                            "firstName": "Mauro",
                            "lastName": "Piccioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Piccioni"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122465951,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8d051c9224c341bd9e1c4c75594725b0d1264962",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Prior knowledge on the space of possible images is given in the form of a function or template in some domain. The set of all possible true images is assumed to be formed by a composition of that function with continuous mappings of the domain into itself. A prior Gaussian distribution is given on the set of continuous mappings. The observed image is assumed to be a degradation of the true image with additive noise. Given the observed image, a posterior distribution is then obtained and has the form of a nonlinear perturbation of the Gaussian measure on the space of mappings. We present simulations of the posterior distribution that lead to structural reconstructions of the true image in the sense that it enables us to determine landmarks and other characteristic features of the image, as well as to spot pathologies in it. Moreover, we show that the reconstruction algorithm is relatively robust when the images are degraded by noise that is not necessarily additive."
            },
            "slug": "Structural-Image-Restoration-through-Deformable-Amit-Grenander",
            "title": {
                "fragments": [],
                "text": "Structural Image Restoration through Deformable Templates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51378744"
                        ],
                        "name": "J. Lamperti",
                        "slug": "J.-Lamperti",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lamperti",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lamperti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37016743,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f274a4cee19d93218c209aa07f47abea0598ae91",
            "isKey": false,
            "numCitedBy": 1027,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "It is clear that for given I,un } and t, the better theorem of this kind would be the one in which (2) is proved for the larger class of functions f. In this paper we shall show that certain known \"invariance principles\" can under some hypotheses be improved by considerably enlarging the class of functions for which (2) holds. This will be done by considering spaces S other than the customary ones. For example, in studying convergence to the Wiener process, it is usual to let S be the space (denoted e) of continuous functions with the uniform topology. However, this choice does not fully exploit the pleasant properties of the Wiener path-functions, which are not only continuous but also Holder continuous of any order up to 1/2. Therefore we shall attempt to use spaces Lip5 in place of e as the function-space S. When weak convergence can be established using such spaces, the class of functionals for which (2) is known to hold becomes much larger than before. To carry out the idea sketched above it is necessary to have a criterion which guarantees that the sample functions of a stochastic process are a.s."
            },
            "slug": "ON-CONVERGENCE-OF-STOCHASTIC-PROCESSES-Lamperti",
            "title": {
                "fragments": [],
                "text": "ON CONVERGENCE OF STOCHASTIC PROCESSES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15652822,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7de372cae64dea5263076b5139c6b79df9e3157b",
            "isKey": false,
            "numCitedBy": 1647,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In regression analysis the response variable Y and the predictor variables X 1 \u2026, Xp are often replaced by functions \u03b8(Y) and O1(X 1), \u2026, O p (Xp ). We discuss a procedure for estimating those functions \u03b8 and O1, \u2026, O p that minimize e 2 = E{[\u03b8(Y) \u2014 \u03a3 O j (Xj )]2}/var[\u03b8(Y)], given only a sample {(yk , xk1 , \u2026, xkp ), 1 \u2a7d k \u2a7d N} and making minimal assumptions concerning the data distribution or the form of the solution functions. For the bivariate case, p = 1, \u03b8 and O satisfy \u03c1 = p(\u03b8, O) = max\u03b8,O\u03c1[\u03b8(Y), O(X)], where \u03c1 is the product moment correlation coefficient and \u03c1 is the maximal correlation between X and Y. Our procedure thus also provides a method for estimating the maximal correlation between two variables."
            },
            "slug": "Estimating-Optimal-Transformations-for-Multiple-and-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Estimating Optimal Transformations for Multiple Regression and Correlation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144928357"
                        ],
                        "name": "Y. Vardi",
                        "slug": "Y.-Vardi",
                        "structuredName": {
                            "firstName": "Yehuda",
                            "lastName": "Vardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Vardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108020"
                        ],
                        "name": "L. Shepp",
                        "slug": "L.-Shepp",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Shepp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10706913"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17836207,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "57f9830c30bb32c324266e51b82b70de62352334",
            "isKey": false,
            "numCitedBy": 890,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Positron emission tomography (PET)\u2014still in its research stages\u2014is a technique that promises to open new medical frontiers by enabling physicians to study the metabolic activity of the body in a pictorial manner. Much as in X-ray transmission tomography and other modes of computerized tomography, the quality of the reconstructed image in PET is very sensitive to the mathematical algorithm to be used for reconstruction. In this article, we tailor a mathematical model to the physics of positron emissions, and we use the model to describe the basic image reconstruction problem of PET as a standard problem in statistical estimation from incomplete data. We describe various estimation procedures, such as the maximum likelihood (ML) method (using the EM algorithm), the method of moments, and the least squares method. A computer simulation of a PET experiment is then used to demonstrate the ML and the least squares reconstructions. The main purposes of this article are to report on what we believe is an..."
            },
            "slug": "A-Statistical-Model-for-Positron-Emission-Vardi-Shepp",
            "title": {
                "fragments": [],
                "text": "A Statistical Model for Positron Emission Tomography"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784213"
                        ],
                        "name": "R. Bajcsy",
                        "slug": "R.-Bajcsy",
                        "structuredName": {
                            "firstName": "Ruzena",
                            "lastName": "Bajcsy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bajcsy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802545"
                        ],
                        "name": "S. Kovacic",
                        "slug": "S.-Kovacic",
                        "structuredName": {
                            "firstName": "Stanislav",
                            "lastName": "Kovacic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kovacic"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19718946,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "36c9356db20ab4eace62d86e0a576fa6382d84b9",
            "isKey": false,
            "numCitedBy": 1217,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multiresolution-elastic-matching-Bajcsy-Kovacic",
            "title": {
                "fragments": [],
                "text": "Multiresolution elastic matching"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741426"
                        ],
                        "name": "P. Gallinari",
                        "slug": "P.-Gallinari",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gallinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gallinari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2201276"
                        ],
                        "name": "S. Thiria",
                        "slug": "S.-Thiria",
                        "structuredName": {
                            "firstName": "Sylvie",
                            "lastName": "Thiria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thiria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3368257"
                        ],
                        "name": "F. Fogelman",
                        "slug": "F.-Fogelman",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18307087,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "3aa580e8ad906a1be3d221819111d716048ae3e7",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Results are presented which permit comparison of classification tasks of multilayer perceptrons with discriminant analysis. The results are illustrated with simulations of both approaches that demonstrate that multilayer perceptrons with nonlinear elements outperform discriminant analysis.<<ETX>>"
            },
            "slug": "Multilayer-perceptrons-and-data-analysis-Gallinari-Thiria",
            "title": {
                "fragments": [],
                "text": "Multilayer perceptrons and data analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Results are presented which permit comparison of classification tasks of multilayer perceptrons with discriminant analysis and simulations of both approaches demonstrate that multilayers perceptron with nonlinear elements outperform discriminantAnalysis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 53
                            }
                        ],
                        "text": "Parzen windows and nearestneighbor rules (see, e.g., Duda and Hart 1973; Hardle 1990), regularization methods (see, e.g., Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 19851, recursive partitioning\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48403,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64903870,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400b45a803d642b752a84147ef547af7811e8f3f",
            "isKey": false,
            "numCitedBy": 19575,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting."
            },
            "slug": "Information-Theory-and-an-Extension-of-the-Maximum-Akaike",
            "title": {
                "fragments": [],
                "text": "Information Theory and an Extension of the Maximum Likelihood Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion to provide answers to many practical problems of statistical model fitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343252"
                        ],
                        "name": "S. Wold",
                        "slug": "S.-Wold",
                        "structuredName": {
                            "firstName": "Svante",
                            "lastName": "Wold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wold"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 6
                            }
                        ],
                        "text": "(From Wahba and Wold 1975) regression, g(X)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120116272,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "99a0f1bc2b4535406d780957cc5cc937d987d176",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The cross validation mean square error technique is used to determine the correct degree of smoothing, in fitting smoothing solines to discrete, noisy observations from some unknown smooth function. Monte Cario results snow amazing success in estimating the true smooth function as well as its derivative."
            },
            "slug": "A-completely-automatic-french-curve:-fitting-spline-Wahba-Wold",
            "title": {
                "fragments": [],
                "text": "A completely automatic french curve: fitting spline functions by cross validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120741100,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On demontre un theoreme fondamental qui donne une borne inferieure pour la longueur de code et donc, pour les erreurs de prediction. On definit les notions \u00abd'information a priori\u00bb et \u00abd'information utile\u00bb dans les donnees"
            },
            "slug": "Stochastic-Complexity-and-Modeling-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity and Modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29458883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8017699564136f93af21575810d557dba1ee6fc6",
            "isKey": false,
            "numCitedBy": 16308,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index."
            },
            "slug": "Classification-and-Regression-Trees-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses tree classification in the context of medicine, where right Sized Trees and Honest Estimates are considered and Bayes Rules and Partitions are used as guides to optimal pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115915283,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "52b950531f18e18344b1dd7f1ef91adc4be45dff",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The Greeks have reduced the multiplicity of materials to a conceptually simple basis: a small number of atomic types and their chemical combination. Such conceptual unification has yet to be attained for the phenomena of mind. One of the important functions of our mind is the construction of models or \u201csymbols\u201d for external objects and situations. What is to be discussed here is the structure of the symbols of mind."
            },
            "slug": "Am-I-Thinking-Assemblies-Malsburg",
            "title": {
                "fragments": [],
                "text": "Am I Thinking Assemblies"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Greeks have reduced the multiplicity of materials to a conceptually simple basis: a small number of atomic types and their chemical combination."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121176122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "88ce531f22108f687cbb576bcb0cd660b2a694bc",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere des procedures de lissage spline etudiees en 1978 et 1983 et leur extension a la resolution d'equations d'operateurs lineaires avec donnees bruitees"
            },
            "slug": "A-Comparison-of-GCV-and-GML-for-Choosing-the-in-the-Wahba",
            "title": {
                "fragments": [],
                "text": "A Comparison of GCV and GML for Choosing the Smoothing Parameter in the Generalized Spline Smoothing Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122230395,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c59250736174b8d80c40de6e1d5e1641366ef416",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When \\(\\mathfrak{F}\\) is a universal Donsker class, then for independent, indetically distributed (i.i.d) observation \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_n\\) with an unknown law P, for any \\(\\mathfrak{f}_i\\)in \\(\\mathfrak{F},\\) \\(i=1,\\ldots,m,\\quad n^{-1/2}\\left\\{ \\mathfrak{f}_1\\left(\\mathbf{X}_1\\right)+\\ldots+\\mathfrak{f}_i\\left(\\mathbf{X}_n\\right)\\right\\}_{1\\leq i\\leq m}\\) is asymptotically normal with mean Vector \\(n^{1/2}\\left\\{\\int\\mathfrak{f}_i\\left(\\mathbf{X}_n\\right)d\\mathbf{P}\\left(x\\right)\\right\\}_{1_\\leq i\\leq m}\\) and covariance matrix \\(\\int\\mathfrak{f}_i\\mathfrak{f}_j d\\mathbf{P}-\\int\\mathfrak{f}_id\\mathbf{P}\\int\\mathfrak{f}_jd\\mathbf{P},\\) uniformly for \\({\\mathfrak{f}_i}\\in \\mathfrak{F}.\\) Then, for certain Statistics formed frome the \\(\\mathfrak{f}_i\\left(\\mathbf{X}_k\\right),\\) even where \\(\\mathfrak{f}_i\\) may be chosen depending on the \\(\\mathbf{X}_k\\) there will be asymptotic distribution as \\(n \\rightarrow \\infty.\\) For example, for \\(\\mathbf{X}^2\\) statistics, where \\(f_i\\) are indicators of disjoint intervals, depending suitably on \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_n\\), whose union is the real line, \\(\\mathbf{X}^2\\) quadratic forms have limiting distributions [Roy (1956) and Watson (1958)] which may, however, not be \\(\\mathbf{X}^2\\) distributions and may depend on P [Chernoff and Lehmann (1954)]. Universal Donsker classes of sets are, up to mild measurability conditions, just classes satisfying the Vapnik\u2013Cervonenkis comdinatorial conditions defined later in this section Donsker the Vapnik-Cervonenkis combinatorial conditions defined later in this section [Durst and Dudley (1981) and Dudley (1984) Chapter 11]. The use of such classes allows a variety of extensions of the Roy\u2013Watson results to general (multidimensional) sample spaces [Pollard (1979) and Moore and Subblebine (1981)]. Vapnik and Cervonenkis (1974) indicated application of their families of sets to classification (pattern recognition) problems. More recently, the classes have been applied to tree-structured classifiacation [Breiman, Friedman, Olshen and Stone (1984), Chapter 12]."
            },
            "slug": "Universal-Donsker-Classes-and-Metric-Entropy-Dudley",
            "title": {
                "fragments": [],
                "text": "Universal Donsker Classes and Metric Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465976"
                        ],
                        "name": "M. Fischler",
                        "slug": "M.-Fischler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Fischler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394928"
                        ],
                        "name": "R. Elschlager",
                        "slug": "R.-Elschlager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Elschlager",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elschlager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14554383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "719da2a0ddd38e78151e1cb2db31703ea8b2e490",
            "isKey": false,
            "numCitedBy": 1527,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "slug": "The-Representation-and-Matching-of-Pictorial-Fischler-Elschlager",
            "title": {
                "fragments": [],
                "text": "The Representation and Matching of Pictorial Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The primary problem dealt with in this paper is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065464844"
                        ],
                        "name": "Isabelle Guyon",
                        "slug": "Isabelle-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isabelle Guyon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 190401580,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "debe613efd2c2aea5b6e0a70aa347583975eb631",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Description des proprietes de reseaux d'architecture differentes et proposition de mecanismes d'apprentissage originaux a la reconnaissance de sequences d'informations. Application a la reconnaissance de caracteres manuscrits."
            },
            "slug": "R\u00e9seaux-de-neurones-pour-la-reconnaissance-des-:-et-Guyon",
            "title": {
                "fragments": [],
                "text": "R\u00e9seaux de neurones pour la reconnaissance des formes : architectures et apprentissage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423299574"
                        ],
                        "name": "F. O\u2019Sullivan",
                        "slug": "F.-O\u2019Sullivan",
                        "structuredName": {
                            "firstName": "Finbarr",
                            "lastName": "O\u2019Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O\u2019Sullivan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123176594,
            "fieldsOfStudy": [
                "Environmental Science",
                "Mathematics"
            ],
            "id": "dc7a0b17f0d646e5120bf23873218131ef64b8f4",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-cross-validated-bayesian-retrieval-algorithm-for-O\u2019Sullivan-Wahba",
            "title": {
                "fragments": [],
                "text": "A cross validated bayesian retrieval algorithm for nonlinear remote sensing experiments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503280"
                        ],
                        "name": "L. Goldman",
                        "slug": "L.-Goldman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Goldman",
                            "middleNames": [
                                "Elizabeth"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060461290"
                        ],
                        "name": "M. Weinberg",
                        "slug": "M.-Weinberg",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Weinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Weinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48451539"
                        ],
                        "name": "M. Weisberg",
                        "slug": "M.-Weisberg",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Weisberg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Weisberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143755504"
                        ],
                        "name": "E. Cook",
                        "slug": "E.-Cook",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Cook",
                            "middleNames": [
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144598389"
                        ],
                        "name": "R. Sargent",
                        "slug": "R.-Sargent",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Sargent",
                            "middleNames": [
                                "K"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sargent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6204279"
                        ],
                        "name": "G. Lamas",
                        "slug": "G.-Lamas",
                        "structuredName": {
                            "firstName": "Gervasio",
                            "lastName": "Lamas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lamas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47486982"
                        ],
                        "name": "C. Dennis",
                        "slug": "C.-Dennis",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Dennis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931713790"
                        ],
                        "name": "C. Wilson",
                        "slug": "C.-Wilson",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Wilson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7212009"
                        ],
                        "name": "L. Deckelbaum",
                        "slug": "L.-Deckelbaum",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Deckelbaum",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": "MD"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deckelbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5680991"
                        ],
                        "name": "H. Fineberg",
                        "slug": "H.-Fineberg",
                        "structuredName": {
                            "firstName": "Harvey",
                            "lastName": "Fineberg",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Fineberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2102199650"
                        ],
                        "name": "R. Stiratelli",
                        "slug": "R.-Stiratelli",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Stiratelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stiratelli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35344955,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "a36bf6c0aaf799478b567fc849cb5f92c6385fc3",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "To determine whether data available to physicians in the emergency room can accurately identify which patients with acute chest pain are having myocardial infarctions, we analyzed 482 patients at one hospital. Using recursive partitioning analysis, we constructed a decision protocol in the format of a simple flow chart to identify infarction on the basis of nine clinical factors. In prospective testing on 468 other patients at a second hospital, the protocol performed as well as the physicians. Moreover, an integration of the protocol with the physicians' judgments resulted in a classification system that preserved sensitivity for detecting infarctions, significantly improved the specificity (from 67 per cent to 77 per cent, P less than 0.01) and positive predictive value (from 34 per cent to 42 per cent, P = 0.016) of admission to an intensive-care area. The protocol identified a subgroup of 107 patients among whom only 5 per cent had infarctions and for whom admission to non-intensive-care areas might be appropriate. This decision protocol warrants further wide-scale prospective testing but is not ready for routine clinical use."
            },
            "slug": "A-computer-derived-protocol-to-aid-in-the-diagnosis-Goldman-Weinberg",
            "title": {
                "fragments": [],
                "text": "A computer-derived protocol to aid in the diagnosis of emergency room patients with acute chest pain."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A decision protocol in the format of a simple flow chart to identify infarction on the basis of nine clinical factors, which preserved for detecting infarctions and significantly improved the specificity and positive predictive value of admission to an intensive-care area."
            },
            "venue": {
                "fragments": [],
                "text": "The New England journal of medicine"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251336"
                        ],
                        "name": "D. Burr",
                        "slug": "D.-Burr",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Burr",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burr"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17709775,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "60b3fe8fa86d3e8e6d74c917f38877e2e88bca07",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently a dynamic elastic model [1] was proposed for automatic image matching. Examples were shown applying the model to dot patterns and gray scale pictures. This paper extends the model to line drawings. Examples are shown on handprint and animation, suggesting the use of dynamic matching for shape recognition and for motion correspondence."
            },
            "slug": "Elastic-Matching-of-Line-Drawings-Burr",
            "title": {
                "fragments": [],
                "text": "Elastic Matching of Line Drawings"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Examples are shown on handprint and animation, suggesting the use of dynamic matching for shape recognition and for motion correspondence, and extending the model to line drawings."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796350"
                        ],
                        "name": "P. Diaconis",
                        "slug": "P.-Diaconis",
                        "structuredName": {
                            "firstName": "Persi",
                            "lastName": "Diaconis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diaconis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32854168"
                        ],
                        "name": "D. Freedman",
                        "slug": "D.-Freedman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Freedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120719658,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5c63ab340634a23b84a233f1a7725c4b5ddffe9c",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "On etudie les proprietes de frequence des regles de Bayes avec une attention particuliere a la consistence"
            },
            "slug": "On-the-consistency-of-Bayes-estimates-Diaconis-Freedman",
            "title": {
                "fragments": [],
                "text": "On the consistency of Bayes estimates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144327830"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Cox",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118361489,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "89ba1b9f42082655999d50270b335d7bf393f653",
            "isKey": false,
            "numCitedBy": 4769,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Binary response variables special logistical analyses some complications some related approaches more complex responses. Appendices: Theoretical background Choice of explanatory variables in multiple regression Review of computational aspects Further results and exercises."
            },
            "slug": "The-analysis-of-binary-data-Cox",
            "title": {
                "fragments": [],
                "text": "The analysis of binary data"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Binary response variables special logistical analyses some complications some related approaches more complex responses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 36
                            }
                        ],
                        "text": "It is by now well-known (see, e.g., White 1990) that a feedforward neural network (with some mild conditions on Ely I x] and network structure, and some optimistic assumptions about minimizing 3.3) can be made consistent by suitably letting the network size grow with the size of the training set,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionists nonparametric regression: multilayer"
            },
            "venue": {
                "fragments": [],
                "text": "Connectionists nonparametric regression: multilayer"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 131
                            }
                        ],
                        "text": "An interesting and difficult problem in industrial \"process specification\" was recently solved at the General Motors Research Labs (Lorenzen 1988) with the help of the already mentioned CART method (Breiman et a/."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "Figure 1 (from Lorenzen 1988) shows a histogram comparison for that variable that was judged to have the most visually disparate histograms among the 80 variables: the left histogram is from a population of scrapped casts, and the right is from a population of accepted casts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 1
                            }
                        ],
                        "text": "(Lorenzen 1988) variabIes summarizing an applicant's financial status, These include, for example, measures of income and income stability, debt and other financial obligations, credit history, and possibly appraised values in the case of mortgages and other secured loans."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 132
                            }
                        ],
                        "text": "An interesting and difficult problem in industrial \"process specification\" was recently solved at the General Motors Research Labs (Lorenzen 1988) with the help of the already mentioned CART method (Breiman et al. 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Setting realistic process specification limits- A case study"
            },
            "venue": {
                "fragments": [],
                "text": "General Motors Research Labs. Publication GMR-6389, Warren, MI."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 490,
                                "start": 473
                            }
                        ],
                        "text": "Parzen windows and nearestneighbor rules (see, e.g., Duda and Hart 1973; Hardle 1990), regularization methods (see, e.g., Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 19851, recursive partitioning methods such as \"CART,\" which stands for \"Classification and Regression Trees\" (Breiman et al. 1984) ), as well as feedforward neural networks (Rumelhart et al. 1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used to construct consistent nonparametric estimators."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 118
                            }
                        ],
                        "text": "Similar remarks apply to likelihood-based (instead of least-squaresbased ) approaches, such as the Boltzmann Machine (Ackley et al. 1985; Hinton and Sejnowski 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 32
                            }
                        ],
                        "text": "1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used to construct consistent nonparametric estimators."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "\u2026and Regression Trees\" (Breiman et al. 1984) ), as well as feedforward neural networks (Rumelhart et al. 1986a,b) and Boltzmann Machines (Ackley et al. 1985; Hinton and Sejnowski 1986), are a few examples of techniques that can be used to construct consistent nonparametric estimators."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Boltzmann Machine implements a Monte Carlo computational algorithm for increasing likelihood."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 116
                            }
                        ],
                        "text": "Similar remarks apply to likelihood-based (instead of least-squaresbased) approaches, such as the Boltzmann Machine (Ackley et al. 1985; Hinton and Sejnowski 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143929773"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125481163,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ebb53a7e5cff86b2b42d1108a0fa81f571d8894",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-projection-pursuit-Jones-Sibson",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14942773"
                        ],
                        "name": "M. Stone",
                        "slug": "M.-Stone",
                        "structuredName": {
                            "firstName": "Mervyn",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116210394,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c61149de2d5aca78932729a16c657f811edc63b5",
            "isKey": false,
            "numCitedBy": 828,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cross\u2010Validatory-Choice-and-Assessment-of-(With-Stone",
            "title": {
                "fragments": [],
                "text": "Cross\u2010Validatory Choice and Assessment of Statistical Predictions (With Discussion)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64831710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1584ea619b59164aaf31148e0cdb2cedc47a6bb8",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Elastic-matching-and-pattern-recognition-in-neural-Bienenstock-Doursat",
            "title": {
                "fragments": [],
                "text": "Elastic matching and pattern recognition in neural networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62668616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a0cd8a28b7668fcd38b98b8e1598c33e1852077",
            "isKey": false,
            "numCitedBy": 846,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "V.-Adaptive-Control-Processes-Bellman",
            "title": {
                "fragments": [],
                "text": "V. Adaptive Control Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": false,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69028931"
                        ],
                        "name": "J. Freidman",
                        "slug": "J.-Freidman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Freidman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Freidman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 139
                            }
                        ],
                        "text": "1984), Alternating Conditional Expectations, or \"ACE\" (Breiman and Friedman 1985), and Multivariate Adaptive Regression Splines, or \"MARS\" (Friedman 1991), as well as feedforward neural networks (Rumelhart et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33779230,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9c980c5477c1f1a369a19dd954c936185bbd8b82",
            "isKey": false,
            "numCitedBy": 3467,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariate-adaptive-regression-splines-Freidman",
            "title": {
                "fragments": [],
                "text": "Multivariate adaptive regression splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network ensembles. IEEE Transact. Pattern Anal. Machine Intell. PAMI-12"
            },
            "venue": {
                "fragments": [],
                "text": "Neural network ensembles. IEEE Transact. Pattern Anal. Machine Intell. PAMI-12"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exhaustive thermodynamic analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks: From Models to Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: From Models to Applications"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differential geometry of Boltzmann machines"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep., Department of Mathematical Engineering and Instrumentation Physics, University of Tokyo, Bunkyo-Ku, Tokyo."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the BiasNariance Dilemma"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks and the BiasNariance Dilemma"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature extraction using an exploratory projection pursuit neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Feature extraction using an exploratory projection pursuit neural network"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionists nonparametric regression: multilayer feedforWidrow, B"
            },
            "venue": {
                "fragments": [],
                "text": "1973. The rubber mask technique, Part I. Pattern Recognition 5, 175Yuille, A. 1990. Generalized deformable models, statistical physics, and match-"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "plex Syst"
            },
            "venue": {
                "fragments": [],
                "text": "plex Syst"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Auto-association by multi-layer perceptrons and singular value decomposition"
            },
            "venue": {
                "fragments": [],
                "text": "Biol. Cybernet"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A cross validated Bayesian retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks can learn arbitrary mappings: Connectionist nonparametric regression with automatic and semi-automatic determination of network complexity"
            },
            "venue": {
                "fragments": [],
                "text": "UCSD Department of Economics Discussion Paper."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object recognition in the dynamic link architecture: Parallel"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Issues of representation in neural networks , In Representations of Vision: Trends and Tacit Assumptions in Vision Reseauch"
            },
            "venue": {
                "fragments": [],
                "text": "Issues of representation in neural networks , In Representations of Vision: Trends and Tacit Assumptions in Vision Reseauch"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The representation and matching of pic- Fodor Connectionism and cognitive architecture: Friedman Multivariate adaptive regression splines Projection pursuit regression"
            },
            "venue": {
                "fragments": [],
                "text": "J. A., and Pylyshyn, Z. J. H. Ann. Statist. J. A m . Statist. Assoc"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ann. Statist"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Internal representation of universal regularities: A challenge for connectionism"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Connections, Mental Computation, L. Nadel, L. A. Cooper, P. Culicover, and R. M. Harnish, eds., pp. 104--134. Bradford /MIT Press, Cambridge, MA, London, England."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Doursat Contract N00014-88-K-0289, and the General Motors Research Laboratories . E. B. was supported by grants from the Commission of European Communities"
            },
            "venue": {
                "fragments": [],
                "text": "ST2J-0416) and the French MinistGre de la Recherche (87C0187)"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Auto-association by multi-layer perceptrons and singular value decomposition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 117
                            }
                        ],
                        "text": "Indeed, it has been found that for many problems a constrained architecture can do better than a general-purpose one (Denker et al. 1987; Waibel et al. 1988; Le Cun et al. 1989; Solla 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 72
                            }
                        ],
                        "text": "A simple example of such a situation is the socalled contiguity problem (Denker et al. 1987; Solla 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and generalization in layered neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exhaustive thermodynamic analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and generalization in layered neural networks: The 11"
            },
            "venue": {
                "fragments": [],
                "text": "Learning and generalization in layered neural networks: The 11"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionism and cognitive architecture: Friedman, J"
            },
            "venue": {
                "fragments": [],
                "text": "H. 1991. Multivariate adaptive regression splines. Ann. Statist. 19, Friedman, J. H., and Stuetzle, W. 1981. Projection pursuit regression. J. A m ."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 97
                            }
                        ],
                        "text": "The study of neural networks in recent years has involved increasingly sophisticated mathematics (cf. Barron and Barron 1988; Barron 1989; Baum and Haussler 1989; Haussler 1989b; White 1989, 1990; Amari 1990; Amari et al. 1990; Azencott 1990; Baum 1990a), often directly connected with the statistical-inference issues discussed in the previous sections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differential geometry of Boltzmann machines"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep., Department of Mathematical Engineering and Instrumentation Physics, University of Tokyo, Bunkyo-Ku, Tokyo."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "\u2026rules (see, e.g., Duda and Hart 1973; Hardle 1990), regularization methods (see, e.g., Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 19851, recursive partitioning methods such as \"CART,\"\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 55
                            }
                        ],
                        "text": ", Wahba 1982) and the closely related method of sieves (Grenander 1981; Geman and Hwang 1982), projection pursuit (Friedman and Stuetzle 1981; Huber 1985), recursive partitioning methods such as \"CART,\" which stands for \"Classification and Regression Trees\" (Breiman et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Abstract Inference"
            },
            "venue": {
                "fragments": [],
                "text": "Wiley, New York."
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HANDS, A Pattern Theoretic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The rubber mask technique, Part I"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating optimal transformations for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What size net gives vaIid generalization? Neural Comp"
            },
            "venue": {
                "fragments": [],
                "text": "What size net gives vaIid generalization? Neural Comp"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Setting realistic process specification limits - A case study"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Global models of natural boundaries: Theory and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. Thesis, Division of Applied Mathematics, Brown University, Providence, Rl."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 117
                            }
                        ],
                        "text": "Indeed, it has been found that for many problems a constrained architecture can do better than a general-purpose one (Denker et al. 1987; Waibel et al. 1988; Le Cun et al. 1989; Solla 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 79
                            }
                        ],
                        "text": "In speech processing there have been successes in isolated phoneme recognition (Waibel et al. 1988; Lippmann 1989) and there is a suggestion of neural networks (or other nonparametric methods) as good \"front-ends\" for hidden Markov models (Bourlard and Wellekens 1990), and, beyond this, of advances in continuous speech recognition via trained neural networks, avoiding the difficult task of estimating parameters in complex hidden Markov models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICASSP-88, New York."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What size net gives vaIid generalization? Neural Comp"
            },
            "venue": {
                "fragments": [],
                "text": "1, 151-160. Baum, L. E. 1972. An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object recognition in the dynamic link architecture: Parallel implementation on a transputer network. In Neural Networks: A Dynamic Systems Approach f o Machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing hand-printed letters"
            },
            "venue": {
                "fragments": [],
                "text": "pirical Econ"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The boostrap Widrow-Hoff rule as a duster-formation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An application of a multiple neu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received 4 May"
            },
            "venue": {
                "fragments": [],
                "text": "Received 4 May"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Skeletonization: A technique for trim"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature extraction using an exploratory projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized deformable models, statistical physics, and matchward networks can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "211. ing problems. Neural Comp"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical learning networks: A unifying view"
            },
            "venue": {
                "fragments": [],
                "text": "Computing Science and Statistics, Proceedings of the 20th Symposium Interface, E. Wegman, ed., pp. 192-203. American Statistical Association, Washington, DC."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 158
                            }
                        ],
                        "text": "In a statistical physics perspective, introducing bias may also be viewed as a means of decreasing an appropriately defined measure of entropy of the machine (Carnevali et al. 1987; Denker et al. 1987; Tishby et al. 1989; Schwartz et al. 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 190
                            }
                        ],
                        "text": "This is also the focus of the statistical sciences, so it is not surprising that statistical tools are increasingly exploited in the development and analysis of these kinds of neural models (Lippmann 1987; Barron and Barron 1988; Gallinari et al. 1988; Barron 1989; Haussler 1989a; Tishby et al. 1989; White 1989; Amari et al. 1990; Baum 1990b; Hinton and Nowlan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Consistent inferences of probabilities in layered networks: Predictions and generalization"
            },
            "venue": {
                "fragments": [],
                "text": "IfCNN International Joint Conference on Neural Networks, Vol. II, pp. 403-409, IEEE, New York."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object recognition in the dynamic link architecture: Parallel implementation on a transputer network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: A Dynamic Systems Approach f o Machine Intelligence, B. Kosko"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Consistent inferences of probabilities in layered networks: Predictions and generalization"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exhaustive learning. Neural Comp"
            },
            "venue": {
                "fragments": [],
                "text": "Exhaustive learning. Neural Comp"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neurocomputing, Foundations of Research, p"
            },
            "venue": {
                "fragments": [],
                "text": "587. MIT Press, Cambridge MA."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Auto-association by multi-layer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1990b. When are k-nearest-neighbor and backpropagation accurate"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma Study of Biological Shapes"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma Study of Biological Shapes"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical learning networks: A unifying view"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 145
                            }
                        ],
                        "text": "1988; Lippmann 1989) and there is a suggestion of neural networks (or other nonparametric methods) as good \"front-ends\" for hidden Markov models (Bourlard and Wellekens 1990), and, beyond this, of advances in continuous speech recognition via trained neural networks, avoiding the difficult task of estimating parameters in complex hidden Markov models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Links between Markov models and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 79
                            }
                        ],
                        "text": "This tradeoff is, in fact, observed in experiments reported by several authors (see, for example, Chauvin 1990; Morgan and Bourlard 1990), as well as in our experiments with artificial data (Section 3."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization and parameter estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How far are automatically"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cross validated spline methods for the estimation of multivariate functions from data on functionals"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics: An Appraisal, Proceedings 50th Anniversary Conference Iowa State Statistical Laboratory, H. A. David and H. T. David, eds., pp. 205-235. Iowa State Univ. Press, Ames."
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma 53"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma 53"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "Figure 1 (from Lorenzen 1988) shows a histogram comparison for that variable that was judged to have the most visually disparate histograms among the 80 variables: the left histogram is from a population of scrapped casts, and the right is from a population of accepted casts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 1
                            }
                        ],
                        "text": "(Lorenzen 1988) variabIes summarizing an applicant's financial status, These include, for example, measures of income and income stability, debt and other financial obligations, credit history, and possibly appraised values in the case of mortgages and other secured loans."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 132
                            }
                        ],
                        "text": "An interesting and difficult problem in industrial \"process specification\" was recently solved at the General Motors Research Labs (Lorenzen 1988) with the help of the already mentioned CART method (Breiman et al. 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Setting realistic process specification limits -A case study. General Motors Research Labs"
            },
            "venue": {
                "fragments": [],
                "text": "Setting realistic process specification limits -A case study. General Motors Research Labs"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 2
                            }
                        ],
                        "text": "\" Poggio and Girosi (1990) have shown how splines and related estimators can be computed with multilayer networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization algorithms for learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Issues of representation in neural networks, In Representations of Vision: Trends and Tacit Assumptions in Vision Reseauch, A"
            },
            "venue": {
                "fragments": [],
                "text": "Gorea, ed., pp. 47-67. Cambridge University Press, Cambridge. Bourlard,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Internal representation of universal regularities: A chal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 13,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 151,
        "totalPages": 16
    },
    "page_url": "https://www.semanticscholar.org/paper/Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock/a34e35dbbc6911fa7b94894dffdc0076a261b6f0?sort=total-citations"
}