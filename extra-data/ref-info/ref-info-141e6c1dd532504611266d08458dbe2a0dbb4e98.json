{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "4 with solving the QCQP (L) using Mosek for two datasets, ionosphere and breast cancer, from the UCI repository, and nested subsets of the adult dataset from Platt (1998). The basis kernels are Gaussian kernels on random subsets of features, with varying widths."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Since the \u03b5-optimality conditions for the MY-regularized SKM are exactly the same as for the SVM, but with a different objective function (Platt, 1998; Keerthi et al., 2001):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", when the SKM reduces to the SVM, this corresponds to the approximate KKT conditions usually employed for the standard SVM (Platt, 1998; Keerthi et al., 2001; Joachims, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, off-the-shelf algorithms do not suffice in large-scale applications of the SVM, and a second major reason for the rise to prominence of the SVM is the development of special-purpose algorithms for solving the QP (Platt, 1998; Joachims, 1998; Keerthi et al., 2001)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": true,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701847"
                        ],
                        "name": "L. Ghaoui",
                        "slug": "L.-Ghaoui",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Ghaoui",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ghaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 108
                            }
                        ],
                        "text": "As we will show, the conic dual problem defining the SKM is exactly the multiple kernel learning problem of Lanckriet et al. (2004).1 Moreover, given this new formulation, we can design a Moreau-Yosida regularization which preserves the sparse SVM structure, and therefore we can apply SMO\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Lanckriet et al. (2004) show that this setup yields the following optimization problem:\nmin \u03b6 \u2212 2e>\u03b1 (L) w.r.t. \u03b6 \u2208 R, \u03b1 \u2208 Rn\ns.t. 0 6 \u03b1 6 C, \u03b1>y = 0\n\u03b1>D(y)KjD(y)\u03b1 6 trKj c \u03b6, j \u2208 {1, . . . ,m},\nwhere D(y) is the diagonal matrix with diagonal y, e \u2208 R n the vector of all ones, and C a positive\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 39
                            }
                        ],
                        "text": "In particular, the QCQP formulation of Lanckriet et al. (2004) does not lead to an MY-regularized problem that can be solved efficiently by SMO techniques."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "Unfortunately, in our setting, this creates a new difficulty\u2014we lose the sparsity that makes the SVM amenable to SMO optimization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 52
                            }
                        ],
                        "text": "In this paper we focus on the framework proposed by Lanckriet et al. (2004), which involves joint optimization of the coefficients in a conic combination of kernel matrices and the coefficients of a discriminative classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 233
                            }
                        ],
                        "text": "While this so-called \u201cmultiple kernel learning\u201d problem can in principle be solved via cross-validation, several recent papers have focused on more efficient methods for kernel learning (Chapelle et al., 2002; Grandvalet & Canu, 2003; Lanckriet et al., 2004; Ong et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 93
                            }
                        ],
                        "text": "In this section, we (1) begin with a brief review of the multiple kernel learning problem of Lanckriet et al. (2004), (2) introduce the support kernel machine (SKM), and (3) show that the dual of the SKM is equivalent to the multiple kernel learning primal."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "We present\n1It is worth noting that this dual problem cannot be obtained directly as the Lagrangian dual of the QCQP problem\u2014Lagrangian duals of QCQPs are semidefinite programming problems.\nthe results of numerical experiments with the new method in Section 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1113875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0948365ef39ef153e61e9569ade541cf881c7c2a",
            "isKey": false,
            "numCitedBy": 2470,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem."
            },
            "slug": "Learning-the-Kernel-Matrix-with-Semidefinite-Lanckriet-Cristianini",
            "title": {
                "fragments": [],
                "text": "Learning the Kernel Matrix with Semidefinite Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques and leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, caching and shrinking techniques ( Joachims, 1998 ) that prevent redundant computations of kernel matrix values can also be employed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Since each SMO optimization is performed on a differentiable function with Lipschitz gradient and SMO is equivalent to steepest descent for the `1norm ( Joachims, 1998 ), classical optimization results show that each of those SMO optimizations is finitely convergent (Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Note that for one kernel, i.e., when the SKM reduces to the SVM, this corresponds to the approximate KKT conditions usually employed for the standard SVM (Platt, 1998; Keerthi et al., 2001;  Joachims, 1998 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, off-the-shelf algorithms do not suffice in large-scale applications of the SVM, and a second major reason for the rise to prominence of the SVM is the development of special-purpose algorithms for solving the QP (Platt, 1998;  Joachims, 1998;  Keerthi et al., 2001)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60502770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb99668d4df98a3f6ff0b9fa3402e09008f22e2c",
            "isKey": true,
            "numCitedBy": 1838,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-support-vector-machine-learning-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large-scale support vector machine learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052864185"
                        ],
                        "name": "P. Bradley",
                        "slug": "P.-Bradley",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bradley",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bradley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 132
                            }
                        ],
                        "text": "On the other hand, if m = k and dj = 1, we minimize the square of the `1-norm of w, which is very similar to the LP-SVM proposed by Bradley and Mangasarian (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "On the other hand, if m = k and dj = 1, we minimize the square of the `1-norm of w, which is very similar to the LP-SVM proposed by Bradley and Mangasarian (1998). The primal problem for the SKM is thus:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5885974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23d9a5273bd9e08eb68cf3b097836d718e91d70c",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational comparison is made between two feature selection approaches for nding a separating plane that discriminates between two point sets in an n-dimensional feature space that utilizes as few of the n features (dimensions) as possible. In the concave minimization approach [19, 5] a separating plane is generated by minimizing a weighted sum of distances of misclassi ed points to two parallel planes that bound the sets and which determine the separating plane midway between them. Furthermore, the number of dimensions of the space used to determine the plane is minimized. In the support vector machine approach [27, 7, 1, 10, 24, 28], in addition to minimizing the weighted sum of distances of misclassi ed points to the bounding planes, we also maximize the distance between the two bounding planes that generate the separating plane. Computational results show that feature suppression is an indirect consequence of the support vector machine approach when an appropriate norm is used. Numerical tests on 6 public data sets show that classi ers trained by the concave minimization approach and those trained by a support vector machine have comparable 10fold cross-validation correctness. However, in all data sets tested, the classi ers obtained by the concave minimization approach selected fewer problem features than those trained by a support vector machine."
            },
            "slug": "Feature-Selection-via-Concave-Minimization-and-Bradley-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Feature Selection via Concave Minimization and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Numerical tests on 6 public data sets show that classi ers trained by the concave minimization approach and those trained by a support vector machine have comparable 10fold cross-validation correctness."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13498098"
                        ],
                        "name": "M. Lobo",
                        "slug": "M.-Lobo",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Lobo",
                            "middleNames": [
                                "Sousa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lobo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2771373"
                        ],
                        "name": "H. Lebret",
                        "slug": "H.-Lebret",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Lebret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lebret"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, Moreau-Yosida (MY) regularization is an effective general solution methodology that is based on inf-convolution ( Lemarechal & Sagastizabal, 1997 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 85
                            }
                        ],
                        "text": "In our particular case, we treat problem (P ) as a second-order cone program (SOCP) (Lobo et al., 1998), which yields the following dual (see Appendix A for the derivation):\nmin 12\u03b3 2 \u2212 \u03b1>e\n(D) w.r.t. \u03b3 \u2208 R, \u03b1 \u2208 Rn\ns.t. 0 6 \u03b1 6 C, \u03b1>y = 0\n||\u2211i \u03b1iyixji||2 6 dj\u03b3, \u2200j \u2208 {1, . . . ,m}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 177
                            }
                        ],
                        "text": "\u2026elements (u, v) and (u\u2032, v\u2032) of a secondorder cone Kd = {(u, v) \u2208 Rd \u00d7 R, ||u||2 6 v} are orthogonal if and only if they both belong to the boundary, and they are \u201canti-proportional\u201d (Lobo et al., 1998); that is, \u2203\u03b7 > 0 such that ||u||2 = v, ||u\u2032||2 = v\u2032, (u, v) = \u03b7(\u2212u\u2032, v\u2032) (see Figure 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "In this paper we show how these problems can be resolved by considering a novel dual formulation of the QCQP as a second-order cone programming (SOCP) problem."
                    },
                    "intents": []
                }
            ],
            "corpusId": 123210079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f49d8df59dff02fadd26152b5585af6e753907c5",
            "isKey": true,
            "numCitedBy": 1416,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-second-order-cone-programming-Lobo-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Applications of second-order cone programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8773884"
                        ],
                        "name": "C. Lemar\u00e9chal",
                        "slug": "C.-Lemar\u00e9chal",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lemar\u00e9chal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemar\u00e9chal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3097463"
                        ],
                        "name": "C. Sagastiz\u00e1bal",
                        "slug": "C.-Sagastiz\u00e1bal",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Sagastiz\u00e1bal",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sagastiz\u00e1bal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 128
                            }
                        ],
                        "text": "In particular, Moreau-Yosida (MY) regularization is an effective general solution methodology that is based on inf-convolution (Lemarechal & Sagastizabal, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 132
                            }
                        ],
                        "text": "One class of solutions to non-smooth optimization problems involves constructing a smooth approximate problem out of a non-smooth problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15734410,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "afa94077afce53837c32014f6af1606b8c28b347",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "When computing the infimal convolution of a convex function f with the squared norm, the so-called Moreau--Yosida regularization of f is obtained. Among other things, this function has a Lipschitzian gradient. We investigate some more of its properties, relevant for optimization. The most important part of our study concerns second-order differentiability: existence of a second-order development of $f$ implies that its regularization has a Hessian. For the converse, we disclose the importance of the decomposition of ${\\Bbb R}^N$ along $\\cal U$ (the subspace where f is \"smooth\") and $\\cal V$ (the subspace parallel to the subdifferential of f)."
            },
            "slug": "Practical-Aspects-of-the-Moreau-Yosida-Theoretical-Lemar\u00e9chal-Sagastiz\u00e1bal",
            "title": {
                "fragments": [],
                "text": "Practical Aspects of the Moreau-Yosida Regularization: Theoretical Preliminaries"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The most important part of this study concerns second-order differentiability: existence of a second- order development of f implies that its regularization has a Hessian."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706780"
                        ],
                        "name": "Cheng Soon Ong",
                        "slug": "Cheng-Soon-Ong",
                        "structuredName": {
                            "firstName": "Cheng Soon",
                            "lastName": "Ong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng Soon Ong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 208981655,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f55aaa941537e2a83f912e0730f8ba1a05a8e71a",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of choosing a kernel suitable for est imation using a Gaussian Process estimator or a Support Vector Machi ne. A novel solution is presented which involves defining a Reprod ucing Kernel Hilbert Space on the space of kernels itself. By utilizin g an analog of the classical representer theorem, the problem of choosi ng a kernel from a parameterized family of kernels (e.g. of varying widt h) is reduced to a statistical estimation problem akin to the problem of mi ni izing a regularized risk functional. Various classical settings f or model or kernel selection are special cases of our framework."
            },
            "slug": "Hyperkernels-Ong-Smola",
            "title": {
                "fragments": [],
                "text": "Hyperkernels"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A novel solution is presented which involves defining a Reprod ucing Kernel Hilbert Space on the space of kernels itself, an analog of the classical representer theorem, which reduces the problem of choosing a kernel from a parameterized family of kernels to a statistical estimation problem akin to a regularized risk functional."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802711"
                        ],
                        "name": "Yves Grandvalet",
                        "slug": "Yves-Grandvalet",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Grandvalet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Grandvalet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794818"
                        ],
                        "name": "S. Canu",
                        "slug": "S.-Canu",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Canu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Canu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 208
                            }
                        ],
                        "text": "While this so-called \u201cmultiple kernel learning\u201d problem can in principle be solved via cross-validation, several recent papers have focused on more efficient methods for kernel learning (Chapelle et al., 2002; Grandvalet & Canu, 2003; Lanckriet et al., 2004; Ong et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 747118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c10bbba5f12ac4d739baa2acd7912c258b6551c",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces an algorithm for the automatic relevance determination of input variables in kernelized Support Vector Machines. Relevance is measured by scale factors defining the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters defining the classifier. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem."
            },
            "slug": "Adaptive-Scaling-for-Feature-Selection-in-SVMs-Grandvalet-Canu",
            "title": {
                "fragments": [],
                "text": "Adaptive Scaling for Feature Selection in SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The resulting algorithm compares favorably to state-of-the-art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153292035"
                        ],
                        "name": "Sayan Mukherjee",
                        "slug": "Sayan-Mukherjee",
                        "structuredName": {
                            "firstName": "Sayan",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sayan Mukherjee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 185
                            }
                        ],
                        "text": "While this so-called \u201cmultiple kernel learning\u201d problem can in principle be solved via cross-validation, several recent papers have focused on more efficient methods for kernel learning (Chapelle et al., 2002; Grandvalet & Canu, 2003; Lanckriet et al., 2004; Ong et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14607075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "626a5da1bfc0f4b38be27f867f95daa061655f94",
            "isKey": false,
            "numCitedBy": 2241,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of automatically tuning multiple parameters for pattern recognition Support Vector Machines (SVMs) is considered. This is done by minimizing some estimates of the generalization error of SVMs using a gradient descent algorithm over the set of parameters. Usual methods for choosing parameters, based on exhaustive search become intractable as soon as the number of parameters exceeds two. Some experimental results assess the feasibility of our approach for a large number of parameters (more than 100) and demonstrate an improvement of generalization performance."
            },
            "slug": "Choosing-Multiple-Parameters-for-Support-Vector-Chapelle-Vapnik",
            "title": {
                "fragments": [],
                "text": "Choosing Multiple Parameters for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The problem of automatically tuning multiple parameters for pattern recognition Support Vector Machines (SVMs) is considered by minimizing some estimates of the generalization error of SVMs using a gradient descent algorithm over the set of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33133899"
                        ],
                        "name": "E. Andersen",
                        "slug": "E.-Andersen",
                        "structuredName": {
                            "firstName": "Erling",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735908"
                        ],
                        "name": "Knud D. Andersen",
                        "slug": "Knud-D.-Andersen",
                        "structuredName": {
                            "firstName": "Knud",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Knud D. Andersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 139
                            }
                        ],
                        "text": "This problem is more challenging than a QP, but it can also be solved in principle by generalpurpose optimization toolboxes such as Mosek (Andersen & Andersen, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 222
                            }
                        ],
                        "text": "In simulations, we use the following values for the free parameters: \u00b5 = 0.5, \u03b51/n = 0.0005, \u03b52 = 0.0001, where the value for \u03b51/n corresponds to the average value this quantity attains when solving the QCQP (L) directly using Mosek."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "We compare the algorithm presented in Section 4.4 with solving the QCQP (L) using Mosek for two datasets, ionosphere and breast cancer, from the UCI repository, and nested subsets of the adult dataset from Platt (1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 93
                            }
                        ],
                        "text": "Thus the algorithm presented in this paper appears to provide a significant improvement over Mosek in computational complexity, both in terms of the number of kernels and the number of data points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "1 for Mosek."
                    },
                    "intents": []
                }
            ],
            "corpusId": 117484770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2b98fbce490feb02f24013413835736af319da6",
            "isKey": true,
            "numCitedBy": 584,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this work is to present the MOSEK optimizer intended for solution of large-scale sparse linear programs. The optimizer is based on the homogeneous interior-point algorithm which in contrast to the primal-dual algorithm detects a possible primal or dual infeasibility reliably. It employs advanced (parallelized) linear algebra, it handles dense columns in the constraint matrix efficiently, and it has a basis identification procedure."
            },
            "slug": "The-Mosek-Interior-Point-Optimizer-for-Linear-An-of-Andersen-Andersen",
            "title": {
                "fragments": [],
                "text": "The Mosek Interior Point Optimizer for Linear Programming: An Implementation of the Homogeneous Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The MOSEK optimizer is based on the homogeneous interior-point algorithm which in contrast to the primal-dual algorithm detects a possible primal or dual infeasibility reliably reliably."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772326"
                        ],
                        "name": "S. Shevade",
                        "slug": "S.-Shevade",
                        "structuredName": {
                            "firstName": "Shirish",
                            "lastName": "Shevade",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shevade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145880755"
                        ],
                        "name": "C. Bhattacharyya",
                        "slug": "C.-Bhattacharyya",
                        "structuredName": {
                            "firstName": "Chiranjib",
                            "lastName": "Bhattacharyya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bhattacharyya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38445965"
                        ],
                        "name": "K. Murthy",
                        "slug": "K.-Murthy",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Murthy",
                            "middleNames": [
                                "R.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 138
                            }
                        ],
                        "text": "Since the \u03b5-optimality conditions for the MY-regularized SKM are exactly the same as for the SVM, but with a different objective function (Platt, 1998; Keerthi et al., 2001):\n(OPT4) max i\u2208IM\u222aI0\u2212\u222aIC+\n{yi\u2207G(\u03b1)i}\n6 min i\u2208IM\u222aI0+\u222aIC\u2212\n{yi\u2207G(\u03b1)i} + 2\u03b5,\nchoosing the pair of indices can be done in a manner similar to that proposed for the SVM, by using the fast heuristics of Platt (1998) and Keerthi et al. (2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 168
                            }
                        ],
                        "text": "Note that for one kernel, i.e., when the SKM reduces to the SVM, this corresponds to the approximate KKT conditions usually employed for the standard SVM (Platt, 1998; Keerthi et al., 2001; Joachims, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 250
                            }
                        ],
                        "text": "Indeed, off-the-shelf algorithms do not suffice in large-scale applications of the SVM, and a second major reason for the rise to prominence of the SVM is the development of special-purpose algorithms for solving the QP (Platt, 1998; Joachims, 1998; Keerthi et al., 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 27
                            }
                        ],
                        "text": "We also define, following (Keerthi et al., 2001), I0+ = I0\u2229{i, yi = 1} and I0\u2212 = I0\u2229{i, yi = \u22121}, IC+ = IC \u2229{i, yi = 1}, IC\u2212 = IC \u2229 {i, yi = \u22121}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 206
                            }
                        ],
                        "text": "We compare the algorithm presented in Section 4.4 with solving the QCQP (L) using Mosek for two datasets, ionosphere and breast cancer, from the UCI repository, and nested subsets of the adult dataset from Platt (1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 264
                            }
                        ],
                        "text": "\u2026different objective function (Platt, 1998; Keerthi et al., 2001):\n(OPT4) max i\u2208IM\u222aI0\u2212\u222aIC+\n{yi\u2207G(\u03b1)i}\n6 min i\u2208IM\u222aI0+\u222aIC\u2212\n{yi\u2207G(\u03b1)i} + 2\u03b5,\nchoosing the pair of indices can be done in a manner similar to that proposed for the SVM, by using the fast heuristics of Platt (1998) and Keerthi et al. (2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026the \u03b5-optimality conditions for the MY-regularized SKM are exactly the same as for the SVM, but with a different objective function (Platt, 1998; Keerthi et al., 2001):\n(OPT4) max i\u2208IM\u222aI0\u2212\u222aIC+\n{yi\u2207G(\u03b1)i}\n6 min i\u2208IM\u222aI0+\u222aIC\u2212\n{yi\u2207G(\u03b1)i} + 2\u03b5,\nchoosing the pair of indices can be done in a manner\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1536643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d95b96d71669f3f4edfcc95cacd428b62b3fcde",
            "isKey": true,
            "numCitedBy": 1804,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This article points out an important source of inefficiency in Platt's sequential minimal optimization (SMO) algorithm that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO. These modified algorithms perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "slug": "Improvements-to-Platt's-SMO-Algorithm-for-SVM-Keerthi-Shevade",
            "title": {
                "fragments": [],
                "text": "Improvements to Platt's SMO Algorithm for SVM Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO that perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107390599"
                        ],
                        "name": "D. Anderson",
                        "slug": "D.-Anderson",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "However, since each line search is the minimization of a convex function, we can use efficient one-dimensional root finding, such as Brent\u2019s method (Brent, 1973)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62598143,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "83615ab0b900c7f6179b6ffdf5b771b05560970f",
            "isKey": false,
            "numCitedBy": 1887,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This monograph describes and analyzes some practical methods for finding approximate zeros and minima of functions."
            },
            "slug": "Algorithms-for-minimization-without-derivatives-Anderson",
            "title": {
                "fragments": [],
                "text": "Algorithms for minimization without derivatives"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This monograph describes and analyzes some practical methods for finding approximate zeros and minima of functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 268
                            }
                        ],
                        "text": "Since each SMO optimization is performed on a differentiable function with Lipschitz gradient and SMO is equivalent to steepest descent for the `1- norm (Joachims, 1998), classical optimization results show that each of those SMO optimizations is finitely convergent (Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Given any function J(\u03b1), the subdifferential of J at \u03b1 \u2202J(\u03b1) is defined as (Bertsekas, 1995):\n\u2202J(\u03b1) = {g \u2208 Rn, \u2200\u03b1\u2032, J(\u03b1\u2032) > J(\u03b1) + g>(\u03b1\u2032 \u2212 \u03b1)}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 140
                            }
                        ],
                        "text": "It is well known that in this situation, steepest descent and coordinate descent methods do not necessarily converge to the global optimum (Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 193
                            }
                        ],
                        "text": "Unfortunately, as is well known in the non-smooth optimization literature, this means that simple local descent algorithms such as SMO may fail to converge or may converge to incorrect values (Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 115
                            }
                        ],
                        "text": "The notion of subdifferential is especially useful for characterizing optimality conditions of nonsmooth problems (Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": true,
            "numCitedBy": 6359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 257
                            }
                        ],
                        "text": "While this so-called \u201cmultiple kernel learning\u201d problem can in principle be solved via cross-validation, several recent papers have focused on more efficient methods for kernel learning (Chapelle et al., 2002; Grandvalet & Canu, 2003; Lanckriet et al., 2004; Ong et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hyperkernels . Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Hyperkernels . Neural Information Processing Systems"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 14,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Multiple-kernel-learning,-conic-duality,-and-the-Bach-Lanckriet/141e6c1dd532504611266d08458dbe2a0dbb4e98?sort=total-citations"
}