{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862682"
                        ],
                        "name": "G. Doddington",
                        "slug": "G.-Doddington",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Doddington",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Doddington"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 42
                            }
                        ],
                        "text": "BLEU and the closely related NIST metric (Doddington, 2002) have been extensively used for comparative evaluation of the various MT systems developed under the DARPA TIDES research program, as well as by other MT researchers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14067706,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f",
            "isKey": false,
            "numCitedBy": 1570,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT research. Their idea, which they call an \"evaluation understudy\", compares MT output with expert reference translations in terms of the statistics of short sequences of words (word N-grams). The more of these N-grams that a translation shares with the reference translations, the better the translation is judged to be. The idea is elegant in its simplicity. But far more important, IBM showed a strong correlation between these automatically generated scores and human judgments of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation facility based on the IBM work. This utility is now available from NIST and serves as the primary evaluation measure for TIDES MT research."
            },
            "slug": "Automatic-evaluation-of-machine-translation-quality-Doddington",
            "title": {
                "fragments": [],
                "text": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "NIST commissioned NIST to develop an MT evaluation facility based on the IBM work, which is now available from NIST and serves as the primary evaluation measure for TIDES MT research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784914"
                        ],
                        "name": "A. Lavie",
                        "slug": "A.-Lavie",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Lavie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lavie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757166"
                        ],
                        "name": "Kenji Sagae",
                        "slug": "Kenji-Sagae",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Sagae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenji Sagae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2008176"
                        ],
                        "name": "Shyamsundar Jayaraman",
                        "slug": "Shyamsundar-Jayaraman",
                        "structuredName": {
                            "firstName": "Shyamsundar",
                            "lastName": "Jayaraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shyamsundar Jayaraman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 18
                            }
                        ],
                        "text": "In previous work (Lavie et al., 2004), we compared METEOR with IBM's BLEU metric and it\u2019s derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3120630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc6ffa9b66734b4c9381cf525159dba8288917e",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent research has shown that a balanced harmonic mean (F1 measure) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality. We show that significantly better correlations can be achieved by placing more weight on recall than on precision. While this may seem unexpected, since BLEU and NIST focus on n-gram precision and disregard recall, our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall. We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics, but also to BLEU and NIST."
            },
            "slug": "The-significance-of-recall-in-automatic-metrics-for-Lavie-Sagae",
            "title": {
                "fragments": [],
                "text": "The significance of recall in automatic metrics for MT evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work shows that correlation with human judgments is highest when almost all of the weight is assigned to recall, and shows that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics, but also to BLEU and NIST."
            },
            "venue": {
                "fragments": [],
                "text": "AMTA"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115527766"
                        ],
                        "name": "Luke Shea",
                        "slug": "Luke-Shea",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Shea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Shea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144599392"
                        ],
                        "name": "I. D. Melamed",
                        "slug": "I.-D.-Melamed",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Melamed",
                            "middleNames": [
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. D. Melamed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9469794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b038147589432947cd47b2c75d46e43613a1a91b",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluation of MT evaluation measures is limited by inconsistent human judgment data. Nonetheless, machine translation can be evaluated using the well-known measures precision, recall, and their average, the F-measure. The unigram-based F-measure has significantly higher correlation with human judgments than recently proposed alternatives. More importantly, this standard measure has an intuitive graphical interpretation, which can facilitate insight into how MT systems might be improved. The relevant software is publicly available from http://nlp.cs.nyu.edu/GTM/."
            },
            "slug": "Evaluation-of-machine-translation-and-its-Turian-Shea",
            "title": {
                "fragments": [],
                "text": "Evaluation of machine translation and its evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The unigram-based F-measure has significantly higher correlation with human judgments than recently proposed alternatives and has an intuitive graphical interpretation, which can facilitate insight into how MT systems might be improved."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "The main principle behind IBM\u2019s BLEU metric (Papineni et al, 2002) is the measurement of the\noverlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 180
                            }
                        ],
                        "text": "Automatic Metrics for machine translation (MT) evaluation have been receiving significant attention in the past two years, since IBM's BLEU metric was proposed and made available (Papineni et al 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372, Ann Arbor, June 2005. c\u00a92005 Association for Computational Linguistics\nmetric for machine translation evaluation that is based on a generalized concept of unigram matching between the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": true,
            "numCitedBy": 16617,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48505446"
                        ],
                        "name": "M. King",
                        "slug": "M.-King",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797819"
                        ],
                        "name": "Andrei Popescu-Belis",
                        "slug": "Andrei-Popescu-Belis",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Popescu-Belis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Popescu-Belis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "We also perform experiments to show the relative contributions of the various mapping modules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 46
                            }
                        ],
                        "text": "Recently developed frameworks, such as FEMTI (King et al, 2003), are attempting to devise effective platforms for combining multi-faceted measures for MT evaluation in effective and user-adjustable ways."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1745463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24da373b7b6350394a36088b221efef8c7bbe0c9",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents FEMTI, a web-based Framework for the Evaluation of Machine Translation in ISLE. FEMTI offers structured descriptions of potential user needs, linked to an overview of technical characteristics of MT systems. The description of possible systems is mainly articulated around the quality characteristics for software product set out in ISO/IEC standard 9126. Following the philosophy set out there and in the related 14598 series of standards, each quality characteristic bottoms out in metrics which may be applied to a particular instance of a system in order to judge how satisfactory the system is with respect to that characteristic. An evaluator can use the description of user needs to help identify the specific needs of his evaluation and the relations between them. He can then follow the pointers to system description to determine what metrics should be applied and how. In the current state of the framework, emphasis is on being exhaustive, including as much as possible of the information available in the literature on machine translation evaluation. Future work will aim at being more analytic, looking at characteristics and metrics to see how they relate to one another, validating metrics and investigating the correlation between particular metrics and human judgement."
            },
            "slug": "FEMTI:-creating-and-using-a-framework-for-MT-King-Popescu-Belis",
            "title": {
                "fragments": [],
                "text": "FEMTI: creating and using a framework for MT evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "FEMTI offers structured descriptions of potential user needs, linked to an overview of technical characteristics of MT systems, including as much as possible of the information available in the literature on machine translation evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11728052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b48af24cd360d6b0a3dd25424550c28bf97bc1ce",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations."
            },
            "slug": "Syntax-based-Alignment-of-Multiple-Translations:-Pang-Knight",
            "title": {
                "fragments": [],
                "text": "Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets that are good representations of paraphrases and can predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "Next we compute Fmean by combining the precision and recall via a harmonic-mean (van Rijsbergen, 1979) that places most of the weight on recall."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "year": 1979
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 7,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/METEOR:-An-Automatic-Metric-for-MT-Evaluation-with-Banerjee-Lavie/0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7?sort=total-citations"
}