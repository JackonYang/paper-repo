{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059814276"
                        ],
                        "name": "Tom Yeh",
                        "slug": "Tom-Yeh",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108488317"
                        ],
                        "name": "John J. Lee",
                        "slug": "John-J.-Lee",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lee",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John J. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Photo-based Question Answering enables users to ask questions that reference an included photograph, and tackles the very difficult problems of automatic computer vision and question answering [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14801863,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "6dac15df6545b883434ab18fbb21f8b956f897d1",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Photo-based question answering is a useful way of finding information about physical objects. Current question answering (QA) systems are text-based and can be difficult to use when a question involves an object with distinct visual features. A photo-based QA system allows direct use of a photo to refer to the object. We develop a three-layer system architecture for photo-based QA that brings together recent technical achievements in question answering and image matching. The first, template-based QA layer matches a query photo to online images and extracts structured data from multimedia databases to answer questions about the photo. To simplify image matching, it exploits the question text to filter images based on categories and keywords. The second, information retrieval QA layer searches an internal repository of resolved photo-based questions to retrieve relevant answers. The third, human-computation QA layer leverages community experts to handle the most difficult cases. A series of experiments performed on a pilot dataset of 30,000 images of books, movie DVD covers, grocery items, and landmarks demonstrate the technical feasibility of this architecture. We present three prototypes to show how photo-based QA can be built into an online album, a text-based QA, and a mobile application."
            },
            "slug": "Photo-based-question-answering-Yeh-Lee",
            "title": {
                "fragments": [],
                "text": "Photo-based question answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work develops a three-layer system architecture for photo-based QA that brings together recent technical achievements in question answering and image matching and leverages community experts to handle the most difficult cases."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066164791"
                        ],
                        "name": "Tara Matthews",
                        "slug": "Tara-Matthews",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Matthews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tara Matthews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712939"
                        ],
                        "name": "S. Carter",
                        "slug": "S.-Carter",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Carter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34423974"
                        ],
                        "name": "Carol Pai",
                        "slug": "Carol-Pai",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Pai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carol Pai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3256404"
                        ],
                        "name": "Janette Fong",
                        "slug": "Janette-Fong",
                        "structuredName": {
                            "firstName": "Janette",
                            "lastName": "Fong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janette Fong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3055754"
                        ],
                        "name": "Jennifer Mankoff",
                        "slug": "Jennifer-Mankoff",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Mankoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Mankoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Prior work has demonstrated that such services need to work quickly [22], and so we have developed an approach (and accompanying implementation) called quikTurkit that provides a layer of abstraction on top of Mechanical Turk to intelligently recruit multiple workers before they are needed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31636321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3b366c319beb463a5fef82aebfcefc0a35948d9",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "People who are deaf or hard-of-hearing may have challenges communicating with others via spoken words and may have challenges being aware of audio events in their environments. This is especially true in public places, which may not have accessible ways of communicating announcements and other audio events. In this paper, we present the design and evaluation of a mobile sound transcription tool for the deaf and hard-of-hearing. Our tool, Scribe4Me, is designed to improve awareness of sound-based information in any location. When a button is pushed on the tool, a transcription of the last 30 seconds of sound is given to the user in a text message. Transcriptions include dialog and descriptions of environmental sounds. We describe a 2-week field study of an exploratory prototype, which shows that our approach is feasible, highlights particular contexts in which it is useful, and provides information about what should be contained in transcriptions."
            },
            "slug": "Scribe4Me:-Evaluating-a-Mobile-Sound-Transcription-Matthews-Carter",
            "title": {
                "fragments": [],
                "text": "Scribe4Me: Evaluating a Mobile Sound Transcription Tool for the Deaf"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A 2-week field study of an exploratory prototype of a mobile sound transcription tool for the deaf and hard-of-hearing shows that the approach is feasible, highlights particular contexts in which it is useful, and provides information about what should be contained in transcriptions."
            },
            "venue": {
                "fragments": [],
                "text": "UbiComp"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066164791"
                        ],
                        "name": "Tara Matthews",
                        "slug": "Tara-Matthews",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Matthews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tara Matthews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3256404"
                        ],
                        "name": "Janette Fong",
                        "slug": "Janette-Fong",
                        "structuredName": {
                            "firstName": "Janette",
                            "lastName": "Fong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janette Fong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403902788"
                        ],
                        "name": "F. W. Ho-Ching",
                        "slug": "F.-W.-Ho-Ching",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Ho-Ching",
                            "middleNames": [
                                "Wai-ling"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. W. Ho-Ching"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69398636"
                        ],
                        "name": "J. Mankoff",
                        "slug": "J.-Mankoff",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Mankoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mankoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14555908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbef0bdab35bf2f74c0bb8afdaa63c5b8f10b26b",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Sounds such as co-workers chatting nearby or a dripping faucet help us maintain awareness of and respond to our surroundings. Without a tool that communicates ambient sounds in a non-auditory manner, maintaining this awareness is difficult for people who are deaf. We present an iterative investigation of peripheral, visual displays of ambient sounds. Our major contributions are: (1) a rich understanding of what ambient sounds are useful to people who are deaf, (2) a set of visual and functional requirements for a peripheral sound display, based on feedback from people who are deaf, (3) lab-based evaluations investigating the characteristics of four prototypes, and (4) a set of design guidelines for successful ambient audio displays, based on a comparison of four implemented prototypes and user feedback. Our work provides valuable information about the sound awareness needs of the deaf and can help to inform further design of such applications."
            },
            "slug": "Evaluating-non-speech-sound-visualizations-for-the-Matthews-Fong",
            "title": {
                "fragments": [],
                "text": "Evaluating non-speech sound visualizations for the deaf"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An iterative investigation of peripheral, visual displays of ambient sounds, providing valuable information about the sound awareness needs of the deaf and can help to inform further design of such applications."
            },
            "venue": {
                "fragments": [],
                "text": "Behav. Inf. Technol."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2893996"
                        ],
                        "name": "Shaun K. Kane",
                        "slug": "Shaun-K.-Kane",
                        "structuredName": {
                            "firstName": "Shaun",
                            "lastName": "Kane",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaun K. Kane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744846"
                        ],
                        "name": "Jeffrey P. Bigham",
                        "slug": "Jeffrey-P.-Bigham",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Bigham",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey P. Bigham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796045"
                        ],
                        "name": "J. Wobbrock",
                        "slug": "J.-Wobbrock",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Wobbrock",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wobbrock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 214
                            }
                        ],
                        "text": "Touchscreen devices like the iPhone were once assumed to be inaccessible to blind users, but well-designed, multitouch interfaces leverage the spatial layout of the screen and can even be preferred by blind people [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207169434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d606b9c9ddd46993802897c2752e68797e5cf4c6",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in touch screen technology have increased the prevalence of touch screens and have prompted a wave of new touch screen-based devices. However, touch screens are still largely inaccessible to blind users, who must adopt error-prone compensatory strategies to use them or find accessible alternatives. This inaccessibility is due to interaction techniques that require the user to visually locate objects on the screen. To address this problem, we introduce Slide Rule, a set of audio-based multi-touch interaction techniques that enable blind users to access touch screen applications. We describe the design of Slide Rule, our interaction techniques, and a user study in which 10 blind people used Slide Rule and a button-based Pocket PC screen reader. Results show that Slide Rule was significantly faster than the button-based system, and was preferred by 7 of 10 users. However, users made more errors when using Slide Rule than when using the more familiar button-based system."
            },
            "slug": "Slide-rule:-making-mobile-touch-screens-accessible-Kane-Bigham",
            "title": {
                "fragments": [],
                "text": "Slide rule: making mobile touch screens accessible to blind people using multi-touch interaction techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Slide Rule is introduced, a set of audio-based multi-touch interaction techniques that enable blind users to access touch screen applications and shows that Slide Rule was significantly faster than the button-based system, and was preferred by 7 of 10 users."
            },
            "venue": {
                "fragments": [],
                "text": "Assets '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077426189"
                        ],
                        "name": "Scott Gifford",
                        "slug": "Scott-Gifford",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Gifford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Gifford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061510173"
                        ],
                        "name": "J. Knox",
                        "slug": "J.-Knox",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Knox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Knox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055803117"
                        ],
                        "name": "J. James",
                        "slug": "J.-James",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "James",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. James"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49428189"
                        ],
                        "name": "A. Prakash",
                        "slug": "A.-Prakash",
                        "structuredName": {
                            "firstName": "Atul",
                            "lastName": "Prakash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Prakash"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 158
                            }
                        ],
                        "text": "For instance, TextScout [33] provides an accessible OCR interface, and Talking Points delivers contextually-relevant navigation information in urban settings [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5323577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "104fc2ff407a457b7df1d2eba86225bb2b217d3f",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of systems, technological and otherwise, exist for helping blind and visually impaired users navigate from place to place. [5, 3] However, in focusing only on the destination, these systems often neglect the journey. We propose to rectify this, with a device to improve a user\u2019s peripheral awareness of their surroundings. This is the motivation and the primary goal of this project. The Talking Points project aims to create a system for attaching information to places and objects, in a format that can be easily converted to speech. It allows blind and visually impaired users to get information about the places and objects they are walking by, and can also be used to provide digital information to other passersby. We have created a proof-of-concept of this system. The system reads passive RFID tags from a mobile reader. It looks up a tag\u2019s ID number to find associated text, then uses text-to-speech software to read that speech to the user."
            },
            "slug": "Introduction-to-the-talking-points-project-Gifford-Knox",
            "title": {
                "fragments": [],
                "text": "Introduction to the talking points project"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Talking Points project aims to create a system for attaching information to places and objects, in a format that can be easily converted to speech, that allows blind and visually impaired users to get information about the places they are walking by, and can also be used to provide digital information to other passersby."
            },
            "venue": {
                "fragments": [],
                "text": "Assets '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784365"
                        ],
                        "name": "Laura A. Dabbish",
                        "slug": "Laura-A.-Dabbish",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Dabbish",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura A. Dabbish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 338469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d4a6e4900ec0f096c87bb2b1272eeceaa584a6",
            "isKey": false,
            "numCitedBy": 2386,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained."
            },
            "slug": "Labeling-images-with-a-computer-game-Ahn-Dabbish",
            "title": {
                "fragments": [],
                "text": "Labeling images with a computer game"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new interactive system: a game that is fun and can be used to create valuable output that addresses the image-labeling problem and encourages people to do the work by taking advantage of their desire to be entertained."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2893996"
                        ],
                        "name": "Shaun K. Kane",
                        "slug": "Shaun-K.-Kane",
                        "structuredName": {
                            "firstName": "Shaun",
                            "lastName": "Kane",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaun K. Kane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712587"
                        ],
                        "name": "C. Jayant",
                        "slug": "C.-Jayant",
                        "structuredName": {
                            "firstName": "Chandrika",
                            "lastName": "Jayant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jayant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796045"
                        ],
                        "name": "J. Wobbrock",
                        "slug": "J.-Wobbrock",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Wobbrock",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wobbrock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762656"
                        ],
                        "name": "R. Ladner",
                        "slug": "R.-Ladner",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Ladner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ladner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "Applications for general-purpose smartphones are beginning to replace special-purpose devices, but blind people still carry devices such as GPS-powered navigation aids, barcode readers, light detectors, color identifiers, and compasses [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1108352,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "578367827a729341912cef92ad2c0d552fc36211",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Mobile devices provide people with disabilities new opportunities to act independently in the world. However, these empowering devices have their own accessibility challenges. We present a formative study that examines how people with visual and motor disabilities select, adapt, and use mobile devices in their daily lives. We interviewed 20 participants with visual and motor disabilities and asked about their current use of mobile devices, including how they select them, how they use them while away from home, and how they adapt to accessibility challenges when on the go. Following the interviews, 19 participants completed a diary study in which they recorded their experiences using mobile devices for one week. Our results show that people with visual and motor disabilities use a variety of strategies to adapt inaccessible mobile devices and successfully use them to perform everyday tasks and navigate independently. We provide guidelines for more accessible and empowering mobile device design."
            },
            "slug": "Freedom-to-roam:-a-study-of-mobile-device-adoption-Kane-Jayant",
            "title": {
                "fragments": [],
                "text": "Freedom to roam: a study of mobile device adoption and accessibility for people with visual and motor disabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This formative study examines how people with visual and motor disabilities select, adapt, and use mobile devices in their daily lives, and provides guidelines for more accessible and empowering mobile device design."
            },
            "venue": {
                "fragments": [],
                "text": "Assets '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145234497"
                        ],
                        "name": "A. Kittur",
                        "slug": "A.-Kittur",
                        "structuredName": {
                            "firstName": "Aniket",
                            "lastName": "Kittur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kittur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2226805"
                        ],
                        "name": "Ed H. Chi",
                        "slug": "Ed-H.-Chi",
                        "structuredName": {
                            "firstName": "Ed",
                            "lastName": "Chi",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ed H. Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3994427"
                        ],
                        "name": "B. Suh",
                        "slug": "B.-Suh",
                        "structuredName": {
                            "firstName": "Bongwon",
                            "lastName": "Suh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Suh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "Mechanical Turk makes outsourcing small paid jobs practical [1] and has been used for a wide variety of purposes, including large user studies [15], labeling image data sets [30], and determining political sentiments in blog snippets [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1442595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2de9b67cabf99432a4d4a7b278a98afdb42c3d6d",
            "isKey": false,
            "numCitedBy": 1981,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach."
            },
            "slug": "Crowdsourcing-user-studies-with-Mechanical-Turk-Kittur-Chi",
            "title": {
                "fragments": [],
                "text": "Crowdsourcing user studies with Mechanical Turk"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Although micro-task markets have great potential for rapidly collecting user measurements at low costs, it is found that special care is needed in formulating tasks in order to harness the capabilities of the approach."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144844426"
                        ],
                        "name": "M. Morris",
                        "slug": "M.-Morris",
                        "structuredName": {
                            "firstName": "Meredith",
                            "lastName": "Morris",
                            "middleNames": [
                                "Ringel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Morris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144113253"
                        ],
                        "name": "J. Teevan",
                        "slug": "J.-Teevan",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Teevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Teevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1814699"
                        ],
                        "name": "Katrina Panovich",
                        "slug": "Katrina-Panovich",
                        "structuredName": {
                            "firstName": "Katrina",
                            "lastName": "Panovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrina Panovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "One option may be to send questions to one\u2019s social network [26] or even to more personal contacts via picture text messaging."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Prior work has explored how people ask and answer questions on their online social networks [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8797180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44440b7e988c3e224439a96617fcf8103e0b2ab",
            "isKey": false,
            "numCitedBy": 678,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "People often turn to their friends, families, and colleagues when they have questions. The recent, rapid rise of online social networking tools has made doing this on a large scale easy and efficient. In this paper we explore the phenomenon of using social network status messages to ask questions. We conducted a survey of 624 people, asking them to share the questions they have asked and answered of their online social networks. We present detailed data on the frequency of this type of question asking, the types of questions asked, and respondents' motivations for asking their social networks rather than using more traditional search tools like Web search engines. We report on the perceived speed and quality of the answers received, as well as what motivates people to respond to questions seen in their friends' status messages. We then discuss the implications of our findings for the design of next-generation search tools."
            },
            "slug": "What-do-people-ask-their-social-networks,-and-why:-Morris-Teevan",
            "title": {
                "fragments": [],
                "text": "What do people ask their social networks, and why?: a survey study of status message q&a behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper explores the phenomenon of using social network status messages to ask questions, and presents detailed data on the frequency of this type of question asking, the types of questions asked, and respondents' motivations for asking their social networks rather than using more traditional search tools like Web search engines."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391237"
                        ],
                        "name": "Patrick E. Lanigan",
                        "slug": "Patrick-E.-Lanigan",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Lanigan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick E. Lanigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783978"
                        ],
                        "name": "Aaron M. Paulos",
                        "slug": "Aaron-M.-Paulos",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Paulos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron M. Paulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110115871"
                        ],
                        "name": "Andrew W. Williams",
                        "slug": "Andrew-W.-Williams",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Williams",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew W. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144833923"
                        ],
                        "name": "P. Narasimhan",
                        "slug": "P.-Narasimhan",
                        "structuredName": {
                            "firstName": "Priya",
                            "lastName": "Narasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Narasimhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "For example, Trinetra connects a portable barcode reader to a phone via Bluetooth [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2098625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22d67b268be8ea31ce8f3c7c7b002c6c18c45c9c",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Trinetra aims for cost-effective, assistive technologies to provide blind people with a greater degree of independence in their daily activities. The overall objective is to improve the quality of life for the blind by harnessing the collective capability of diverse networked embedded devices to support grocery shopping, transportation, etc. This paper describes our research and development of the Trinetra system, a barcode-based solution comprising COTS components, such as an Internetand Bluetooth-enabled cell phone, text-to-speech software and a portable barcode reader. We describe our experiences with the first deployment of Trinetra at the Carnegie Mellon University\u2019s campus store, Entropy."
            },
            "slug": "Trinetra:-Assistive-Technologies-for-the-Blind-Lanigan-Paulos",
            "title": {
                "fragments": [],
                "text": "Trinetra: Assistive Technologies for the Blind"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The research and development of the Trinetra system, a barcode-based solution comprising COTS components, such as an Internetand Bluetooth-enabled cell phone, text-to-speech software and a portable barcode reader, is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3086992"
                        ],
                        "name": "Hironobu Takagi",
                        "slug": "Hironobu-Takagi",
                        "structuredName": {
                            "firstName": "Hironobu",
                            "lastName": "Takagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hironobu Takagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48169254"
                        ],
                        "name": "Shinya Kawanaka",
                        "slug": "Shinya-Kawanaka",
                        "structuredName": {
                            "firstName": "Shinya",
                            "lastName": "Kawanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shinya Kawanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48045067"
                        ],
                        "name": "Masatomo Kobayashi",
                        "slug": "Masatomo-Kobayashi",
                        "structuredName": {
                            "firstName": "Masatomo",
                            "lastName": "Kobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masatomo Kobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072440438"
                        ],
                        "name": "Takashi Itoh",
                        "slug": "Takashi-Itoh",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Itoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takashi Itoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1851536"
                        ],
                        "name": "C. Asakawa",
                        "slug": "C.-Asakawa",
                        "structuredName": {
                            "firstName": "Chieko",
                            "lastName": "Asakawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Asakawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8748002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43e0c4d833f34e747dfcf9b6036c35c670f02199",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Web content is under the control of site owners, and therefore the site owners have the responsibility to make their content accessible. This is a basic assumption of Web accessibility. Users who want access to inaccessible content must ask the site owners for help. However, the process is slow and too often the need is mooted before the content becomes accessible. Social Accessibility is an approach to drastically reduce the burden on site owners and to shorten the time to provide accessible Web content by allowing volunteers worldwide to - renovate' any webpage on the Internet. Users encountering Web access problems anywhere at any time will be able to immediately report the problems to a social computing service. Volunteers can be quickly notified, and they can easily respond by creating and publishing the requested accessibility metadata--also helping any other users who encounter the same problems. Site owners can learn about the methods for future accessibility renovations based on the volunteers' external metadata. There are two key technologies to enable this process, the external metadata that allows volunteers to annotate existing Web content, and the social computing service that supports the collaborative renovations. In this paper, we will first review previous approaches, and then propose the Social Accessibility approach. The scenario, implementation, and results of a pilot service are introduced, followed by discussion of future directions."
            },
            "slug": "Social-accessibility:-achieving-accessibility-Takagi-Kawanaka",
            "title": {
                "fragments": [],
                "text": "Social accessibility: achieving accessibility through collaborative metadata authoring"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper will first review previous approaches, and then propose the Social Accessibility approach, to drastically reduce the burden on site owners and to shorten the time to provide accessible Web content by allowing volunteers worldwide to - renovate' any webpage on the Internet."
            },
            "venue": {
                "fragments": [],
                "text": "Assets '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10442652"
                        ],
                        "name": "Mary R. Power",
                        "slug": "Mary-R.-Power",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Power",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary R. Power"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35855217"
                        ],
                        "name": "D. Power",
                        "slug": "D.-Power",
                        "structuredName": {
                            "firstName": "Desmond",
                            "lastName": "Power",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Power"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50376843"
                        ],
                        "name": "Louise Horstmanshof",
                        "slug": "Louise-Horstmanshof",
                        "structuredName": {
                            "firstName": "Louise",
                            "lastName": "Horstmanshof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Louise Horstmanshof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "Other common remote services include relay services for deaf and hard of hearing people (which requires trained employees) [27], and the retroactive nearly real-time audio captioning by dedicated workers in Scribe4Me [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24654000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce5d45652d37b31a8af369722323f8a09684dda2",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the expansion of Deaf people's use of communication technology little is published about how they use electronic communication in their social and working lives and the implications for their concepts of identity and community. Australia is an ideal research base because the use of a range of technologies is widespread there. To gain access to a wide age range of people who identify as Deaf, members of the national organization, the Australian Association of the Deaf, were surveyed by mail. Results showed that Short Message Service (SMS), telephone typewriters (TTY), voice/TTY relay services, fax, and e-mail were used regularly. Deaf users are discerning of the purposes for which they use each method: SMS for social and personal interactions, TTY for longer communications and (via the relay service) with people and services without TTYs, fax for business and social contact, and computers for personal and business e-mails as well as Web browsing, accessing chat rooms, word processing, games, and study."
            },
            "slug": "Deaf-people-communicating-via-SMS,-TTY,-relay-fax,-Power-Power",
            "title": {
                "fragments": [],
                "text": "Deaf people communicating via SMS, TTY, relay service, fax, and computers in Australia."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Results showed that Short Message Service, telephone typewriters, TTY, voice/TTY relay services, fax, and e-mail were used regularly and Deaf users are discerning of the purposes for which they use each method."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of deaf studies and deaf education"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091924025"
                        ],
                        "name": "P. Hsueh",
                        "slug": "P.-Hsueh",
                        "structuredName": {
                            "firstName": "Pei-yun",
                            "lastName": "Hsueh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hsueh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780489"
                        ],
                        "name": "Prem Melville",
                        "slug": "Prem-Melville",
                        "structuredName": {
                            "firstName": "Prem",
                            "lastName": "Melville",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prem Melville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 234
                            }
                        ],
                        "text": "Mechanical Turk makes outsourcing small paid jobs practical [1] and has been used for a wide variety of purposes, including large user studies [15], labeling image data sets [30], and determining political sentiments in blog snippets [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3954835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a749f461a4194f1d42d06bdd3071dd19b03ac26",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency."
            },
            "slug": "Data-Quality-from-Crowdsourcing:-A-Study-of-Hsueh-Melville",
            "title": {
                "fragments": [],
                "text": "Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An empirical study is conducted to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2009"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40913460"
                        ],
                        "name": "X. Liu",
                        "slug": "X.-Liu",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 128
                            }
                        ],
                        "text": "Some accessible applications that use the camera on existing phones include currency-reading applications and color identifiers [20, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11617888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c80bf58a00127f7cf3f355902fa7ac12f1629142",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a camera phone-based currency reader for the visually impaired that can identify the value of U.S. paper currency. Currently, U.S. paper currency can only be identified visually and this situation will continue for a foreseeable future. Our solution harvests the imaging and computational power on camera phones to read these bills. Considering it is impractical for the visually impaired to capture high quality image, our currency reader performs real time processing for each captured frame as the camera approaches the bill. We developed efficient background subtraction and perspective correction algorithms and trained our currency reader using an efficient Ada-boost framework. Our currency reader processes 10 frames/second and achieves a false positive rate of approximately 1/10000. Major smart phone platforms, including Symbian and Windows Mobile, are supported."
            },
            "slug": "A-camera-phone-based-currency-reader-for-the-Liu",
            "title": {
                "fragments": [],
                "text": "A camera phone based currency reader for the visually impaired"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A camera phone-based currency reader for the visually impaired that can identify the value of U.S. paper currency by developing efficient background subtraction and perspective correction algorithms and trained the currency reader using an efficient Ada-boost framework."
            },
            "venue": {
                "fragments": [],
                "text": "Assets '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2765021"
                        ],
                        "name": "D. Horowitz",
                        "slug": "D.-Horowitz",
                        "structuredName": {
                            "firstName": "Damon",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Horowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2833700"
                        ],
                        "name": "S. Kamvar",
                        "slug": "S.-Kamvar",
                        "structuredName": {
                            "firstName": "Sepandar",
                            "lastName": "Kamvar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kamvar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "\u201d [28] VizWiz and quikTurkit explore how to use microtask marketplaces like Amazon\u2019s Mechanical Turk to get answers back even faster."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8169387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75191fd7331e505b42b61770cb10025e65708a60",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Aardvark, a social search engine. With Aardvark, users ask a question, either by instant message, email, web input, text message, or voice. Aardvark then routes the question to the person in the user's extended social network most likely to be able to answer that question. As compared to a traditional web search engine, where the challenge lies in finding the right document to satisfy a user's information need, the challenge in a social search engine like Aardvark lies in finding the right person to satisfy a user's information need. Further, while trust in a traditional search engine is based on authority, in a social search engine like Aardvark, trust is based on intimacy. We describe how these considerations inform the architecture, algorithms, and user interface of Aardvark, and how they are reflected in the behavior of Aardvark users."
            },
            "slug": "The-anatomy-of-a-large-scale-social-search-engine-Horowitz-Kamvar",
            "title": {
                "fragments": [],
                "text": "The anatomy of a large-scale social search engine"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "How trust is based on intimacy and other considerations inform the architecture, algorithms, and user interface of Aardvark, and how they are reflected in the behavior of AARDvark users are described."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2842983"
                        ],
                        "name": "D. Hong",
                        "slug": "D.-Hong",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Hong",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32507491"
                        ],
                        "name": "Shawn C. Kimmel",
                        "slug": "Shawn-C.-Kimmel",
                        "structuredName": {
                            "firstName": "Shawn",
                            "lastName": "Kimmel",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shawn C. Kimmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3367945"
                        ],
                        "name": "Rett Boehling",
                        "slug": "Rett-Boehling",
                        "structuredName": {
                            "firstName": "Rett",
                            "lastName": "Boehling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rett Boehling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3363073"
                        ],
                        "name": "Nina Camoriano",
                        "slug": "Nina-Camoriano",
                        "structuredName": {
                            "firstName": "Nina",
                            "lastName": "Camoriano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nina Camoriano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3360446"
                        ],
                        "name": "W. Cardwell",
                        "slug": "W.-Cardwell",
                        "structuredName": {
                            "firstName": "Wes",
                            "lastName": "Cardwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Cardwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3367317"
                        ],
                        "name": "Greg Jannaman",
                        "slug": "Greg-Jannaman",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Jannaman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Jannaman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360617"
                        ],
                        "name": "A. Purcell",
                        "slug": "A.-Purcell",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Purcell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Purcell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067867618"
                        ],
                        "name": "D. Ross",
                        "slug": "D.-Ross",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48359551"
                        ],
                        "name": "Eric Russel",
                        "slug": "Eric-Russel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Russel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Russel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 166
                            }
                        ],
                        "text": ", chirps and cuckoos crosswalk signal sounds), verbal instructions, or a combination of output methods, many of which are used in other applications for blind people [9, 32, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18304227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a739500252163d0ed24e114fab363f5d30304d1",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the development of a system that will allow a visually-impaired person to safely operate a motor vehicle. Named the blind driver challenge, the purpose of the project is to improve the independence of the visually-impaired by allowing them to travel at their convenience. The system development is targeted to be deployed on Team Victor Tangopsilas DARPA Urban Challenge vehicle ldquoOdin.rdquo The system uses tactile and audio interfaces to relay information to the driver about vehicle heading and speed. The driver then corrects his steering and speed using a joystick. The tactile interface is a modified massage chair, which directs the driver to accelerate or brake. The audio interface is a pair of headphones, which directs the driver where to turn. Testing software has been developed to evaluate the effectiveness of the system by tracking the userspsila ability to follow signals generated by the blind driver challenge code. With these results the team hopes to improve the system, and eventually through a partnership with the Virginia School for the Blind, expand testing to include the visually-impaired."
            },
            "slug": "Development-of-a-semi-autonomous-vehicle-operable-Hong-Kimmel",
            "title": {
                "fragments": [],
                "text": "Development of a semi-autonomous vehicle operable by the visually-impaired"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The development of a system that will allow a visually-impaired person to safely operate a motor vehicle and eventually through a partnership with the Virginia School for the Blind, expandTesting software has been developed to evaluate the effectiveness of the system."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195306"
                        ],
                        "name": "H. Bay",
                        "slug": "H.-Bay",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Bay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433494"
                        ],
                        "name": "Andreas Ess",
                        "slug": "Andreas-Ess",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14777911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdbb606ae47c64049262dfbd3bb147d3f4ba8420",
            "isKey": false,
            "numCitedBy": 11653,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speeded-Up-Robust-Features-(SURF)-Bay-Ess",
            "title": {
                "fragments": [],
                "text": "Speeded-Up Robust Features (SURF)"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48155668"
                        ],
                        "name": "Greg Little",
                        "slug": "Greg-Little",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Little",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Little"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912259"
                        ],
                        "name": "Lydia B. Chilton",
                        "slug": "Lydia-B.-Chilton",
                        "structuredName": {
                            "firstName": "Lydia",
                            "lastName": "Chilton",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lydia B. Chilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34874616"
                        ],
                        "name": "Max Goldman",
                        "slug": "Max-Goldman",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Goldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Goldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152160465"
                        ],
                        "name": "Rob Miller",
                        "slug": "Rob-Miller",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rob Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5707841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "752fa5dd69f4b4c270a1a7ab6008dca8be7d2c3c",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Mechanical Turk (MTurk) provides an on-demand source of human computation. This provides a tremendous opportunity to explore algorithms which incorporate human computation as a function call. However, various systems challenges make this difficult in practice, and most uses of MTurk post large numbers of independent tasks. TurKit is a toolkit for prototyping and exploring algorithmic human computation, while maintaining a straight-forward imperative programming style. We present the crash-and-rerun programming model that makes TurKit possible, along with a variety of applications for human computation algorithms. We also present case studies of TurKit used for real experiments across different fields."
            },
            "slug": "TurKit:-human-computation-algorithms-on-mechanical-Little-Chilton",
            "title": {
                "fragments": [],
                "text": "TurKit: human computation algorithms on mechanical turk"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents the crash-and-rerun programming model that makes TurKit possible, along with a variety of applications for human computation algorithms, and presents case studies of TurKit used for real experiments across different fields."
            },
            "venue": {
                "fragments": [],
                "text": "UIST"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144080148"
                        ],
                        "name": "A. Sorokin",
                        "slug": "A.-Sorokin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Sorokin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sorokin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "Mechanical Turk makes outsourcing small paid jobs practical [1] and has been used for a wide variety of purposes, including large user studies [15], labeling image data sets [30], and determining political sentiments in blog snippets [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1206581,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d513d8b6470c7cbbeca8563505de8711325a3179",
            "isKey": false,
            "numCitedBy": 645,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to outsource data annotation to Amazon Mechanical Turk. Doing so has produced annotations in quite large numbers relatively cheaply. The quality is good, and can be checked and controlled. Annotations are produced quickly. We describe results for several different annotation problems. We describe some strategies for determining when the task is well specified and properly priced."
            },
            "slug": "Utility-data-annotation-with-Amazon-Mechanical-Turk-Sorokin-Forsyth",
            "title": {
                "fragments": [],
                "text": "Utility data annotation with Amazon Mechanical Turk"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work shows how to outsource data annotation to Amazon Mechanical Turk, and describes results for several different annotation problems, including some strategies for determining when the task is well specified and properly priced."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068255843"
                        ],
                        "name": "Christopher Hunt",
                        "slug": "Christopher-Hunt",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Hunt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Hunt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "In the first scheme, a homography between each captured frame and the overview image is computed based on the correspondences of SURF features [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 161878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c7cf406a47048730c1a08d46cb0166b16566524",
            "isKey": false,
            "numCitedBy": 6212,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this document, the SURF detector-descriptor scheme used in the OpenSURF library is discussed in detail. First the algorithm is analysed from a theoretical standpoint to provide a detailed overview of how and why it works. Next the design and development choices for the implementation of the library are discussed and justified. During the implementation of the library, it was found that some of the finer details of the algorithm had been omitted or overlooked, so Section 1.5 serves to make clear the concepts which are not explicitly defined in the SURF paper [1]."
            },
            "slug": "SURF:-Speeded-Up-Robust-Features-Hunt",
            "title": {
                "fragments": [],
                "text": "SURF: Speeded-Up Robust Features"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "In this document, the SURF detector-descriptor scheme used in the OpenSURF library is discussed in detail and the algorithm is analysed from a theoretical standpoint to provide a detailed overview of how and why it works."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3340410"
                        ],
                        "name": "Jaeseung Ko",
                        "slug": "Jaeseung-Ko",
                        "structuredName": {
                            "firstName": "Jaeseung",
                            "lastName": "Ko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaeseung Ko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145568138"
                        ],
                        "name": "Changick Kim",
                        "slug": "Changick-Kim",
                        "structuredName": {
                            "firstName": "Changick",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changick Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 216
                            }
                        ],
                        "text": "Blur is estimated by computing the mean and standard deviation of an image from its binary map and evaluating these values using a set of pre-built covariance matrices created from images known to be blurry or sharp [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42680732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a88daf6a79f5681fc7d3847aa7431b2adfb07610",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose low-cost blur detection and estimation method which can be applied to improving image display on mobile phones. We detect the blurred image by constructing the Bayes discriminant function with the statistics of the gradients of the input image. Once the blurred image is detected, we extract the main ripple components from the log spectrum of the blurred image and measure the angle and width of the main ripple components to estimate the angle and extent of the blur kernel. The experimental results are provided to show the performance of the proposed blur kernel estimation and image restoration algorithm."
            },
            "slug": "Low-cost-blur-image-detection-and-estimation-for-Ko-Kim",
            "title": {
                "fragments": [],
                "text": "Low cost blur image detection and estimation for mobile devices"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Low-cost blur detection and estimation method which can be applied to improving image display on mobile phones and the performance of the proposed blur kernel estimation and image restoration algorithm is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2009 11th International Conference on Advanced Communication Technology"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intel reader"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Photo-based question answering. MM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Voiceover: Macintosh OS X"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "KGB"
            },
            "venue": {
                "fragments": [],
                "text": "KGB"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Talking signs Testscout-your mobile reader"
            },
            "venue": {
                "fragments": [],
                "text": "Talking signs Testscout-your mobile reader"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Amazon Remembers Surf: Speeded up robust features"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of CVIU"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind with camera: Changing lives with photography"
            },
            "venue": {
                "fragments": [],
                "text": "Blind with camera: Changing lives with photography"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Google Goggles"
            },
            "venue": {
                "fragments": [],
                "text": "Google Goggles"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Macintosh OS X , 2007"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "The ESP Game was originally motivated (in part) by the desire to provide descriptions of web images for blind people [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Labeling images with a computer"
            },
            "venue": {
                "fragments": [],
                "text": "game. CHI 2004,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Testscoutyour mobile reader"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mobile speak screen readers. Code Factory"
            },
            "venue": {
                "fragments": [],
                "text": "Mobile speak screen readers. Code Factory"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Amazon Mechanical Turk Amazon Remembers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eyes-free. http://code.google.com/p/eyes-free"
            },
            "venue": {
                "fragments": [],
                "text": "Eyes-free. http://code.google.com/p/eyes-free"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "Other common remote services include relay services for deaf and hard of hearing people (which requires trained employees) [27], and the retroactive nearly real-time audio captioning by dedicated workers in Scribe4Me [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 16
                            }
                        ],
                        "text": "A user study of Scribe4Me found that participants felt waiting the required 3-5 minutes was too long because it \u201cleaves one as an observer rather than an active participant.\u201d"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluating visualizations of non-speech sounds for the deaf"
            },
            "venue": {
                "fragments": [],
                "text": "Behavior and Information Technology"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "16. kNFB reader. knfb Reading Technology"
            },
            "venue": {
                "fragments": [],
                "text": "16. kNFB reader. knfb Reading Technology"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intel reader. http://www.intel.com/healthcare/reader"
            },
            "venue": {
                "fragments": [],
                "text": "Intel reader. http://www.intel.com/healthcare/reader"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mobile speak screen readers. Code Factory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knocking live video. ustream"
            },
            "venue": {
                "fragments": [],
                "text": "Knocking live video. ustream"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 40,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/VizWiz:-nearly-real-time-answers-to-visual-Bigham-Jayant/8a960ea40fe7b32a1ee702a84f64ec1de5c3e7fe?sort=total-citations"
}