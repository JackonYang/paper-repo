{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709446"
                        ],
                        "name": "R. Rosipal",
                        "slug": "R.-Rosipal",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Rosipal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosipal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36599774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6becb2082773ee8efb2d93c2b44be763053d5134",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The proposal of considering nonlinear principal component analysis as a kernel eigenvalue problem has provided an extremely powerful method of extracting nonlinear features for a number of classification and regression applications. Whereas the utilization of Mercer kernels makes the problem of computing principal components in, possibly, infinite-dimensional feature spaces tractable, there are still the attendant numerical problems of diagonalizing large matrices. In this contribution, we propose an expectation-maximization approach for performing kernel principal component analysis and show this to be a computationally efficient method, especially when the number of data points is large."
            },
            "slug": "An-Expectation-Maximization-Approach-to-Nonlinear-Rosipal-Girolami",
            "title": {
                "fragments": [],
                "text": "An Expectation-Maximization Approach to Nonlinear Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work proposes an expectation-maximization approach for performing kernel principal component analysis and shows this to be a computationally efficient method, especially when the number of data points is large."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69577099"
                        ],
                        "name": "T. W. Anderson",
                        "slug": "T.-W.-Anderson",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. W. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 242
                            }
                        ],
                        "text": "\u2026case, it is simple to show that the observation covariance model WWT 2I can be made exact (assuming the correct choice of q), and both W and 2 may then be determined analytically through eigendecomposition of S, without resort to iteration (Anderson (1963) and Basilevsky (1994), pages 361\u00b1363)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1291,
                                "start": 252
                            }
                        ],
                        "text": "In that case, it is simple to show that the observation covariance model WW \u0087 (2)I can be made exact (assuming the correct choice of q), and both W and 2 may then be determined analytically through eigendecomposition of S, without resort to iteration (Anderson (1963) and Basilevsky (1994), pages 361\u00b1363). However, it is restrictive (and rarely justi\u00aeed in practice) to assume that either 2 is known or that the model of the second-order statistics of the data is exact. Indeed, in the presence of additive observation noise, an exact covariance model is generally undesirable. This is particularly true in the practical application of PCA, where we often do not require an exact characterization of the covariance structure in the minor subspace, since this information is e\u0080ectively `discarded' in the dimensionality reduction process. In the remainder of this paper we therefore focus on the case of most interest and consider the nature of the maximum likelihood estimators for W and 2 in the realistic case where the model covariance proposed is not equal to its sample counterpart, and where 2 must be estimated from the data (and so enters the likelihood function). This case has indeed been investigated, and related to PCA, in the early factor analysis literature by Lawley (1953) and by Anderson and Rubin (1956), although this work does not appear widely known."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 252
                            }
                        ],
                        "text": "In that case, it is simple to show that the observation covariance model WW \u0087 (2)I can be made exact (assuming the correct choice of q), and both W and 2 may then be determined analytically through eigendecomposition of S, without resort to iteration (Anderson (1963) and Basilevsky (1994), pages 361\u00b1363)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 70
                            }
                        ],
                        "text": "We have reiterated and extended the earlier work of Lawley (1953) and Anderson and Rubin (1956) and shown how PCA may be viewed as a maximum likelihood procedure based on a probability density model of the observed data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124296805,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dae4dd6eabc455e40236bd1a2f5fa410e2e069c2",
            "isKey": true,
            "numCitedBy": 1187,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The asymptotic distribution of the characteristic roots and (normalized) vectors of a sample covariance matrix is given when the observations are from a multivariate normal distribution whose covariance matrix has characteristic roots of arbitrary multiplicity. The elements of each characteristic vector are the coefficients of a principal component (with sum of squares of coefficients being unity), and the corresponding characteristic root is the variance of the principal component. Tests of hypotheses of equality of population roots are treated, and confidence intervals for assumed equal roots are given; these are useful in assessing the importance of principal components. A similar study for correlation matrices is considered. (Author)"
            },
            "slug": "ASYMPTOTIC-THEORY-FOR-PRINCIPAL-COMPONENT-ANALYSIS-Anderson",
            "title": {
                "fragments": [],
                "text": "ASYMPTOTIC THEORY FOR PRINCIPAL COMPONENT ANALYSIS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7882,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69495445"
                        ],
                        "name": "Dorothy T. Thayer",
                        "slug": "Dorothy-T.-Thayer",
                        "structuredName": {
                            "firstName": "Dorothy",
                            "lastName": "Thayer",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dorothy T. Thayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123437256,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e9222ee44916c976c80f11303002e850de0c63e",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for aq \u00d7q symmetric matrix whereq is the matrix of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation."
            },
            "slug": "EM-algorithms-for-ML-factor-analysis-Rubin-Thayer",
            "title": {
                "fragments": [],
                "text": "EM algorithms for ML factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144458696"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Kim",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1414411436"
                        ],
                        "name": "S. H. Park",
                        "slug": "S.-H.-Park",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Park",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. H. Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48206096"
                        ],
                        "name": "H. Kim",
                        "slug": "H.-Kim",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16960463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "461d22241ce2f8f714be37e229628fc715efdc4d",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel principal component analysis (PCA) has recently been proposed as a nonlinear extension of PCA. The basic idea is to first map the input space into a feature space via a nonlinear map and then compute the principal components in that feature space. This letter illustrates the potential of kernel PCA for texture classification. Accordingly, supervised texture classification was performed using kernel PCA for texture feature extraction. By adopting a polynomial kernel, the principal components were computed within the product space of the input pixels making up the texture patterns, thereby producing a good performance."
            },
            "slug": "Kernel-principal-component-analysis-for-texture-Kim-Park",
            "title": {
                "fragments": [],
                "text": "Kernel principal component analysis for texture classification"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152823751"
                        ],
                        "name": "A. Ruiz",
                        "slug": "A.-Ruiz",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Ruiz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ruiz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388396823"
                        ],
                        "name": "P. E. L\u00f3pez-de-Teruel",
                        "slug": "P.-E.-L\u00f3pez-de-Teruel",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "L\u00f3pez-de-Teruel",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. E. L\u00f3pez-de-Teruel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8803997,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c35436b8731ea9ce3971a6031d465076628a2a4",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "The eigenstructure of the second-order statistics of a multivariate random population can be inferred from the matrix of pairwise combinations of inner products of the samples. Therefore, it can be also efficiently obtained in the implicit, high-dimensional feature spaces defined by kernel functions. We elaborate on this property to obtain general expressions for immediate derivation of nonlinear counterparts of a number of standard pattern analysis algorithms, including principal component analysis, data compression and denoising, and Fisher's discriminant. The connection between kernel methods and nonparametric density estimation is also illustrated. Using these results we introduce the kernel version of Mahalanobis distance, which originates nonparametric models with unexpected and interesting properties, and also propose a kernel version of the minimum squared error (MSE) linear discriminant function. This learning machine is particularly simple and includes a number of generalized linear models such as the potential functions method or the radial basis function (RBF) network. Our results shed some light on the relative merit of feature spaces and inductive bias in the remarkable generalization properties of the support vector machine (SVM). Although in most situations the SVM obtains the lowest error rates, exhaustive experiments with synthetic and natural data show that simple kernel machines based on pseudoinversion are competitive in problems with appreciable class overlapping."
            },
            "slug": "Nonlinear-kernel-based-statistical-pattern-analysis-Ruiz-L\u00f3pez-de-Teruel",
            "title": {
                "fragments": [],
                "text": "Nonlinear kernel-based statistical pattern analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Kernel version of Mahalanobis distance is introduced, which originates nonparametric models with unexpected and interesting properties, and a kernel version of the minimum squared error (MSE) linear discriminant function is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "86970661"
                        ],
                        "name": "G. Young",
                        "slug": "G.-Young",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Young",
                            "middleNames": [
                                "Marion"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 75
                            }
                        ],
                        "text": "This approach was adopted in the early Young\u00b1Whittle factor analysis model (Young, 1940; Whittle, 1952), where in addition the residual variance 2 was presumed known (i.e. the model likelihood was a function of W alone)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 75
                            }
                        ],
                        "text": "This approach was adopted in the early Young-Whittle factor analysis model (Young 1940; Whittle 1952), where in addition, the residual variance 2 was presumed known (i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120941345,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4d73b11a0d141c616441ad5280e782578e8d3d9e",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Fisher's method of maximum likelihood is applied to the problem of estimation in factor analysis, as initiated by Lawley, and found to lead to a generalization of the Eckart matrix approximation problem. The solution of this in a special case is applied to show how test fallability enters into factor determination, it being noted that the method of communalities underestimates the number of factors."
            },
            "slug": "Maximum-likelihood-estimation-and-factor-analysis-Young",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation and factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1941
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109271982"
                        ],
                        "name": "Kil-Ung Kim",
                        "slug": "Kil-Ung-Kim",
                        "structuredName": {
                            "firstName": "Kil-Ung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kil-Ung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144268073"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "K",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107880232"
                        ],
                        "name": "Sung-Pa Park",
                        "slug": "Sung-Pa-Park",
                        "structuredName": {
                            "firstName": "Sung-Pa",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sung-Pa Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109564980"
                        ],
                        "name": "Hyerim Kim",
                        "slug": "Hyerim-Kim",
                        "structuredName": {
                            "firstName": "Hyerim",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyerim Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121972345,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95a6c4f917489f843bcbf44879af7c3af5ffa8ee",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel principal component analysis (PCA) is presented as a mechanism for extracting textural information. Using the polynomial kernel, higher order correlations of input pixels can be easily used as features for classification. As a result, supervised texture classification can be performed using a neural network."
            },
            "slug": "Texture-classification-with-kernel-principal-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture classification with kernel principal component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Kernel principal component analysis (PCA) is presented as a mechanism for extracting textural information using the polynomial kernel, and supervised texture classification can be performed using a neural network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34628173"
                        ],
                        "name": "K. Tsuda",
                        "slug": "K.-Tsuda",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Tsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tsuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5894296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fcbefeb0beae4470cf40df74cd116b1d4bdcae4",
            "isKey": false,
            "numCitedBy": 3519,
            "numCiting": 211,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis."
            },
            "slug": "An-introduction-to-kernel-based-learning-algorithms-M\u00fcller-Mika",
            "title": {
                "fragments": [],
                "text": "An introduction to kernel-based learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69577099"
                        ],
                        "name": "T. W. Anderson",
                        "slug": "T.-W.-Anderson",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. W. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145656106"
                        ],
                        "name": "H. Rubin",
                        "slug": "H.-Rubin",
                        "structuredName": {
                            "firstName": "Herman",
                            "lastName": "Rubin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 124
                            }
                        ],
                        "text": "This case has indeed been investigated, and related to PCA, in the early factor analysis literature by Lawley (1953) and by Anderson and Rubin (1956), although this work does not appear widely known."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 70
                            }
                        ],
                        "text": "We have reiterated and extended the earlier work of Lawley (1953) and Anderson and Rubin (1956) and shown how PCA may be viewed as a maximum likelihood procedure based on a probability density model of the observed data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26463186,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f3ce8002d531a5edc85bbdfcb7e1512a0275152",
            "isKey": false,
            "numCitedBy": 997,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we discuss some methods of factor analysis. The entire discussion is centered around one general probability model. We consider some mathematical problems of the model, such as whether certain kinds of observed data determine the model uniquely. We treat the statistical problems of estimation and tests of certain hypotheses. For these purposes the asymptotic distribution theory of some statistics is treated. The primary aim of this paper is to give a unified exposition of this part of factor analysis from the viewpoint of the mathematical statistician. The literature on factor analysis is scattered; moreover, the many papers and books have been written from many different points of view. By confining ourselves to one model and by emphasizing statistical inferences for this model we hope to present a clear picture to the statistician. The development given here is expected to point up features of model-building and statistical inference that occur in other areas where statistical theories are being developed. For example, nearly all of the problems met in factor analysis are met in latent structure analysis. There are also some new results given in this paper. The proofs of these are mainly given in a technical Part II of the paper. In confining.ourselves to the mathematical and statistical aspects of one model, we are leaving out of consideration many important and interesting topics. We shall not consider how useful this model may be nor in what substantive areas one may expect to find data (and problems) that fit the model. We also do not consider methods based on other models. In doing this, we do not mean to imply that the model considered here is the most useful or important. It seems that this model has some usefulness and importance, it has been studied considerably, and one can give a fairly unified exposition of it. Extensive discussion of the purposes and applications (as well as other developments) of factor analysis is given in books by psychologists (for example, Holzinger and Harmon [10], Thomson [23], Thurstone [24]). Some general discussion of statistical inference has been given in papers by Bartlett [9] and Kendall [12]."
            },
            "slug": "Statistical-Inference-in-Factor-Analysis-Anderson-Rubin",
            "title": {
                "fragments": [],
                "text": "Statistical Inference in Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper discusses some methods of factor analysis and considers some mathematical problems of the model, such as whether certain kinds of observed data determine the model uniquely, and treats the statistical problems of estimation and tests of certain hypotheses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094876634"
                        ],
                        "name": "R. Lotlikar",
                        "slug": "R.-Lotlikar",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Lotlikar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lotlikar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144174561"
                        ],
                        "name": "R. Kothari",
                        "slug": "R.-Kothari",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kothari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kothari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22978688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb26321549031a32d70f8faeff28caa340c0ce3a",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Dimensionality reduction is the process of mapping high-dimension patterns to a lower dimension subspace. When done prior to classification, estimates obtained in the lower dimension subspace are more reliable. For some classifiers, there is also an improvement in performance due to the removal of the diluting effect of redundant information. A majority of the present approaches to dimensionality reduction are based on scatter matrices or other statistics of the data which do not directly correlate to classification accuracy. The optimality criteria of choice for the purposes of classification is the Bayes error. Usually however, Bayes error is difficult to express analytically. We propose an optimality criteria based on an approximation of the Bayes error and use it to formulate a linear and a nonlinear method of dimensionality reduction. The nonlinear method we propose, relies on using a multilayered perceptron which produces as output the lower dimensional representation. It thus differs from autoassociative like multilayered perceptrons which have been proposed and used for dimensionality reduction. Our results show that the nonlinear method is, as anticipated, superior to the linear method in that it can perform unfolding of a nonlinear manifold. In addition, the nonlinear method we propose provides substantially better lower dimension representation (for classification purposes) than Fisher's linear discriminant (FLD) and two other nonlinear methods of dimensionality reduction that are often used."
            },
            "slug": "Bayes-optimality-motivated-linear-and-multilayered-Lotlikar-Kothari",
            "title": {
                "fragments": [],
                "text": "Bayes-optimality motivated linear and multilayered perceptron-based dimensionality reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that the nonlinear method is, as anticipated, superior to the linear method in that it can perform unfolding of a nonlinear manifold and provides substantially better lower dimension representation than Fisher's linear discriminant (FLD) and two other nonlinear methods of dimensionality reduction that are often used."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 287
                            }
                        ],
                        "text": "Because PCA defines a single linear projection and is thus a relatively simplistic technique, there has been significant recent interest in obtaining more complex projection methods by combining multiple PCA models, notably for image compression (Dony and Haykin 1995) and visualisation (Bishop and Tipping 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 267
                            }
                        ],
                        "text": "\u2026a single linear projection and is thus a relatively simplistic technique, there has been signi\u00aecant recent interest in obtaining more complex projection methods by combining multiple PCA models, notably for image compression (Dony and Haykin, 1995) and visualization (Bishop and Tipping, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2403387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de3ec11fe51e9abc6a6c1a639044ae5a70c43e72",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Visualization has proven to be a powerful and widely-applicable tool for the analysis and interpretation of multivariate data. Most visualization algorithms aim to find a projection from the data space down to a two-dimensional visualization space. However, for complex data sets living in a high-dimensional space, it is unlikely that a single two-dimensional projection can reveal all of the interesting structure. We therefore introduce a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and subclusters of data points visualized at deeper levels. The algorithm is based on a hierarchical mixture of latent variable models, whose parameters are estimated using the expectation-maximization algorithm. We demonstrate the principle of the approach on a toy data set, and we then apply the algorithm to the visualization of a synthetic data set in 12 dimensions obtained from a simulation of multiphase flows in oil pipelines, and to data in 36 dimensions derived from satellite images."
            },
            "slug": "A-Hierarchical-Latent-Variable-Model-for-Data-Bishop-Tipping",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Latent Variable Model for Data Visualization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and subclusters of data points visualization at deeper levels."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110576212"
                        ],
                        "name": "D. Wilson",
                        "slug": "D.-Wilson",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Wilson",
                            "middleNames": [
                                "J.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751846"
                        ],
                        "name": "G. Irwin",
                        "slug": "G.-Irwin",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Irwin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Irwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752416"
                        ],
                        "name": "G. Lightbody",
                        "slug": "G.-Lightbody",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Lightbody",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lightbody"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2109380,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1cadd70479463bcccef3ab118a33e241ec9a5b60",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel means for creating a nonlinear extension of principal component analysis (PCA) using radial basis function (RBF) networks. This algorithm comprises two distinct stages: projection and self-consistency. The projection stage contains a single network, trained to project data from a high- to a low-dimensional space. Training requires solution of a generalized eigenvector equation. The second stage, trained using a novel hybrid nonlinear optimization algorithm, then performs the inverse transformation. Issues relating to the practical implementation of the procedure are discussed, and the algorithm is demonstrated on a nonlinear test problem. An example of the application of the algorithm to data from a benchmark simulation of an industrial overheads condenser and reflux drum rig is also included. This shows the usefulness of the procedure in detecting and isolating both sensor and process faults. Pointers for future research in this area are also given."
            },
            "slug": "RBF-principal-manifolds-for-process-monitoring-Wilson-Irwin",
            "title": {
                "fragments": [],
                "text": "RBF principal manifolds for process monitoring"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper describes a novel means for creating a nonlinear extension of principal component analysis (PCA) using radial basis function (RBF) networks, which comprises two distinct stages: projection and self-consistency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172776"
                        ],
                        "name": "G. Baudat",
                        "slug": "G.-Baudat",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Baudat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baudat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1989127"
                        ],
                        "name": "F. Anouar",
                        "slug": "F.-Anouar",
                        "structuredName": {
                            "firstName": "Fatiha",
                            "lastName": "Anouar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Anouar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7036341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "994e91efb53a4a6b04a562ec10751cd0bbcdeac5",
            "isKey": false,
            "numCitedBy": 1727,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method that we call generalized discriminant analysis (GDA) to deal with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. In the transformed space, linear properties make it easy to extend and generalize the classical linear discriminant analysis (LDA) to nonlinear discriminant analysis. The formulation is expressed as an eigenvalue problem resolution. Using a different kernel, one can cover a wide class of nonlinearities. For both simulated data and alternate kernels, we give classification results, as well as the shape of the decision function. The results are confirmed using real data to perform seed classification."
            },
            "slug": "Generalized-Discriminant-Analysis-Using-a-Kernel-Baudat-Anouar",
            "title": {
                "fragments": [],
                "text": "Generalized Discriminant Analysis Using a Kernel Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new method that is close to the support vector machines insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space to deal with nonlinear discriminant analysis using kernel function operator."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111170937"
                        ],
                        "name": "Martin Brown",
                        "slug": "Martin-Brown",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3299838"
                        ],
                        "name": "H. Lewis",
                        "slug": "H.-Lewis",
                        "structuredName": {
                            "firstName": "Hugh",
                            "lastName": "Lewis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2317393"
                        ],
                        "name": "S. Gunn",
                        "slug": "S.-Gunn",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Gunn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gunn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16520624,
            "fieldsOfStudy": [
                "Environmental Science",
                "Computer Science"
            ],
            "id": "94a1092bd202dda5f014cfa0e5afce959bb0d69f",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Mixture modeling is becoming an increasingly important tool in the remote sensing community as researchers attempt to resolve subpixel, area information. This paper compares a well-established technique, linear spectral mixture models (LSMM), with a much newer idea based on data selection, support vector machines (SVM). It is shown that the constrained least squares LSMM is equivalent to the linear SVM, which relies on proving that the LSMM algorithm possesses the \"maximum margin\" property. This in turn shows that the LSMM algorithm can be derived from the same optimality conditions as the linear SVM, which provides important insights about the role of the bias term and rank deficiency in the pure pixel matrix within the LSMM algorithm. It also highlights one of the main advantages for using the linear SVM algorithm in that it performs automatic \"pure pixel\" selection from a much larger database. In addition, extensions to the basic SVM algorithm allow the technique to be applied to data sets that exhibit spectral confusion (overlapping sets of pure pixels) and to data sets that have nonlinear mixture regions. Several illustrative examples, based on an area-labeled Landsat dataset, are used to demonstrate the potential of this approach."
            },
            "slug": "Linear-spectral-mixture-models-and-support-vector-Brown-Lewis",
            "title": {
                "fragments": [],
                "text": "Linear spectral mixture models and support vector machines for remote sensing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the constrained least squares LSMM is equivalent to the linear SVM, which relies on proving that the LSMM algorithm possesses the \"maximum margin\" property, which provides important insights about the role of the bias term and rank deficiency in the pure pixel matrix within the LS MM algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Geosci. Remote. Sens."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6463600"
                        ],
                        "name": "G. Dunteman",
                        "slug": "G.-Dunteman",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dunteman",
                            "middleNames": [
                                "Henry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dunteman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60013557,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "bc3d9f84a9e6550ec95af565e086b437b8b2822e",
            "isKey": false,
            "numCitedBy": 853,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Picking up the theme of a companion volume, Introduction to Linear Models, Dr Dunteman extends his clear exposition to the case of multiple dependent variables. He clarifies advanced concepts for both students and researchers in an intuitive, non-rigorous manner, and every technique is illustrated (step-by-step) on small, hypothetical, yet meaningful social science data bases. The reader who works through both of Dunteman's texts will have built a basic understanding of statistical model-building and a strong foundation for advanced studies."
            },
            "slug": "Introduction-To-Multivariate-Analysis-Dunteman",
            "title": {
                "fragments": [],
                "text": "Introduction To Multivariate Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Dr Dunteman clarifies advanced concepts for both students and researchers in an intuitive, non-rigorous manner, and every technique is illustrated (step-by-step) on small, hypothetical, yet meaningful social science data bases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34628173"
                        ],
                        "name": "K. Tsuda",
                        "slug": "K.-Tsuda",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Tsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tsuda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40164596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddbd78be51b18162bf719be836f16c54a9aa20a3",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The subspace method has usually been applied to a multidimensional space (i.e., feature space) which uses features as its basis. A subspace method can also be applied to a functional space, since the subspace can be defined by an arbitrary linear space. This paper proposes the mapping of a feature space onto the Hilbert subspace so that pattern recognition can be performed by the subspace method. The proposed method has been experimentally applied to the recognition of the Japanese hiragana syllabary and has achieved a higher recognition rate than the conventional method. This improvement is based on the fact that the dimension of the common part in all of the classes of the subspaces in the Hilbert function space becomes zero. \u00a9 2001 Scripta Technica, Syst Comp Jpn, 32(6): 55\u201361, 2001"
            },
            "slug": "The-subspace-method-in-Hilbert-space-Tsuda",
            "title": {
                "fragments": [],
                "text": "The subspace method in Hilbert space"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The mapping of a feature space onto the Hilbert subspace so that pattern recognition can be performed by the subspace method has been experimentally applied to the recognition of the Japanese hiragana syllabary and has achieved a higher recognition rate than the conventional method."
            },
            "venue": {
                "fragments": [],
                "text": "Systems and Computers in Japan"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The general question that function k does correspond to a dot product in some space F has been discussed by Boser et al. (1992) and Vapnik (1995): Mercer\u2019s theorem of functional analysis implies that if k is a continuous kernel of a positive integral operator, there exists a mapping into a space where k acts as a dot product (for details, see appendix C)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This method was used by Boser et al. (1992) to extend the Generalized Portrait hyperplane classifier of Vapnik and Chervonenkis (1974) to nonlinear support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10843,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803921"
                        ],
                        "name": "S. Lipovetsky",
                        "slug": "S.-Lipovetsky",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Lipovetsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lipovetsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 54
                            }
                        ],
                        "text": "Perhaps the most common such model is factor analysis (Bartholomew 1987; Basilevsky 1994) where the relationship is linear: t Wx (1) The d q matrix W relates the two sets of variables, while the parameter vector permits the model to have non-zero mean."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 55
                            }
                        ],
                        "text": "Perhaps the most common such model is factor analysis (Bartholomew, 1987; Basilevsky, 1994) where the relationship is linear:\nt Wx : 1 The d qmatrix W relates the two sets of variables, while the parameter vector permits the model to have non-zero mean."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33849266,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "dc45f718f4345396933c7066f82247eb400b970c",
            "isKey": false,
            "numCitedBy": 923,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "representation of the Volterra model. One of the main issues addressed in Chapter 6 is the criterion by which local models are selected. These models will be designated as input, output, or general selected. The author, in this chapter, also presents similarities and dissimilarities between input-selected multimodels and NMAX, output-selected and NARX, and general selected and NARMAX. Chapter 7 focuses on the relationships that exist between these different model classes. Finally, Chapter 8 concludes by emphasizing the four steps\u2014selection, physical phenomenon, goodness of \u008e t, and assessment of the validity of the model. Overall, the author\u2019s presentation is insightful and consistent. It is well written and nicely organized. The book\u2019s strength is its verbal explanation of the mathematical de\u008e nitions and results; intuitive discussions and applications from various physical \u008e elds are frequently supplied. From the very beginning of the book, the author goes to a great length to explain the material to ensure that the reader understands the differences among the models discussed. The book\u2019s appeal is that it illustrates a wide range of results for many kinds of models that appear in stochastic processes and time series literature. It provides general insights on the nonlinear model classes that have been discussed in the modeling and control literature. Finally, this book is an important addition in the area of nonlinear time series, and it has much to offer that is hard to \u008e nd elsewhere."
            },
            "slug": "Latent-Variable-Models-and-Factor-Analysis-Lipovetsky",
            "title": {
                "fragments": [],
                "text": "Latent Variable Models and Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The book\u2019s appeal is that it illustrates a wide range of results for many kinds of models that appear in stochastic processes and time series literature and provides general insights on the nonlinear model classes that have been discussed in the modeling and control literature."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580190"
                        ],
                        "name": "U. Makov",
                        "slug": "U.-Makov",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Makov",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Makov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124992180,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures."
            },
            "slug": "Statistical-analysis-of-finite-mixture-Titterington-Smith",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of finite mixture distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This course discusses Mathematical Aspects of Mixtures, Sequential Problems and Procedures, and Applications of Finite Mixture Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738575"
                        ],
                        "name": "T. V. Gestel",
                        "slug": "T.-V.-Gestel",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Gestel",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. V. Gestel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744439"
                        ],
                        "name": "J. Suykens",
                        "slug": "J.-Suykens",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Suykens",
                            "middleNames": [
                                "A.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suykens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360480"
                        ],
                        "name": "Dirk-Emma Baestaens",
                        "slug": "Dirk-Emma-Baestaens",
                        "structuredName": {
                            "firstName": "Dirk-Emma",
                            "lastName": "Baestaens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dirk-Emma Baestaens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46821184"
                        ],
                        "name": "A. Lambrechts",
                        "slug": "A.-Lambrechts",
                        "structuredName": {
                            "firstName": "Annemie",
                            "lastName": "Lambrechts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lambrechts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2478377"
                        ],
                        "name": "Bruno Vandaele",
                        "slug": "Bruno-Vandaele",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Vandaele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bruno Vandaele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750713"
                        ],
                        "name": "B. Moor",
                        "slug": "B.-Moor",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Moor",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704135"
                        ],
                        "name": "J. Vandewalle",
                        "slug": "J.-Vandewalle",
                        "structuredName": {
                            "firstName": "Joos",
                            "lastName": "Vandewalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vandewalle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3061177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "785eb1a41b9ed1901d155a26a0ba97a0a89826ed",
            "isKey": false,
            "numCitedBy": 498,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian evidence framework is applied in this paper to least squares support vector machine (LS-SVM) regression in order to infer nonlinear models for predicting a financial time series and the related volatility. On the first level of inference, a statistical framework is related to the LS-SVM formulation which allows one to include the time-varying volatility of the market by an appropriate choice of several hyper-parameters. The hyper-parameters of the model are inferred on the second level of inference. The inferred hyper-parameters, related to the volatility, are used to construct a volatility model within the evidence framework. Model comparison is performed on the third level of inference in order to automatically tune the parameters of the kernel function and to select the relevant inputs. The LS-SVM formulation allows one to derive analytic expressions in the feature space and practical expressions are obtained in the dual space replacing the inner product by the related kernel function using Mercer's theorem. The one step ahead prediction performances obtained on the prediction of the weekly 90-day T-bill rate and the daily DAX30 closing prices show that significant out of sample sign predictions can be made with respect to the Pesaran-Timmerman test statistic."
            },
            "slug": "Financial-time-series-prediction-using-least-vector-Gestel-Suykens",
            "title": {
                "fragments": [],
                "text": "Financial time series prediction using least squares support vector machines within the evidence framework"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The one step ahead prediction performances obtained on the prediction of the weekly 90-day T-bill rate and the daily DAX30 closing prices show that significant out of sample sign predictions can be made with respect to the Pesaran-Timmerman test statistic."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This can be done by a technique proposed by Burges (1996) in the context of support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52810328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1061ff8a216a8d00f5f189d7ea593c6f0703b771",
            "isKey": false,
            "numCitedBy": 511,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Support Vector Machine SVM is a uni versal learning machine whose decision sur face is parameterized by a set of support vec tors and by a set of corresponding weights An SVM is also characterized by a kernel function Choice of the kernel determines whether the resulting SVM is a polynomial classi er a two layer neural network a ra dial basis function machine or some other learning machine SVMs are currently considerably slower in test phase than other approaches with sim ilar generalization performance To address this we present a general method to signif icantly decrease the complexity of the deci sion rule obtained using an SVM The pro posed method computes an approximation to the decision rule in terms of a reduced set of vectors These reduced set vectors are not support vectors and can in some cases be computed analytically We give ex perimental results for three pattern recogni tion problems The results show that the method can decrease the computational com plexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods Fur ther the method allows the generalization performance complexity trade o to be di rectly controlled The proposed method is not speci c to pattern recognition and can be applied to any problem where the Sup port Vector algorithm is used for example regression INTRODUCTION SUPPORT VECTOR MACHINES Consider a two class classi er for which the decision rule takes the form"
            },
            "slug": "Simplified-Support-Vector-Decision-Rules-Burges",
            "title": {
                "fragments": [],
                "text": "Simplified Support Vector Decision Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show that the method can decrease the computational complexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 192934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3626f388371b678b2f02f6eefc44fa5abc53ceb3",
            "isKey": false,
            "numCitedBy": 6535,
            "numCiting": 473,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field."
            },
            "slug": "Statistical-Pattern-Recognition:-A-Review-Jain-Duin",
            "title": {
                "fragments": [],
                "text": "Statistical Pattern Recognition: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399027888"
                        ],
                        "name": "\u00c1. Navia-V\u00e1zquez",
                        "slug": "\u00c1.-Navia-V\u00e1zquez",
                        "structuredName": {
                            "firstName": "\u00c1ngel",
                            "lastName": "Navia-V\u00e1zquez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1. Navia-V\u00e1zquez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388508441"
                        ],
                        "name": "F. P\u00e9rez-Cruz",
                        "slug": "F.-P\u00e9rez-Cruz",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "P\u00e9rez-Cruz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. P\u00e9rez-Cruz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398903195"
                        ],
                        "name": "Antonio Art\u00e9s-Rodr\u00edguez",
                        "slug": "Antonio-Art\u00e9s-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Art\u00e9s-Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Art\u00e9s-Rodr\u00edguez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398493114"
                        ],
                        "name": "A. Figueiras-Vidal",
                        "slug": "A.-Figueiras-Vidal",
                        "structuredName": {
                            "firstName": "An\u00edbal",
                            "lastName": "Figueiras-Vidal",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Figueiras-Vidal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27161171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46f684fbb5f82d109f23ab621a090a8cdd76bc21",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "An iterative block training method for support vector classifiers (SVCs) based on weighted least squares (WLS) optimization is presented. The algorithm, which minimizes structural risk in the primal space, is applicable to both linear and nonlinear machines. In some nonlinear cases, it is necessary to previously find a projection of data onto an intermediate-dimensional space by means of either principal component analysis or clustering techniques. The proposed approach yields very compact machines, the complexity reduction with respect to the SVC solution is especially notable in problems with highly overlapped classes. Furthermore, the formulation in terms of WLS minimization makes the development of adaptive SVCs straightforward, opening up new fields of application for this type of model, mainly online processing of large amounts of (static/stationary) data, as well as online update in nonstationary scenarios (adaptive solutions). The performance of this new type of algorithm is analyzed by means of several simulations."
            },
            "slug": "Weighted-least-squares-training-of-support-vector-Navia-V\u00e1zquez-P\u00e9rez-Cruz",
            "title": {
                "fragments": [],
                "text": "Weighted least squares training of support vector classifiers leading to compact and adaptive schemes"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "An iterative block training method for support vector classifiers (SVCs) based on weighted least squares (WLS) optimization is presented, which minimizes structural risk in the primal space and yields very compact machines."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 777816,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ee177aacf6b3697d079579ce558cdb2ee58cee39",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive new bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers. The proofs make use of a viewpoint that is apparently novel in the field of statistical learning theory. The hypothesis class is described in terms of a linear operator mapping from a possibly infinite-dimensional unit ball in feature space into a finite-dimensional space. The covering numbers of the class are then determined via the entropy numbers of the operator. These numbers, which characterize the degree of compactness of the operator can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine. As a consequence, we are able to theoretically explain the effect of the choice of kernel function on the generalization performance of support vector machines."
            },
            "slug": "Generalization-performance-of-regularization-and-of-Williamson-Smola",
            "title": {
                "fragments": [],
                "text": "Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "New bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks, are derived by obtaining new bounds on their covering numbers by using the eigenvalues of an integral operator induced by the kernel function used by the machine."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125657"
                        ],
                        "name": "Phil Knirsch",
                        "slug": "Phil-Knirsch",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Knirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phil Knirsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14669541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a3e0028d99439ce2741d0e147b6e9a34bc4267",
            "isKey": false,
            "numCitedBy": 1231,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector (SV) kernel functions. We first discuss the geometry of feature space. In particular, we review what is known about the shape of the image of input space under the feature space map, and how this influences the capacity of SV methods. Following this, we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel, using the example of the class of inhomogeneous polynomial kernels, which are often used in SV pattern recognition. We then discuss the connection between feature space and input space by dealing with the question of how one can, given some vector in feature space, find a preimage (exact or approximate) in input space. We describe algorithms to tackle this issue, and show their utility in two applications of kernel methods. First, we use it to reduce the computational complexity of SV decision functions; second, we combine it with the Kernel PCA algorithm, thereby constructing a nonlinear statistical denoising technique which is shown to perform well on real-world data."
            },
            "slug": "Input-space-versus-feature-space-in-kernel-based-Sch\u00f6lkopf-Mika",
            "title": {
                "fragments": [],
                "text": "Input space versus feature space in kernel-based methods"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The geometry of feature space is reviewed, and the connection between feature space and input space is discussed by dealing with the question of how one can, given some vector in feature space, find a preimage in input space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2387242"
                        ],
                        "name": "G. W\u00fcbbeler",
                        "slug": "G.-W\u00fcbbeler",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "W\u00fcbbeler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. W\u00fcbbeler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522238"
                        ],
                        "name": "A. Ziehe",
                        "slug": "A.-Ziehe",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ziehe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ziehe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144100806"
                        ],
                        "name": "B. Mackert",
                        "slug": "B.-Mackert",
                        "structuredName": {
                            "firstName": "Bruno-Marcel",
                            "lastName": "Mackert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mackert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32373270"
                        ],
                        "name": "L. Trahms",
                        "slug": "L.-Trahms",
                        "structuredName": {
                            "firstName": "Lutz",
                            "lastName": "Trahms",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Trahms"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073253"
                        ],
                        "name": "G. Curio",
                        "slug": "G.-Curio",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Curio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Curio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1935590,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d6a61aa44bd6cb28f180a768b7589888b0132cf5",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply a recently developed multivariate statistical data analysis technique-so called blind source separation (BSS) by independent component analysis-to process magnetoencephalogram recordings of near-DC fields. The extraction of near-DC fields from MEG recordings has great relevance for medical applications since slowly varying DC-phenomena have been found, e.g., in cerebral anoxia and spreading depression in animals. Comparing several BSS approaches, it turns out that an algorithm based on temporal decorrelation successfully extracted a DC-component which was induced in the auditory cortex by presentation of music. The task is challenging because of the limited amount of available data and the corruption by outliers, which makes it an interesting real-world testbed for studying the robustness of ICA methods."
            },
            "slug": "Independent-component-analysis-of-noninvasively-in-W\u00fcbbeler-Ziehe",
            "title": {
                "fragments": [],
                "text": "Independent component analysis of noninvasively recorded cortical magnetic DC-fields in humans"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It turns out that an algorithm based on temporal decorrelation successfully extracted a DC-component which was induced in the auditory cortex by presentation of music, which is an interesting real-world testbed for studying the robustness of ICA methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Biomedical Engineering"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 36
                            }
                        ],
                        "text": "Initiated by the pioneering work of Oja (1982), a number of unsupervised neural network algorithms computing principal components have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16577977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e00dd12caea7c4dab1633a35d1da3cb2e76b420",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence."
            },
            "slug": "Simplified-neuron-model-as-a-principal-component-Oja",
            "title": {
                "fragments": [],
                "text": "Simplified neuron model as a principal component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 102
                            }
                        ],
                        "text": "However, in contrast with factor analysis, maximum likelihood estimators for W and 2 may be obtained explicitly, as we see shortly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15339,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8783809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56efe7bf4bd52a6369d9ebbe55033e81e716f7d0",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition."
            },
            "slug": "Surface-Learning-with-Applications-to-Lipreading-Bregler-Omohundro",
            "title": {
                "fragments": [],
                "text": "Surface Learning with Applications to Lipreading"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data and shows its utility in learning the space of lip images in a system for improving speech recognition by lip reading."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850667"
                        ],
                        "name": "R. Dony",
                        "slug": "R.-Dony",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Dony",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24907373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8b5565910ce2590f0e5408957bcc3967c6b90d4",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The optimal linear block transform for coding images is well known to be the Karhunen-Loeve transformation (KLT). However, the assumption of stationarity in the optimality condition is far from valid for images. Images are composed of regions whose local statistics may vary widely across an image. While the use of adaptation can result in improved performance, there has been little investigation into the optimality of the criterion upon which the adaptation is based. In this paper we propose a new transform coding method in which the adaptation is optimal. The system is modular, consisting of a number of modules corresponding to different classes of the input data. Each module consists of a linear transformation, whose bases are calculated during an initial training period. The appropriate class for a given input vector is determined by the subspace classifier. The performance of the resulting adaptive system is shown to be superior to that of the optimal nonadaptive linear transformation. This method can also be used as a segmentor. The segmentation it performs is independent of variations in illumination. In addition, the resulting class representations are analogous to the arrangement of the directionally sensitive columns in the visual cortex."
            },
            "slug": "Optimally-adaptive-transform-coding-Dony-Haykin",
            "title": {
                "fragments": [],
                "text": "Optimally adaptive transform coding"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new transform coding method in which the adaptation is optimal is proposed, consisting of a number of modules corresponding to different classes of the input data, and the performance of the resulting adaptive system is shown to be superior to that of the optimal nonadaptive linear transformation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To develop this technique, we made use of a kernel method so far used only in supervised learning (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For instance (Vapnik, 1995), if x = (x1, x2), then C2(x) = (x1, x2, x1x2, x2x1), or, yielding the same value of the dot product,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although this general fact was known (Burges, private communication), the machine learning community has made little use of it, the exception being support vector machines (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38757,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 570648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d75a5fe9e1b6511c5135d68e9ce8c0da5a7374",
            "isKey": false,
            "numCitedBy": 2853,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion. This results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix, without increasing the complexity of the calculation. The resulting approximation of faces projected from outside of the data set onto this optimal basis is improved on average. >"
            },
            "slug": "Application-of-the-Karhunen-Loeve-Procedure-for-the-Kirby-Sirovich",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Loeve Procedure for the Characterization of Human Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion, which results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 502,
                                "start": 193
                            }
                        ],
                        "text": "Drawing on the standard methodology for maximizing the likelihood of a Gaussian model in the presence of missing values (Little and Rubin, 1987) and the EM algorithm for PPCA given in Appendix B, we may derive an iterative algorithm for maximum likelihood estimation of the principal axes, where both the latent variables fxng and the missing observations ftnjg make up the `complete' data. Fig. 1(a) shows a projection of 38 examples from the 18-dimensional Tobamovirus data utilized by Ripley (1996), p."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 44
                            }
                        ],
                        "text": "by using the EM algorithm given in Appendix B, which is based on the algorithm for standard factor analysis of Rubin and Thayer (1982). However, in contrast with factor analysis, maximum likelihood estimators for W and 2 may be obtained explicitly, as we see shortly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4042,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49688742"
                        ],
                        "name": "K. Jerger",
                        "slug": "K.-Jerger",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Jerger",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jerger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762805"
                        ],
                        "name": "T. Netoff",
                        "slug": "T.-Netoff",
                        "structuredName": {
                            "firstName": "Th\u00e9oden",
                            "lastName": "Netoff",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Netoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32645462"
                        ],
                        "name": "J. Francis",
                        "slug": "J.-Francis",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Francis",
                            "middleNames": [
                                "Thachil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Francis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101591448"
                        ],
                        "name": "T. Sauer",
                        "slug": "T.-Sauer",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Sauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39646657"
                        ],
                        "name": "L. Pecora",
                        "slug": "L.-Pecora",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Pecora",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pecora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5943015"
                        ],
                        "name": "S. Weinstein",
                        "slug": "S.-Weinstein",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Weinstein",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weinstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766099"
                        ],
                        "name": "S. Schiff",
                        "slug": "S.-Schiff",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Schiff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schiff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5998777,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "c6fe304e828f7c99356727d9ad3f3e0663982c2c",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary: For patients with medically intractable epilepsy, there have been few effective alternatives to resective surgery, a destructive, irreversible treatment. A strategy receiving increased attention is using interictal spike patterns and continuous EEG measurements from epileptic patients to predict and ultimately control seizure activity via chemical or electrical control systems. This work compares results of seven linear and nonlinear methods (analysis of power spectra, cross\u2010correlation, principal components, phase, wavelets, correlation integral, and mutual prediction) in detecting the earliest dynamical changes preceding 12 intracranially\u2010recorded seizures from 4 patients. A method of counting standard deviations was used to compare across methods, and the earliest departures from thresholds determined from non\u2010seizure EEG were compared to a neurologist's judgement. For these data, the nonlinear methods offered no predictive advantage over the linear methods. All the methods described here were successful in detecting changes leading to a seizure between one and two minutes before the first changes noted by the neurologist, although analysis of phase correlation proved the most robust. The success of phase analysis may be due in part to its complete insensitivity to amplitude, which may provide a significant source of error."
            },
            "slug": "Early-Seizure-Detection-Jerger-Netoff",
            "title": {
                "fragments": [],
                "text": "Early Seizure Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "All the methods described here were successful in detecting changes leading to a seizure between one and two minutes before the first changes noted by the neurologist, although analysis of phase correlation proved the most robust."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of clinical neurophysiology : official publication of the American Electroencephalographic Society"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206447772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "455d9a4ff96561d543acbcb2aa81d6cd8fcd20df",
            "isKey": false,
            "numCitedBy": 2522,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently."
            },
            "slug": "Trends-&-Controversies:-Support-Vector-Machines-Hearst",
            "title": {
                "fragments": [],
                "text": "Trends & Controversies: Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This issue's collection of essays should help familiarize readers with this interesting new racehorse in the Machine Learning stable, and give a practical guide and a new technique for implementing the algorithm efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intell. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941532"
                        ],
                        "name": "E. Bagarinao",
                        "slug": "E.-Bagarinao",
                        "structuredName": {
                            "firstName": "Epifanio",
                            "lastName": "Bagarinao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bagarinao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702940"
                        ],
                        "name": "K. Pakdaman",
                        "slug": "K.-Pakdaman",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Pakdaman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pakdaman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685177"
                        ],
                        "name": "T. Nomura",
                        "slug": "T.-Nomura",
                        "structuredName": {
                            "firstName": "Taishin",
                            "lastName": "Nomura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nomura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107826793"
                        ],
                        "name": "S. Sato",
                        "slug": "S.-Sato",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23152985,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "cdf6aef34cc3f0e779d44811ff9b2cb5eae87398",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a formalism for the reconstruction of bifurcation diagrams from noisy time series. The method consists in finding a parametrized predictor function whose bifurcation structure is similar to that of the given system. The reconstruction algorithm is composed of two stages: model selection and bifurcation parameter identification. In the first stage, an appropriate model that best represents all the given time series is selected. A nonlinear autoregressive model with polynomial terms is employed in this study. The identification of the bifurcation parameters from among the many model parameters is done in the second stage. The algorithm works well even for a limited number of time series."
            },
            "slug": "Reconstructing-bifurcation-diagrams-from-noisy-time-Bagarinao-Pakdaman",
            "title": {
                "fragments": [],
                "text": "Reconstructing bifurcation diagrams from noisy time series using nonlinear autoregressive models."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A nonlinear autoregressive model with polynomial terms is employed in this study, and it is shown that this model works well even for a limited number of time series."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1432639666"
                        ],
                        "name": "Karl Pearson F.R.S.",
                        "slug": "Karl-Pearson-F.R.S.",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "F.R.S.",
                            "middleNames": [
                                "Pearson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl Pearson F.R.S."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125037489,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "cac33f91e59f0a137b46176d74cee55c7010c3f8",
            "isKey": false,
            "numCitedBy": 9520,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "(1901). LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science: Vol. 2, No. 11, pp. 559-572."
            },
            "slug": "LIII.-On-lines-and-planes-of-closest-fit-to-systems-F.R.S.",
            "title": {
                "fragments": [],
                "text": "LIII. On lines and planes of closest fit to systems of points in space"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper is concerned with the construction of planes of closest fit to systems of points in space and the relationships between these planes and the planes themselves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1901
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 134
                            }
                        ],
                        "text": "The most common derivation of PCA is in terms of a standardised linear projection which maximises the variance in the projected space (Hotelling 1933)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "The most common derivation of PCA is in terms of a standardized linear projection which maximizes the variance in the projected space (Hotelling, 1933)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144828484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9ebb5c0d6d54707a4d6181a693b6f755ec8a45a9",
            "isKey": false,
            "numCitedBy": 8491,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-a-complex-of-statistical-variables-into-Hotelling",
            "title": {
                "fragments": [],
                "text": "Analysis of a complex of statistical variables into principal components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522238"
                        ],
                        "name": "A. Ziehe",
                        "slug": "A.-Ziehe",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ziehe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ziehe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145218329"
                        ],
                        "name": "G. Nolte",
                        "slug": "G.-Nolte",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Nolte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nolte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144100806"
                        ],
                        "name": "B. Mackert",
                        "slug": "B.-Mackert",
                        "structuredName": {
                            "firstName": "Bruno-Marcel",
                            "lastName": "Mackert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mackert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073253"
                        ],
                        "name": "G. Curio",
                        "slug": "G.-Curio",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Curio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Curio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 286706,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "66c14d8a53dbebb7b56abc6b4669dcb1df01a78d",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Artifacts in magnetoneurography data due to endogenous biological noise sources, like the cardiac signal, can be four orders of magnitude higher than the signal of interest. Therefore, it is important to establish effective artifact reduction methods. We propose a blind source separation algorithm using only second-order temporal correlations for cleaning biomagnetic measurements of evoked responses in the peripheral nervous system. The algorithm showed its efficiency by eliminating disturbances originating from biological and technical noise sources and successfully extracting the signal of interest. This yields a significant improvement of the neuro-magnetic source analysis."
            },
            "slug": "Artifact-reduction-in-magnetoneurography-based-on-Ziehe-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Artifact reduction in magnetoneurography based on time-delayed second-order correlations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A blind source separation algorithm using only second-order temporal correlations for cleaning biomagnetic measurements of evoked responses in the peripheral nervous system is proposed and shows its efficiency by eliminating disturbances originating from biological and technical noise sources and successfully extracting the signal of interest."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Biomed. Eng."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 69
                            }
                        ],
                        "text": "However, in contrast with factor analysis, maximum likelihood estimators for W and 2 may be obtained explicitly, as we see shortly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18694007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355af3c3adbb17d25f0d2a4193e3daadffc0d4e8",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 144,
            "paperAbstract": {
                "fragments": [],
                "text": "\u201cWhat being walks sometimes on two feet, sometimes on three, and sometimes on four, and is weakest when it has the most?\u201d \u2014The Sphinx's Riddle"
            },
            "slug": "Pattern-recognition:-Historical-perspective-and-Rosenfeld-Wechsler",
            "title": {
                "fragments": [],
                "text": "Pattern recognition: Historical perspective and future directions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The sections of this paper deal with the categorization and functional approximation problems; the four components of a pattern recognition system; and trends in predictive learning, feature selection using \u201cnatural\u201d bases, and the use of mixtures of experts in classification."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Imaging Syst. Technol."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5992254"
                        ],
                        "name": "A. Basilevsky",
                        "slug": "A.-Basilevsky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Basilevsky",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Basilevsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 262
                            }
                        ],
                        "text": "\u2026case, it is simple to show that the observation covariance model WWT 2I can be made exact (assuming the correct choice of q), and both W and 2 may then be determined analytically through eigendecomposition of S, without resort to iteration (Anderson (1963) and Basilevsky (1994), pages 361\u00b1363)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 54
                            }
                        ],
                        "text": "Perhaps the most common such model is factor analysis (Bartholomew 1987; Basilevsky 1994) where the relationship is linear: t Wx (1) The d q matrix W relates the two sets of variables, while the parameter vector permits the model to have non-zero mean."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Perhaps the most common such model is factor analysis (Bartholomew, 1987; Basilevsky, 1994) where the relationship is linear:\nt Wx : 1 The d qmatrix W relates the two sets of variables, while the parameter vector permits the model to have non-zero mean."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59689658,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "7ea51b80e91af5f09f3fa8e610d34b061886c3ed",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A solar panel including photovoltaic cells encapsulated in a silicone resin, in which the base member to which the silicone resin adheres is a glass mat polyester in laminate or molded form."
            },
            "slug": "Statistical-Factor-Analysis-and-Related-Methods-Basilevsky",
            "title": {
                "fragments": [],
                "text": "Statistical Factor Analysis and Related Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A solar panel including photovoltaic cells encapsulated in a silicone resin, in which the base member to which the silicone resin adheres is a glass mat polyester in laminate or molded form."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 134
                            }
                        ],
                        "text": "The most common derivation of PCA is in terms of a standardised linear projection which maximises the variance in the projected space (Hotelling 1933)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "The most common derivation of PCA is in terms of a standardized linear projection which maximizes the variance in the projected space (Hotelling, 1933)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144828484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9ebb5c0d6d54707a4d6181a693b6f755ec8a45a9",
            "isKey": false,
            "numCitedBy": 8491,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-a-complex-of-statistical-variables-into-Hotelling",
            "title": {
                "fragments": [],
                "text": "Analysis of a complex of statistical variables into principal components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16729445"
                        ],
                        "name": "R. Courant",
                        "slug": "R.-Courant",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Courant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Courant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6234493"
                        ],
                        "name": "D. Hilbert",
                        "slug": "D.-Hilbert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hilbert",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hilbert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125547342,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "1f5ba2142aa78a34a4077dafb337caea0aefaa04",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Methods-of-Mathematical-Physics,-Vol.-I-Courant-Hilbert",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics, Vol. I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98415342"
                        ],
                        "name": "G. Reuter",
                        "slug": "G.-Reuter",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Reuter",
                            "middleNames": [
                                "E.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Reuter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125617136,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "be86151293961999ca211ee164d6c6359aaa4e26",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LINEAR-OPERATORS-PART-II-(SPECTRAL-THEORY)-Reuter",
            "title": {
                "fragments": [],
                "text": "LINEAR OPERATORS PART II (SPECTRAL THEORY)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69449918"
                        ],
                        "name": "R. Shanmugam",
                        "slug": "R.-Shanmugam",
                        "structuredName": {
                            "firstName": "Ramalingam",
                            "lastName": "Shanmugam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shanmugam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123327330,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5ecf1c08fee8444f402583c2bd97d33e263a2c46",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariate-Analysis:-Part-2:-Classification,-and-Shanmugam",
            "title": {
                "fragments": [],
                "text": "Multivariate Analysis: Part 2: Classification, Covariance Structures and Repeated Measurements"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69449918"
                        ],
                        "name": "R. Shanmugam",
                        "slug": "R.-Shanmugam",
                        "structuredName": {
                            "firstName": "Ramalingam",
                            "lastName": "Shanmugam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shanmugam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 9
                            }
                        ],
                        "text": "This is where factor analysis fundamentally di ers from standard PCA, which e ectively treats covariance and variance identically."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 150
                            }
                        ],
                        "text": "\u2026the log-likelihood The gradient of log-likelihood (4) with respect to W may be obtained from standard matrix di erentiation results (for example see Krzanowski and Marriott (1994), p. 133):\n@L @W N C\u00ff1SC\u00ff1W\u00ff C\u00ff1W : 10\nAt the stationary points\nSC\u00ff1W W, 11 assuming that C\u00ff1 exists, which we shall\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121922720,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "386c23cc49ec2a6ba62693176b5628466b7be340",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariate-Analysis:-Part-1:-Distributions,-and-Shanmugam",
            "title": {
                "fragments": [],
                "text": "Multivariate Analysis: Part 1: Distributions, Ordination and Inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 94
                            }
                        ],
                        "text": "A complementary property of PCA, and that most closely related to the original discussions of Pearson (1901), is that, of all orthogonal linear projections xn WT tn \u00ff t , the principal component projection minimizes the squared reconstruction error n ktn \u00ff t\u0302nk2, where the optimal linear\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On lines and planes of closest fit to systems of points in space. The London"
            },
            "venue": {
                "fragments": [],
                "text": "Edinburgh and Dublin Philosophical Magazine and Journal of Science"
            },
            "year": 1901
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 110
                            }
                        ],
                        "text": "Rather than giving a full review of this field here, we briefly describe five approaches and refer readers to Diamantaras and Kung (1996) for more details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal component neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "This case has indeed been investigated, and related to PCA, in the early factor analysis literature by Lawley (1953) and by Anderson and Rubin (1956), although this work does not appear widely known."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "We have reiterated and extended the earlier work of Lawley (1953) and Anderson and Rubin (1956) and shown how PCA may be viewed as a maximum likelihood procedure based on a probability density model of the observed data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A modified method of estimation in factor analysis and some large sample results"
            },
            "venue": {
                "fragments": [],
                "text": "Uppsala Symposium on Psychological Factor Analysis, Number 3 in Nordisk Psykologi Monograph Series, pp. 35\u201342. Uppsala: Almqvist and Wiksell."
            },
            "year": 1953
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 54
                            }
                        ],
                        "text": "Perhaps the most common such model is factor analysis (Bartholomew, 1987; Basilevsky, 1994) where the relationship is linear:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 55
                            }
                        ],
                        "text": "Perhaps the most common such model is factor analysis (Bartholomew, 1987; Basilevsky, 1994) where the relationship is linear:\nt Wx : 1 The d qmatrix W relates the two sets of variables, while the parameter vector permits the model to have non-zero mean."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent Variable Models and Factor Analysis. London: Gri\u0081n"
            },
            "venue": {
                "fragments": [],
                "text": "Basilevsky, A"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of pattern recognition [in Russian"
            },
            "venue": {
                "fragments": [],
                "text": "Nauka, Moscow,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On lines and planes of closest \u00ae t to systems of points in space"
            },
            "venue": {
                "fragments": [],
                "text": "Lond . Edinb . Dub . Phil . Mag . J . Sci ."
            },
            "year": 1901
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 274
                            }
                        ],
                        "text": "\u2026of the data, in which case S is the sample covariance matrix of the observations ftng. Estimates for W and 2 may be obtained by iterative maximization of L, e.g. by using the EM algorithm given in Appendix B, which is based on the algorithm for standard factor analysis of Rubin and Thayer (1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "EM algorithms forML factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Psychometrika 47(1), 69\u201376."
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "1(a) shows a projection of 38 examples from the 18-dimensional Tobamovirus data utilized by Ripley (1996), p. 291, to illustrate standard PCA."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "This case has indeed been investigated, and related to PCA, in the early factor analysis literature by Lawley (1953) and by Anderson and Rubin (1956), although this work does not appear widely known."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "We have reiterated and extended the earlier work of Lawley (1953) and Anderson and Rubin (1956) and shown how PCA may be viewed as a maximum likelihood procedure based on a probability density model of the observed data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A modi\u00aeed method of estimation in factor analysis and some large sample results"
            },
            "venue": {
                "fragments": [],
                "text": "Nord . Psykol . Monogr . Ser ."
            },
            "year": 1953
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 15
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 59,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Probabilistic-Principal-Component-Analysis-Tipping-Bishop/ff4b3bbb455c9cc561ddec097a869140b3c1303d?sort=total-citations"
}