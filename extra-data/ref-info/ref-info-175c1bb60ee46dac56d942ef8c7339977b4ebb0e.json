{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791069"
                        ],
                        "name": "D. Musicant",
                        "slug": "D.-Musicant",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Musicant",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Musicant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107978833"
                        ],
                        "name": "Vipin Kumar",
                        "slug": "Vipin-Kumar",
                        "structuredName": {
                            "firstName": "Vipin",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vipin Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7460490"
                        ],
                        "name": "Aysel Ozgur",
                        "slug": "Aysel-Ozgur",
                        "structuredName": {
                            "firstName": "Aysel",
                            "lastName": "Ozgur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aysel Ozgur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 13
                            }
                        ],
                        "text": "Furthermore, Musicant et al. (2003) make a theoretical argument that such cost models approximately optimize F1-score."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 158
                            }
                        ],
                        "text": "However, for non-linear performance measures like F1-score, the few previous attempts towards their direct optimization noted their computational difficulty (Musicant et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14398042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c6e8c365ee78cbbb2e6278b5329710063a297ea",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs) are regularly used for classification of unbalanced data by weighting more heavily the error contribution from the rare class. This heuristic technique is often used to learn classifiers with high F-measure, although this particular application of SVMs has not been rigorously examined. We provide significant and new theoretical results that support this popular heuristic. Specifically, we demonstrate that with the right parameter settings SVMs approximately optimize F-measure in the same way that SVMs have already been known to approximately optimize accuracy. This finding has a number of theoretical and practical implications for using SVMs in F-measure optimization."
            },
            "slug": "Optimizing-F-Measure-with-Support-Vector-Machines-Musicant-Kumar",
            "title": {
                "fragments": [],
                "text": "Optimizing F-Measure with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is demonstrated that with the right parameter settings SVMs approximately optimize F-measure in the same way that SVMs have already been known to approximately optimize accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "FLAIRS Conference"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Support Vector Machines (SVMs) were developed by Vapnik et al. ( Boser et al., 1992;  Cortes & Vapnik, 1995; Vapnik, 1998) as a method for learning linear and, through the use of Kernels, non-linear rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 64
                            }
                        ],
                        "text": "Support Vector Machines (SVMs) were developed by Vapnik et al. (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998) as a method for learning linear and, through the use of Kernels, non-linear rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10843,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 93
                            }
                        ],
                        "text": "Support Vector Machines (SVMs) were developed by Vapnik et al. (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998) as a method for learning linear and, through the use of Kernels, non-linear rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": false,
            "numCitedBy": 33436,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399048849"
                        ],
                        "name": "Alexandru Niculescu-Mizil",
                        "slug": "Alexandru-Niculescu-Mizil",
                        "structuredName": {
                            "firstName": "Alexandru",
                            "lastName": "Niculescu-Mizil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandru Niculescu-Mizil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 254
                            }
                        ],
                        "text": "\u2026many different variants of convenient and tractable performance measures, aiming to find one that performs well for the application specific performance measure after post-processing the resulting model (e.g. (Lewis, 2001; Yang, 2001; Abe et al., 2004; Caruana & Niculescu-Mizil, 2004))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(Lewis, 2001; Yang, 2001; Abe et al., 2004;  Caruana & Niculescu-Mizil, 2004 ))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 40592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77f8a60da1af42112e49d3b1b32243123066e09e",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Many criteria can be used to evaluate the performance of supervised learning. Different criteria are appropriate in different settings, and it is not always clear which criteria to use. A further complication is that learning methods that perform well on one criterion may not perform well on other criteria. For example, SVMs and boosting are designed to optimize accuracy, whereas neural nets typically optimize squared error or cross entropy. We conducted an empirical study using a variety of learning methods (SVMs, neural nets, k-nearest neighbor, bagged and boosted trees, and boosted stumps) to compare nine boolean classification performance metrics: Accuracy, Lift, F-Score, Area under the ROC Curve, Average Precision, Precision/Recall Break-Even Point, Squared Error, Cross Entropy, and Probability Calibration. Multidimensional scaling (MDS) shows that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are interpreted as probabilities: squared error, cross entropy, and calibration, lay in one part of metric space far away from metrics that depend on the relative order of the predicted values: ROC area, average precision, break-even point, and lift. In between them fall two metrics that depend on comparing predictions to a threshold: accuracy and F-score. As expected, maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy, but perform poorly on probability metrics such as squared error. What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision. We introduce a new metric, SAR, that combines squared error, accuracy, and ROC area into one metric. MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics, suggesting that it is a good general purpose metric to use when more specific criteria are not known."
            },
            "slug": "Data-mining-in-metric-space:-an-empirical-analysis-Caruana-Niculescu-Mizil",
            "title": {
                "fragments": [],
                "text": "Data mining in metric space: an empirical analysis of supervised learning performance criteria"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new metric is introduced, SAR, that combines squared error, accuracy, and ROC area into one metric, and MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics, suggesting that it is a good general purpose metric to use when more specific criteria are not known."
            },
            "venue": {
                "fragments": [],
                "text": "ROCAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 65
                            }
                        ],
                        "text": "Based on the sparse approximation algorithm for structural SVMs (Tsochantaridis et al., 2004), we propose a method with which the training problem can be solved in polynomial time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 13
                            }
                        ],
                        "text": "Furthermore, Tsochantaridis et al. (2004) show that the algorithm terminates after a polynomial number of iterations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 39
                            }
                        ],
                        "text": "The sparse approximation algorithm of (Tsochantaridis et al., 2004) does not have this restriction, and we will show in the following how it can be used to solve Optimization Problem 2 in polynomial time for a large class of loss functions \u2206."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 79
                            }
                        ],
                        "text": "This problem is a special case of the multivariate prediction formulations in (Tsochantaridis et al., 2004) as well as in (Taskar et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 11
                            }
                        ],
                        "text": "Following (Tsochantaridis et al., 2004), we formulate the following alternative optimization problem for non-negative \u2206."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 59
                            }
                        ],
                        "text": "However, by adapting the sparse approximation algorithm of (Tsochantaridis et al., 2004) implemented in SVM3, we will show that this problem can be solved in polynomial time for many types of multivariate loss functions \u2206."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 60
                            }
                        ],
                        "text": "However, by adapting the sparse approximation algorithm of (Tsochantaridis et al., 2004) implemented in SVMstruct3, we will show that this problem can be solved in polynomial time for many types of multivariate loss functions \u2206."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 190
                            }
                        ],
                        "text": "How can the optimization problem of the multivariate SVMmulti be solved despite the huge number of constraints? This problem is a special case of the multivariate prediction formulations in (Tsochantaridis et al., 2004) as well as in (Taskar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 564746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93aa298b40bb3ec23c25239089284fdf61ded917",
            "isKey": false,
            "numCitedBy": 1455,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "slug": "Support-vector-machine-learning-for-interdependent-Tsochantaridis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Support vector machine learning for interdependent and structured output spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs, and demonstrates the versatility and effectiveness of the method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 211
                            }
                        ],
                        "text": "\u2026many different variants of convenient and tractable performance measures, aiming to find one that performs well for the application specific performance measure after post-processing the resulting model (e.g. (Lewis, 2001; Yang, 2001; Abe et al., 2004; Caruana & Niculescu-Mizil, 2004))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 71
                            }
                        ],
                        "text": "For example, David Lewis won the TREC-2001 Batch Filtering Evaluation (Lewis, 2001) using SVMlight with such cost models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 70
                            }
                        ],
                        "text": "For example, David Lewis won the TREC-2001 Batch Filtering Evaluation (Lewis, 2001) using SVM with such cost models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 761952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a7d40ba2e553ab481526b2b6e6df5bd2cd9952f",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Given the large training set available for batch filtering, choosing a supervised learning algorithm that would make effective use of this data was critical. The support vector machine approach (SVM) to training linear classifiers has outperformed competing approaches in a number of recent text categorization studies, particularly for categories with substantial numbers of positive training examples. SVMs require little or no feature selection, since they avoid overfitting by optimizing a margin-based criterion rather than one based on number of features. This minimizes the complexity of the software and processing. Finally, Thorsten Joachims has made publicly available an efficient implementation of SVMs, SVM_Light [Joachims 1999]: http://www.joachims.org/svm_light/"
            },
            "slug": "Applying-Support-Vector-Machines-to-the-TREC-2001-Lewis",
            "title": {
                "fragments": [],
                "text": "Applying Support Vector Machines to the TREC-2001 Batch Filtering and Routing Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The support vector machine approach (SVM) to training linear classifiers has outperformed competing approaches in a number of recent text categorization studies, particularly for categories with substantial numbers of positive training examples."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785006"
                        ],
                        "name": "C. Ferri",
                        "slug": "C.-Ferri",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Ferri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ferri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47840704"
                        ],
                        "name": "Peter A. Flach",
                        "slug": "Peter-A.-Flach",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Flach",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter A. Flach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398777358"
                        ],
                        "name": "J. Hern\u00e1ndez-Orallo",
                        "slug": "J.-Hern\u00e1ndez-Orallo",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Hern\u00e1ndez-Orallo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hern\u00e1ndez-Orallo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 86
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 87
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8869874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e53f0f01cc17e28557235dd9a9d08923de7dfa8a",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "ROC analysis is increasingly being recognised as an important tool for evaluation and comparison of classifiers when the operating characteristics (i.e. class distribution and cost parameters) are not known at training time. Usually, each classifier is characterised by its estimated true and false positive rates and is represented by a single point in the ROC diagram. In this paper, we show how a single decision tree can represent a set of classifiers by choosing different labellings of its leaves, or equivalently, an ordering on the leaves. In this setting, rather than estimating the accuracy of a single tree, it makes more sense to use the area under the ROC curve (AUC) as a quality metric. We also propose a novel splitting criterion which chooses the split with the highest local AUC. To the best of our knowledge, this is the first probabilistic splitting criterion that is not based on weighted average impurity. We present experiments suggesting that the AUC splitting criterion leads to trees with equal or better AUC value, without sacrificing accuracy if a single labelling is chosen."
            },
            "slug": "Learning-Decision-Trees-Using-the-Area-Under-the-Ferri-Flach",
            "title": {
                "fragments": [],
                "text": "Learning Decision Trees Using the Area Under the ROC Curve"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper shows how a single decision tree can represent a set of classifiers by choosing different labellings of its leaves, or equivalently, an ordering on the leaves, and proposes a novel splitting criterion which chooses the split with the highest local AUC."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 236
                            }
                        ],
                        "text": "\u2026many different variants of convenient and tractable performance measures, aiming to find one that performs well for the application specific performance measure after post-processing the resulting model (e.g. (Lewis, 2001; Yang, 2001; Abe et al., 2004; Caruana & Niculescu-Mizil, 2004))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(Lewis, 2001; Yang, 2001; Abe et al., 2004; Caruana & Niculescu-Mizil, 2004))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7092024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9063125fa5222f1b051d9228aea4241b0a533c2e",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification. In this paper, we present a method for solving multi-class cost-sensitive learning problems using any binary classification algorithm. This algorithm is derived using hree key ideas: 1) iterative weighting; 2) expanding data space; and 3) gradient boosting with stochastic ensembles. We establish some theoretical guarantees concerning the performance of this method. In particular, we show that a certain variant possesses the boosting property, given a form of weak learning assumption on the component binary classifier. We also empirically evaluate the performance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning, in terms of predictive performance (cost minimization) and, in many cases, computational efficiency."
            },
            "slug": "An-iterative-method-for-multi-class-cost-sensitive-Abe-Zadrozny",
            "title": {
                "fragments": [],
                "text": "An iterative method for multi-class cost-sensitive learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper empirically evaluates the performance of the proposed method using benchmark data sets and proves that the method generally achieves better results than representative methods for cost-sensitive learning, in terms of predictive performance (cost minimization) and, in many cases, computational efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2284699"
                        ],
                        "name": "A. Herschtal",
                        "slug": "A.-Herschtal",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Herschtal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Herschtal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693373"
                        ],
                        "name": "Bhavani Raskutti",
                        "slug": "Bhavani-Raskutti",
                        "structuredName": {
                            "firstName": "Bhavani",
                            "lastName": "Raskutti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bhavani Raskutti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003;  Herschtal & Raskutti, 2004 ), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 143
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8571129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "588f23c7f9c8c85e05d575c81f1d7dbda3b026ca",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces RankOpt, a linear binary classifier which optimises the area under the ROC curve (the AUC). Unlike standard binary classifiers, RankOpt adopts the AUC statistic as its objective function, and optimises it directly using gradient descent. The problems with using the AUC statistic as an objective function are that it is non-differentiable, and of complexity O(n2) in the number of data observations. RankOpt uses a differentiable approximation to the AUC which is accurate, and computationally efficient, being of complexity O(n.) This enables the gradient descent to be performed in reasonable time. The performance of RankOpt is compared with a number of other linear binary classifiers, over a number of different classification problems. In almost all cases it is found that the performance of RankOpt is significantly better than the other classifiers tested."
            },
            "slug": "Optimising-area-under-the-ROC-curve-using-gradient-Herschtal-Raskutti",
            "title": {
                "fragments": [],
                "text": "Optimising area under the ROC curve using gradient descent"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper introduces RankOpt, a linear binary classifier which optimises the area under the ROC curve (the AUC)."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145245719"
                        ],
                        "name": "Lian Yan",
                        "slug": "Lian-Yan",
                        "structuredName": {
                            "firstName": "Lian",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lian Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2479874"
                        ],
                        "name": "R. Dodier",
                        "slug": "R.-Dodier",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Dodier",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dodier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2523711"
                        ],
                        "name": "R. Wolniewicz",
                        "slug": "R.-Wolniewicz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wolniewicz",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wolniewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks ( Yan et al., 2003;  Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 125
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3135985,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df27dde10589455d290eeee6d0ae6ceeb83d0c6b",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "When the goal is to achieve the best correct classification rate, cross entropy and mean squared error are typical cost functions used to optimize classifier performance. However, for many real-world classification problems, the ROC curve is a more meaningful performance measure. We demonstrate that minimizing cross entropy or mean squared error does not necessarily maximize the area under the ROC curve (AUC). We then consider alternative objective functions for training a classifier to maximize the AUC directly. We propose an objective function that is an approximation to the Wilcoxon-Mann-Whitney statistic, which is equivalent to the AUC. The proposed objective function is differentiable, so gradient-based methods can be used to train the classifier. We apply the new objective function to real-world customer behavior prediction problems for a wireless service provider and a cable service provider, and achieve reliable improvements in the ROC curve."
            },
            "slug": "Optimizing-Classifier-Performance-via-an-to-the-Yan-Dodier",
            "title": {
                "fragments": [],
                "text": "Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes an objective function that is an approximation to the Wilcoxon-Mann-Whitney statistic, which is equivalent to the AUC, and applies it to real-world customer behavior prediction problems for a wireless service provider and a cable service provider, and achieves reliable improvements in the ROC curve."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026y\u0304\u2032)]} 6: if \u2206(y\u0304\u2032, y\u0304)\u2212wT [\u03a8(x\u0304, y\u0304)\u2212\u03a8(x\u0304, y\u0304\u2032)] > \u03be+ then 7: C \u2190 C \u222a {y\u0304\u2032} 8: w\u2190 optimize SVM\u2206multi objective over C 9: end if\n10: until C has not changed during iteration 11: return(w)\nalgorithm proposed in (Taskar et al., 2003) for solving these types of large quadratic programs is not\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 123
                            }
                        ],
                        "text": "This problem is a special case of the multivariate prediction formulations in (Tsochantaridis et al., 2004) as well as in (Taskar et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": false,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108066200"
                        ],
                        "name": "Yi Lin",
                        "slug": "Yi-Lin",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4628585"
                        ],
                        "name": "Yoonkyung Lee",
                        "slug": "Yoonkyung-Lee",
                        "structuredName": {
                            "firstName": "Yoonkyung",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoonkyung Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Morik et al., 1999; Lin et al., 2002) in the context of SVMs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 159
                            }
                        ],
                        "text": "In particular, most learning algorithms can be extended to incorporate unbalanced misclassification costs via linear loss functions (e.g. (Morik et al., 1999; Lin et al., 2002) in the context of SVMs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5801524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6be52ea38859d6a4cf1fd7804745ddb88216e01",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The majority of classification algorithms are developed for the standard situation in which it is assumed that the examples in the training set come from the same distribution as that of the target population, and that the cost of misclassification into different classes are the same. However, these assumptions are often violated in real world settings. For some classification methods, this can often be taken care of simply with a change of threshold; for others, additional effort is required. In this paper, we explain why the standard support vector machine is not suitable for the nonstandard situation, and introduce a simple procedure for adapting the support vector machine methodology to the nonstandard situation. Theoretical justification for the procedure is provided. Simulation study illustrates that the modified support vector machine significantly improves upon the standard support vector machine in the nonstandard situation. The computational load of the proposed procedure is the same as that of the standard support vector machine. The procedure reduces to the standard support vector machine in the standard situation."
            },
            "slug": "Support-Vector-Machines-for-Classification-in-Lin-Lee",
            "title": {
                "fragments": [],
                "text": "Support Vector Machines for Classification in Nonstandard Situations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper explains why the standard support vectors machine is not suitable for the nonstandard situation, and introduces a simple procedure for adapting the support vector machine methodology to the non standard situation."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 224
                            }
                        ],
                        "text": "\u2026many different variants of convenient and tractable performance measures, aiming to find one that performs well for the application specific performance measure after post-processing the resulting model (e.g. (Lewis, 2001; Yang, 2001; Abe et al., 2004; Caruana & Niculescu-Mizil, 2004))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(Lewis, 2001; Yang, 2001; Abe et al., 2004; Caruana & Niculescu-Mizil, 2004))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3342722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01f3a2d40a748fdd571f9cdb6fc402118919307e",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Thresholding strategies in automated text categorization are an underexplored area of research. This paper presents an examination of the effect of thresholding strategies on the performance of a classifier under various conditions. Using k-Nearest Neighbor (kNN) as the classifier and five evaluation benchmark collections as the testbets, three common thresholding methods were investigated, including rank-based thresholding (RCut), proportion-based assignments (PCut) and score-based local optimization (SCut); in addition, new variants of these methods are proposed to overcome significant problems in the existing approaches. Experimental results show that the choice of thresholding strategy can significantly influence the performance of kNN, and that the ``optimal'' strategy may vary by application. SCut is potentially better for fine-tuning but risks overfitting. PCut copes better with rare categories and exhibits a smoother trade-off in recall versus precision, but is not suitable for online decision making. RCut is most natural for online response but is too coarse-grained for global or local optimization. RTCut, a new method combining the strength of category ranking and scoring, outperforms both PCut and RCut significantly."
            },
            "slug": "A-study-of-thresholding-strategies-for-text-Yang",
            "title": {
                "fragments": [],
                "text": "A study of thresholding strategies for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results show that the choice of thresholding strategy can significantly influence the performance of kNN, and that the ``optimal'' strategy may vary by application."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81080659"
                        ],
                        "name": "M. Mohri",
                        "slug": "M.-Mohri",
                        "structuredName": {
                            "firstName": "Mehryar",
                            "lastName": "Mohri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mohri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting ( Cortes & Mohri, 2003;  Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 182
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 873512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa12af4231669db1b8f978b829079ca2684df5cc",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classification algorithm. However, the objective function optimized in most of these algorithms is the error rate and not the AUC value. We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate. Our results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable. Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values. We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets demonstrating the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC."
            },
            "slug": "AUC-Optimization-vs.-Error-Rate-Minimization-Cortes-Mohri",
            "title": {
                "fragments": [],
                "text": "AUC Optimization vs. Error Rate Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable, so algorithms designed to minimize the error rate may not lead to the best possible AUC values."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068005320"
                        ],
                        "name": "Raj D. Iyer",
                        "slug": "Raj-D.-Iyer",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Iyer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raj D. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 46
                            }
                        ],
                        "text": ", 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 204
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16692650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f",
            "isKey": false,
            "numCitedBy": 2182,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations."
            },
            "slug": "An-Efficient-Boosting-Algorithm-for-Combining-Freund-Iyer",
            "title": {
                "fragments": [],
                "text": "An Efficient Boosting Algorithm for Combining Preferences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning, and gives theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 95
                            }
                        ],
                        "text": "While based on these probabilities many performance measures can be (approximately) optimized (Lewis, 1995), estimating the probabilities accurately is a difficult problem and arguably harder than the original problem of optimizing the particular performance measure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17260485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91cbbe24c807473b7b935d39b63df5b15da9bb32",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Text retrieval systems typically produce a ranking of documents and let a user decide how far down that ranking to go. In contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other IR systems must make decisions without human input or supervision. It is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed. We show how to do this for binary text classification systems, emphasizing that different goals for the system le ad to different optimal behaviors. Optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used. Ranked retrieval is the information retrieval (IR) researc her\u2019s favorite tool for dealing with information overload. Ranked retrieval systems display documents in order of probability of releva nce or some similar measure. Users see the best documents first, anddecide how far down the ranking to go in examining the available information. The central role played by ranking in this appr oach has led researchers to evaluate IR systems primarily, often exclusively, on the quality of their rankings. (See, for instance , the TREC evaluations [1].) In some IR applications, however, ranking is not enough: A company provides an SDI (selective dissemination of information) service which filters newswire feeds. Relevant articles are faxed each morning to clients. Interaction between customer and system takes place infrequently. The cost of resources (tying up phone lines, fax machine paper, etc.) is a factor to consider in operating the system. A text categorization system assigns controlled vocabulary categories to incoming documents as they are stored in a text database. Cost cutting has eliminated manual checking of category assignments."
            },
            "slug": "Evaluating-and-optimizing-autonomous-text-systems-Lewis",
            "title": {
                "fragments": [],
                "text": "Evaluating and optimizing autonomous text classification systems"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work shows how to define what constitutes good effectiveness for binary text classification systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752599"
                        ],
                        "name": "K. Morik",
                        "slug": "K.-Morik",
                        "structuredName": {
                            "firstName": "Katharina",
                            "lastName": "Morik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Morik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406679"
                        ],
                        "name": "Peter Brockhausen",
                        "slug": "Peter-Brockhausen",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brockhausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Brockhausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "In particular, most learning algorithms can be extended to incorporate unbalanced misclassification costs via linear loss functions (e.g. (Morik et al., 1999; Lin et al., 2002) in the context of SVMs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 113
                            }
                        ],
                        "text": "The cost model is implemented by allowing different regularization constants for positive and negative examples (Morik et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10842128,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "id": "28f734ca252df1709a1314197838c781a7248c59",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes a case study in combining different methods for acquiring medical knowledge. Given a huge amount of noisy, high dimensional numerical time series data describing patients in intensive care, the support vector machine is used to learn when and how to change the dose of which drug. Given medical knowledge about and expertise in clinical decision making, a first-order logic knowledge base about effects of therapeutical interventions has been built. As a preprocessing mechanism it uses another statistical method. The integration of numerical and knowledge-based procedures eases the task of validation in two ways. On one hand, the knowledge base is validated with respect to past patients records. On the other hand, medical interventions that are recommended by learning results are justified by the knowledge base."
            },
            "slug": "Combining-Statistical-Learning-with-a-Approach-A-in-Morik-Brockhausen",
            "title": {
                "fragments": [],
                "text": "Combining Statistical Learning with a Knowledge-Based Approach - A Case Study in Intensive Care Monitoring"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A case study in combining different methods for acquiring medical knowledge given a huge amount of noisy, high dimensional numerical time series data describing patients in intensive care, the support vector machine is used to learn when and how to change the dose of which drug."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "In this section we first review the typical assumptions (often implicitly) made by most existing learning algorithms (Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "Support Vector Machines (SVMs) were developed by Vapnik et al. (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998) as a method for learning linear and, through the use of Kernels, non-linear rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 118
                            }
                        ],
                        "text": "Depending on the application, measuring the success of a learning algorithm requires application specific performance measures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234984"
                        ],
                        "name": "R. Herbrich",
                        "slug": "R.-Herbrich",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Herbrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herbrich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 209928701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81b8bb14524665e3dbaa772f297cd5e5e79ba0d6",
            "isKey": false,
            "numCitedBy": 423,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Large-margin-rank-boundaries-for-ordinal-regression-Herbrich",
            "title": {
                "fragments": [],
                "text": "Large margin rank boundaries for ordinal regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 140
                            }
                        ],
                        "text": "Approaches of the first type aim to produce accurate estimates of the probabilities of class membership of each example (e.g. (Platt, 2000; Langford & Zadrozny, 2005))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46043061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf9e2d179f1e07d1320332ed83ac1b33df5bc38",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-Class-Membership-Probabilities-using-Langford-Zadrozny",
            "title": {
                "fragments": [],
                "text": "Estimating Class Membership Probabilities using Classifier Learners"
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061462269"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Platt, 2000; Langford & Zadrozny, 2005))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 56563878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7",
            "isKey": false,
            "numCitedBy": 5101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Outputs-for-Support-vector-Machines-Platt",
            "title": {
                "fragments": [],
                "text": "Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "Support Vector Machines (SVMs) were developed by Vapnik et al. (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998) as a method for learning linear and, through the use of Kernels, non-linear rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support\u2013vector networks. Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Support\u2013vector networks. Machine Learning"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 259
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Svms and area under roc curve (Technical Report)"
            },
            "venue": {
                "fragments": [],
                "text": "PSI-INSA de Rouen"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 259
                            }
                        ],
                        "text": "Also, methods for optimizing ROCArea have been proposed in the area of decision trees (Ferri et al., 2002), neural networks (Yan et al., 2003; Herschtal & Raskutti, 2004), boosting (Cortes & Mohri, 2003; Freund et al., 1998), and SVMs (Herbrich et al., 2000; Rakotomamonjy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Svms and area under roc curve"
            },
            "venue": {
                "fragments": [],
                "text": "Svms and area under roc curve"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum-margin markov networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. NIPS"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "In particular, most learning algorithms can be extended to incorporate unbalanced misclassification costs via linear loss functions (e.g. (Morik et al., 1999; Lin et al., 2002) in the context of SVMs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 113
                            }
                        ],
                        "text": "The cost model is implemented by allowing different regularization constants for positive and negative examples (Morik et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining statistical learning with a knowledgebased approach"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICML"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum - margin markov networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 15
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-support-vector-method-for-multivariate-measures-Joachims/175c1bb60ee46dac56d942ef8c7339977b4ebb0e?sort=total-citations"
}