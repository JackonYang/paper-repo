{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068068778"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 83
                            }
                        ],
                        "text": "We use data augmentation in novel ways, much more aggressively than previous work (Howard, 2013), (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "We also adopt some augmentations that were proposed by the previous work, such as flipping and cropping (Howard, 2013), (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 651286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d67175d17c450ab0ac9c256103828f9e9a0acb85",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner."
            },
            "slug": "Some-Improvements-on-Deep-Convolutional-Neural-Howard",
            "title": {
                "fragments": [],
                "text": "Some Improvements on Deep Convolutional Neural Network Based Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper summarizes the entry in the Imagenet Large Scale Visual Recognition Challenge 2013, which achieved a top 5 classification error rate and achieved over a 20% relative improvement on the previous year's winner."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 119
                            }
                        ],
                        "text": "4VGG team\u2019s single model achieves top-1 error of 24.4% and top-5 error of 7.1% on validation set after the competition (Simonyan & Zisserman, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 88
                            }
                        ],
                        "text": "In ILSVRC 2014, the winner GoogLeNet (Szegedy et al., 2014) and the runner-up VGG team (Simonyan & Zisserman, 2014) both increased the depth of the network significantly, and achieved top-5 classification error 6.66% and 7.32%, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 92
                            }
                        ],
                        "text": "VGG team achieves top-5 test set error of 6.8% using multiple models after the competition (Simonyan & Zisserman, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 91
                            }
                        ],
                        "text": "As listed in Table 2, one basic configuration has 16 layers and is similar with VGG\u2019s work (Simonyan & Zisserman, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 61
                            }
                        ],
                        "text": "Besides the depth, GoogLeNet (Szegedy et al., 2014) and VGG (Simonyan & Zisserman, 2014) used multi-scale data to improve the accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 25
                            }
                        ],
                        "text": "Different from the work (Simonyan & Zisserman, 2014), (Szegedy et al., 2014) that uses scale-jittering method, we have trained separate models at different scales, including high-resolution ones (such as 512x512), and combined them by averaging softmax class posteriors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": true,
            "numCitedBy": 62181,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 20
                            }
                        ],
                        "text": "Most previous work (Krizhevsky et al., 2012), (Zeiler & Fergus, 2014) operates on downsized images with a resolution of approximately 256x256."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 190
                            }
                        ],
                        "text": "The SuperVision team made a significant breakthrough in ILSVRC 2012, where they trained a deep convolutional neural network with 60 million parameters using an efficient GPU implementation (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026algorithms, given enough data and computational resources, dominate many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 15
                            }
                        ],
                        "text": "Previous work (Krizhevsky et al., 2012), (Zeiler & Fergus, 2014) usually downsizes the images to a fixed resolution, such as 256x256, and then randomly crops out slightly smaller areas, such as 224x224, and uses those crops for training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 99
                            }
                        ],
                        "text": "We use data augmentation in novel ways, much more aggressively than previous work (Howard, 2013), (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "Different from the color shifting in (Krizhevsky et al., 2012), we perform color casting to alter the intensities of the RGB channels in training images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 121
                            }
                        ],
                        "text": "We also adopt some augmentations that were proposed by the previous work, such as flipping and cropping (Howard, 2013), (Krizhevsky et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": true,
            "numCitedBy": 80897,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 38
                            }
                        ],
                        "text": "In ILSVRC 2014, the winner GoogLeNet (Szegedy et al., 2014) and the runner-up VGG team (Simonyan & Zisserman, 2014) both increased the depth of the network significantly, and achieved top-5 classification error 6.66% and 7.32%, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 145
                            }
                        ],
                        "text": "\u2026data and computational resources, dominate many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "Besides the depth, GoogLeNet (Szegedy et al., 2014) and VGG (Simonyan & Zisserman, 2014) used multi-scale data to improve the accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026data and computational resources, dominate many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 55
                            }
                        ],
                        "text": "Different from the work (Simonyan & Zisserman, 2014), (Szegedy et al., 2014) that uses scale-jittering method, we have trained separate models at different scales, including high-resolution ones (such as 512x512), and combined them by averaging softmax class posteriors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": true,
            "numCitedBy": 29470,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2705825"
                        ],
                        "name": "Omry Yadan",
                        "slug": "Omry-Yadan",
                        "structuredName": {
                            "firstName": "Omry",
                            "lastName": "Yadan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omry Yadan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067963295"
                        ],
                        "name": "Keith Adams",
                        "slug": "Keith-Adams",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Adams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keith Adams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2188620"
                        ],
                        "name": "Yaniv Taigman",
                        "slug": "Yaniv-Taigman",
                        "structuredName": {
                            "firstName": "Yaniv",
                            "lastName": "Taigman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaniv Taigman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 80
                            }
                        ],
                        "text": "Similar strategies have been proposed in the previous work (Krizhevsky, 2014), (Yadan et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 27
                            }
                        ],
                        "text": "In (Krizhevsky, 2014) and (Yadan et al., 2013), the authors also employed data and model parallelism or hybrid parallelization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15336613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ce50b6d21e299d60e3ae2f46408ef2b6f29cdd4",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs. In this work, we used up to 4 NVIDIA TITAN GPUs with 6GB of RAM. While our experiments are performed on a single server, our GPUs have disjoint memory spaces, and just as in the distributed setting, communication overheads are an important consideration. Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm. Instead, we isolate the impact of parallelism, while using standard supervised back-propagation and synchronous mini-batch stochastic gradient descent."
            },
            "slug": "Multi-GPU-Training-of-ConvNets-Yadan-Adams",
            "title": {
                "fragments": [],
                "text": "Multi-GPU Training of ConvNets"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work isolates the impact of parallelism, while using standard supervised back-propagation and synchronous mini-batch stochastic gradient descent to investigate methods to speed convergence by parallelizing training across multiple GPUs."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50196944"
                        ],
                        "name": "Judy Hoffman",
                        "slug": "Judy-Hoffman",
                        "structuredName": {
                            "firstName": "Judy",
                            "lastName": "Hoffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judy Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368132"
                        ],
                        "name": "Eric Tzeng",
                        "slug": "Eric-Tzeng",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Tzeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Tzeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN [4] \u221a \u221a 75.7 DPD [53] \u221a \u221a \u221a 51.0 POOF [3] \u221a \u221a \u221a 56.8 Nonparametric [18] \u221a \u221a \u221a 57.8 Symbiotic [6] \u221a \u221a \u221a 61.0 DPD+DeCAF [16] \u221a \u221a \u221a 65.0 PB R-CNN [52] \u221a \u221a \u221a 76.4 Symbiotic [6] \u221a \u221a \u221a \u221a 69.5 POOF [3] \u221a \u221a \u221a \u221a 73.3 PB R-CNN [52] \u221a \u221a \u221a \u221a 82.0 PN-DCN [4] \u221a \u221a \u221a \u221a 85.4 (a) (b) Figure 8: Basic configurations of our models: (a) 16 la"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6161478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8de958fead0d8a9619b55c7299df3257c624a96",
            "isKey": true,
            "numCitedBy": 4234,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "slug": "DeCAF:-A-Deep-Convolutional-Activation-Feature-for-Donahue-Jia",
            "title": {
                "fragments": [],
                "text": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "DeCAF, an open-source implementation of deep convolutional activation features, along with all associated network parameters, are released to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2835963"
                        ],
                        "name": "A. Razavian",
                        "slug": "A.-Razavian",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Razavian",
                            "middleNames": [
                                "Sharif"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Razavian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2622491"
                        ],
                        "name": "Hossein Azizpour",
                        "slug": "Hossein-Azizpour",
                        "structuredName": {
                            "firstName": "Hossein",
                            "lastName": "Azizpour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hossein Azizpour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50626295"
                        ],
                        "name": "J. Sullivan",
                        "slug": "J.-Sullivan",
                        "structuredName": {
                            "firstName": "Josephine",
                            "lastName": "Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sullivan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153120475"
                        ],
                        "name": "S. Carlsson",
                        "slug": "S.-Carlsson",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Carlsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carlsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "fferent from outdoor scenes, it has large variability across different samples within each category. The work [39] created a dataset of 67 indoor scenes categories, which was used by many researchers [8, 15, 19, 26, 40]. Quattoni et al. propose a scene model which uses image prototypes to define a mapping between images and scene labels. Juneja et al. [26] present an automatic discovery of distinctive parts for an o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ng and evaluate this method on the task of scene recognition, outperforming the work [15]. Different from these work, several work uses CNN to address the indoor scene recognition. The published work [40] using a CNN off-theshelf representation with linear SVMs training improves the baselines. The scheme presented by Gong et al. extracts CNN activations for local patches at multiple scale levels, and "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ".1%, still better than best published result of 95.34%. Table 3. Species categorization results on Oxford flowers Method Accuracy(%) Angelova et al. [1] 80.66 Murray et al. [34] 84.60 Razavian et al. [40] 86.80 Azizpour et al. [2] 91.30 No parts (AlexNet) [44] 90.35 Const. (AlexNet) [44] 91.74 No parts (VGG19) [44] 93.07 Const. (VGG19) [44] 95.34 Ours w/o seg 98.73 Oxford-IIIT Pets The Oxford- IIIT Pe"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "o the model. We achieve accuracy of 82.4% using multi-scale testing. Table 6. Accuracy comparison of different methods on MIT-67 indoor scenes Method Accuracy(%) IFV [26] 60.8 MLrep [15] 64.0 CNN-SVM [40] 58.4 CNNaug-SVM [40] 69.0 CNN(AlexConvNet)+multiscale pooling [40] 68.9 MOP-CNN [19] 68.9 D-CNN-VD [8] 81.1 Ours 82.4 5.4 Generic Classification We have also tested our system on the ImageNet LargeSc"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6383532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6270baedeba28001cd1b563a199335720d6e0fe0",
            "isKey": true,
            "numCitedBy": 4285,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
            },
            "slug": "CNN-Features-Off-the-Shelf:-An-Astounding-Baseline-Razavian-Azizpour",
            "title": {
                "fragments": [],
                "text": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13 suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3191220"
                        ],
                        "name": "Trishul M. Chilimbi",
                        "slug": "Trishul-M.-Chilimbi",
                        "structuredName": {
                            "firstName": "Trishul",
                            "lastName": "Chilimbi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trishul M. Chilimbi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312079"
                        ],
                        "name": "Yutaka Suzue",
                        "slug": "Yutaka-Suzue",
                        "structuredName": {
                            "firstName": "Yutaka",
                            "lastName": "Suzue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yutaka Suzue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2300600"
                        ],
                        "name": "Johnson Apacible",
                        "slug": "Johnson-Apacible",
                        "structuredName": {
                            "firstName": "Johnson",
                            "lastName": "Apacible",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johnson Apacible"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40444389"
                        ],
                        "name": "Karthik Kalyanaraman",
                        "slug": "Karthik-Kalyanaraman",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Kalyanaraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Kalyanaraman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Thanks to Adam Coates and Andrew Ng for many insightful conversations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 14
                            }
                        ],
                        "text": "Project Adam (Chilimbi et al., 2014) is another distributed system built by Microsoft."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2185117,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e69c8b5df8a4178b1c8c7f154a761147a6f030be",
            "isKey": false,
            "numCitedBy": 664,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Large deep neural network models have recently demonstrated state-of-the-art accuracy on hard visual recognition tasks. Unfortunately such models are extremely time consuming to train and require large amount of compute cycles. We describe the design and implementation of a distributed system called Adam comprised of commodity server machines to train such models that exhibits world-class performance, scaling and task accuracy on visual recognition tasks. Adam achieves high efficiency and scalability through whole system co-design that optimizes and balances workload computation and communication. We exploit asynchrony throughout the system to improve performance and show that it additionally improves the accuracy of trained models. Adam is significantly more efficient and scalable than was previously thought possible and used 30x fewer machines to train a large 2 billion connection model to 2x higher accuracy in comparable time on the ImageNet 22,000 category image classification task than the system that previously held the record for this benchmark. We also show that task accuracy improves with larger models. Our results provide compelling evidence that a distributed systems-driven approach to deep learning using current training algorithms is worth pursuing."
            },
            "slug": "Project-Adam:-Building-an-Efficient-and-Scalable-Chilimbi-Suzue",
            "title": {
                "fragments": [],
                "text": "Project Adam: Building an Efficient and Scalable Deep Learning Training System"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The design and implementation of a distributed system called Adam comprised of commodity server machines to train large deep neural network models that exhibits world-class performance, scaling and task accuracy on visual recognition tasks and shows that task accuracy improves with larger models."
            },
            "venue": {
                "fragments": [],
                "text": "OSDI"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145423516"
                        ],
                        "name": "S. Ma",
                        "slug": "S.-Ma",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 114
                            }
                        ],
                        "text": "The best result of ILSVRC 2014, top-5 error rate of 6.66%, is not far from human recognition performance of 5.1% (Russakovsky et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 37
                            }
                        ],
                        "text": "3This result is obtained from paper (Russakovsky et al., 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 79
                            }
                        ],
                        "text": "This work is related to much previous work around this most popular challenge (Russakovsky et al., 2014), which has become the\nstandard benchmark for large-scale object classification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2930547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "isKey": true,
            "numCitedBy": 25477,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements."
            },
            "slug": "ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng",
            "title": {
                "fragments": [],
                "text": "ImageNet Large Scale Visual Recognition Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40470211"
                        ],
                        "name": "T. Paine",
                        "slug": "T.-Paine",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Paine",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Paine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41151701"
                        ],
                        "name": "Hailin Jin",
                        "slug": "Hailin-Jin",
                        "structuredName": {
                            "firstName": "Hailin",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hailin Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145527707"
                        ],
                        "name": "Zhe L. Lin",
                        "slug": "Zhe-L.-Lin",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Lin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe L. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 64
                            }
                        ],
                        "text": "Similar to this work but at a much smaller scale is the work of Paine et al. (2013), in which an asynchronous SGD algorithm based on a GPU cluster was implemented."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17826787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba461c849065809acf7f8d48ccced77004c155ca",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time."
            },
            "slug": "GPU-Asynchronous-Stochastic-Gradient-Descent-to-Up-Paine-Jin",
            "title": {
                "fragments": [],
                "text": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision, and it is believed this will make it possible to train larger networks on larger training sets in a reasonable amount of time."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144284"
                        ],
                        "name": "Tsung-Yu Lin",
                        "slug": "Tsung-Yu-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2895705"
                        ],
                        "name": "Aruni RoyChowdhury",
                        "slug": "Aruni-RoyChowdhury",
                        "structuredName": {
                            "firstName": "Aruni",
                            "lastName": "RoyChowdhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aruni RoyChowdhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Species categorization results on aircraft variant Method Accuracy(%) B-CNN(M,M), w/ ft [30] 73."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[30] directly use one CNN for parts detector and another CNN for classify the parts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Another work [30] achieves mean per-class accuracy of 79."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1331231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d3f789a56dca288b2c8e23ef047a2b342184950",
            "isKey": true,
            "numCitedBy": 1271,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization. It also generalizes various orderless texture descriptors such as the Fisher vector, VLAD and O2P. We present experiments with bilinear models where the feature extractors are based on convolutional neural networks. The bilinear form simplifies gradient computation and allows end-to-end training of both networks using image labels only. Using networks initialized from the ImageNet dataset followed by domain specific fine-tuning we obtain 84.1% accuracy of the CUB-200-2011 dataset requiring only category labels at training time. We present experiments and visualizations that analyze the effects of fine-tuning and the choice two networks on the speed and accuracy of the models. Results show that the architecture compares favorably to the existing state of the art on a number of fine-grained datasets while being substantially simpler and easier to train. Moreover, our most accurate model is fairly efficient running at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete system will be made available at http://vis-www.cs.umass.edu/bcnn."
            },
            "slug": "Bilinear-CNN-Models-for-Fine-Grained-Visual-Lin-RoyChowdhury",
            "title": {
                "fragments": [],
                "text": "Bilinear CNN Models for Fine-Grained Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Blinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor, are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5115386"
                        ],
                        "name": "Yunchao Gong",
                        "slug": "Yunchao-Gong",
                        "structuredName": {
                            "firstName": "Yunchao",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunchao Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39060743"
                        ],
                        "name": "Liwei Wang",
                        "slug": "Liwei-Wang",
                        "structuredName": {
                            "firstName": "Liwei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liwei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37495246"
                        ],
                        "name": "Ruiqi Guo",
                        "slug": "Ruiqi-Guo",
                        "structuredName": {
                            "firstName": "Ruiqi",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruiqi Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " each class for training and 20 images for testing. The best result in published work [8] uses a D-CNN descriptor, which is a texture descriptor based on the Fisher Vector (FV), related to the method [19]. The features are extracted using VGG 19 layer model and then a SVM classifier is used. This is the best result on MIT Indoor scene with accuracy of 81.1%. In our experiments, we fine-tune the last t"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "y comparison of different methods on MIT-67 indoor scenes Method Accuracy(%) IFV [26] 60.8 MLrep [15] 64.0 CNN-SVM [40] 58.4 CNNaug-SVM [40] 69.0 CNN(AlexConvNet)+multiscale pooling [40] 68.9 MOP-CNN [19] 68.9 D-CNN-VD [8] 81.1 Ours 82.4 5.4 Generic Classification We have also tested our system on the ImageNet LargeScale Visual Recognition Challenge (ILSVRC), a popular dataset for generic object class"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "fferent from outdoor scenes, it has large variability across different samples within each category. The work [39] created a dataset of 67 indoor scenes categories, which was used by many researchers [8, 15, 19, 26, 40]. Quattoni et al. propose a scene model which uses image prototypes to define a mapping between images and scene labels. Juneja et al. [26] present an automatic discovery of distinctive parts for an o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1346519,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "a99add9d76d849a8d47b93532703e4ca0f683b92",
            "isKey": true,
            "numCitedBy": 1000,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets."
            },
            "slug": "Multi-scale-Orderless-Pooling-of-Deep-Convolutional-Gong-Wang",
            "title": {
                "fragments": [],
                "text": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simple but effective scheme called multi-scale orderless pooling (MOP-CNN), which extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2543016"
                        ],
                        "name": "M. Cimpoi",
                        "slug": "M.-Cimpoi",
                        "structuredName": {
                            "firstName": "Mircea",
                            "lastName": "Cimpoi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cimpoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "h as different types of birds and flowers, which is different from generic classification. Much work has been devoted on the learning of object parts that benefits from the alignment and segmentation [1, 8, 6, 18]. Current state-of-the-art results for bird species recognition have been achieved in [4], where the strength of deep feature learning is integrated with part modeling. Most of these methods need extr"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ing: leisure, working places, home, stores and public space categories. It consists of 15,620 images, 80 images of each class for training and 20 images for testing. The best result in published work [8] uses a D-CNN descriptor, which is a texture descriptor based on the Fisher Vector (FV), related to the method [19]. The features are extracted using VGG 19 layer model and then a SVM classifier is us"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ferent methods on MIT-67 indoor scenes Method Accuracy(%) IFV [26] 60.8 MLrep [15] 64.0 CNN-SVM [40] 58.4 CNNaug-SVM [40] 69.0 CNN(AlexConvNet)+multiscale pooling [40] 68.9 MOP-CNN [19] 68.9 D-CNN-VD [8] 81.1 Ours 82.4 5.4 Generic Classification We have also tested our system on the ImageNet LargeScale Visual Recognition Challenge (ILSVRC), a popular dataset for generic object classification. Only th"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "d by Gong et al. extracts CNN activations for local patches at multiple scale levels, and then performs orderless VLAD encoding. Finally, they train a linear SVMs classifier. The previous best result [8] is obtained by Fisher Vector pooling of a deep CNN filter bank. In contrast, we only fine-tune our pre-trained model with our aggressive data augmentation methods and use the softmax layer as the cla"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "fferent from outdoor scenes, it has large variability across different samples within each category. The work [39] created a dataset of 67 indoor scenes categories, which was used by many researchers [8, 15, 19, 26, 40]. Quattoni et al. propose a scene model which uses image prototypes to define a mapping between images and scene labels. Juneja et al. [26] present an automatic discovery of distinctive parts for an o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195347837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6afd65d1b72e8726f4ae19367a399da1de506edf",
            "isKey": true,
            "numCitedBy": 51,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture at- tributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. Motivated by the challenge posed by this problem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank. D-CNN substantially improves the state-of-the-art in texture, mate- rial and scene recognition. Our approach achieves 82.3% accuracy on Flickr material dataset and 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than 10% over existing approaches. D-CNN easily trans- fers across domains without requiring feature adaptation as for methods that build on the fully-connected layers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scale information and describe regions of arbitrary shapes and sizes. Our approach is particularly suited at lo- calizing stuff categories and obtains state-of-the-art re- sults on MSRC segmentation dataset, as well as promising results on recognizing materials and surface attributes in clutter on the OpenSurfaces dataset."
            },
            "slug": "Deep-convolutional-filter-banks-for-texture-and-Cimpoi-Maji",
            "title": {
                "fragments": [],
                "text": "Deep convolutional filter banks for texture recognition and segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a new texture descriptor, D-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank, which substantially improves the state-of-the-art in texture, mate- rial and scene recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 13
                            }
                        ],
                        "text": "Inspired by (Sermanet et al., 2013), a transformed network with only convolutional layers is used to test images and the class scores from the feature map of the last layer are averaged to attain the final score."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5507caf1210b6bfbd04fe02b2669bc14292e23a1",
            "isKey": false,
            "numCitedBy": 4352,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143781496"
                        ],
                        "name": "Ke Yang",
                        "slug": "Ke-Yang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "iminative classifiers. Due to the huge amount of computation, a lot of efforts have been made to build a distributed system to scale the network to very large models. Dean et al. developed DistBelief [14] to train a deep network with billions of parameters using tens of thousands of CPU cores. Within this framework, they developed asynchronous stochastic gradient descent procedure. Coates et al. built"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 372467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "isKey": false,
            "numCitedBy": 3026,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."
            },
            "slug": "Large-Scale-Distributed-Deep-Networks-Dean-Corrado",
            "title": {
                "fragments": [],
                "text": "Large Scale Distributed Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper considers the problem of training a deep network with billions of parameters using tens of thousands of CPU cores and develops two algorithms for large-scale distributed training, Downpour SGD and Sandblaster L-BFGS, which increase the scale and speed of deep network training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "h as different types of birds and flowers, which is different from generic classification. Much work has been devoted on the learning of object parts that benefits from the alignment and segmentation [1, 8, 6, 18]. Current state-of-the-art results for bird species recognition have been achieved in [4], where the strength of deep feature learning is integrated with part modeling. Most of these methods need extr"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN [4] \u221a \u221a 75.7 DPD [53] \u221a \u221a \u221a 51.0 POOF [3] \u221a \u221a \u221a 56.8 Nonparametric [18] \u221a \u221a \u221a 57.8 Symbiotic [6] \u221a \u221a \u221a 61.0 DPD+DeCAF [16] \u221a \u221a \u221a 65.0 PB R-CNN [52] \u221a \u221a \u221a 76.4 Symbiotic [6] \u221a \u221a \u221a \u221a 69.5 POOF [3] \u221a \u221a \u221a \u221a 73.3 PB R-CNN [52] \u221a \u221a \u221a \u221a 82.0 PN-DCN [4] \u221a \u221a \u221a \u221a 85.4 (a) (b) Figu"
                    },
                    "intents": []
                }
            ],
            "corpusId": 215827080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "isKey": true,
            "numCitedBy": 17075,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
            },
            "slug": "Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue",
            "title": {
                "fragments": [],
                "text": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39102205"
                        ],
                        "name": "Tianjun Xiao",
                        "slug": "Tianjun-Xiao",
                        "structuredName": {
                            "firstName": "Tianjun",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianjun Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2636690"
                        ],
                        "name": "Yichong Xu",
                        "slug": "Yichong-Xu",
                        "structuredName": {
                            "firstName": "Yichong",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichong Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976163"
                        ],
                        "name": "Kuiyuan Yang",
                        "slug": "Kuiyuan-Yang",
                        "structuredName": {
                            "firstName": "Kuiyuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuiyuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2478181"
                        ],
                        "name": "Jiaxing Zhang",
                        "slug": "Jiaxing-Zhang",
                        "structuredName": {
                            "firstName": "Jiaxing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaxing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753918"
                        ],
                        "name": "Yuxin Peng",
                        "slug": "Yuxin-Peng",
                        "structuredName": {
                            "firstName": "Yuxin",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxin Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38448016"
                        ],
                        "name": "Zheng Zhang",
                        "slug": "Zheng-Zhang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "% by adding bounding box annotations only at test time. Table 2. Species categorization results on CUB-200-2011 Method Training Testing Accura BBox Part BBox Part cy(%) Alignments [17] 53.6 Attention [49] 77.9 B-CNN (D,M) [30] 80.9 Part discovery [44] 81.0 Ours 84.9 GMTL [38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6725492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d5fd29a02f7fc50f7c36f1a74563d529620aa66",
            "isKey": true,
            "numCitedBy": 607,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200 2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations."
            },
            "slug": "The-application-of-two-level-attention-models-in-Xiao-Xu",
            "title": {
                "fragments": [],
                "text": "The application of two-level attention models in deep convolutional neural network for fine-grained image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to apply visual attention to fine-grained classification task using deep neural network and achieves the best accuracy under the weakest supervision condition, and is competitive against other methods that rely on additional annotations."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144479015"
                        ],
                        "name": "Awni Y. Hannun",
                        "slug": "Awni-Y.-Hannun",
                        "structuredName": {
                            "firstName": "Awni",
                            "lastName": "Hannun",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Awni Y. Hannun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065131508"
                        ],
                        "name": "Carl Case",
                        "slug": "Carl-Case",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Case",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Case"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48991386"
                        ],
                        "name": "J. Casper",
                        "slug": "J.-Casper",
                        "structuredName": {
                            "firstName": "Jared",
                            "lastName": "Casper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Casper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040049"
                        ],
                        "name": "G. Diamos",
                        "slug": "G.-Diamos",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Diamos",
                            "middleNames": [
                                "Frederick"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Diamos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152585800"
                        ],
                        "name": "Erich Elsen",
                        "slug": "Erich-Elsen",
                        "structuredName": {
                            "firstName": "Erich",
                            "lastName": "Elsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erich Elsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3283879"
                        ],
                        "name": "R. Prenger",
                        "slug": "R.-Prenger",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Prenger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prenger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264597"
                        ],
                        "name": "Shubho Sengupta",
                        "slug": "Shubho-Sengupta",
                        "structuredName": {
                            "firstName": "Shubho",
                            "lastName": "Sengupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shubho Sengupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "day, history is repeating itself: simple, scalable algorithms, given enough data and computational resources, dominate many fields, including visual object recognition [9, 27, 48], speech recognition [13, 21] and natural language processing [11, 32, 33]. Although neural networks have been studied for many decades, only recently have they come into their own, thanks to the availability of larger training d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 166
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16979536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24741d280869ad9c60321f5ab6e5f01b7852507d",
            "isKey": false,
            "numCitedBy": 1526,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems."
            },
            "slug": "Deep-Speech:-Scaling-up-end-to-end-speech-Hannun-Case",
            "title": {
                "fragments": [],
                "text": "Deep Speech: Scaling up end-to-end speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Deep Speech, a state-of-the-art speech recognition system developed using end-to-end deep learning, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 47
                            }
                        ],
                        "text": "Most previous work (Krizhevsky et al., 2012), (Zeiler & Fergus, 2014) operates on downsized images with a resolution of approximately 256x256."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 162
                            }
                        ],
                        "text": "Following the success of SuperVision, the winner of the classification task in ILSVRC 2013 was Clarifai, which was designed by using the visualization technique (Zeiler & Fergus, 2014) to guide the adjustment of the network architectures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 42
                            }
                        ],
                        "text": "Previous work (Krizhevsky et al., 2012), (Zeiler & Fergus, 2014) usually downsizes the images to a fixed resolution, such as 256x256, and then randomly crops out slightly smaller areas, such as 224x224, and uses those crops for training."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3960646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
            "isKey": false,
            "numCitedBy": 11803,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
            },
            "slug": "Visualizing-and-Understanding-Convolutional-Zeiler-Fergus",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 98
                            }
                        ],
                        "text": "All the models are trained using standard training techniques and mostly default hyper-parameters [22, 25, 45, 46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13740328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "isKey": false,
            "numCitedBy": 12372,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset."
            },
            "slug": "Delving-Deep-into-Rectifiers:-Surpassing-on-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit and derives a robust initialization method that particularly considers the rectifier nonlinearities."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2622491"
                        ],
                        "name": "Hossein Azizpour",
                        "slug": "Hossein-Azizpour",
                        "structuredName": {
                            "firstName": "Hossein",
                            "lastName": "Azizpour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hossein Azizpour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2835963"
                        ],
                        "name": "A. Razavian",
                        "slug": "A.-Razavian",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Razavian",
                            "middleNames": [
                                "Sharif"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Razavian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50626295"
                        ],
                        "name": "J. Sullivan",
                        "slug": "J.-Sullivan",
                        "structuredName": {
                            "firstName": "Josephine",
                            "lastName": "Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sullivan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801052"
                        ],
                        "name": "A. Maki",
                        "slug": "A.-Maki",
                        "structuredName": {
                            "firstName": "Atsuto",
                            "lastName": "Maki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Maki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153120475"
                        ],
                        "name": "S. Carlsson",
                        "slug": "S.-Carlsson",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Carlsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carlsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1226440,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbb684f6a0f95cfd0ddbe324ab476d8e95613cb0",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Evidence is mounting that ConvNets are the best representation learning method for recognition. In the common scenario, a ConvNet is trained on a large labeled dataset and the feed-forward units activation, at a certain layer of the network, is used as a generic representation of an input image. Recent studies have shown this form of representation to be astoundingly effective for a wide range of recognition tasks. This paper thoroughly investigates the transferability of such representations w.r.t. several factors. It includes parameters for training the network such as its architecture and parameters of feature extraction. We further show that different visual recognition tasks can be categorically ordered based on their distance from the source task. We then show interesting results indicating a clear correlation between the performance of tasks and their distance from the source task conditioned on proposed factors. Furthermore, by optimizing these factors, we achieve state-of-the-art performances on 16 visual recognition tasks."
            },
            "slug": "From-generic-to-specific-deep-representations-for-Azizpour-Razavian",
            "title": {
                "fragments": [],
                "text": "From generic to specific deep representations for visual recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper thoroughly investigates the transferability of ConvNet representations w.r.t. several factors, and shows that different visual recognition tasks can be categorically ordered based on their distance from the source task."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49675814"
                        ],
                        "name": "Marcel Simon",
                        "slug": "Marcel-Simon",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcel Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679449"
                        ],
                        "name": "E. Rodner",
                        "slug": "E.-Rodner",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Rodner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rodner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Table 3. Species categorization results on Oxford flowers Method Accuracy(%) Angelova et al. [1] 80.66 Murray et al. [34] 84.60 Razavian et al. [40] 86.80 Azizpour et al. [2] 91.30 No parts (AlexNet) [44] 90.35 Const. (AlexNet) [44] 91.74 No parts (VGG19) [44] 93.07 Const. (VGG19) [44] 95.34 Ours w/o seg 98.73 Oxford-IIIT Pets The Oxford- IIIT Pets dataset [37] is created by Visual Geometry Group (VGG"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "st time. Table 2. Species categorization results on CUB-200-2011 Method Training Testing Accura BBox Part BBox Part cy(%) Alignments [17] 53.6 Attention [49] 77.9 B-CNN (D,M) [30] 80.9 Part discovery [44] 81.0 Ours 84.9 GMTL [38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN [4] \u221a \u221a 75.7 DPD [53] \u221a \u221a \u221a 51.0 POOF [3] \u221a \u221a "
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "vantage of large models and data augmentation techniques, we have achieved state-of-the-art results using only our end-to-end deep learning system on four popular fine-grained classification datasets [44]2 . The only exception is for CUB-200-2011 dataset, where bounding box at test time was used. CUB-200-2011 CUB-200-2011 [47] is a 200 bird species recognition dataset which contains 11,788 images for "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the color, shape, patterns on the petal etc. The dataset is divided into a training set (6,149 images), a validation set (1,020 images) and a test set (1,020 images). The best result published so far [44] was heavily reliant on part model. Their best part selection scheme achieves 95.34% mean accuracy using VGG 19-layer model. The best result without part information is 93.07% [44]. We fine-tune all l"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "lgorithm for part normalization. It achieves a state-of-the-art 85.4% accuracy with both parts and bounding box annotations, and 75.7% accuracy with an unsupervised part model. Work from Simon et al. [44] uses CNN based unsupervised part model to achieve 81.0% accuracy. Lin et al. [30] directly use one CNN for parts detector and another CNN for classify the parts. It achieves 80.9% and 81.9% accuracy "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10605093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e704f7b9f0b7218715888b7b6f23960245588d02",
            "isKey": true,
            "numCitedBy": 347,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios."
            },
            "slug": "Neural-Activation-Constellations:-Unsupervised-Part-Simon-Rodner",
            "title": {
                "fragments": [],
                "text": "Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An approach is presented that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning, to find constellations of neural activation patterns computed using convolutional neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 60
                            }
                        ],
                        "text": "Similar strategies have been proposed in the previous work (Krizhevsky, 2014), (Yadan et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 46
                            }
                        ],
                        "text": "This is similar to the approach presented in (Krizhevsky, 2014), even though we are doing it in a scaled up fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 4
                            }
                        ],
                        "text": "In (Krizhevsky, 2014) and (Yadan et al., 2013), the authors also employed data and model parallelism or hybrid parallelization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5556470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80d800dfadbe2e6c7b2367d9229cc82912d55889",
            "isKey": false,
            "numCitedBy": 883,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks."
            },
            "slug": "One-weird-trick-for-parallelizing-convolutional-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "One weird trick for parallelizing convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new way to parallelize the training of convolutional neural networks across multiple GPUs is presented, which scales significantly better than all alternatives when applied to modern convolutionAL neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5808102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d376d6978dad0374edfa6709c9556b42d3594d3",
            "isKey": false,
            "numCitedBy": 29218,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
            },
            "slug": "Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy",
            "title": {
                "fragments": [],
                "text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279670"
                        ],
                        "name": "Andrea Frome",
                        "slug": "Andrea-Frome",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Frome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Frome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2892780"
                        ],
                        "name": "Esteban Real",
                        "slug": "Esteban-Real",
                        "structuredName": {
                            "firstName": "Esteban",
                            "lastName": "Real",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Esteban Real"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18882526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ef259c2f6d50373abfec14fcb8fa924f7b7af0b",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input."
            },
            "slug": "Attention-for-Fine-Grained-Categorization-Sermanet-Frome",
            "title": {
                "fragments": [],
                "text": "Attention for Fine-Grained Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set using an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570381"
                        ],
                        "name": "Brody Huval",
                        "slug": "Brody-Huval",
                        "structuredName": {
                            "firstName": "Brody",
                            "lastName": "Huval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brody Huval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156632012"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25629078"
                        ],
                        "name": "David J. Wu",
                        "slug": "David-J.-Wu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 276
                            }
                        ],
                        "text": "\u2026recently have they come into their own, thanks to the availability of larger training data sets along\nAll the authors are with Baidu Research, Baidu, Inc. Ren Wu (wuren@baidu.com) is the corresponding author.\nwith increased computation power through heterogeneous computing (Coates et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 173
                            }
                        ],
                        "text": "A more recent example shows that three GPU servers with 4 GPUs each, rival the same performance of a 1000 nodes (16000 cores) CPU cluster, used by the Google Brain project (Coates et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8604637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95",
            "isKey": false,
            "numCitedBy": 674,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks."
            },
            "slug": "Deep-learning-with-COTS-HPC-systems-Coates-Huval",
            "title": {
                "fragments": [],
                "text": "Deep learning with COTS HPC systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents technical details and results from their own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI, and shows that it can scale to networks with over 11 billion parameters using just 16 machines."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786693"
                        ],
                        "name": "Carl Doersch",
                        "slug": "Carl-Doersch",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Doersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Doersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ow more examples to the model. We achieve accuracy of 82.4% using multi-scale testing. Table 6. Accuracy comparison of different methods on MIT-67 indoor scenes Method Accuracy(%) IFV [26] 60.8 MLrep [15] 64.0 CNN-SVM [40] 58.4 CNNaug-SVM [40] 69.0 CNN(AlexConvNet)+multiscale pooling [40] 68.9 MOP-CNN [19] 68.9 D-CNN-VD [8] 81.1 Ours 82.4 5.4 Generic Classification We have also tested our system on th"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "s well on this scene dataset. Doersch et al. pose a mid-level visual element discovery as discriminative mode seeking and evaluate this method on the task of scene recognition, outperforming the work [15]. Different from these work, several work uses CNN to address the indoor scene recognition. The published work [40] using a CNN off-theshelf representation with linear SVMs training improves the basel"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "fferent from outdoor scenes, it has large variability across different samples within each category. The work [39] created a dataset of 67 indoor scenes categories, which was used by many researchers [8, 15, 19, 26, 40]. Quattoni et al. propose a scene model which uses image prototypes to define a mapping between images and scene labels. Juneja et al. [26] present an automatic discovery of distinctive parts for an o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6334137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b222b3b05d8d313420dbde8b163e4336a85dcde9",
            "isKey": true,
            "numCitedBy": 268,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical \"visual words\", but lower than full-blown semantic objects. Several approaches [5,6,12,23] have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difficult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the MIT Scene-67 dataset."
            },
            "slug": "Mid-level-Visual-Element-Discovery-as-Mode-Seeking-Doersch-Gupta",
            "title": {
                "fragments": [],
                "text": "Mid-level Visual Element Discovery as Discriminative Mode Seeking"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Given a weakly-labeled image collection, this method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels, and proposes the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "\u2026itself: simple, scalable algorithms, given enough data and computational resources, dominate many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1918673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
            "isKey": false,
            "numCitedBy": 875,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning."
            },
            "slug": "Deep,-Big,-Simple-Neural-Nets-for-Handwritten-Digit-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31872381"
                        ],
                        "name": "P. Gosselin",
                        "slug": "P.-Gosselin",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Gosselin",
                            "middleNames": [
                                "Henri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gosselin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26734366"
                        ],
                        "name": "N. Murray",
                        "slug": "N.-Murray",
                        "structuredName": {
                            "firstName": "Naila",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681054"
                        ],
                        "name": "H. J\u00e9gou",
                        "slug": "H.-J\u00e9gou",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "J\u00e9gou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J\u00e9gou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "4%. FGVC-Aircraft The FGVC-aircraft dataset [31] consists of 10,000 images of 100 aircraft variants, which is a part of the fine-grained recognition challenge FGComp 2013. The method in previous work [20] is an ensemble approach based on a heavily engineered version of FV-SIFT. They achieve mean per-class accuracy of 80.74% when using the provided bounding boxes information during the training stage. "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "d to model \u201cwhere\u201d parts are in an image and another is used to model \u201cwhat\u201d the parts look like. The outputs of the two networks are combined bilinearly. We use the same data split strategy as paper [20] on this dataset. By fine-tuning all layers of our 19-layer model with our data augmentation approaches, we improve the mean per-class accuracy to 85.17%, setting the new record without using any boun"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "rmation. Table 5. Species categorization results on aircraft variant Method Accuracy(%) B-CNN(M,M), w/ ft [30] 73.5 B-CNN(D,M), w/ ft [30] 79.4 Philippe et al. track2 [20] 80.7 Philippe et al. track1 [20] 81.5 Ours 85.2 5.3 Scene Recognition Indoor scene recognition is another challenging open problem in vision, namely place categorization. The location of meaningful regions and objects varies drastic"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2436160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4589a778eab11277ac7e9cd3e78e68c56aa34ea3",
            "isKey": true,
            "numCitedBy": 124,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Revisiting-the-Fisher-vector-for-fine-grained-Gosselin-Murray",
            "title": {
                "fragments": [],
                "text": "Revisiting the Fisher vector for fine-grained classification"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11710343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98bb60748eb8ef7a671cdd22faa87e377fd13060",
            "isKey": false,
            "numCitedBy": 925,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time."
            },
            "slug": "Part-Based-R-CNNs-for-Fine-Grained-Category-Zhang-Donahue",
            "title": {
                "fragments": [],
                "text": "Part-Based R-CNNs for Fine-Grained Category Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a model for fine-grained categorization that overcomes limitations by leveraging deep convolutional features computed on bottom-up region proposals, and learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine- grained category from a pose-normalized representation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053402806"
                        ],
                        "name": "Thomas Berg",
                        "slug": "Thomas-Berg",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Berg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ery [44] 81.0 Ours 84.9 GMTL [38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN [4] \u221a \u221a 75.7 DPD [53] \u221a \u221a \u221a 51.0 POOF [3] \u221a \u221a \u221a 56.8 Nonparametric [18] \u221a \u221a \u221a 57.8 Symbiotic [6] \u221a \u221a \u221a 61.0 DPD+DeCAF [16] \u221a \u221a \u221a 65.0 PB R-CNN [52] \u221a \u221a \u221a 76.4 Symbiotic [6] \u221a \u221a \u221a \u221a 69.5 POOF [3] \u221a \u221a \u221a \u221a 73.3 PB R-CNN [52] \u221a \u221a \u221a \u221a 82.0 PN-DCN"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14125992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23efd4b0aef1ae0b356fe88141da085526ed3df0",
            "isKey": true,
            "numCitedBy": 338,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "From a set of images in a particular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs.-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce."
            },
            "slug": "POOF:-Part-Based-One-vs.-One-Features-for-Face-and-Berg-Belhumeur",
            "title": {
                "fragments": [],
                "text": "POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A method to automatically learn a large and diverse set of highly discriminative intermediate features that are called Part-based One-vs-One Features (POOFs), each of these features specializes in discrimination between two particular classes based on the appearance at a particular part."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26734366"
                        ],
                        "name": "N. Murray",
                        "slug": "N.-Murray",
                        "structuredName": {
                            "firstName": "Naila",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "in which an 88.76% mean accuracy on 37 classes is achieved. Table 4. Species categorization results on Oxford-IIIT pets Method Accuracy(%) Bo et al. [42] 53.40 Angelova et al. [1] 54.30 Murray et al. [34] 56.80 Azizpour et al. [2] 88.10 No parts (AlexNet) [44] 78.55 Const. (AlexNet) [44] 85.20 No parts (VGG19) [44] 88.76 Const. (VGG19) [44] 91.60 Ours 93.10 Different from adopting the part model in pa"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " can achieve accuracy of 96.1%, still better than best published result of 95.34%. Table 3. Species categorization results on Oxford flowers Method Accuracy(%) Angelova et al. [1] 80.66 Murray et al. [34] 84.60 Razavian et al. [40] 86.80 Azizpour et al. [2] 91.30 No parts (AlexNet) [44] 90.35 Const. (AlexNet) [44] 91.74 No parts (VGG19) [44] 93.07 Const. (VGG19) [44] 95.34 Ours w/o seg 98.73 Oxford-II"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 10618410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31da0fffce755a17759c059e9badb6a8557cb587",
            "isKey": true,
            "numCitedBy": 155,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art patch-based image representations involve a pooling operation that aggregates statistics computed from local descriptors. Standard pooling operations include sum- and max-pooling. Sum-pooling lacks discriminability because the resulting representation is strongly influenced by frequent yet often uninformative descriptors, but only weakly influenced by rare yet potentially highly-informative ones. Max-pooling equalizes the influence of frequent and rare descriptors but is only applicable to representations that rely on count statistics, such as the bag-of-visual-words (BOV)and its soft- and sparse-coding extensions. We propose a novel pooling mechanism that achieves the same effect as max-pooling but is applicable beyond the BOV and especially to the state-of-the-art Fisher Vector -- hence the name Generalized Max Pooling (GMP). It involves equalizing the similarity between each patch and the pooled representation, which is shown to be equivalent to re-weighting the per-patch statistics. We show on five public image classification benchmarks that the proposed GMP can lead to significant performance gains with respect to heuristic alternatives."
            },
            "slug": "Generalized-Max-Pooling-Murray-Perronnin",
            "title": {
                "fragments": [],
                "text": "Generalized Max Pooling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel pooling mechanism that achieves the same effect as max-pooling but is applicable beyond the BOV and especially to the state-of-the-art Fisher Vector -- hence the name Generalized Max Pooling (GMP)."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367820"
                        ],
                        "name": "C. Wah",
                        "slug": "C.-Wah",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Wah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3251767"
                        ],
                        "name": "Steve Branson",
                        "slug": "Steve-Branson",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Branson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Branson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2930640"
                        ],
                        "name": "P. Welinder",
                        "slug": "P.-Welinder",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Welinder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Welinder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " trained several models with the images of different scales and they are complementary. For example, two models pre-trained using ILSVRC [41] training images and fine-tuned using CUB-200-2011 dataset [47], at the resolution of 256x256 and 512x512 respectively, could give a top-1 accuracy rate of 83.7% and 81.6% for the test set, but the fused model by simple averaging gives an error rate of 84.9% - an"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "d deep learning system on four popular fine-grained classification datasets [44]2 . The only exception is for CUB-200-2011 dataset, where bounding box at test time was used. CUB-200-2011 CUB-200-2011 [47] is a 200 bird species recognition dataset which contains 11,788 images for both training and 2 The Stanford dogs dataset is a subset of ImageNet. There are some test images appearing in the ImageNet "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16119123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c069629a51f6c1c301eb20ed77bc6b586c24ce32",
            "isKey": true,
            "numCitedBy": 2636,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization."
            },
            "slug": "The-Caltech-UCSD-Birds-200-2011-Dataset-Wah-Branson",
            "title": {
                "fragments": [],
                "text": "The Caltech-UCSD Birds-200-2011 Dataset"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428157"
                        ],
                        "name": "R. Wu",
                        "slug": "R.-Wu",
                        "structuredName": {
                            "firstName": "Ren",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48335600"
                        ],
                        "name": "Bin Zhang",
                        "slug": "Bin-Zhang",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49975266"
                        ],
                        "name": "M. Hsu",
                        "slug": "M.-Hsu",
                        "structuredName": {
                            "firstName": "Meichun",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 199
                            }
                        ],
                        "text": "Early work shows that for clustering algorithms, a single GPU offers 10x more performance than top-of-the-line 8 cores workstations, even on very large datasets with more than a billion data points (Wu et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14210790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "273da9706996ae3543fc28095d7feabf5460c067",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we report our research on using GPUs to accelerate clustering of very large data sets, which are common in today's real world applications. While many published works have shown that GPUs can be used to accelerate various general purpose applications with respectable performance gains, few attempts have been made to tackle very large problems. Our goal here is to investigate if GPUs can be useful accelerators even with very large data sets that cannot fit into GPU's onboard memory.\n Using a popular clustering algorithm, K-Means, as an example, our results have been very positive. On a data set with a billion data points, our GPU-accelerated implementation achieved an order of magnitude performance gain over a highly optimized CPU-only version running on 8 cores, and more than two orders of magnitude gain over a popular benchmark, MineBench, running on a single core."
            },
            "slug": "Clustering-billions-of-data-points-using-GPUs-Wu-Zhang",
            "title": {
                "fragments": [],
                "text": "Clustering billions of data points using GPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper investigates if GPUs can be useful accelerators even with very large data sets that cannot fit into GPU's onboard memory, and uses a popular clustering algorithm, K-Means, as an example."
            },
            "venue": {
                "fragments": [],
                "text": "UCHPC-MAW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2304222"
                        ],
                        "name": "E. Gavves",
                        "slug": "E.-Gavves",
                        "structuredName": {
                            "firstName": "Efstratios",
                            "lastName": "Gavves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gavves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688071"
                        ],
                        "name": "Basura Fernando",
                        "slug": "Basura-Fernando",
                        "structuredName": {
                            "firstName": "Basura",
                            "lastName": "Fernando",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Basura Fernando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145404204"
                        ],
                        "name": "Cees G. M. Snoek",
                        "slug": "Cees-G.-M.-Snoek",
                        "structuredName": {
                            "firstName": "Cees",
                            "lastName": "Snoek",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cees G. M. Snoek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ved accuracy of 85.6% by adding bounding box annotations only at test time. Table 2. Species categorization results on CUB-200-2011 Method Training Testing Accura BBox Part BBox Part cy(%) Alignments [17] 53.6 Attention [49] 77.9 B-CNN (D,M) [30] 80.9 Part discovery [44] 81.0 Ours 84.9 GMTL [38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8052866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0205405ca59aaf38c529f62c4718f5b22c1480dc",
            "isKey": true,
            "numCitedBy": 78,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape. Then, one may proceed to the classification by examining the corresponding regions of the alignments. More specifically, the alignments are used to transfer part annotations from training images to unseen images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We further argue that for the distinction of sub-classes, distribution-based features like color Fisher vectors are better suited for describing localized appearance of fine-grained categories than popular matching oriented shape-sensitive features, like HOG. They allow capturing the subtle local differences between subclasses, while at the same time being robust to misalignments between distinctive details. We evaluate the local alignments on the CUB-2011 and on the Stanford Dogs datasets, composed of 200 and 120, visually very hard to distinguish bird and dog species. In our experiments we study and show the benefit of the color Fisher vector parameterization, the influence of the alignment partitioning, and the significance of object segmentation on fine-grained categorization. We, furthermore, show that by using object detectors as voters to generate object confidence saliency maps, we arrive at fully unsupervised, yet highly accurate fine-grained categorization. The proposed local alignments set a new state-of-the-art on both the fine-grained birds and dogs datasets, even without any human intervention. What is more, the local alignments reveal what appearance details are most decisive per fine-grained object category."
            },
            "slug": "Local-Alignments-for-Fine-Grained-Categorization-Gavves-Fernando",
            "title": {
                "fragments": [],
                "text": "Local Alignments for Fine-Grained Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "By using object detectors as voters to generate object confidence saliency maps, the proposed local alignments set a new state-of-the-art on both the fine-grained birds and dogs datasets, even without any human intervention."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34858254"
                        ],
                        "name": "Yuning Chai",
                        "slug": "Yuning-Chai",
                        "structuredName": {
                            "firstName": "Yuning",
                            "lastName": "Chai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuning Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740145"
                        ],
                        "name": "V. Lempitsky",
                        "slug": "V.-Lempitsky",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lempitsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lempitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3258810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a26477bd1e302219a065eea16495566ead2cede",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method for the task of fine-grained visual categorization. The method builds a model of the base-level category that can be fitted to images, producing high-quality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful Grab Cut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently."
            },
            "slug": "Symbiotic-Segmentation-and-Part-Localization-for-Chai-Lempitsky",
            "title": {
                "fragments": [],
                "text": "Symbiotic Segmentation and Part Localization for Fine-Grained Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The model builds a model of the base-level category that can be fitted to images, producing high-quality foreground segmentation and mid-level part localizations, and improves the categorization accuracy over the state-of-the-art."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827962"
                        ],
                        "name": "Esa Rahtu",
                        "slug": "Esa-Rahtu",
                        "structuredName": {
                            "firstName": "Esa",
                            "lastName": "Rahtu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Esa Rahtu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776374"
                        ],
                        "name": "Juho Kannala",
                        "slug": "Juho-Kannala",
                        "structuredName": {
                            "firstName": "Juho",
                            "lastName": "Kannala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juho Kannala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2118703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "522d65a3db7431015aeaa201a7fc4450a57e40c3",
            "isKey": false,
            "numCitedBy": 786,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding."
            },
            "slug": "Fine-Grained-Visual-Classification-of-Aircraft-Maji-Rahtu",
            "title": {
                "fragments": [],
                "text": "Fine-Grained Visual Classification of Aircraft"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable, however, they present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145426908"
                        ],
                        "name": "A. Angelova",
                        "slug": "A.-Angelova",
                        "structuredName": {
                            "firstName": "Anelia",
                            "lastName": "Angelova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Angelova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682028"
                        ],
                        "name": "Shenghuo Zhu",
                        "slug": "Shenghuo-Zhu",
                        "structuredName": {
                            "firstName": "Shenghuo",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenghuo Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "h as different types of birds and flowers, which is different from generic classification. Much work has been devoted on the learning of object parts that benefits from the alignment and segmentation [1, 8, 6, 18]. Current state-of-the-art results for bird species recognition have been achieved in [4], where the strength of deep feature learning is integrated with part modeling. Most of these methods need extr"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "e last softmax layer, we can achieve accuracy of 96.1%, still better than best published result of 95.34%. Table 3. Species categorization results on Oxford flowers Method Accuracy(%) Angelova et al. [1] 80.66 Murray et al. [34] 84.60 Razavian et al. [40] 86.80 Azizpour et al. [2] 91.30 No parts (AlexNet) [44] 90.35 Const. (AlexNet) [44] 91.74 No parts (VGG19) [44] 93.07 Const. (VGG19) [44] 95.34 Our"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "thout using part model, in which an 88.76% mean accuracy on 37 classes is achieved. Table 4. Species categorization results on Oxford-IIIT pets Method Accuracy(%) Bo et al. [42] 53.40 Angelova et al. [1] 54.30 Murray et al. [34] 56.80 Azizpour et al. [2] 88.10 No parts (AlexNet) [44] 78.55 Const. (AlexNet) [44] 85.20 No parts (VGG19) [44] 88.76 Const. (VGG19) [44] 91.60 Ours 93.10 Different from adop"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9527101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21256be13869da1c98160e3498209daa6497d99c",
            "isKey": true,
            "numCitedBy": 212,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also `zoom in' on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging large-scale flower dataset, containing 578 species of flowers and 250,000 images."
            },
            "slug": "Efficient-Object-Detection-and-Segmentation-for-Angelova-Zhu",
            "title": {
                "fragments": [],
                "text": "Efficient Object Detection and Segmentation for Fine-Grained Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 220
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5023,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49791682"
                        ],
                        "name": "Ryan Farrell",
                        "slug": "Ryan-Farrell",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Farrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Farrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346186"
                        ],
                        "name": "Forrest N. Iandola",
                        "slug": "Forrest-N.-Iandola",
                        "structuredName": {
                            "firstName": "Forrest",
                            "lastName": "Iandola",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Forrest N. Iandola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[30] 80.9 Part discovery [44] 81.0 Ours 84.9 GMTL [38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN [4] \u221a \u221a 75.7 DPD [53] \u221a \u221a \u221a 51.0 POOF [3] \u221a \u221a \u221a 56.8 Nonparametric [18] \u221a \u221a \u221a 57.8 Symbiotic [6] \u221a \u221a \u221a 61.0 DPD+DeCAF [16] \u221a \u221a \u221a 65.0 PB R-CNN [52] \u221a \u221a \u221a 76.4 Symbiotic [6] \u221a \u221a \u221a \u221a 69.5 POOF [3] \u221a \u221a \u221a \u221a 73.3 PB R-CNN [52]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 8657174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19c9ac899d5c1a008eaee887556bc1b61ff8132e",
            "isKey": true,
            "numCitedBy": 228,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements of our approach over state-of-art algorithms."
            },
            "slug": "Deformable-Part-Descriptors-for-Fine-Grained-and-Zhang-Farrell",
            "title": {
                "fragments": [],
                "text": "Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models based on strongly-supervised DPM parts, which enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40417846"
                        ],
                        "name": "Mayank Juneja",
                        "slug": "Mayank-Juneja",
                        "structuredName": {
                            "firstName": "Mayank",
                            "lastName": "Juneja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mayank Juneja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "fferent from outdoor scenes, it has large variability across different samples within each category. The work [39] created a dataset of 67 indoor scenes categories, which was used by many researchers [8, 15, 19, 26, 40]. Quattoni et al. propose a scene model which uses image prototypes to define a mapping between images and scene labels. Juneja et al. [26] present an automatic discovery of distinctive parts for an o"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "again just to show more examples to the model. We achieve accuracy of 82.4% using multi-scale testing. Table 6. Accuracy comparison of different methods on MIT-67 indoor scenes Method Accuracy(%) IFV [26] 60.8 MLrep [15] 64.0 CNN-SVM [40] 58.4 CNNaug-SVM [40] 69.0 CNN(AlexConvNet)+multiscale pooling [40] 68.9 MOP-CNN [19] 68.9 D-CNN-VD [8] 81.1 Ours 82.4 5.4 Generic Classification We have also tested "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8763431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b730f5dfbd73172a4bba2d00d377a145c046bca",
            "isKey": true,
            "numCitedBy": 416,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previous part-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of-the-art classification performance on this data."
            },
            "slug": "Blocks-That-Shout:-Distinctive-Parts-for-Scene-Juneja-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Blocks That Shout: Distinctive Parts for Scene Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a simple, efficient, and effective method to learn parts incrementally, starting from a single part occurrence with an Exemplar SVM, and can learn parts which are significantly more informative and for a fraction of the cost, compared to previous part-learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "day, history is repeating itself: simple, scalable algorithms, given enough data and computational resources, dominate many fields, including visual object recognition [9, 27, 48], speech recognition [13, 21] and natural language processing [11, 32, 33]. Although neural networks have been studied for many decades, only recently have they come into their own, thanks to the availability of larger training d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 145
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14862572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6658bbf68995731b2083195054ff45b4eca38b3a",
            "isKey": false,
            "numCitedBy": 2677,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively."
            },
            "slug": "Context-Dependent-Pre-Trained-Deep-Neural-Networks-Dahl-Yu",
            "title": {
                "fragments": [],
                "text": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output that can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646632"
                        ],
                        "name": "M. Nilsback",
                        "slug": "M.-Nilsback",
                        "structuredName": {
                            "firstName": "Maria-Elena",
                            "lastName": "Nilsback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nilsback"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " rate of 84.9% - an even better result than each individual model. 5. Experiments We test our system\u2019s performance on several popular computer vision benchmarks: CUB-200-2011 [47], Oxford 102 Flowers [35], Oxford-IIIT Pets [37], FGVC-Aircraft [31], MIT indoor scene [39], and ILSVRC. The first four datasets are fine-grained recognition. The fifth is for the indoor scene recognition. And the last datase"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "OF [3] \u221a \u221a \u221a \u221a 73.3 PB R-CNN [52] \u221a \u221a \u221a \u221a 82.0 PN-DCN [4] \u221a \u221a \u221a \u221a 85.4 (a) (b) Figure 8: Basic configurations of our models: (a) 16 layers; (b) 19layers. Oxford 102 Flowers Oxford 102 flowers dataset [35] consists of 102 different categories of flowers with 8,189 images, which vary in scale, resolution, lighting clutter, quality, etc. Distinguishing features in flowers are the color, shape, patterns o"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15193013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02b28f3b71138a06e40dbd614abf8568420ae183",
            "isKey": true,
            "numCitedBy": 1800,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray, which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for the best single feature to 72.8% for the combination of all features."
            },
            "slug": "Automated-Flower-Classification-over-a-Large-Number-Nilsback-Zisserman",
            "title": {
                "fragments": [],
                "text": "Automated Flower Classification over a Large Number of Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for the best single feature to 72.8% forThe combination of all features."
            },
            "venue": {
                "fragments": [],
                "text": "2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143698722"
                        ],
                        "name": "Jian Pu",
                        "slug": "Jian-Pu",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Pu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717861"
                        ],
                        "name": "Yu-Gang Jiang",
                        "slug": "Yu-Gang-Jiang",
                        "structuredName": {
                            "firstName": "Yu-Gang",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Gang Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39811558"
                        ],
                        "name": "Jun Wang",
                        "slug": "Jun-Wang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905953"
                        ],
                        "name": "X. Xue",
                        "slug": "X.-Xue",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Xue"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " categorization results on CUB-200-2011 Method Training Testing Accura BBox Part BBox Part cy(%) Alignments [17] 53.6 Attention [49] 77.9 B-CNN (D,M) [30] 80.9 Part discovery [44] 81.0 Ours 84.9 GMTL [38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN [4] \u221a \u221a 75.7 DPD [53] \u221a \u221a \u221a 51.0 POOF [3] \u221a \u221a \u221a 56.8 Nonparametric [18]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6424243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b43e31b1823f84641464ad2de2d0b9e35ec728c",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Fine-grained visual categorization aims at classifying visual data at a subordinate level, e.g., identifying different species of birds. It is a highly challenging topic receiving significant research attention recently. Most existing works focused on the design of more discriminative feature representations to capture the subtle visual differences among categories. Very limited efforts were spent on the design of robust model learning algorithms. In this paper, we treat the training of each category classifier as a single learning task, and formulate a generic multiple task learning (MTL) framework to train multiple classifiers simultaneously. Different from the existing MTL methods, the proposed generic MTL algorithm enforces no structure assumptions and thus is more flexible in handling complex inter-class relationships. In particular, it is able to automatically discover both clusters of similar categories and outliers. We show that the objective of our generic MTL formulation can be solved using an iterative reweighted l2 method. Through an extensive experimental validation, we demonstrate that our method outperforms several state-of-the-art approaches."
            },
            "slug": "Which-Looks-Like-Which:-Exploring-Inter-class-in-Pu-Jiang",
            "title": {
                "fragments": [],
                "text": "Which Looks Like Which: Exploring Inter-class Relationships in Fine-Grained Visual Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper treats the training of each category classifier as a single learning task, and formulates a generic multiple task learning (MTL) framework to train multiple classifiers simultaneously, which is able to automatically discover both clusters of similar categories and outliers."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188342"
                        ],
                        "name": "O. Parkhi",
                        "slug": "O.-Parkhi",
                        "structuredName": {
                            "firstName": "Omkar",
                            "lastName": "Parkhi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Parkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " the classification model. We simply fine-tune the last three fully-connected layers of our best single model. The same as paper [44], the training and evaluation process follow the protocol of paper [37]. The learning rate starts from 0.001 and decreased to 0.0001. Together with our data augmentation techniques, 93.1% mean accuracy is achieved with the multi-scale testing. Compared with the previous "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "pour et al. [2] 91.30 No parts (AlexNet) [44] 90.35 Const. (AlexNet) [44] 91.74 No parts (VGG19) [44] 93.07 Const. (VGG19) [44] 95.34 Ours w/o seg 98.73 Oxford-IIIT Pets The Oxford- IIIT Pets dataset [37] is created by Visual Geometry Group (VGG). The dataset contains 37 classes, 7,349 images (each class with roughly 200 images). The best result on this dataset is the work by paper [44]. They fine-tun"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "n better result than each individual model. 5. Experiments We test our system\u2019s performance on several popular computer vision benchmarks: CUB-200-2011 [47], Oxford 102 Flowers [35], Oxford-IIIT Pets [37], FGVC-Aircraft [31], MIT indoor scene [39], and ILSVRC. The first four datasets are fine-grained recognition. The fifth is for the indoor scene recognition. And the last dataset is for generic classi"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 383200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84b50ebe85f7a1721800125e7882fce8c45b5c5a",
            "isKey": true,
            "numCitedBy": 638,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the fine grained object categorization problem of determining the breed of animal from an image. To this end we introduce a new annotated dataset of pets covering 37 different breeds of cats and dogs. The visual problem is very challenging as these animals, particularly cats, are very deformable and there can be quite subtle differences between the breeds. We make a number of contributions: first, we introduce a model to classify a pet breed automatically from an image. The model combines shape, captured by a deformable part model detecting the pet face, and appearance, captured by a bag-of-words model that describes the pet fur. Fitting the model involves automatically segmenting the animal in the image. Second, we compare two classification approaches: a hierarchical one, in which a pet is first assigned to the cat or dog family and then to a breed, and a flat one, in which the breed is obtained directly. We also investigate a number of animal and image orientated spatial layouts. These models are very good: they beat all previously published results on the challenging ASIRRA test (cat vs dog discrimination). When applied to the task of discriminating the 37 different breeds of pets, the models obtain an average accuracy of about 59%, a very encouraging result considering the difficulty of the problem."
            },
            "slug": "Cats-and-dogs-Parkhi-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Cats and dogs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "These models are very good: they beat all previously published results on the challenging ASIRRA test (cat vs dog discrimination) when applied to the task of discriminating the 37 different breeds of pets, and obtain an average accuracy of about 59%, a very encouraging result considering the difficulty of the problem."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10097073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
            "isKey": false,
            "numCitedBy": 938,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models."
            },
            "slug": "A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton",
            "title": {
                "fragments": [],
                "text": "A Scalable Hierarchical Distributed Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data are introduced and it is shown that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143903370"
                        ],
                        "name": "Murray Campbell",
                        "slug": "Murray-Campbell",
                        "structuredName": {
                            "firstName": "Murray",
                            "lastName": "Campbell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Murray Campbell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32498609"
                        ],
                        "name": "A. J. Hoane",
                        "slug": "A.-J.-Hoane",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Hoane",
                            "middleNames": [
                                "Joseph"
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Hoane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144844988"
                        ],
                        "name": "Feng-hsiung Hsu",
                        "slug": "Feng-hsiung-Hsu",
                        "structuredName": {
                            "firstName": "Feng-hsiung",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng-hsiung Hsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 131
                            }
                        ],
                        "text": "On May 11th, 1997, IBM\u2019s Deep Blue achieved a historic victory by defeating world chess champion Gary Kasparov in a six-game match (Campbell et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 662187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e4bc1aa55c752918ae99b1a125f6adef61afad2",
            "isKey": false,
            "numCitedBy": 946,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-Blue-Campbell-Hoane",
            "title": {
                "fragments": [],
                "text": "Deep Blue"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 248
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 577005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd7d93193aad6c4b71cc8942e808753019e87706",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models."
            },
            "slug": "Three-new-graphical-models-for-statistical-language-Mnih-Hinton",
            "title": {
                "fragments": [],
                "text": "Three new graphical models for statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1871661"
                        ],
                        "name": "Bradley C. Kuszmaul",
                        "slug": "Bradley-C.-Kuszmaul",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Kuszmaul",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bradley C. Kuszmaul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 169
                            }
                        ],
                        "text": "It came as a surprise to some, but with the correct algorithms, chess performance is a function of computational power (Condon & Thompson, 1982), (Hyatt et al., 1990), (Kuszmaul, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14158457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fbd597e1d563da70fb423c2b4ab71038a60b72d",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "The StarTech massively parallel chess program, running on a 512-processor Connection Machine CM-5 supercomputer, tied for third place at the 1993 ACM International Computer Chess Championship. StarTech employs the Jamboree search algorithm, a natural extension of J. Pearl\u2019s Scout search algorithm, to find parallelism in game-tree searches. StarTech\u2019s work-stealing scheduler distributes the work specified by the search algorithm across the processors of the CM-5. StarTech uses a global transposition table shared among the processors. StarTech has an informally estimated rating of over 2400 USCF. Two performance measures help in understanding the performance of the StarTech program: the work, , and the critical path length, . The Jamboree search algorithm used in StarTech seems to perform about 2 to 3 times more work than does our best serial implementation. The critical path length, under tournament conditions, is less than 0.1% of the total work, yielding an average parallelism of over 1000. The StarTech scheduler achieves actual performance of approximately 1 02 1 5 on processors. The critical path and work can be used to tune performance by allowing development of the program on a small, readily accessable, machine while predicting the performance on a big, tournament-sized, machine."
            },
            "slug": "The-STARTECH-Massively-Parallel-Chess-Program-Kuszmaul",
            "title": {
                "fragments": [],
                "text": "The STARTECH Massively-Parallel Chess Program"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Jamboree search algorithm used in StarTech seems to perform about 2 to 3 times more work than does the best serial implementation, and the critical path length can be used to tune performance by allowing development of the program on a small, readily accessable, machine while predicting the performance on a big, tournament-sized, machine."
            },
            "venue": {
                "fragments": [],
                "text": "J. Int. Comput. Games Assoc."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 131
                            }
                        ],
                        "text": "On May 11th, 1997, IBM\u2019s Deep Blue achieved a historic victory by defeating world chess champion Gary Kasparov in a six-game match (Campbell et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1028275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b886f2c097b635ee9550ca29fff7dcbbb7727ff7",
            "isKey": false,
            "numCitedBy": 5908,
            "numCiting": 271,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a survey of Artifici'al Intelligence (AI). It divides the field into four cor~ topics (embodying the base fo\u00b7r a science of intelligence) and eight applications topics (in which research has been contributing to core ideas).. The paper discusses the history, the major landmarks, and some of the controversies in each of these twelve topics. Each topic is represented by a chart citing the major references. These references are contained in an extensive bibliography. The paper concludes with a discussion of some of the criticisms of 'AI and with some predictions about the course of future research."
            },
            "slug": "Artificial-Intelligence-Nilsson",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The history, the major landmarks, and some of the controversies in each of these twelve topics are discussed, as well as some predictions about the course of future research."
            },
            "venue": {
                "fragments": [],
                "text": "IFIP Congress"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145216556"
                        ],
                        "name": "J. H. Condon",
                        "slug": "J.-H.-Condon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Condon",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Condon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1453698480"
                        ],
                        "name": "K. Thompson",
                        "slug": "K.-Thompson",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 120
                            }
                        ],
                        "text": "It came as a surprise to some, but with the correct algorithms, chess performance is a function of computational power (Condon & Thompson, 1982), (Hyatt et al., 1990), (Kuszmaul, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60819952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dd850cb594855dd561d98edff178554fa9de729",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The computer chess program BELLE is currently the World Computer Chess Champion and the North American Computer Chess Champion. In human play, BELLE has consistently obtained master performance ratings. This paper describes the special-purpose hardware that gives BELLE its advantage: speed."
            },
            "slug": "BELLE:-chess-hardware-Condon-Thompson",
            "title": {
                "fragments": [],
                "text": "BELLE: chess hardware"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The computer chess program BELLE is currently the World Computer Chess Champion and the North American Computer Chess Champions and the special-purpose hardware that gives it its advantage: speed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3251767"
                        ],
                        "name": "Steve Branson",
                        "slug": "Steve-Branson",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Branson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Branson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2996914"
                        ],
                        "name": "G. Horn",
                        "slug": "G.-Horn",
                        "structuredName": {
                            "firstName": "Grant",
                            "lastName": "Horn",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "are pre-trained on ILSVRC and then fine-tuned of all the layers with CUB-200-2011 dataset at the resolution of 256x256 and 512x512 respectively. The result is better than all previous work except one [4], despite the fact that no part or bounding box annotations were used. We achieved accuracy of 85.6% by adding bounding box annotations only at test time. Table 2. Species categorization results on CU"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "77.9 B-CNN (D,M) [30] 80.9 Part discovery [44] 81.0 Ours 84.9 GMTL [38] \u221a \u221a 44.2 Symbiotic [6] \u221a \u221a 59.4 Alignments [17] \u221a \u221a 67.0 B-CNN (D,M) [30] \u221a 81.9 Ours+BBox \u221a 85.6 PB R-CNN [52] \u221a \u221a 73.9 PN-DCN [4] \u221a \u221a 75.7 DPD [53] \u221a \u221a \u221a 51.0 POOF [3] \u221a \u221a \u221a 56.8 Nonparametric [18] \u221a \u221a \u221a 57.8 Symbiotic [6] \u221a \u221a \u221a 61.0 DPD+DeCAF [16] \u221a \u221a \u221a 65.0 PB R-CNN [52] \u221a \u221a \u221a 76.4 Symbiotic [6] \u221a \u221a \u221a \u221a 69.5 POOF [3] \u221a \u221a \u221a \u221a "
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "k has been devoted on the learning of object parts that benefits from the alignment and segmentation [1, 8, 6, 18]. Current state-of-the-art results for bird species recognition have been achieved in [4], where the strength of deep feature learning is integrated with part modeling. Most of these methods need extra bounding box and part annotations for training. However, these requirements limit their"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "erformance has been largely improved using CNNs recently, object parts (e.g. forehead or tail) information, either supervised annotations or unsupervised part extraction, are heavily relied on. PNDCN [4] proposes higher order geometric warping functions and a novel graph-based clustering algorithm for part normalization. It achieves a state-of-the-art 85.4% accuracy with both parts and bounding box a"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45200974,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "384d86497715b6900871ebc6ec3b5fbdd71c04cc",
            "isKey": true,
            "numCitedBy": 106,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improved-Bird-Species-Recognition-Using-Pose-Deep-Branson-Horn",
            "title": {
                "fragments": [],
                "text": "Improved Bird Species Recognition Using Pose Normalized Deep Convolutional Nets"
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "Early work shows that for clustering algorithms, a single GPU offers 10x more performance than top-of-the-line 8 cores workstations, even on very large datasets with more than a billion data points [50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "In the work [28] and [50], the authors also employed data and model parallelism or hybrid parallelization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 59
                            }
                        ],
                        "text": "Similar strategies have been proposed in the previous work [28, 50]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 27
                            }
                        ],
                        "text": "In (Krizhevsky, 2014) and (Yadan et al., 2013), the authors also employed data and model parallelism or hybrid parallelization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 80
                            }
                        ],
                        "text": "Similar strategies have been proposed in the previous work (Krizhevsky, 2014), (Yadan et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multigpu training of convnets"
            },
            "venue": {
                "fragments": [],
                "text": "arXiv:1312.5853,"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " networks). Indoor scene classification is another challenging task in computer vision. Different from outdoor scenes, it has large variability across different samples within each category. The work [39] created a dataset of 67 indoor scenes categories, which was used by many researchers [8, 15, 19, 26, 40]. Quattoni et al. propose a scene model which uses image prototypes to define a mapping between"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " 5. Experiments We test our system\u2019s performance on several popular computer vision benchmarks: CUB-200-2011 [47], Oxford 102 Flowers [35], Oxford-IIIT Pets [37], FGVC-Aircraft [31], MIT indoor scene [39], and ILSVRC. The first four datasets are fine-grained recognition. The fifth is for the indoor scene recognition. And the last dataset is for generic classification. Provided training and test data s"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "kind of scene categorization requires a model that can exploit both local and global discriminative information and in general it is a very hard problem for computer vision. MIT indoor scenes dataset [39] contains 67 indoor scenes covering a wide range of domains including: leisure, working places, home, stores and public space categories. It consists of 15,620 images, 80 images of each class for trai"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61863364,
            "fieldsOfStudy": [],
            "id": "d65b8a4c07146e617a0029f98fcf5317a0a68a39",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing indoor scenes"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 276
                            }
                        ],
                        "text": "\u2026recently have they come into their own, thanks to the availability of larger training data sets along\nAll the authors are with Baidu Research, Baidu, Inc. Ren Wu (wuren@baidu.com) is the corresponding author.\nwith increased computation power through heterogeneous computing (Coates et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 173
                            }
                        ],
                        "text": "A more recent example shows that three GPU servers with 4 GPUs each, rival the same performance of a 1000 nodes (16000 cores) CPU cluster, used by the Google Brain project (Coates et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 233
                            }
                        ],
                        "text": "Although neural networks have been studied for many decades, only recently have they come into their own, thanks to the availability of larger training data sets along with increased computation power through heterogeneous computing [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "servers with 4 GPUs each, rival the same performance of a 1000 nodes (16000 cores) CPU cluster, used in the system [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep learning with COTS HPC"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "Experiments We test our system\u2019s performance on several popular computer vision benchmarks: CUB-200-2011 [47], Oxford 102 Flowers [35], Oxford-IIIT Pets [37], FGVC-Aircraft [31], MIT indoor scene [39], and ILSVRC."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "The Oxford- IIIT Pets dataset [37] is created by Visual Geometry Group (VGG)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "The same as paper [44], the training and evaluation process follow the protocol of paper [37]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Jawahar, Cats and dogs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32370068"
                        ],
                        "name": "R. Hyatt",
                        "slug": "R.-Hyatt",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hyatt",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hyatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2348501"
                        ],
                        "name": "A. E. Gower",
                        "slug": "A.-E.-Gower",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gower",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. E. Gower"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39278265"
                        ],
                        "name": "H. L. Nelson",
                        "slug": "H.-L.-Nelson",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Nelson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. L. Nelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64258491,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "26ea3af3fe1ee9604d789c5efecda5b5b3e08de5",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cray-Blitz-Hyatt-Gower",
            "title": {
                "fragments": [],
                "text": "Cray Blitz"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45157843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e2964d36f154cc13b0af39c5b36cb3d76b27da1",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-Big-Simple-Neural-Nets-Excel-on-Handwritten-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 80
                            }
                        ],
                        "text": "Similar strategies have been proposed in the previous work (Krizhevsky, 2014), (Yadan et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 27
                            }
                        ],
                        "text": "In (Krizhevsky, 2014) and (Yadan et al., 2013), the authors also employed data and model parallelism or hybrid parallelization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-gpu training of convnets. CoRR, abs/1312"
            },
            "venue": {
                "fragments": [],
                "text": "Multi-gpu training of convnets. CoRR, abs/1312"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Overfeat: Integrated recognition, localization Deep Image: Scaling up Image Recognition and detection using convolutional networks"
            },
            "venue": {
                "fragments": [],
                "text": "Overfeat: Integrated recognition, localization Deep Image: Scaling up Image Recognition and detection using convolutional networks"
            },
            "year": 1312
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "In this work, we show how aggressive data augmentation can prevent overfitting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visualizing and understanding convolutional networks. CoRR, abs"
            },
            "venue": {
                "fragments": [],
                "text": "Visualizing and understanding convolutional networks. CoRR, abs"
            },
            "year": 1311
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large scale distributed deep networks. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Large scale distributed deep networks. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 147
                            }
                        ],
                        "text": "It came as a surprise to some, but with the correct algorithms, chess performance is a function of computational power (Condon & Thompson, 1982), (Hyatt et al., 1990), (Kuszmaul, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cray blitz. Computers, Chess, and Cognition, abs/1312.5402"
            },
            "venue": {
                "fragments": [],
                "text": "Cray blitz. Computers, Chess, and Cognition, abs/1312.5402"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 202
                            }
                        ],
                        "text": "Today, history is repeating itself: simple, scalable algorithms, given enough data and computational resources, dominate many fields, including visual object recognition [9, 27, 48], speech recognition [13, 21] and natural language processing [11, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 166
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DeepSpeech: Scaling up end-to-end speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "arXiv:1412.5567,"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 145
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contextcependent pre-trained deep neural networks for largevocabulary speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech & Language Processing"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 166
                            }
                        ],
                        "text": "\u2026many fields, including visual object recognition (Ciresan et al., 2010), (Krizhevsky et al., 2012), (Szegedy et al., 2014), speech recognition (Dahl et al., 2012), (Hannun et al., 2014) and natural language processing (Collobert & Weston, 2008), (Mnih & Hinton, 2007), (Mnih & Hinton, 2008)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deepspeech: Scaling up endto-end speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Deepspeech: Scaling up endto-end speech recognition"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Attention for finegrained categorization"
            },
            "venue": {
                "fragments": [],
                "text": "arXiv:1412.7054,"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 120
                            }
                        ],
                        "text": "It came as a surprise to some, but with the correct algorithms, chess performance is a function of computational power (Condon & Thompson, 1982), (Hyatt et al., 1990), (Kuszmaul, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Belle chess hardware Advances in Computer Chess 3"
            },
            "venue": {
                "fragments": [],
                "text": "Belle chess hardware Advances in Computer Chess 3"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 147
                            }
                        ],
                        "text": "It came as a surprise to some, but with the correct algorithms, chess performance is a function of computational power (Condon & Thompson, 1982), (Hyatt et al., 1990), (Kuszmaul, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cray Blitz. Computers, Chess, and Cognition"
            },
            "venue": {
                "fragments": [],
                "text": "Cray Blitz. Computers, Chess, and Cognition"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large scale distributed deep networks. In NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Large scale distributed deep networks. In NIPS"
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 32,
            "methodology": 30,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 70,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-Image:-Scaling-up-Image-Recognition-Wu-Yan/f5ce3abf942cdd685fb0f290f3e741f7b4749f0a?sort=total-citations"
}