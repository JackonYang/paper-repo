{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116023048"
                        ],
                        "name": "S. Amar",
                        "slug": "S.-Amar",
                        "structuredName": {
                            "firstName": "Shun-ichi",
                            "lastName": "Amar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 219
                            }
                        ],
                        "text": "What we exploit here is that although a covariance matrix needs many gradients to be estimated, we can take advantage of an observed property that it generally varies smoothly as training proceeds and moves in parameter space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "[9] proposes a method specific to the case of multilayer perceptrons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16093180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52ff21aabaecdffe8dff063d0bba113a6a432fac",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The main diiculty in implementing the natural gradient learning rule is to compute the inverse of the Fisher information matrix when the input dimension is large. We have found a new scheme to represent the Fisher information matrix. Based on this scheme, we have designed an algorithm to compute the inverse of the Fisher information matrix. When the input dimension n is much larger than the number of hidden neurons, the complexity of this algorithm is of order O(n 2) while the complexity of conventional algorithms for the same purpose is of order O(n 3). The simulation has connrmed the eecience and robustness of the natural gradient learning rule."
            },
            "slug": "Natural-Gradient-Descent-for-Training-Multi-Layer-Yang-Amar",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Descent for Training Multi-Layer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An algorithm to compute the inverse of the Fisher information matrix when the input dimension is large is designed and the simulation has connrmed the eecience and robustness of the natural gradient learning rule."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "As the end objective is a good solution with respect to generalization, one often uses early stopping: optimizing the training error while monitoring the validation error to fight overfitting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "The approach consists in progressively updating \u03b8 using the gradient g\u0303 = d eL\nd\u03b8 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207585383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a767a341364de1f75bea85e0b12ba7d3586a461",
            "isKey": false,
            "numCitedBy": 2730,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."
            },
            "slug": "Natural-Gradient-Works-Efficiently-in-Learning-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Works Efficiently in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "What we exploit here is that although a covariance matrix needs many gradients to be estimated, we can take advantage of an observed property that it generally varies smoothly as training proceeds and moves in parameter space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22304550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e88407f3a5591ca5c46c4c26751bdeba1e42a41",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The natural gradient descent method is applied to train an n-m-1 multilayer perceptron. Based on an efficient scheme to represent the Fisher information matrix for an n-m-1 stochastic multilayer perceptron, a new algorithm is proposed to calculate the natural gradient without inverting the Fisher information matrix explicitly. When the input dimension n is much larger than the number of hidden neurons m, the time complexity of computing the natural gradient is O(n)."
            },
            "slug": "Complexity-Issues-in-Natural-Gradient-Descent-for-Yang-Amari",
            "title": {
                "fragments": [],
                "text": "Complexity Issues in Natural Gradient Descent Method for Training Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new algorithm is proposed to calculate the natural gradient without inverting the Fisher information matrix explicitly, when the input dimension n is much larger than the number of hidden neurons m."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "What we exploit here is that although a covariance matrix needs many gradients to be estimated, we can take advantage of an observed property that it generally varies smoothly as training proceeds and moves in parameter space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11017566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffa94bba647817fa5e8f8d3250fc977435b5ca76",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a generic method for iteratively approximating various second-order gradient steps-Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD."
            },
            "slug": "Fast-Curvature-Matrix-Vector-Products-for-Gradient-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A generic method for iteratively approximating various second-order gradient steps-Newton, Gauss- newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051036"
                        ],
                        "name": "Hyeyoung Park",
                        "slug": "Hyeyoung-Park",
                        "structuredName": {
                            "firstName": "Hyeyoung",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeyoung Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "TONGA was designed to address these issues, which it does this by maintaining a low rank approximation of the covariance and by casting both problems of finding the low rank approximation and of computing the natural gradient in a lower dimensional space, thereby attaining a much lower complexity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6468689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c20d6c5bd45b7f8d484bf24b01f43d3f7d46d57",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The natural gradient learning method is known to have ideal performances for on-line training of multilayer perceptrons. It avoids plateaus, which give rise to slow convergence of the backpropagation method. It is Fisher efficient, whereas the conventional method is not. However, for implementing the method, it is necessary to calculate the Fisher information matrix and its inverse, which is practically very difficult. This article proposes an adaptive method of directly obtaining the inverse of the Fisher information matrix. It generalizes the adaptive Gauss-Newton algorithms and provides a solid theoretical justification of them. Simulations show that the proposed adaptive method works very well for realizing natural gradient learning."
            },
            "slug": "Adaptive-Method-of-Realizing-Natural-Gradient-for-Amari-Park",
            "title": {
                "fragments": [],
                "text": "Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An adaptive method of directly obtaining the inverse of the Fisher information matrix is proposed and it generalizes the adaptive Gauss-Newton algorithms and provides a solid theoretical justification of them."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14805281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "isKey": false,
            "numCitedBy": 973,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks."
            },
            "slug": "An-empirical-evaluation-of-deep-architectures-on-of-Larochelle-Erhan",
            "title": {
                "fragments": [],
                "text": "An empirical evaluation of deep architectures on problems with many factors of variation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A series of experiments indicate that these models with deep architectures show promise in solving harder learning problems that exhibit many factors of variation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7040882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b0db6135b8dd3e2a9efa86163e91c0cd0fdf660",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution presents an overview of the theoretical and practical aspects of the broad family of learning algorithms based on Stochastic Gradient Descent, including Perceptrons, Adalines, K-Means, LVQ, Multi-Layer Networks, and Graph Transformer Networks."
            },
            "slug": "Stochastic-Learning-Bottou",
            "title": {
                "fragments": [],
                "text": "Stochastic Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This contribution presents an overview of the theoretical and practical aspects of the broad family of learning algorithms based on Stochastic Gradient Descent, including Perceptrons, Adalines, K-Means, LVQ, Multi-Layer Networks, and Graph Transformer Networks."
            },
            "venue": {
                "fragments": [],
                "text": "Advanced Lectures on Machine Learning"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3299292"
                        ],
                        "name": "K. B. Petersen",
                        "slug": "K.-B.-Petersen",
                        "structuredName": {
                            "firstName": "Kaare",
                            "lastName": "Petersen",
                            "middleNames": [
                                "Brandt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. B. Petersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122191331"
                        ],
                        "name": "M. S. Pedersen",
                        "slug": "M.-S.-Pedersen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "Syskind"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1221763,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "9682782c26bb418741304436bb3a721ffc96c0f0",
            "isKey": false,
            "numCitedBy": 2371,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Acknowledgements: We would like to thank the following for contributions and suggestions: Bill Baxter, Brian Templeton, Christian Rishoj, Christian Schroppel Douglas L. Theobald, Esben Hoegh-Rasmussen, Glynne Casteel, Jan Larsen, Jun Bin Gao, Jurgen Struckmeier, Kamil Dedecius, Korbinian Strimmer, Lars Christiansen, Lars Kai Hansen, Leland Wilkinson, Liguo He, Loic Thibaut, Miguel Barao, Ole Winther, Pavel Sakov, Stephan Hattinger, Vasile Sima, Vincent Rabaud, Zhaoshui He. We would also like thank The Oticon Foundation for funding our PhD studies."
            },
            "slug": "The-Matrix-Cookbook-Petersen-Pedersen",
            "title": {
                "fragments": [],
                "text": "The Matrix Cookbook"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124525506,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "493cd5e15d6b8726ce27ed0595f1993c27525670",
            "isKey": false,
            "numCitedBy": 17356,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-recipes-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 9,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Topmoumoute-Online-Natural-Gradient-Algorithm-Roux-Manzagol/6ed460701019072ee2e364a1a491f73dd931f27f?sort=total-citations"
}