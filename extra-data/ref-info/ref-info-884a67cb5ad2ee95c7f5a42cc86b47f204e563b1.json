{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] takes a systematic approach to integrating the evidence sources they use."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] describe a text word, line and block segmentation algorithm for horizontal rectangular layouts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63629817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "068bc334298658330e9df2ceebeb5f4c76b60b90",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this study is to apply solid statistical methods to systematically model and extract various layout structures on document images, such as words, text lines and text blocks. \nWe first establish the computation theory of the recursive morphological transforms, namely the recursive erosion transform, the recursive dilation transform, the recursive opening transform, and the recursive closing transform. The transforms serve as a set of powerful tools for the document image shape analysis. \nThen we describe our efforts to construct a series of carefully ground-truthed document image databases, such as the UW English document image database (I). The database offers a platform based on which we can develop, train and evaluate our document layout analysis system. \nWe present three sub-components of our document layout analysis system. They are the text skew estimation, the word segmentation, and the object spatial analysis: \nThe text skew estimation finds the text skew angle of a document image. We develop an automatic text skew estimation algorithm using the recursive opening and closing transforms. It computes the estimated text skew angles which are within 0.5$\\sp\\circ$ of the true text skew angles with a probability of 0.95 on real images. \nThe word segmentation detects all the words on a document image. We describe a word segmentation algorithm that utilizes the recursive closing transform. We derive the quantitative measures, such as the rates of miss, false, correct, splitting, merging and spurious detections, to evaluate its performance. The results show that the algorithm correctly detects the words on a document image at a rate of about 95%. \nThe object spatial analysis treats the detected words as atomic and employs a probabilistic linear displacement model (PLDM) and an augmented PLDM model to model and extract the text lines and text blocks in a document image. By gathering statistics from a large population of document images, we are able to validate our models and determine the proper model parameters. The correct text line and text block detection rates are about 92% and 81% respectively."
            },
            "slug": "Document-layout-analysis-using-recursive-transforms-Chen",
            "title": {
                "fragments": [],
                "text": "Document layout analysis using recursive morphological transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The aim of this study is to apply solid statistical methods to systematically model and extract various layout structures on document images, such as words, text lines and text blocks, through the computation theory of the recursive morphological transforms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36738748"
                        ],
                        "name": "Debashish Niyogi",
                        "slug": "Debashish-Niyogi",
                        "structuredName": {
                            "firstName": "Debashish",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debashish Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 2314813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9349824a7e0a513667071df1c07833adb6f197bb",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The analysis of a document image to derive a symbolic description of its structure and contents involves using spatial domain knowledge to classify the different printed blocks (e.g., text paragraphs), group them into logical units (e.g., newspaper stories), and determine the reading order of the text blocks within each unit. These steps describe the conversion of the physical structure of a document into its logical structure. We have developed a computational model for document logical structure derivation, in which a rule-based control strategy utilizes the data obtained from analyzing a digitized document image, and makes inferences using a multi-level knowledge base of document layout rules. The knowledge-based document logical structure derivation system (DeLoS) based on this model consists of a hierarchical rule-based control system to guide the block classification, grouping and read-ordering operations; a global data structure to store the document image data and incremental inferences; and a domain knowledge base to encode the rules governing document layout."
            },
            "slug": "Knowledge-based-derivation-of-document-logical-Niyogi-Srihari",
            "title": {
                "fragments": [],
                "text": "Knowledge-based derivation of document logical structure"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A computational model for document logical structure derivation is developed, in which a rule-based control strategy utilizes the data obtained from analyzing a digitized document image, and makes inferences using a multi-level knowledge base of document layout rules."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112912897"
                        ],
                        "name": "Tao Hu",
                        "slug": "Tao-Hu",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680326"
                        ],
                        "name": "R. Ingold",
                        "slug": "R.-Ingold",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Ingold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "[12] T. Hu and R. Ingold."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 67
                            }
                        ],
                        "text": "The approximate reasoning, based on fuzzy logic, is used by Hu and Ingold [12]to improve the system robustness."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "The approximate reasoning, based on fuzzy logic, is used by Hu and Ingold [12] to improve the system robustness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Hu and Ingold [12] present a document description language that is similar to an attributed grammar and includes composition rules and presentation rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 7
                            }
                        ],
                        "text": "Hu and Ingold [12] present a document description language that is similar toan attributed grammar and includes composition rules and presentation rules."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16784012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9de4c3f28473c65128a0a070a0c0edeb153d7cf2",
            "isKey": true,
            "numCitedBy": 24,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY This paper presents our efforts to improve the efficiency of a document structure analysis system, which intends to analyse the complete logical structure of a document. The usage of fuzzy logic improves the system robustness; however, the problem of system efficiency was revealed to be critical. Different techniques have been studied to overcome this problem. Dynamic programming, heuristics, and dynamic threshold are used for parsing, which achieves a linear complexity. A new concept of key step, based on the principle of sub-goals, is incorporated with a multi-pass and mixed top-down analysis strategy, which avoids the combinatorial explosion of the number of search paths. Finally, the paper shows that the error-tolerating parser based on an analysis graph seems more realistic and efficient than an error-correcting parser."
            },
            "slug": "A-Mixed-Approach-Toward-an-Efficient-Logical-from-Hu-Ingold",
            "title": {
                "fragments": [],
                "text": "A Mixed Approach Toward an Efficient Logical Structure Recognition from Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The paper shows that the error-tolerating parser based on an analysis graph seems more realistic and efficient than an error-correcting parser."
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Publ."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780258"
                        ],
                        "name": "Jisheng Liang",
                        "slug": "Jisheng-Liang",
                        "structuredName": {
                            "firstName": "Jisheng",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jisheng Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1365695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44015ab8d9ba89ac004421e3c56bcbeecbd0a47",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A performance evaluation protocol for the layout analysis is discussed in this paper. In the University of Washington English Document Image Database-III, there are 1600 English document images that come with manually edited ground truth of entity bounding boxes. These bounding boxes enclose text and non-text zones, text-lines, and words. We describe a performance metric for the comparison of the detected entities and the ground truth in terms of their bounding boxes. The Document Attribute Format Specification is used as the standard data representation. The protocol is intended to serve as a model for using the UW-III database to evaluate the document analysis algorithms. A set of layout analysis algorithms which detect different entities have been tested based on the data set and the performance metric. The evaluation results are presented in this paper."
            },
            "slug": "Performance-evaluation-of-document-layout-analysis-Liang-Phillips",
            "title": {
                "fragments": [],
                "text": "Performance evaluation of document layout analysis algorithms on the UW data set"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A performance metric is described for the comparison of the detected entities and the ground truth in terms of their bounding boxes and the evaluation results are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780258"
                        ],
                        "name": "Jisheng Liang",
                        "slug": "Jisheng-Liang",
                        "structuredName": {
                            "firstName": "Jisheng",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jisheng Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122930"
                        ],
                        "name": "J. Ha",
                        "slug": "J.-Ha",
                        "structuredName": {
                            "firstName": "Jaekyu",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[11, 15] present an algorithm based on recursive cutting of connected component projection pro les for segmenting binary document images into zones; zones are classi ed as textual and non-textual; then the text zones are decomposed to text blocks, text line, and words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "1 Performance evaluation of rule-based document layout analysis algorithms In this section, we brie y present a rule-based algorithm that extracts document layout structure using the bounding boxes of di erent entities [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16777771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e5f3263d6670e98b5f0b7c929501eb1cd1d36f2",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents an efficient technique for document page layout structure extraction and classification by analyzing the spatial configuration of the bounding boxes of different entities on the given image. The algorithm segments an image into a list of homogeneous zones. The classification algorithm labels each zone as test, table, line-drawing, halftone, ruling, or noise. The text lines and words are extracted within text zones and neighboring text lines are merged to form text blocks. The tabular structure is further decomposed into row and column items. Finally, the document layout hierarchy is produced from these extracted entities."
            },
            "slug": "Document-layout-structure-extraction-using-bounding-Liang-Ha",
            "title": {
                "fragments": [],
                "text": "Document layout structure extraction using bounding boxes of different entitles"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "An efficient technique for document page layout structure extraction and classification by analyzing the spatial configuration of the bounding boxes of different entities on the given image by segments an image into a list of homogeneous zones."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE Workshop on Applications of Computer Vision. WACV'96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34286355"
                        ],
                        "name": "Shin-Ywan Wang",
                        "slug": "Shin-Ywan-Wang",
                        "structuredName": {
                            "firstName": "Shin-Ywan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shin-Ywan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128300"
                        ],
                        "name": "T. Yagasaki",
                        "slug": "T.-Yagasaki",
                        "structuredName": {
                            "firstName": "Toshiaki",
                            "lastName": "Yagasaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yagasaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "In the second approach, the low-level segments (connected components [23], line segments [21], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5434785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1997866ee54ab0b22c90d3d6086932e40a5118e",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a page segmentation method called block selection which not only segments the page image into categorized blocks but also provides a novel tree structure to represent the page blocks for selection. Block selection, more than classifying the text and nontext areas only, can identify the major document elements, such as text, picture, table, frame and line. This ability fits block selection into a wider range of document processing applications. In order to make the usage of block selection more practical to various document styles, many restrictions set on the document by some existing technologies are freed. The language on the document could be English-like, Kanji-like or both. The direction of text could be horizontal, vertical, slanted, or mixed. The editing style of the document is unconstrained. No skew correction is involved regardless of the document style. The formed blocks are described by a hierarchical tree to reflect the page arrangement in the \"object\" sense. This structural result can be efficiently used for further storage, retrieval or other manipulation purposes. The possible applications using this proposed method are discussed."
            },
            "slug": "Block-selection:-a-method-for-segmenting-a-page-of-Wang-Yagasaki",
            "title": {
                "fragments": [],
                "text": "Block selection: a method for segmenting a page image of various editing styles"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A page segmentation method called block selection is presented which not only segments the page image into categorized blocks but also provides a novel tree structure to represent the page blocks for selection that can be efficiently used for further storage, retrieval or other manipulation purposes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47530341"
                        ],
                        "name": "T. Bayer",
                        "slug": "T.-Bayer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19851329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfbac9745a17c42ebf05c4bd0b509767fefffde4",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A document analysis system which is capable of extracting the semantics of specific text portions of structured documents is presented. The architecture of the analysis system is based on a knowledge representation scheme, a semantic network, called Resco-Frame Representation of Structured Documents. It allows the definition of knowledge about document components as well as knowledge about analysis algorithms in a uniform, simple, but powerful representation formalism. Hence, this architecture enables the analysis system to exploit the specific power of both the algorithmic knowledge describing the properties of algorithms and the declarative knowledge about properties of text objects in documents. The inference engine and the control algorithm show how these two knowledge sources are combined and utilized. The flexibility of the representation formalism Fresco, the recognition results and the computational complexity of the inference algorithm are presented in two different applications.<<ETX>>"
            },
            "slug": "Understanding-structured-text-documents-by-a-model-Bayer",
            "title": {
                "fragments": [],
                "text": "Understanding structured text documents by a model based document analysis system"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A document analysis system which is capable of extracting the semantics of specific text portions of structured documents is presented and the inference engine and the control algorithm show how these two knowledge sources are combined and utilized."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122930"
                        ],
                        "name": "J. Ha",
                        "slug": "J.-Ha",
                        "structuredName": {
                            "firstName": "Jaekyu",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "The analysis can be done by projecting bounding boxes onto vertical and horizontal lines [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[11, 15] present an algorithm based on recursive cutting of connected component projection pro les for segmenting binary document images into zones; zones are classi ed as textual and non-textual; then the text zones are decomposed to text blocks, text line, and words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6826352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebbbb7c50c5a5bc6f402230c8c77b83c83db4b48",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Segmentation of document images can be performed by projecting image pixels. This pixel projection approach is one of widely used top-down segmentation methods and is based on the assumption that the document image has been correctly deskewed. Unfortunately, the pixel projection approach is computationally inefficient. It is because each symbol is not treated as a computational unit. In this paper, we explain a new technique which is highly tactical in the profiling analysis. Instead of projecting image pixels, we first compute the bounding box of each connected component in a document image and then we project those bounding boxes. Using the new technique, this paper describes how to extract words, text lines, and text blocks (e.g., paragraphs). This bounding box projection approach has many advantages over the pixel projection approach. It is less computationally involved. When applied to text zones, it is also possible to infer from the projection profiles how bounding boxes (and, therefore, primitive symbols) are aligned and/or where significant horizontal and vertical gaps are present. Since the new technique manipulates only bounding boxes, it can be applied to any noncursive language documents."
            },
            "slug": "Document-page-decomposition-using-bounding-boxes-of-Ha-Phillips",
            "title": {
                "fragments": [],
                "text": "Document page decomposition using bounding boxes of connected components of black pixels"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper describes how to extract words, text lines, and text blocks (e.g., paragraphs) using a new technique which is highly tactical in the profiling analysis and has many advantages over the pixel projection approach."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 191
                            }
                        ],
                        "text": "This means that they have to be proved out on signi cant sized data sets and there must be suitable performance metrics for each kind of information a document understanding technique infers [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "A survey of the document physical and logical structure extraction algorithms can be found in [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 241
                            }
                        ],
                        "text": "2 UW Document Image Databases It is clear that much of the early work on document analysis system provided illustrative results and hardly any had their techniques tested on signi cant sized data sets and to measure quantitative performance [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 225
                            }
                        ],
                        "text": "However, it is clear that many of the early works on document analysis systems provide illustrative results and hardly any have their techniques tested on signi cant sized data sets and give quantitative performance measures [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13615381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d91e0d202fa23b7a2e81c5b3b04eb4cc5327b0f9",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs. The physical reader of the paper document is the scanner just like the physical reader of the floppy is the floppy drive and the physical reader of the tape cartridge is the tape cartridge drive, and the physical reader of the CDROM is the CDROM drive. In the survey presented, we restrict ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences. Understanding such documents involves estimating the rotation skew of each document page, determining the geometric page layout, labeling blocks as text or non-text, determining the read order for text blocks, recognizing the text of text blocks through an OCR system, determining the logical page layout, and formatting the data and information of the document in a suitable way for use by a word processing system or by an information retrieval system.<<ETX>>"
            },
            "slug": "Document-image-understanding:-geometric-and-logical-Haralick",
            "title": {
                "fragments": [],
                "text": "Document image understanding: geometric and logical layout"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs and restricts ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34736316"
                        ],
                        "name": "K. Summers",
                        "slug": "K.-Summers",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Summers",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Summers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42137386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69e8dc0a769b5ef941cf8b6c66265d2f33c8b222",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic derivation of logical document structure from generic layout would enable the development of many highly flexible electronic document manipulation tools. This problem can be divided into the segmentation of text into pieces and the classification of these pieces as particular logical structures. This paper proposes an approach to the classification of logical document structures, according to their distance from predefined prototypes. The prototypes consider linguistic information minimally, thus relying minimally on the accuracy of OCR and decreasing language-dependence. Different classes of logical structures and the differences in the requisite information for classifying them are discussed. A prototype format is proposed, existing prototypes and a distance measurement are described, and performance results are provided."
            },
            "slug": "Near-wordless-document-structure-classification-Summers",
            "title": {
                "fragments": [],
                "text": "Near-wordless document structure classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes an approach to the classification of logical document structures, according to their distance from predefined prototypes, thus relying minimally on the accuracy of OCR and decreasing language-dependence."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2270291"
                        ],
                        "name": "S. Tsujimoto",
                        "slug": "S.-Tsujimoto",
                        "structuredName": {
                            "firstName": "Shuichi",
                            "lastName": "Tsujimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tsujimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3358140"
                        ],
                        "name": "H. Asada",
                        "slug": "H.-Asada",
                        "structuredName": {
                            "firstName": "Haruo",
                            "lastName": "Asada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Asada"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "In the second approach, the low-level segments (connected components [23], line segments [21], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62000868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1248ffebf372e65e2ced57f014d8dd933050ab30",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The document image processes used in a recently developed text reading system are described. The system consists of three major components: document analysis, document understanding, and character segmentation/recognition. The document analysis component extracts lines of text from a page for recognition. The document understanding component extracts logical relationships between the document constituents. The character segmentation/recognition component extracts characters from a text line and recognizes them. Experiments on more than a hundred documents have proved that the proposed approaches to document analysis and document understanding are robust even for multicolumned and multiarticle documents containing graphics and photographs, and that the proposed character segmentation/recognition method is robust enough to cope with omnifont characters which frequently touch each other. >"
            },
            "slug": "Major-components-of-a-complete-text-reading-system-Tsujimoto-Asada",
            "title": {
                "fragments": [],
                "text": "Major components of a complete text reading system"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experiments have proved that the proposed approaches to document analysis and document understanding are robust even for multicolumned and multiarticle documents containing graphics and photographs, and thatThe proposed character segmentation/recognition method is robust enough to cope with omnifont characters which frequently touch each other."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137629"
                        ],
                        "name": "P. Fankhauser",
                        "slug": "P.-Fankhauser",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Fankhauser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fankhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110289906"
                        ],
                        "name": "Yi Xu",
                        "slug": "Yi-Xu",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6075601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb03f6b18ce17fc63d7a8032734fcd9fa9ccb758",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY This paper presents MarkItUp!, a system to recognize the structure of untagged electronic documents which contain sub\u2010documents with similar format. For these kinds of documents manual structure recognition is a highly repetitive task. On the other hand, the specification of recognition grammars requires significant intellectual effort. Our approach uses manually structured examples to incrementally generate recognition grammars by means of techniques for learning by example. Users can structure example portions of a document by inserting mark\u2010ups. MarkItUp! then abstracts and unifies the structure of the examples. On this basis it tries to structure another example with similar format. Users can correct or accept the produced structure. With every accepted example thereby a grammar is acquired and gradually refined, which can be used to successfully structure the other portions of the document."
            },
            "slug": "MarkItUp!-An-Incremental-Approach-to-Document-Fankhauser-Xu",
            "title": {
                "fragments": [],
                "text": "MarkItUp! An Incremental Approach to Document Structure Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "MarkItUp!, a system to recognize the structure of untagged electronic documents which contain sub\u2010documents with similar format is presented, which uses manually structured examples to incrementally generate recognition grammars by means of techniques for learning by example."
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Publ."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1896610"
                        ],
                        "name": "S. Randriamasy",
                        "slug": "S.-Randriamasy",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Randriamasy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Randriamasy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820595"
                        ],
                        "name": "L. Vincent",
                        "slug": "L.-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 98
                            }
                        ],
                        "text": "Document page decomposition usingbounding boxes of connected components of Black Pixels, in: L.M. Vincent andH."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Randriamasy and Vincent [39] proposed a pixel-level and region-based approach to compare segmentation results and manually generated regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Randriamasy and Vincent [39] proposed apixel-level and region-based approach to compare segmentation results and manuallygenerated regions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 27
                            }
                        ],
                        "text": "[39] S. Randriamasy and L. Vincent."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6941199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f05833210f2b2e2f495594aeb98ff66e30c8fa1",
            "isKey": true,
            "numCitedBy": 38,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for automatically evaluating the quality of document page segmentation algorithms is introduced. Many different zoning techniques are now available but there is no robust method available to benchmark and evaluate them reliably. Our proposed strategy is a region-based approach, in which segmentation results are compared with manually generated \"ground truth files\", describing all possible correct segmentations. A segmentation ground truthing scheme has been proposed. The evaluation of segmentation quality is achieved by testing the overlap between the two sets of regions. In fact, the regions are defined as the \"black\" pixels contained in the extracted polygons. An explicit specification of segmentation errors and a numerical evaluation are derived. The algorithm is simple and fast, and provides a multi-level output for each segmentation.<<ETX>>"
            },
            "slug": "Benchmarking-page-segmentation-algorithms-Randriamasy-Vincent",
            "title": {
                "fragments": [],
                "text": "Benchmarking page segmentation algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A method for automatically evaluating the quality of document page segmentation algorithms is introduced, in which segmentation results are compared with manually generated \"ground truth files\", describing all possible correct segmentations."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "Hori and Doermann [73] instantiate and extend Haralick's framework for performance characterization in image analysis [76], in an application-dependent manner, for measuring the performance of raster to vector conversion algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3337900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7a628c2c6ec1017a66fc5a4c33174f1a32e6da2",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Performance-characterization-in-image-analysis:-a-Haralick",
            "title": {
                "fragments": [],
                "text": "Performance characterization in image analysis: thinning, a case in point"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741916"
                        ],
                        "name": "M. Garris",
                        "slug": "M.-Garris",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Garris",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Garris [29] proposed a scoring method which computes the coverage and e ciency of zone segmentation algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16555647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8087d6c784d46534bb796d32f2342a0c8e9d2fc7",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces scoring methods developed to automatically assess the performance of document recognition systems, specifically, to evaluate the spatial correspondence of zones produced by a document segmentor. Two different approaches are discussed. The first approach (based on zone overlap and nearest-neighbors) is better applied to merged zones, whereas the second approach (based on zone alignments) is better applied to nested zones (such as those found in tables and graphs). Definitions of coverage and efficiency error are presented, and scoring results on real system output is provided that validates the usefulness of these methods to compare different document recognition algorithms. Currently, no standard testing procedures exist for measuring and comparing algorithms within a complex document recognition system. Scoring methods, like the ones introduced in this paper, serve as design and validations tools, expediting the development and deployment of document analysis technology for system developers and end users."
            },
            "slug": "Evaluating-spatial-correspondence-of-zones-in-Garris",
            "title": {
                "fragments": [],
                "text": "Evaluating spatial correspondence of zones in document recognition systems"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper introduces scoring methods developed to automatically assess the performance of document recognition systems, specifically, to evaluate the spatial correspondence of zones produced by a document segmentor."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., International Conference on Image Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642099"
                        ],
                        "name": "F. Dubiel",
                        "slug": "F.-Dubiel",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Dubiel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dubiel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19200855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ea4167f9eb9ad893be32ba35147fe4539b6cdd4",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a system which is capable of learning the presentation of document logical structures, exemplarily shown for business letters. Presenting a set of instances to the system, it clusters them into structural concepts and induces a concept hierarchy. This concept hierarchy is taken as a source for classifying future input. The paper introduces the different learning steps, describes how the resulting concept hierarchy is applied for logical labeling and reports on the results."
            },
            "slug": "Clustering-and-classification-of-document-machine-Dengel-Dubiel",
            "title": {
                "fragments": [],
                "text": "Clustering and classification of document structure-a machine learning approach"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A system capable of learning the presentation of document logical structures, exemplarily shown for business letters, is described, which clusters them into structural concepts and induces a concept hierarchy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098546196"
                        ],
                        "name": "ScienceCornell UniversityIthaca",
                        "slug": "ScienceCornell-UniversityIthaca",
                        "structuredName": {
                            "firstName": "ScienceCornell",
                            "lastName": "UniversityIthaca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ScienceCornell UniversityIthaca"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7045239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d505f687741d1be3bfb31cb124b34dc90587847",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The automated discovery of logical structure in text documents is an important problem that has recently received a good deal of attention; it can enable the creation of exible and sophisticated document manipulation tools that will greatly increase the impact of electronic documents. This paper addresses aspects of the nature of the logical structures to be found, in order to develop categories of structures that reeect the variance in requirements for discovery and the variance in signiicance for applications. A complete taxonomy is not developed, but relevant attributes are identiied in three forms of categoriza-tion: fundamental, based on structure deenitions; discovery, based on required observables to nd structures ; and usage, based on roles structures play in applications. The attributes themselves are independent of the choice of particular logical structures to consider in a given application, and their direct implications are discussed."
            },
            "slug": "Toward-a-Taxonomy-of-Logical-Document-of-Computer-UniversityIthaca",
            "title": {
                "fragments": [],
                "text": "Toward a Taxonomy of Logical Document StructuresKristen SummersDepartment of Computer"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper addresses aspects of the nature of the logical structures to be found, in order to develop categories of structures that reflect the variance in requirements for discovery and thevariance in signiicance for applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5733118"
                        ],
                        "name": "A. Conway",
                        "slug": "A.-Conway",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Conway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conway"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42019065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44e29d6e54fc2971b7ec16b7cd65f6ff33388190",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a syntactic approach to deducing the logical structure of printed documents from their physical layout. Page layout is described by a two-dimensional grammar, similar to a context-free string grammar, and a chart parser is used to parse segmented page images according to the grammar. This process is part of a system which reads scanned document images and produces computer-readable text in a logical mark-up format such as SGML. The system is briefly outlined, the grammar formalism and the parsing algorithm are described in detail, and some experimental results are reported.<<ETX>>"
            },
            "slug": "Page-grammars-and-page-parsing.-A-syntactic-to-Conway",
            "title": {
                "fragments": [],
                "text": "Page grammars and page parsing. A syntactic approach to document layout recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A syntactic approach to deducing the logical structure of printed documents from their physical layout by a two-dimensional grammar, similar to a context-free string grammar, and a chart parser is used to parse segmented page images according to the grammar."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086576246"
                        ],
                        "name": "Joachim Kreich",
                        "slug": "Joachim-Kreich",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Kreich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joachim Kreich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45337688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0c9b1a64b71667db3de4d229dd164d11c273c74",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "To make interpretation of scanned documents much more robust, the system IDA uses different knowledge about geometry and text. This knowledge is handled by an analysis process, which is relatively free in what to do and how to do it. These degrees of freedom are filled by utilization of control knowledge. Decisions are made with regards to context. The analysis process calculates truth values as well as numerically based test candidates. All analysis states are stored in justification networks similar to truth maintenance systems. This is used to support the solution of inconsistencies and ambiguities. The results of analysis as senders, dates, and authors are reliable enough so that they can be used by storing documents in databases.<<ETX>>"
            },
            "slug": "Robust-recognition-of-documents-Kreich",
            "title": {
                "fragments": [],
                "text": "Robust recognition of documents"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "To make interpretation of scanned documents much more robust, the system IDA uses different knowledge about geometry and text that is handled by an analysis process, which is relatively free in what to do and how to do it."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59645368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b97e3ef2621928d32a20cbbabba0f002fc3cc1",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Printing, photocopying and scanning processes degrade the image quality of a document. Although research in document understanding started in the sixties, only two document degradation models have been proposed thus far. Furthermore, no attempts have been made to rigorously validate them. In document understanding research, models for image degradations are crucial in many ways. Models allow us to (i) conduct controlled experiments to study the break-down points of the systems, (ii) create large data sets with groundtruth for training classifiers, (iii) design optimal noise removal algorithms, (iv) choose values for the free parameters of the algorithms, etc. \nIn this thesis two document degradation models are described. The first model accounts for local pixel-level degradations that occur while printing, photocopying and scanning a document. The second model accounts for the perspective and illumination distortions that occur while photocopying or scanning a thick, bound document. The local distortion model allows us the create large data sets of synthetically generated documents, in any language, along with the associated groundtruth information quite easily. Unlike isolated character databases, our data sets are a much better representation of the real world since they account for the real-world character and word occurrence probabilities, and character and word bi-gram probabilities naturally. Moreover, since our methodology puts the text, layout, formatting, resolution, and font details of the document image under the experimenter's control, a large variety of controlled experiments that were not possible earlier are now possible. \nNext, an automatic document registration and character groundtruthing procedure is described. This procedure produces very accurate character groundtruth for scanned documents in any language, which had not been possible until now. The method essentially registers the ideal image to a scanned version and then transforms the groundtruth associated with the ideal image through the registration transformation. This method can be used to generate groundtruth for documents in any language, and even FAXed documents. A data set having 33 English scanned document images with character groundtruth for 62000 symbols was created using this procedure. \nA non-parametric statistical procedure for estimating the parameters of the local degradation model from a sample of real degraded documents is then discussed. The estimation procedure allows researchers to generate large data sets from small samples of real data. Such procedures for estimating parameters do not exist for other document degradation models. In fact, our approach can be easily adapted to estimate the parameters of other models as well. \nFinally, a statistical methodology that can be used to validate the local degradation models is described. This method is based on a non-parametric, two-sample permutation test. A variant of the method allows approximate validation tests instead. Another standard statistical device--the power function--is then used to choose between algorithm variables such as distance functions. Since the validation and power function procedures are independent of the model, they can be used to validate any other degradation model. A method for comparing any two models is also described. It uses p-values associated with the estimated models to select the model that is closer to the real world."
            },
            "slug": "Document-degradation-models-and-a-methodology-for-Kanungo",
            "title": {
                "fragments": [],
                "text": "Document degradation models and a methodology for degradation model validation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Since the methodology puts the text, layout, formatting, resolution, and font details of the document image under the experimenter's control, a large variety of controlled experiments that were not possible earlier are now possible."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3141168"
                        ],
                        "name": "P. W. Palumbo",
                        "slug": "P.-W.-Palumbo",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Palumbo",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. W. Palumbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064971340"
                        ],
                        "name": "Jung Soh",
                        "slug": "Jung-Soh",
                        "structuredName": {
                            "firstName": "Jung",
                            "lastName": "Soh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung Soh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145912825"
                        ],
                        "name": "R. Sridhar",
                        "slug": "R.-Sridhar",
                        "structuredName": {
                            "firstName": "Ramalingam",
                            "lastName": "Sridhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sridhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3145150"
                        ],
                        "name": "V. Demjanenko",
                        "slug": "V.-Demjanenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Demjanenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Demjanenko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[18] developed a postal automation system that locates and interprets destination address blocks on letter mail pieces with a high success rate and high speed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16249736,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "3a9e7bd7f8911d5b2d7b4a5a755dfd4f6770589b",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The CEDAR real-time address block location system, which determines candidates for the location of the destination address from a scanned mail piece image, is described. For each candidate destination address block (DAB), the address block location (ABL) system determines the line segmentation, global orientation, block skew, an indication of whether the address appears to be handwritten or machine printed, and a value indicating the degree of confidence that the block actually contains the destination address. With 20-MHz Sparc processors, the average time per mail piece for the combined hardware and software system components is 0.210 seconds. The system located 89.0% of the addresses as the top choice. Recent developments in the system include the use of a top-down segmentation tool, address syntax analysis using only connected component data, and improvements to the segmentation refinement routines. This has increased top choice performance to 91.4%.<<ETX>>"
            },
            "slug": "Postal-address-block-location-in-real-time-Palumbo-Srihari",
            "title": {
                "fragments": [],
                "text": "Postal address block location in real time"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Recent developments in the CEDAR real-time address block location system include the use of a top-down segmentation tool, address syntax analysis using only connected component data, and improvements to the segmentation refinement routines."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70286642"
                        ],
                        "name": "Mindy Bokser",
                        "slug": "Mindy-Bokser",
                        "structuredName": {
                            "firstName": "Mindy",
                            "lastName": "Bokser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mindy Bokser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "We will experiment on estimating trigram probabilities by combining several group ngram probabilities, rather than by direct lookup in a trigram table [66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61670519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c7e86cee5a41818b5307eefc1e31b71233c21b2",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "An optical character recognition (OCR) engine that is omnifont and reasonably robust on individual degraded characters is presented. The weakest link is its handling of characters which are difficult to segment. The engine is divided into four phases: segmentation, image recognition, ambiguity resolution, and document analysis. The features are zonal and reduce the image to a blurred, gray-level representation. The classifier is data-driven, trained offline, and model-free. Handcrafted features and decision trees tend to be brittle in the presence of noise. To satisfy the needs of full-text applications, the system captures the structure of the document so that, when viewed in a word processor or spreadsheet program, the formatting of the optically recognized document reflects that of the original document. To satisfy the needs of the forms market, a proofing and correction tool displays 'pop-up' images of uncertain characters. >"
            },
            "slug": "Omnidocument-technologies-Bokser",
            "title": {
                "fragments": [],
                "text": "Omnidocument technologies"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An optical character recognition (OCR) engine that is omnifont and reasonably robust on individual degraded characters is presented and captures the structure of the document so that the formatting of the optically recognized document reflects that of the original document."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258400"
                        ],
                        "name": "\u00d8. Trier",
                        "slug": "\u00d8.-Trier",
                        "structuredName": {
                            "firstName": "\u00d8ivind",
                            "lastName": "Trier",
                            "middleNames": [
                                "Due"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d8. Trier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48717516"
                        ],
                        "name": "T. Taxt",
                        "slug": "T.-Taxt",
                        "structuredName": {
                            "firstName": "Torfinn",
                            "lastName": "Taxt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Taxt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "1 Introduction Optical character recognition (OCR) is one of the most popular and successful applications of automatic pattern recognition [67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205015030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8b7804abc030ee93eff2f5baa306b8b95361c57",
            "isKey": false,
            "numCitedBy": 1450,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-extraction-methods-for-character-survey-Trier-Jain",
            "title": {
                "fragments": [],
                "text": "Feature extraction methods for character recognition-A survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700821"
                        ],
                        "name": "F. Esposito",
                        "slug": "F.-Esposito",
                        "structuredName": {
                            "firstName": "Floriana",
                            "lastName": "Esposito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Esposito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738657"
                        ],
                        "name": "D. Malerba",
                        "slug": "D.-Malerba",
                        "structuredName": {
                            "firstName": "Donato",
                            "lastName": "Malerba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malerba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467353"
                        ],
                        "name": "G. Semeraro",
                        "slug": "G.-Semeraro",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Semeraro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Semeraro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35702208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8f460360e932140827d341b2d1bfa65cb614b4a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A study on the possibility of adopting a supervised inductive learning approach to the problem of document understanding is presented. A representation language used to describe a page layout is introduced and the opportunity of extending such a language by means of intentionally defined predicates is discussed. Experimental results obtained by using a well-known learning system, FOCL, are presented. They confirm the exigency of redefining the problem of document understanding in terms of a new strategy of supervised inductive learning, called contextual learning. Some experiments in which a dependence hierarchy between concepts is defined show that contextual rules increase predictive accuracy and decrease learning time for labeling problems like document understanding.<<ETX>>"
            },
            "slug": "Automated-acquisition-of-rules-for-document-Esposito-Malerba",
            "title": {
                "fragments": [],
                "text": "Automated acquisition of rules for document understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experimental results obtained by using a well-known learning system, FOCL, confirm the exigency of redefining the problem of document understanding in terms of a new strategy of supervised inductive learning, called contextual learning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2902145"
                        ],
                        "name": "Gary S. D. Farrow",
                        "slug": "Gary-S.-D.-Farrow",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Farrow",
                            "middleNames": [
                                "S.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary S. D. Farrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743769"
                        ],
                        "name": "C. Xydeas",
                        "slug": "C.-Xydeas",
                        "structuredName": {
                            "firstName": "Costas",
                            "lastName": "Xydeas",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Xydeas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841968"
                        ],
                        "name": "J. Oakley",
                        "slug": "J.-Oakley",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Oakley",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Oakley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8814153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fe149c25eea4fb70080761b7d1d240d4b761c8e",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a system for the conversion of scanned documents into the open document architecture. Unlike previous work in this field the authors use a combination of evidence sources to achieve greater robustness to document defects and noise introduced in the scanning process. Furthermore, they use optical character recognition in conjunction with other forms of image analysis as a means of detecting document structure. This enables enhanced document feature extraction and improved performance. They demonstrate the performance of the system on a specific class of input document.<<ETX>>"
            },
            "slug": "Conversion-of-scanned-documents-to-the-open-Farrow-Xydeas",
            "title": {
                "fragments": [],
                "text": "Conversion of scanned documents to the open document architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The authors use a combination of evidence sources to achieve greater robustness to document defects and noise introduced in the scanning process and use optical character recognition in conjunction with other forms of image analysis as a means of detecting document structure."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "If the angleskew > thresholdskew, we rotate the image by angleskew using the technique given in [59], and the process repeats from step 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14079548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a24502cae14f773dc2628bf286a0205b3ef9d1e5",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an algorithm to estimate the text skew angle in a document image. The algorithm utilizes the recursive morphological transforms and yields accurate estimates of text skew angles on a large document image data set. The algorithm computes the optimal parameter settings on the fly without any human interaction. In this automatic mode, experimental results indicate that the algorithm generates estimated text skew angles within 0.5/spl deg/ of the true text skew angles with a probability of 99%. To process a 300 dpi document image, the algorithm takes 10 seconds on SUN Sparc 10 machines."
            },
            "slug": "Automatic-text-skew-estimation-in-document-images-Chen-Haralick",
            "title": {
                "fragments": [],
                "text": "Automatic text skew estimation in document images"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The algorithm utilizes the recursive morphological transforms and yields accurate estimates of text skew angles on a large document image data set and computes the optimal parameter settings on the fly without any human interaction."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143772136"
                        ],
                        "name": "O. Hori",
                        "slug": "O.-Hori",
                        "structuredName": {
                            "firstName": "Osamu",
                            "lastName": "Hori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Hori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Hori and Doermann [73] instantiate and extend Haralick's framework for performance characterization in image analysis [76], in an application-dependent manner, for measuring the performance of raster to vector conversion algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Hori and Doermann'sprotocol does not distinguish between detection rate and false alarm rate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Hori and Doermann [73] propose a quantitative performance measurement methodology for task-speci c raster to vector conversion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "Benchmarking and Performance Evaluation web site. http://pandora.imag.fr/ECVNet/benchmarking.html.[73] O. Hori and S. Doermann."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 181
                            }
                        ],
                        "text": "Performance evaluation of graphics recognition is still a very young eld; objective and quantitative methods for evaluation of graphics recognition have been proposed very recently [71, 73, 74, 75]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Hori and Doermann [73] instantiate and extend Haralick's framework for perfor-mance characterization in image analysis [76], in an application-dependent manner,for measuring the performance of raster to vector conversion algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Hori and Doermann [73] propose a quantitativeperformance measurement methodology for task-speci c raster to vector conversion."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11365767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1614c13a800696c1df243d19cb0794e02e5f8900",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a methodology for measuring the performance of application-specific raster-to-vector conversion algorithms. In designing and building image analysis systems, comparison of several algorithms is often required. Unfortunately, many methods of comparison do not give quantitative performance measurements, but rather qualitative, and often subjective, evaluations. Our key observation is that there is a need for domain, or task-dependent evaluation of the output. By specifying the input data in the same parameter space as the intended output of the system, we are able to evaluate the quality of the output and how well it conforms to the intended representation. We provide a set of basic metrics, but we emphasize that in general, such metrics may be task-specific. In this paper, the performance of three approaches to raster-to-vector conversion \u2014 thinning, medial line finding, and line fitting \u2014 are compared using this methodology."
            },
            "slug": "Quantitative-Measurement-of-the-Performance-of-Hori-Doermann",
            "title": {
                "fragments": [],
                "text": "Quantitative Measurement of the Performance of Raster-to-Vector Conversion Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The performance of three approaches to raster-to-vector conversion \u2014 thinning, medial line finding, and line fitting \u2014 are compared using this methodology and it is shown that there is a need for domain, or task-dependent evaluation of the output."
            },
            "venue": {
                "fragments": [],
                "text": "GREC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128453"
                        ],
                        "name": "A. Bela\u00efd",
                        "slug": "A.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Abdel",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bela\u00efd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586538"
                        ],
                        "name": "J. C. Anigbogu",
                        "slug": "J.-C.-Anigbogu",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Anigbogu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Anigbogu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3297415"
                        ],
                        "name": "Y. Chenevoy",
                        "slug": "Y.-Chenevoy",
                        "structuredName": {
                            "firstName": "Yannick",
                            "lastName": "Chenevoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chenevoy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32775920,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "6cea7a427ed61ff7d898861603aa28862df7bce5",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY This paper presents a qualitative approach to logical structure recognition of library references. The system is driven by a generic model of a reference class and by an OCR flow, given in SGML format, that include ASCII code of the characters and information about the typographic style and the lexical affiliation of words. The approach used is based on hypotheses production and verification about the existence of sub-field limits in the reference area. At each step of the analysis, the generated hypotheses are sorted on the basis of their confidence scores and the most likely hypothesis is analyzed. The result is a structured flow containing, in UNIMARC format, the list of different sub-fields recognized, accompanied with their confidence score."
            },
            "slug": "Qualitative-Analysis-of-Low-Level-Logical-Bela\u00efd-Anigbogu",
            "title": {
                "fragments": [],
                "text": "Qualitative Analysis of Low-Level Logical Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A qualitative approach to logical structure recognition of library references is presented, based on hypotheses production and verification about the existence of sub-field limits in the reference area, containing a structured flow containing the list of different sub-fields recognized."
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Publ."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16489337,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7f62ed0e11910cf534f705a0f81527e9a3007f29",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Two sources of document degradation are modeled: i) perspective distortion that occurs while photocopying or scanning thick, bound documents, and ii) degradation due to perturbations in the optical scanning and digitization process: speckle, blurr, jitter, thresholding. Perspective distortion is modeled by studying the underlying perspective geometry of the optical system of photocopiers and scanners. An illumination model is described to account for the nonlinear intensity change occuring across a page in a perspective-distorted document. The optical distortion process is modeled morphologically. First, a distance transform on the foreground is performed, followed by a random inversion of binary pixels where the probability of flip is a function of the distance of the pixel to the boundary of the foreground. Correlating the flipped pixels is modeled by a morphological closing operation.<<ETX>>"
            },
            "slug": "Global-and-local-document-degradation-models-Kanungo-Haralick",
            "title": {
                "fragments": [],
                "text": "Global and local document degradation models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An illumination model is described to account for the nonlinear intensity change occuring across a page in a perspective-distorted document."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780258"
                        ],
                        "name": "Jisheng Liang",
                        "slug": "Jisheng-Liang",
                        "structuredName": {
                            "firstName": "Jisheng",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jisheng Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34817063"
                        ],
                        "name": "A. Chhabra",
                        "slug": "A.-Chhabra",
                        "structuredName": {
                            "firstName": "Atul",
                            "lastName": "Chhabra",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chhabra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "We developed a protocol [34] to evaluate the performance of engineering diagram recognition systems on images that contain engineering drawings of straight lines (solid or dashed), circles (solid or dashed), partial arcs of circles (solid or dashed), as well as, bounding boxes of text blocks within the images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5083726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "335e19d744b147a597a6102463bee969d474c296",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper defines a computational protocol for evaluating the performance of raster to vector conversion systems. The graphical entities handled by this protocol are continuous and dashed lines, arcs, and circles, and text regions. The protocol allows matches of the type one-to-one, one-to-many, and many-to-one between the ground truth and the recognition results."
            },
            "slug": "A-Performance-Evaluation-Protocol-for-Graphics-Phillips-Liang",
            "title": {
                "fragments": [],
                "text": "A Performance Evaluation Protocol for Graphics Recognition Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A computational protocol for evaluating the performance of raster to vector conversion systems and graphical entities handled are continuous and dashed lines, arcs, and circles, and text regions."
            },
            "venue": {
                "fragments": [],
                "text": "GREC"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803621"
                        ],
                        "name": "A. Khotanzad",
                        "slug": "A.-Khotanzad",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Khotanzad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khotanzad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3354183"
                        ],
                        "name": "Yaw Hua Hong",
                        "slug": "Yaw-Hua-Hong",
                        "structuredName": {
                            "firstName": "Yaw",
                            "lastName": "Hong",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaw Hua Hong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2176918,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f12b2b698d586e219bfa07a56615d1cefb8557e1",
            "isKey": false,
            "numCitedBy": 2005,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of rotation-, scale-, and translation-invariant recognition of images is discussed. A set of rotation-invariant features are introduced. They are the magnitudes of a set of orthogonal complex moments of the image known as Zernike moments. Scale and translation invariance are obtained by first normalizing the image with respect to these parameters using its regular geometrical moments. A systematic reconstruction-based method for deciding the highest-order Zernike moments required in a classification problem is developed. The quality of the reconstructed image is examined through its comparison to the original one. The orthogonality property of the Zernike moments, which simplifies the process of image reconstruction, make the suggest feature selection approach practical. Features of each order can also be weighted according to their contribution to the reconstruction process. The superiority of Zernike moment features over regular moments and moment invariants was experimentally verified. >"
            },
            "slug": "Invariant-Image-Recognition-by-Zernike-Moments-Khotanzad-Hong",
            "title": {
                "fragments": [],
                "text": "Invariant Image Recognition by Zernike Moments"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A systematic reconstruction-based method for deciding the highest-order ZERNike moments required in a classification problem is developed and the superiority of Zernike moment features over regular moments and moment invariants was experimentally verified."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822856"
                        ],
                        "name": "W. Masek",
                        "slug": "W.-Masek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Masek",
                            "middleNames": [
                                "Joseph"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Masek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143607434"
                        ],
                        "name": "M. Paterson",
                        "slug": "M.-Paterson",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Paterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Paterson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "The edit distance may be computed e ciently using dynamic programming [37] [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205007870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1e15b063a0da756de133c3b040f2d9173b29180",
            "isKey": false,
            "numCitedBy": 679,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Faster-Algorithm-Computing-String-Edit-Distances-Masek-Paterson",
            "title": {
                "fragments": [],
                "text": "A Faster Algorithm Computing String Edit Distances"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153064717"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053933759"
                        ],
                        "name": "S. Subramaniam",
                        "slug": "S.-Subramaniam",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Subramaniam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Subramaniam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "The OCR Performance Evaluation (OPE) Software [26] [27] compares the groundtruth and OCR output and outputs a le containing: single character contingency table, error substring contingency table, statistics of line insertions, deletions and substitutions, and statistics of symbol insertions, deletions and substitutions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59804271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0367abf0adf83f31be5ac917cee6cd0e9d8bd8ab",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An experimental protocol for the performance evaluation of Optical Character Recognition (OCR) algorithms is described. The protocol is intended to serve as a model for using the University of Washington English Document Image Database-I to evaluate OCR systems. The plain text zones (without special symbols) in this database have over 2,300,000 characters. The performances of two UNIX-based OCR systems, namely Caere OCR v109a and Xerox ScanWorX v2.0, are measured. The results suggest that Caere OCR outperforms ScanWorX in terms of recognition accuracy; however, ScanWorX is more robust in the presence of image flaws."
            },
            "slug": "Performance-evaluation-of-two-OCR-systems-Chen-Subramaniam",
            "title": {
                "fragments": [],
                "text": "Performance evaluation of two OCR systems"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An experimental protocol for the performance evaluation of Optical Character Recognition (OCR) algorithms is described, and the results suggest that Caere OCR outperforms ScanworX in terms of recognition accuracy; however, ScanWorX is more robust in the presence of image flaws."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056612934"
                        ],
                        "name": "Bin Kong",
                        "slug": "Bin-Kong",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Kong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Kong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153819718"
                        ],
                        "name": "Arathi Prasad",
                        "slug": "Arathi-Prasad",
                        "structuredName": {
                            "firstName": "Arathi",
                            "lastName": "Prasad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arathi Prasad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[71] propose a quantitative method for evaluating"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "A benchmark[71] was developed and used in that competition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[71] use angle, distance, relative overlap, and o set between line segments for evaluating line matches and for detecting line styles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 181
                            }
                        ],
                        "text": "Performance evaluation of graphics recognition is still a very young eld; objective and quantitative methods for evaluation of graphics recognition have been proposed very recently [71, 73, 74, 75]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38861772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dee44442738a70481c06a2f36b4be1042618d38",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a protocol for systematically evaluating the performance of dashed-line detection algorithms. It includes a test image generator which creates random line patterns subject to prespecified constraints. The generator also outputs ground truth data for each line in the image. The output of the dashed line detection algorithm is then compared to these ground truths and evaluated using a set of criteria."
            },
            "slug": "A-Benchmark:-Performance-Evaluation-of-Dashed-Line-Kong-Phillips",
            "title": {
                "fragments": [],
                "text": "A Benchmark: Performance Evaluation of Dashed-Line Detection Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper describes a protocol for systematically evaluating the performance of dashed-line detection algorithms that includes a test image generator which creates random line patterns subject to prespecified constraints."
            },
            "venue": {
                "fragments": [],
                "text": "GREC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499975"
                        ],
                        "name": "J. Kanai",
                        "slug": "J.-Kanai",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Kanai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kanai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[28] proposed a text-based method to evaluate the zone segmentation performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30733052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bd99ebf0bbe9a8513350d9eae4f3570c99d559b",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Many current optical character recognition (OCR) systems attempt to decompose printed pages into a set of zones, each containing a single column of text, before converting the characters into coded form. The authors present a methodology for automatically assessing the accuracy of such decompositions, and demonstrate its use in evaluating six OCR systems. >"
            },
            "slug": "Automated-Evaluation-of-OCR-Zoning-Kanai-Rice",
            "title": {
                "fragments": [],
                "text": "Automated Evaluation of OCR Zoning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A methodology for automatically assessing the accuracy of optical character recognition decompositions is presented, and its use in evaluating six OCR systems is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97129855"
                        ],
                        "name": "M. Hu",
                        "slug": "M.-Hu",
                        "structuredName": {
                            "firstName": "Ming-Kuei",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "For general linear transformation, Hu [70] and Reiss [68] gave the relative invariants that are functions of the secondto fourth-order central moments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6431165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce1e3528047cd01937f6a8aa760640f6b3c8d531",
            "isKey": false,
            "numCitedBy": 8010,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection."
            },
            "slug": "Visual-pattern-recognition-by-moment-invariants-Hu",
            "title": {
                "fragments": [],
                "text": "Visual pattern recognition by moment invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished and it is indicated that generalization is possible to include invariance with parallel projection."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 128
                            }
                        ],
                        "text": "Sponsored by the Advanced Research Project Agency (ARPA), we have produced the University of Washington Document Image Database [42, 43] series of ground-truth databases on CDROM for the document image analysis and recognition communities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 92
                            }
                        ],
                        "text": "Sponsored by the Advanced Research Project Agency (ARPA), we haveproduced the University of Washington Document Image Database [42, 43] series ofground-truth databases on CDROM for the document image analysis and recognitioncommunities."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3258255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de1580d4cc0c47e4985c8ce52d106c6a7432ff70",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The design of a comprehensive standard document database for machine-printed documents is presented. The effort to produce a series of carefully ground-truthed document databases to be issued on CD-ROMs is described in detail. The databases can be utilized by the OCR and document understanding community as a common platform to develop, test, and evaluate their algorithms.<<ETX>>"
            },
            "slug": "CD-ROM-document-database-standard-Phillips-Chen",
            "title": {
                "fragments": [],
                "text": "CD-ROM document database standard"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The design of a comprehensive standard document database for machine-printed documents is presented and can be utilized by the OCR and document understanding community as a common platform to develop, test, and evaluate their algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144389145"
                        ],
                        "name": "A. Gelman",
                        "slug": "A.-Gelman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097256"
                        ],
                        "name": "Xiao-Li Meng",
                        "slug": "Xiao-Li-Meng",
                        "structuredName": {
                            "firstName": "Xiao-Li",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Li Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38818938"
                        ],
                        "name": "H. Stern",
                        "slug": "H.-Stern",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Stern",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Stern"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17309196,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "377339cd55087d503b855ae89d2126495cf104ee",
            "isKey": false,
            "numCitedBy": 1981,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper considers Bayesian counterparts of the classical tests for good- ness of fit and their use in judging the fit of a single Bayesian model to the observed data. We focus on posterior predictive assessment, in a framework that also includes conditioning on auxiliary statistics. The Bayesian formulation facilitates the con- struction and calculation of a meaningful reference distribution not only for any (classical) statistic, but also for any parameter-dependent \"statistic\" or discrep- ancy. The latter allows us to propose the realized discrepancy assessment of model fitness, which directly measures the true discrepancy between data and the posited model, for any aspect of the model which we want to explore. The computation required for the realized discrepancy assessment is a straightforward byproduct of the posterior simulation used for the original Bayesian analysis. We illustrate with three applied examples. The first example, which serves mainly to motivate the work, illustrates the difficulty of classical tests in assessing the fitness of a Poisson model to a positron emission tomography image that is constrained to be nonnegative. The second and third examples illustrate the details of the posterior predictive approach in two problems: estimation in a model with inequality constraints on the parameters, and estimation in a mixture model. In all three examples, standard test statistics (either a \u03c7 2 or a likelihood ratio) are not pivotal: the difficulty is not just how to compute the reference distribution for the test, but that in the classical framework no such distribution exists, independent of the unknown model parameters."
            },
            "slug": "POSTERIOR-PREDICTIVE-ASSESSMENT-OF-MODEL-FITNESS-Gelman-Meng",
            "title": {
                "fragments": [],
                "text": "POSTERIOR PREDICTIVE ASSESSMENT OF MODEL FITNESS VIA REALIZED DISCREPANCIES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1604429801"
                        ],
                        "name": "Robert M. Haralock",
                        "slug": "Robert-M.-Haralock",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralock",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert M. Haralock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809809"
                        ],
                        "name": "L. Shapiro",
                        "slug": "L.-Shapiro",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Shapiro",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shapiro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 277
                            }
                        ],
                        "text": "4 Experimental Protocol Controlled experiments are an important component of computer vision, for the controlled experiment demonstrates that the algorithm, designed by the computer-vision researcher, recognizes, locates, and measures what it is designed to do from image data [57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "1) where P (g; d) is the probability of the algorithm output is d while the ground truth is g [57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61087042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9eaaecf23f3a4b7822e4bcca924e02cd5b4dc4e",
            "isKey": false,
            "numCitedBy": 3343,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis two-volume set is an authoritative, comprehensive, modern work on computer vision that covers all of the different areas of vision with a balanced and unified approach. The discussion in \"Volume I\" focuses on image in, and image out or feature set out. \"Volume II\" covers the higher level techniques of illumination, perspective projection, analytical photogrammetry, motion, image matching, consistent labeling, model matching, and knowledge-based vision systems."
            },
            "slug": "Computer-and-Robot-Vision-Haralock-Shapiro",
            "title": {
                "fragments": [],
                "text": "Computer and Robot Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This two-volume set is an authoritative, comprehensive, modern work on computer vision that covers all of the different areas of vision with a balanced and unified approach."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145793042"
                        ],
                        "name": "R. Wagner",
                        "slug": "R.-Wagner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wagner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wagner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298802"
                        ],
                        "name": "M. Fischer",
                        "slug": "M.-Fischer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fischer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "The edit distance may be computed e ciently using dynamic programming [37] [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13381535,
            "fieldsOfStudy": [
                "Mathematics",
                "Education",
                "Physics"
            ],
            "id": "455e1168304e0eb2909093d5ab9b5ec85cda5028",
            "isKey": false,
            "numCitedBy": 3190,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings."
            },
            "slug": "The-String-to-String-Correction-Problem-Wagner-Fischer",
            "title": {
                "fragments": [],
                "text": "The String-to-String Correction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm is presented which solves the string-to-string correction problem in time proportional to the product of the lengths of the two strings."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144389145"
                        ],
                        "name": "A. Gelman",
                        "slug": "A.-Gelman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2891565"
                        ],
                        "name": "Y. Goegebeur",
                        "slug": "Y.-Goegebeur",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Goegebeur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Goegebeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109141"
                        ],
                        "name": "F. Tuerlinckx",
                        "slug": "F.-Tuerlinckx",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Tuerlinckx",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Tuerlinckx"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116978358"
                        ],
                        "name": "I. van Mechelen",
                        "slug": "I.-van-Mechelen",
                        "structuredName": {
                            "firstName": "Iven",
                            "lastName": "van Mechelen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. van Mechelen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11657226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca4e9741e19ac47503b28438671366dba43ce8d6",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Model checking with discrete data regressions can be difficult because the usual methods such as residual plots have complicated reference distributions that depend on the parameters in the model. Posterior predictive checks have been proposed as a Bayesian way to average the results of goodness\u2010of\u2010fit tests in the presence of uncertainty in estimation of the parameters. We try this approach using a variety of discrepancy variables for generalized linear models fitted to a historical data set on behavioural learning. We then discuss the general applicability of our findings in the context of a recent applied example on which we have worked. We find that the following discrepancy variables work well, in the sense of being easy to interpret and sensitive to important model failures: structured displays of the entire data set, general discrepancy variables based on plots of binned or smoothed residuals versus predictors and specific discrepancy variables created on the basis of the particular concerns arising in an application. Plots of binned residuals are especially easy to use because their predictive distributions under the model are sufficiently simple that model checks can often be made implicitly. The following discrepancy variables did not work well: scatterplots of latent residuals defined from an underlying continuous model and quantile\u2013quantile plots of these residuals."
            },
            "slug": "Diagnostic-checks-for-discrete-data-regression-Gelman-Goegebeur",
            "title": {
                "fragments": [],
                "text": "Diagnostic checks for discrete data regression models using posterior predictive simulations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work tries a variety of discrepancy variables for generalized linear models fitted to a historical data set on behavioural learning and finds that the following discrepancy variables work well, in the sense of being easy to interpret and sensitive to important model failures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111973920"
                        ],
                        "name": "R. Shyamasundar",
                        "slug": "R.-Shyamasundar",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Shyamasundar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shyamasundar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "The LCS problem can solved e ciently using dynamic programming [63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123556377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2c1830eedc095b92a78dc3afa39a36343ed81ad",
            "isKey": false,
            "numCitedBy": 6417,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary purpose of a programming language is to assist the programmer in the practice of her art. Each language is either designed for a class of problems or supports a different style of programming. In other words, a programming language turns the computer into a \u2018virtual machine\u2019 whose features and capabilities are unlimited. In this article, we illustrate these aspects through a language similar tologo. Programs are developed to draw geometric pictures using this language."
            },
            "slug": "Introduction-to-algorithms-Shyamasundar",
            "title": {
                "fragments": [],
                "text": "Introduction to algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This article illustrates aspects of programming through a language similar tologo, which turns the computer into a \u2018virtual machine\u2019 whose features and capabilities are unlimited."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "The complete procedure for nding the best state sequence is stated as follows [61]: Algorithm 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34795932"
                        ],
                        "name": "T. Reiss",
                        "slug": "T.-Reiss",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Reiss",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Reiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "[68] T. H. Reiss."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 46
                            }
                        ],
                        "text": "For general linear transformation,Hu [70] and Reiss [68] gave the relative invariants that are functions of the second-to fourth-order central moments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "For general linear transformation, Hu [70] and Reiss [68] gave the relative invariants that are functions of the secondto fourth-order central moments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10835747,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "af7762ac0538c848393094c6fe80c019954c58cd",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Translation, rotation, scale and contrast invariants.- Algebraic and projective invariants.- Invariance to affine transformations.- Invariance to projective transformations.- Recognizing partially occluded objects.- Summary and conclusions."
            },
            "slug": "Recognizing-Planar-Objects-Using-Invariant-Image-Reiss",
            "title": {
                "fragments": [],
                "text": "Recognizing Planar Objects Using Invariant Image Features"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Translation, rotation, scale and contrast invariants, algebraic and projective invariant, and recognition of partially occluded objects are summarized."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "TRU)\nIMAGEBIN/ (L0XXREAL.TIF)CHAR_TRU/\nOVERLAY/\n27Table 3.4: Ground-truth information provided for each page in the UW-III documentimage database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "In the University of Washington English Document Image Database-III [44], there are 1600 English document images that come with manually edited ground-truth of entity bounding boxes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 82
                            }
                        ],
                        "text": "183.2 The object-process diagram of graphics recognition evaluator. . . . . 233.3 UW-III Document Image Database's directory structure . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "UW-III English/Technical Document Image Database Man-ual, 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "We test the algorithm on 1600 pages from the UW-III database, and use thecross-validation method to estimate the algorithm's performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "The following is the UW-IIIcontent summary: 25 TIFF images of journal document pages with chemical formulae and theirground truth in X g."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 90
                            }
                        ],
                        "text": "In order to facilitate the trainingand evaluation of document layout analysis algorithms, UW-III [44] includes textline bounding box and word bounding box ground-truth, in addition to the ground-truth information provided in the previous database releases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 64
                            }
                        ],
                        "text": "In this experiment, the ground truth text words provided by the UW-III databaseare used as the input."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 79
                            }
                        ],
                        "text": "An environment for evaluating document analysis algorithms and systems:(a) The UW-III document image database with 1600 manually ground-truthedtechnical journal document images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 210
                            }
                        ],
                        "text": "5.1 Performance of text line and text block extraction from scanned document im-agesThe text structure extraction algorithms described in the previous sections are ap-plied to the total of 1600 images from the UW-III Document Image Database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "The algorithm was evaluated on the UW-III database of some 1600scanned document image pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 36
                            }
                        ],
                        "text": "The algorithms wereevaluated on the UW-III database of some 1600 scanned document image pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 99
                            }
                        ],
                        "text": "964.9 Illustrates the text alignment classi cation results on the ground truthtext blocks from the UW-III database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 189
                            }
                        ],
                        "text": "For each algorithm, we start with their default parameters obtained by observinga small number of images, then we tune the parameters by minimizing the algorithm'scost on the images in the UW-III database, until the improvement of performance isless than a threshold."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 9
                            }
                        ],
                        "text": "97In the UW-III database, there are total of 21,779 ground truth text blocks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 158
                            }
                        ],
                        "text": "Performance of text block extractionIn this experiment, we apply the PLDM-based text block extraction algorithm tothe ground truth text lines provided by the UW-III database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 289
                            }
                        ],
                        "text": "Justi ed Left Center Right Justi ed- Left-hanging hangingJusti ed 8254 129 7 2 5 0Left 19 747 0 0 1 5Center 20 2 122 1 1 0right 1 0 1 18 0 0Justi ed-hanging 22 1 0 0 1983 128Left-hanging 4 5 0 0 51 224Then, we apply the text block extraction algorithm on the ground truth textlines in the UW-III database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 27
                            }
                        ],
                        "text": "The document images in the UW-III data set have been deskewed andthe bounding boxes of text lines are manually ground truthed and veri ed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 225
                            }
                        ],
                        "text": "Figure 3.4 illustrates the process of evaluating documentanalysis algorithms using the UW document image database.Document Format Attribute Speci cation (DAFS) [46] is used as the representa-tion of document structure in the UW-III database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "UW-III contains a total of 1600 English document images randomly selected,copied, and scanned from scienti c and technical journals."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "Then, we reportthe performance of each module on the images from the UW-III Document ImageDatabase."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 100
                            }
                        ],
                        "text": "Finally, the performance evaluation results of a groupof document layout analysis algorithms on the UW-III database are presented inSection 3.4.3.1 Performance Assessment of Document Structure AnalysisLet g designate the ground truth polygonal structure, and d designate the detectedstructure, and u(g; d) designate the utility of (g; d), the expected performance of analgorithm is f =Xi Xf u(g; d)P (g; d); (3.1)where P (g; d) is the probability of the algorithm output is d while the ground truthis g [57]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 30
                            }
                        ],
                        "text": "The directory structureof the UW-III document image database is shown in Figure 3.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 180
                            }
                        ],
                        "text": "Row represents the true categories and columnrepresents the assigned categories. . . . . . . . . . . . . . . . . . . . 213.4 Ground-truth information provided for each page in the UW-III docu-ment image database. . . . . . . . . . . . . . . . . . . . . . . . . . . . 273.5 Illustrates the performance of algorithms before and after the optimiza-tion process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393.6 The default and optimized parameters for the page segmentation. . . 393.7 Performance of the X-Y cut page segmentation with respect to theground-truth and the algorithm output. . . . . . . . . . . . . . . . . . 403.8 The default and optimized parameters for the text line segmentation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "Several dozen images are selected from the UW-III database as the testing set forthe experiment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 104
                            }
                        ],
                        "text": "Table 4.9: Illustrates the text alignment classi cation results on the ground truthtext blocks from the UW-III database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 52
                            }
                        ],
                        "text": "Section 3.2 describes\n16content and creation of the UW-III document image database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "In order to facilitate the training and evaluation of document layout analysis algorithms, UW-III [44] includes text line bounding box and word bounding box ground-truth, in addition to the groundtruth information provided in the previous database releases."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 83
                            }
                        ],
                        "text": "Text structure extraction from the connected components within the text aresIn the UW-III database, a page is partition into a set of zones and each zone is assigneda physical content type, such as text, math, table, drawing, gure, ruling, etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "The algorithm was tested on the 1600 pages of technical documents within theUW-III database."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "User's Reference Manual for the UW English/Technical Document  Image Database III"
            },
            "venue": {
                "fragments": [],
                "text": "UW-III English/Technical Document Image Database Man-  ual"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Wenyin and Dori [75] propose a protocol for evaluating text-graphics separation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori [75] propose aprotocol for evaluating text-graphics separation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori [74] present a protocol for evaluating the recognition of straight andcircular lines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori weightall true positives and false positives by their respective lengths."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "[75] L. Wenyin and D. Dori."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Moreover, the protocol of [75] does not allow one to accept one of the N identical text boxes as a good match and to label others as false alarms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori [74] propose performance evaluation indices for straight and cir-cular line detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "[74] L. Wenyin and D. Dori."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 181
                            }
                        ],
                        "text": "Performance evaluation of graphics recognition is still a very young eld; objective and quantitative methods for evaluation of graphics recognition have been proposed very recently [71, 73, 74, 75]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A protocol for performance evaluation of algorithms for  text segmentation from graphics-rich documents"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Second IAPR  Workshop on Graphics Recognition,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori [75] propose aprotocol for evaluating text-graphics separation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori [74] present a protocol for evaluating the recognition of straight andcircular lines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori weightall true positives and false positives by their respective lengths."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "[75] L. Wenyin and D. Dori."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Wenyin and Dori [74] propose performance evaluation indices for straight and circular line detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Wenyin and Dori [74] propose performance evaluation indices for straight and cir-cular line detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Wenyin and Dori [74] present a protocol for evaluating the recognition of straight and circular lines."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "[74] L. Wenyin and D. Dori."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 181
                            }
                        ],
                        "text": "Performance evaluation of graphics recognition is still a very young eld; objective and quantitative methods for evaluation of graphics recognition have been proposed very recently [71, 73, 74, 75]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A protocol for performance evaluation of line detec-  tion algorithms.Machine Vision and Applications: Special Issue on Performance  Characteristics of Vision Algorithms, pp 240{250"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "OCR Performance Evaluation Software User's Manual, UW EnglishDocument Image Database - (I) Manual, 1993."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "The OCR Performance Evaluation (OPE) Software [26] [27] compares the ground-truth and OCR output and outputs a le containing: single character contingencytable, error substring contingency table, statistics of line insertions, deletions andsubstitutions, and statistics of symbol insertions, deletions and substitutions.3.1.6 Performance metrics for graphics recognitionDriven by the need to convert a large number of hard copy engineering drawings intoCAD les, raster to vector conversion has been a eld of intense research for the lastthree decades."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "The OCR Performance Evaluation (OPE) Software [26] [27] compares the groundtruth and OCR output and outputs a le containing: single character contingency table, error substring contingency table, statistics of line insertions, deletions and substitutions, and statistics of symbol insertions, deletions and substitutions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCR Performance Evaluation Software User's Manual"
            },
            "venue": {
                "fragments": [],
                "text": "UW English  Document Image Database - (I) Manual"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032282"
                        ],
                        "name": "G. Vanderplaats",
                        "slug": "G.-Vanderplaats",
                        "structuredName": {
                            "firstName": "Garret",
                            "lastName": "Vanderplaats",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Vanderplaats"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "10) with the constant a = (1 +p5)=2 [41]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 108777646,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "384755167e85888286e54bd756e9b12e441c6ecf",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-optimization-techniques-for-engineering-Vanderplaats",
            "title": {
                "fragments": [],
                "text": "Numerical optimization techniques for engineering design"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36422303"
                        ],
                        "name": "R. Hoch",
                        "slug": "R.-Hoch",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Hoch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hoch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083596961"
                        ],
                        "name": "Frank H\u00f6nes",
                        "slug": "Frank-H\u00f6nes",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "H\u00f6nes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank H\u00f6nes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145601766"
                        ],
                        "name": "T. J\u00e4ger",
                        "slug": "T.-J\u00e4ger",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "J\u00e4ger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J\u00e4ger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941002"
                        ],
                        "name": "M. Malburg",
                        "slug": "M.-Malburg",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Malburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Malburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2881875"
                        ],
                        "name": "A. Weigel",
                        "slug": "A.-Weigel",
                        "structuredName": {
                            "firstName": "Achim",
                            "lastName": "Weigel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59783506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "067af990a78f2691ce286a90a8c265524791dcc5",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "TECHNIQUES-FOR-IMPROVING-OCR-RESULTS-Dengel-Hoch",
            "title": {
                "fragments": [],
                "text": "TECHNIQUES FOR IMPROVING OCR RESULTS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "A document image can be segmented using a recursive X{Y cut [38] procedure based on the bounding boxes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[38] discussed a prototype of document image analysis system for technical journals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59683040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc45263226de157763006aef70b681dbac744dcc",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "HIERARCHICAL-REPRESENTATION-OF-OPTICALLY-SCANNED-Nagy-Seth",
            "title": {
                "fragments": [],
                "text": "HIERARCHICAL REPRESENTATION OF OPTICALLY SCANNED DOCUMENTS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 128
                            }
                        ],
                        "text": "Sponsored by the Advanced Research Project Agency (ARPA), we have produced the University of Washington Document Image Database [42, 43] series of ground-truth databases on CDROM for the document image analysis and recognition communities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 92
                            }
                        ],
                        "text": "Sponsored by the Advanced Research Project Agency (ARPA), we haveproduced the University of Washington Document Image Database [42, 43] series ofground-truth databases on CDROM for the document image analysis and recognitioncommunities."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English document database  design and implementation methodology"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the second Annual Symposium  on Document Analysis and Information Retrieval, pp 65-104, April 26-28"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Validation and estimation of docu-  ment degradation models"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the Fourth Annual Symposium on Document  Analysis and Information Retrieval, Las Vegas"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Although trade magazines have published surveys of the functionality and ease of use of vectorization products [31], a scienti c, well designed, comparison of the autovectorization capability of the products is not available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Raster-to-vector comes of age with AutoCAD release"
            },
            "venue": {
                "fragments": [],
                "text": "CADA-  LYST, pp 48{70,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Dengel [6] developed a system for partitioning raster images of business letters into logically labeled area items."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "About the logical partitioning of document images"
            },
            "venue": {
                "fragments": [],
                "text": "3rd Symposium  on Document Analysis and Information Retrieval, pp 209-218, Las Vegas"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "2document content can be accessed by its structure rather than simply by its imaged form [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document reconstruction: a system for  recovering document structure from layout"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Electronic Publishing,  pp 127-141, Lausanne, Switzerland"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "An erroneous word bounding box agging algorithm makes use of the database's existing ground-truth information to detect errors in the word bounding boxes [45]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semiautomatic production of highly  accurate word bounding box ground-truth"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings ICPR Workshop on Doc-  ument Analysis Systems pp 375{387, Malvern, PA"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An experimental environment for  model based document analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1st Int. Conf. on Document Analysis and  Recognition, pp 50{58, France"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A model based layout  understanding method for the document recognition System"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1st Int. Conf.  on Document Analysis and Recognition, pp 130-138, Saint-Malo, France"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "113 line- tting algorithm [58] to estimate the text line's baseline coordinates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Otherwise, a document page segmentation is needed to group the connected components into text lines, words, and text blocks [58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Uni ed Approach for Extracting Document Structure"
            },
            "venue": {
                "fragments": [],
                "text": "ISL Technical  Report, Intelligent Systems Laboratory, University of Washington"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 21
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 60,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Document-structure-analysis-and-performance-Liang-Haralick/884a67cb5ad2ee95c7f5a42cc86b47f204e563b1?sort=total-citations"
}